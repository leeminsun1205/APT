\clearpage
\appendix
\newpage

\section{Related works}
\label{app: related works}
\subsection{Adversarial Defense for VLMs}

Research on adversarial defense has concentrated on vision models, and few previous works consider VLMs \citep{gan_large-scale_2020,yang_defending_2021,mao_understanding_2023}.
\citet{gan_large-scale_2020} first employ adversarial training to train VLMs from scratch for improved clean performance. 
\citet{yang_defending_2021} equip standard VLMs with an auxiliary network and a robust feature fusion layer and performs adversarial training on these new modules. 
\citet{mao_understanding_2023} uses full adversarial fine-tuning to adapt a pre-trained CLIP model for zero-shot adversarial robustness. 
All of the above works study the adversarial robustness of VLMs through the lens of model weights, which leaves unexplored the question of how the text prompt affect adversarial robustness.
To fill this blank, we analyze the sensitivity of adversarial robustness to text prompts and propose a new method to optimize the text prompt to increase adversarial robustness.

\subsection{Adversarial Visual Prompting}
\label{app: related works avp}
AVP \citep{chen_visual_2023} combines visual prompting \citep{bahng_exploring_2022} with adversarial training to counteract adversarial perturbations. The visual prompt perturbation, parameterized by $\bm{\phi}$, is applied to the input image, $\bm{x}$, so that the prompted image is given by $\bm{x}_{vp} = \bm{x} + \bm{\phi}$. AVP optimizes visual prompt $\bm{\phi}$ to jointly minimize both clean and adversarial losses:
\begin{equation}
    \text{arg} \min_{\bm{\phi}} \mathcal{L}(\bm{x}+\bm{\phi}, \bm{t}, y;\bm{\theta_v}, \bm{\theta_t}) + \mathcal{L}(\bm{x}+\bm{\delta}+\bm{\phi}, \bm{t}, y;\bm{\theta_v}, \bm{\theta_t})
\end{equation}
where $\bm{\delta}$ is generated by \cref{algo: adv attack} independent of $\bm{\phi}$. 

\subsection{Partial Adversarial Fine-Tuning}
\label{app: related works paft}
PAFT \citep{chen_adversarial_2020} discards the text encoder branch of CLIP and attaches an extra linear layer, parameterized by $\bm{\theta_l}$, on top of the frozen image encoder, $\bm{\theta_{v}}$, to form a new classifier.
The output of the linear classifier has the same dimension as the number of classes. 
PAFT optimizes $\bm{\theta_l}$ to minimize the adversarial loss:
\begin{equation}
    \text{arg} \min_{\bm{\theta_l}} \mathcal{L}(\bm{x}+\bm{\delta}, y;\bm{\theta_v}, \bm{\theta_l})    
\end{equation}
where $\bm{\delta}$ is generated using the conventional PGD attack on the new classifier.

\section{Experiment Setting}
\label{app: additional experiment setting}

\textbf{Data}.
The 11 datasets were selected to constitute a comprehensive benchmark, encompassing a diverse array of vision tasks including generic objects classification, scene recognition, action classification, fine-grained classification, textures recognition and satellite imagery analysis.
They were split into training and test sets in the same way as \citet{zhou_learning_2022}. 
The $N$-shot images, once sampled, are fixed so that all evaluated methods are trained on the exactly same data for a fair comparison.


\textbf{Model}.
The text encoder is the default pre-trained model from CLIP \citep{radford_learning_2021}. 
For both image and text branches, we adopt the same data pre-processing as CLIP \citep{radford_learning_2021}.


\subsection{General Training Set-ups}
\textbf{APT}
was trained by Stochastic Gradient Descent (SGD) using a cosine learning rate schedule with an initial learning rate of 0.002. The batch size was 32. 
For ImageNet, the number of epochs was 20, 20, 50 and 20 for 1, 4, 16 and all shots respectively, and for other datasets was 50, 100, 200, 200.
The number of epochs used with ImageNet was reduced due to limited computational resources. 
Results were reported for the last checkpoint.


\textbf{AVP}
was implemented in the mode of padding with a prompt size of 52 to match the number of parameters of our method for the unified context. 
Although a class-specific variant of AVP was proposed in \citep{chen_visual_2023} and was observed to outperform its unified variant on CIFAR10 with ResNet18, we found  that the class-specific variant of AVP generalized poorly to our experimental set-ups, \ie, CLIP plus 11 diverse datasets. We therefore decided to use the unified variant of AVP.
Following \citep{chen_visual_2023}, a cosine learning rate schedule with an initial learning rate of 0.1 was used. 
The number of epochs, corresponding to 1/4/16/all shots, was 20/50/100/100 for non-ImageNet datasets and 20/20/50/20 for ImageNet. 
The implementation was based on the open-source code of \citet{chen_visual_2023}. 


\textbf{PAFT}
was trained, following \citet{chen_adversarial_2020}, for 20/50/100/100 epochs with an initial learning rate of 0.1 decayed by 0.1 at epochs 5/15/30/30 and 10/25/50/50 for 1/4/16/all shots,  respectively, for non-ImageNet datasets. 
For ImageNet, the number of training epochs was 20/20/50/20 for 1/4/16/100 shots respectively. 


\input{figs/dataset}


\section{Additional Results}

\subsection{In-Distribution Performance for $\epsilon=1/255$}

\cref{fig: shot results eps 1} depicts the results for $\epsilon=1/255$ on each dataset. 
The trends are generally consistent to those observed for $\epsilon=4/255$ in \cref{fig: shot results eps4}.

\input{figs/shot-result-full}


\subsection{Generalization to Other Architectures}
\label{app: resnet50}
\input{figs/rn50}

As shown in \cref{tab: rn50}, the results of our method with the ResNet50 vision encoder demonstrates a performance advantage consistent with that achieved based on ViT. Our method exhibits substantial enhancements over the zero-shot baseline. Furthermore, our approach attains robustness comparable to that achieved by LP, while achieving significantly higher accuracy.

\subsection{Ablation Study}
\label{app: ablation study}

The ablation study was conducted using the following set-up unless otherwise specified. 
The number of context vectors, $M$, was 16, the class embedding was placed at the \textit{end} of the prompt, the prompting strategy for training the adversary was \textit{on-the-fly}.
Models were trained with 16 shots for $\epsilon = 4/255$.

\subsubsection{Prompt Context Length}
\label{sec: prompt context length}
\input{figs/nctx}

As shown in \cref{tab: ablation context length}, it is notable that as the number of context vectors increases, a significant enhancement in performance becomes evident for variant UC.
In the case of CSC, while there is a marginal increase in accuracy associated with the number of context vectors, there is no improvement in robustness. 
A longer context has a greater number of parameters and, consequently, it is expected to possess a larger capacity for improved performance. 
However, the observation that robustness is not notably improved for CSC may be attributed to a fact that even when the context length is one (the minimum), CSC inherently incorporates more parameters than UC with a context length of 16, provided that the number of classes exceeds 16. 
This phenomenon exists in the majority of our test datasets. The default context length was set to 16 based based on there results, due to its better performance.

\subsubsection{Class Embedding Position}
\label{sec: ablation study class embedding position}
\input{figs/ctxpos}

The performance of our method was evaluated with the class embedded at different locations in the context (\cref{tab: ablation class embedding position}). This experiment reveals that no statistically significant differences are observed for both UC and CSC. As a result, \textit{end} was selected as the default position due to its slightly better performance and simplicity.


\subsubsection{Prompting for Training Adversarial Generation}
\label{sec: ablation adversarial generation strategy}
\input{figs/prompts}

\cref{tab: adv prompt} compares the performance of different variants of our method when training adversarial examples are generated by the different prompting strategies introduced in \cref{sec: method context optimization}.
We observe that the prompting strategies \textit{on-the-fly} and \textit{perturbed}, if $\alpha'$ is properly configured, achieve very close accuracy and robustness, while \textit{perturbed} seems to be slightly advantageous regarding accuracy under the unified context. 
Both of them perform much better than the strategy \textit{constant}.
Inside the strategy \textit{perturbed}, the perturbation magnitude increases as $\alpha'$ increases so that the perturbed prompts deviate more from the original ones, which results in the drop of performance as shown in the data. 

\subsection{Verification}
\label{sec: verification on adversarial evaluation}

\input{figs/moreattacks}

To verify that our evaluation of adversarial robustness is reliable, we first additionally evaluated the adversarial robustness of our methods using a diverse set of attacks including TPGD \citep{zhang_theoretically_2019}, CW \citep{carlini_towards_2017} and AutoAttack \citep{croce_reliable_2020}.
AutoAttack, particularly, is widely accepted as a reliable attack.
Note that for AutoAttack only we reduce the size of the ImageNet test set to 5000 following the practice used in RobustBench \citep{croce_robustbench_2021}, since running AutoAttack on the full ImageNet test set is very expensive. 
As shown in \cref{tab: more attacks eval}, the performance of our methods degrade by a reasonable amount but still improve over the baseline method by a large margin under the stronger attacks CW and AutoAttack. This suggests that the robustness advantage of our methods is not a consequence of overfitting to the particular attack or to obfuscated gradients \citep{athalye_obfuscated_2018}. 

To further exclude the possibility of our method masking the gradients for the particular attack prompts \citep{athalye_obfuscated_2018}, we evaluated our methods using adversarial examples transferred from other prompts with the same model as defined in \cref{fig: robustness varied prompts}.
It is shown in \cref{tab: adv eval more prompts} that our method achieves higher robustness against the adversarial examples generated by six other different prompts. 
This suggests that the robustness achieved by our method is not specific to the attack from a particular prompting source and can generalize well to attacks using different prompting methods.

\subsection{Interpretability of the Learned Context}
\label{app: interpreting learned contexts}
\input{figs/vis}

This section studies the interpretability of the context vectors learned by APT through the lens of the nearest words. 
The nearest word is, following the implementation of \citet{zhou_learning_2022}, the vocabulary whose embedding vector is closest to the learned vector based on the Euclidean distance. 
\cref{tab: context vis} shows the decoded nearest words for sixteen learned, unified, vectors ($M=16$) on the selected datasets. 
We find that, in contrast to the hand-engineered prompt contexts (\cref{tab: dataset}), the decoded nearest words appear to lack semantic relevance with the respective dataset.
It seems that APT may have learned something beyond current human vocabulary in order to defend against human-imperceptible  adversarial perturbations. 
On the other hand, the nearest words, despite being used in the previous works like \citep{zhou_learning_2022}, may not accurately reflect the semantic meaning of the learned vectors. 
To be more specific, there is a lack of well-established distance threshold above which the decoded words can be considered as faithful interpretation. 
Besides, some decoded words are non-Latin characters, \eg, the second word for DTD in \cref{tab: context vis}, which are uninterpretable to human.


\subsection{Dependency on the Robust Visual Backbone}
\label{app: dependency on robust backbone}

This section discusses the dependency of APT on the pre-trained adversarially robust image encoder. 
It was observed in our experiments that APT failed to improve robustness over the HEP baseline, or even collapsed during training, when the image encoder of CLIP is not pre-trained using a robust training methods like TeCoA \citep{mao_understanding_2023} to attain adversarial robustness. 
This suggests that a robust visual backbone is necessary for APT to be effective. 

We note that it was reported in \citet{chen_visual_2023} that AVP achieved 27.7\% for accuracy and 16.7\% for robustness against PGD100 with a standardly-trained ResNet18 \citep{he_deep_2016} on CIFAR10. 
The accuracy is dramatically compromised when compared to 94.9\% accuracy of the pre-trained model without AVP. 
Although the robustness increases from 0 to 16.7\%, the trade-off between accuracy and robustness seems unacceptable. 