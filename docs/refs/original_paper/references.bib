
@inproceedings{deng_rlprompt_2022,
	address = {Abu Dhabi, United Arab Emirates},
	title = {{RLPrompt}: {Optimizing} {Discrete} {Text} {Prompts} with {Reinforcement} {Learning}},
	shorttitle = {{RLPrompt}},
	url = {https://aclanthology.org/2022.emnlp-main.222},
	doi = {10.18653/v1/2022.emnlp-main.222},
	abstract = {Prompting has shown impressive success in enabling large pre-trained language models (LMs) to perform diverse NLP tasks, especially with only few downstream data. Automatically finding the optimal prompt for each task, however, is challenging. Most existing work resorts to tuning *soft* prompts (e.g., embeddings) which fall short of interpretability, reusability across LMs, and applicability when gradients are not accessible. *Discrete* prompts, on the other hand, are difficult to optimize, and are often created by “enumeration (e.g., paraphrasing)-then-selection” heuristics that do not explore the prompt space systematically. This paper proposes RLPrompt, an efficient discrete prompt optimization approach with reinforcement learning (RL). RLPrompt formulates a parameter-efficient policy network that generates the optimized discrete prompt after training with reward. To harness the complex and stochastic reward signals from the large LM environment, we incorporate effective reward stabilization that substantially enhances training efficiency. RLPrompt is flexibly applicable to different types of LMs, such as masked (e.g., BERT) and left-to-right models (e.g., GPTs), for both classification and generation tasks. Experiments on few-shot classification and unsupervised text style transfer show superior performance over a wide range of existing fine-tuning or prompting methods. Interestingly, the resulting optimized prompts are often ungrammatical gibberish text; and surprisingly, those gibberish prompts are transferrable between different LMs to retain significant performance, indicating that LM prompting may not follow human language patterns.},
	urldate = {2023-10-22},
	booktitle = {Proceedings of the 2022 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Deng, Mingkai and Wang, Jianyu and Hsieh, Cheng-Ping and Wang, Yihan and Guo, Han and Shu, Tianmin and Song, Meng and Xing, Eric and Hu, Zhiting},
	month = dec,
	year = {2022},
	pages = {3369--3391},
}

@inproceedings{shi_online_2020,
	title = {Online {Adversarial} {Purification} based on {Self}-supervised {Learning}},
	url = {https://openreview.net/forum?id=_i3ASPp12WS},
	abstract = {Deep neural networks are known to be vulnerable to adversarial examples, where a perturbation in the input space leads to an amplified shift in the latent network representation. In this paper, we combine canonical supervised learning with self-supervised representation learning, and present Self-supervised Online Adversarial Purification (SOAP), a novel defense strategy that uses a self-supervised loss to purify adversarial examples at test-time. Our approach leverages the label-independent nature of self-supervised signals and counters the adversarial perturbation with respect to the self-supervised tasks. SOAP yields competitive robust accuracy against state-of-the-art adversarial training and purification methods, with considerably less training complexity. In addition, our approach is robust even when adversaries are given the knowledge of the purification defense strategy. To the best of our knowledge, our paper is the first that generalizes the idea of using self-supervised signals to perform online test-time purification.},
	language = {en},
	urldate = {2023-07-30},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Shi, Changhao and Holtz, Chester and Mishne, Gal},
	month = oct,
	year = {2020},
}

@inproceedings{hill_stochastic_2020,
	title = {Stochastic {Security}: {Adversarial} {Defense} {Using} {Long}-{Run} {Dynamics} of {Energy}-{Based} {Models}},
	shorttitle = {Stochastic {Security}},
	url = {https://openreview.net/forum?id=gwFTuzxJW0},
	abstract = {The vulnerability of deep networks to adversarial attacks is a central problem for deep learning from the perspective of both cognition and security. The current most successful defense method is to train a classifier using adversarial images created during learning. Another defense approach involves transformation or purification of the original input to remove adversarial signals before the image is classified. We focus on defending naturally-trained classifiers using Markov Chain Monte Carlo (MCMC) sampling with an Energy-Based Model (EBM) for adversarial purification. In contrast to adversarial training, our approach is intended to secure highly vulnerable pre-existing classifiers. To our knowledge, no prior defensive transformation is capable of securing naturally-trained classifiers, and our method is the first to validate a post-training defense approach that is distinct from current successful defenses which modify classifier training. The memoryless behavior of long-run MCMC sampling will eventually remove adversarial signals, while metastable behavior preserves consistent appearance of MCMC samples after many steps to allow accurate long-run prediction. Balancing these factors can lead to effective purification and robust classification. We evaluate adversarial defense with an EBM using the strongest known attacks against purification. Our contributions are 1) an improved method for training EBM's with realistic long-run MCMC samples for effective purification, 2) an Expectation-Over-Transformation (EOT) defense that resolves ambiguities for evaluating stochastic defenses and from which the EOT attack naturally follows, and 3) state-of-the-art adversarial defense for naturally-trained classifiers and competitive defense compared to adversarial training on CIFAR-10, SVHN, and CIFAR-100. Our code and pre-trained models are available at https://github.com/point0bar1/ebm-defense.},
	language = {en},
	urldate = {2023-07-30},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Hill, Mitch and Mitchell, Jonathan Craig and Zhu, Song-Chun},
	month = oct,
	year = {2020},
}

@inproceedings{song_pixeldefend_2018,
	title = {{PixelDefend}: {Leveraging} {Generative} {Models} to {Understand} and {Defend} against {Adversarial} {Examples}},
	shorttitle = {{PixelDefend}},
	url = {https://openreview.net/forum?id=rJUYGxbCW},
	abstract = {Adversarial perturbations of normal images are usually imperceptible to humans, but they can seriously confuse state-of-the-art machine learning models. What makes them so special in the eyes of image classifiers? In this paper, we show empirically that adversarial examples mainly lie in the low probability regions of the training distribution, regardless of attack types and targeted models. Using statistical hypothesis testing, we find that modern neural density models are surprisingly good at detecting imperceptible image perturbations. Based on this discovery, we devised PixelDefend, a new approach that purifies a maliciously perturbed image by moving it back towards the distribution seen in the training data. The purified image is then run through an unmodified classifier, making our method agnostic to both the classifier and the attacking method. As a result, PixelDefend can be used to protect already deployed models and be combined with other model-specific defenses. Experiments show that our method greatly improves resilience across a wide variety of state-of-the-art attacking methods, increasing accuracy on the strongest attack from 63\% to 84\% for Fashion MNIST and from 32\% to 70\% for CIFAR-10.},
	language = {en},
	urldate = {2023-07-30},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Song, Yang and Kim, Taesup and Nowozin, Sebastian and Ermon, Stefano and Kushman, Nate},
	month = feb,
	year = {2018},
}

@inproceedings{byun_effectiveness_2022,
	title = {On the {Effectiveness} of {Small} {Input} {Noise} for {Defending} {Against} {Query}-{Based} {Black}-{Box} {Attacks}},
	url = {https://openaccess.thecvf.com/content/WACV2022/html/Byun_On_the_Effectiveness_of_Small_Input_Noise_for_Defending_Against_WACV_2022_paper.html},
	language = {en},
	urldate = {2023-07-30},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Winter} {Conference} on {Applications} of {Computer} {Vision}},
	author = {Byun, Junyoung and Go, Hyojun and Kim, Changick},
	year = {2022},
	pages = {3051--3060},
}

@misc{dong_how_2023,
	title = {How {Robust} is {Google}'s {Bard} to {Adversarial} {Image} {Attacks}?},
	url = {http://arxiv.org/abs/2309.11751},
	doi = {10.48550/arXiv.2309.11751},
	abstract = {Multimodal Large Language Models (MLLMs) that integrate text and other modalities (especially vision) have achieved unprecedented performance in various multimodal tasks. However, due to the unsolved adversarial robustness problem of vision models, MLLMs can have more severe safety and security risks by introducing the vision inputs. In this work, we study the adversarial robustness of Google's Bard, a competitive chatbot to ChatGPT that released its multimodal capability recently, to better understand the vulnerabilities of commercial MLLMs. By attacking white-box surrogate vision encoders or MLLMs, the generated adversarial examples can mislead Bard to output wrong image descriptions with a 22\% success rate based solely on the transferability. We show that the adversarial examples can also attack other MLLMs, e.g., a 26\% attack success rate against Bing Chat and a 86\% attack success rate against ERNIE bot. Moreover, we identify two defense mechanisms of Bard, including face detection and toxicity detection of images. We design corresponding attacks to evade these defenses, demonstrating that the current defenses of Bard are also vulnerable. We hope this work can deepen our understanding on the robustness of MLLMs and facilitate future research on defenses. Our code is available at https://github.com/thu-ml/Attack-Bard. Update: GPT-4V is available at October 2023. We further evaluate its robustness under the same set of adversarial examples, achieving a 45\% attack success rate.},
	urldate = {2023-10-18},
	publisher = {arXiv},
	author = {Dong, Yinpeng and Chen, Huanran and Chen, Jiawei and Fang, Zhengwei and Yang, Xiao and Zhang, Yichi and Tian, Yu and Su, Hang and Zhu, Jun},
	month = oct,
	year = {2023},
	note = {arXiv:2309.11751 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@misc{bai_touchstone_2023,
	title = {{TouchStone}: {Evaluating} {Vision}-{Language} {Models} by {Language} {Models}},
	shorttitle = {{TouchStone}},
	url = {http://arxiv.org/abs/2308.16890},
	doi = {10.48550/arXiv.2308.16890},
	abstract = {Large vision-language models (LVLMs) have recently witnessed rapid advancements, exhibiting a remarkable capacity for perceiving, understanding, and processing visual information by connecting visual receptor with large language models (LLMs). However, current assessments mainly focus on recognizing and reasoning abilities, lacking direct evaluation of conversational skills and neglecting visual storytelling abilities. In this paper, we propose an evaluation method that uses strong LLMs as judges to comprehensively evaluate the various abilities of LVLMs. Firstly, we construct a comprehensive visual dialogue dataset TouchStone, consisting of open-world images and questions, covering five major categories of abilities and 27 subtasks. This dataset not only covers fundamental recognition and comprehension but also extends to literary creation. Secondly, by integrating detailed image annotations we effectively transform the multimodal input content into a form understandable by LLMs. This enables us to employ advanced LLMs for directly evaluating the quality of the multimodal dialogue without requiring human intervention. Through validation, we demonstrate that powerful LVLMs, such as GPT-4, can effectively score dialogue quality by leveraging their textual capabilities alone, aligning with human preferences. We hope our work can serve as a touchstone for LVLMs' evaluation and pave the way for building stronger LVLMs. The evaluation code is available at https://github.com/OFA-Sys/TouchStone.},
	urldate = {2023-10-18},
	publisher = {arXiv},
	author = {Bai, Shuai and Yang, Shusheng and Bai, Jinze and Wang, Peng and Zhang, Xingxuan and Lin, Junyang and Wang, Xinggang and Zhou, Chang and Zhou, Jingren},
	month = sep,
	year = {2023},
	note = {arXiv:2308.16890 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{chang_survey_2023,
	title = {A {Survey} on {Evaluation} of {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2307.03109},
	doi = {10.48550/arXiv.2307.03109},
	abstract = {Large language models (LLMs) are gaining increasing popularity in both academia and industry, owing to their unprecedented performance in various applications. As LLMs continue to play a vital role in both research and daily use, their evaluation becomes increasingly critical, not only at the task level, but also at the society level for better understanding of their potential risks. Over the past years, significant efforts have been made to examine LLMs from various perspectives. This paper presents a comprehensive review of these evaluation methods for LLMs, focusing on three key dimensions: what to evaluate, where to evaluate, and how to evaluate. Firstly, we provide an overview from the perspective of evaluation tasks, encompassing general natural language processing tasks, reasoning, medical usage, ethics, educations, natural and social sciences, agent applications, and other areas. Secondly, we answer the `where' and `how' questions by diving into the evaluation methods and benchmarks, which serve as crucial components in assessing performance of LLMs. Then, we summarize the success and failure cases of LLMs in different tasks. Finally, we shed light on several future challenges that lie ahead in LLMs evaluation. Our aim is to offer invaluable insights to researchers in the realm of LLMs evaluation, thereby aiding the development of more proficient LLMs. Our key point is that evaluation should be treated as an essential discipline to better assist the development of LLMs. We consistently maintain the related open-source materials at: https://github.com/MLGroupJLU/LLM-eval-survey.},
	urldate = {2023-10-18},
	publisher = {arXiv},
	author = {Chang, Yupeng and Wang, Xu and Wang, Jindong and Wu, Yuan and Yang, Linyi and Zhu, Kaijie and Chen, Hao and Yi, Xiaoyuan and Wang, Cunxiang and Wang, Yidong and Ye, Wei and Zhang, Yue and Chang, Yi and Yu, Philip S. and Yang, Qiang and Xie, Xing},
	month = oct,
	year = {2023},
	note = {arXiv:2307.03109 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@inproceedings{chen_adversarial_2020,
	title = {Adversarial {Robustness}: {From} {Self}-{Supervised} {Pre}-{Training} to {Fine}-{Tuning}},
	shorttitle = {Adversarial {Robustness}},
	url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Chen_Adversarial_Robustness_From_Self-Supervised_Pre-Training_to_Fine-Tuning_CVPR_2020_paper.html},
	urldate = {2023-02-19},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Chen, Tianlong and Liu, Sijia and Chang, Shiyu and Cheng, Yu and Amini, Lisa and Wang, Zhangyang},
	year = {2020},
	pages = {699--708},
}

@inproceedings{wang_improving_2023,
	title = {Improving {Zero}-{Shot} {Generalization} for {CLIP} with {Synthesized} {Prompts}},
	url = {https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Improving_Zero-Shot_Generalization_for_CLIP_with_Synthesized_Prompts_ICCV_2023_paper.html},
	language = {en},
	urldate = {2023-10-12},
	author = {Wang, Zhengbo and Liang, Jian and He, Ran and Xu, Nan and Wang, Zilei and Tan, Tieniu},
	year = {2023},
	pages = {3032--3042},
}

@inproceedings{dong_improving_2022,
	title = {Improving {Adversarially} {Robust} {Few}-{Shot} {Image} {Classification} {With} {Generalizable} {Representations}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Dong_Improving_Adversarially_Robust_Few-Shot_Image_Classification_With_Generalizable_Representations_CVPR_2022_paper.html},
	language = {en},
	urldate = {2023-07-20},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Dong, Junhao and Wang, Yuan and Lai, Jian-Huang and Xie, Xiaohua},
	year = {2022},
	pages = {9025--9034},
}

@misc{wang_semantic_2023,
	title = {Semantic {Adversarial} {Attacks} via {Diffusion} {Models}},
	url = {http://arxiv.org/abs/2309.07398},
	doi = {10.48550/arXiv.2309.07398},
	abstract = {Traditional adversarial attacks concentrate on manipulating clean examples in the pixel space by adding adversarial perturbations. By contrast, semantic adversarial attacks focus on changing semantic attributes of clean examples, such as color, context, and features, which are more feasible in the real world. In this paper, we propose a framework to quickly generate a semantic adversarial attack by leveraging recent diffusion models since semantic information is included in the latent space of well-trained diffusion models. Then there are two variants of this framework: 1) the Semantic Transformation (ST) approach fine-tunes the latent space of the generated image and/or the diffusion model itself; 2) the Latent Masking (LM) approach masks the latent space with another target image and local backpropagation-based interpretation methods. Additionally, the ST approach can be applied in either white-box or black-box settings. Extensive experiments are conducted on CelebA-HQ and AFHQ datasets, and our framework demonstrates great fidelity, generalizability, and transferability compared to other baselines. Our approaches achieve approximately 100\% attack success rate in multiple settings with the best FID as 36.61. Code is available at https://github.com/steven202/semantic\_adv\_via\_dm.},
	urldate = {2023-10-10},
	publisher = {arXiv},
	author = {Wang, Chenan and Duan, Jinhao and Xiao, Chaowei and Kim, Edward and Stamm, Matthew and Xu, Kaidi},
	month = sep,
	year = {2023},
	note = {arXiv:2309.07398 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@inproceedings{song_constructing_2018,
	title = {Constructing {Unrestricted} {Adversarial} {Examples} with {Generative} {Models}},
	volume = {31},
	url = {https://proceedings.neurips.cc/paper/2018/hash/8cea559c47e4fbdb73b23e0223d04e79-Abstract.html},
	abstract = {Adversarial examples are typically constructed by perturbing an existing data point within a small matrix norm, and current defense methods are focused on guarding against this type of attack. In this paper, we propose a new class of adversarial examples that are synthesized entirely from scratch using a conditional generative model, without being restricted to norm-bounded perturbations. We first train an Auxiliary Classifier Generative Adversarial Network (AC-GAN) to model the class-conditional distribution over data samples. Then, conditioned on a desired class, we search over the AC-GAN latent space to find images that are likely under the generative model and are misclassified by a target classifier. We demonstrate through human evaluation that these new kind of adversarial images, which we call Generative Adversarial Examples, are legitimate and belong to the desired class. Our empirical results on the MNIST, SVHN, and CelebA datasets show that generative adversarial examples can bypass strong adversarial training and certified defense methods designed for traditional adversarial attacks.},
	urldate = {2023-05-14},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Song, Yang and Shu, Rui and Kushman, Nate and Ermon, Stefano},
	year = {2018},
}

@inproceedings{zhao_generating_2018,
	title = {Generating {Natural} {Adversarial} {Examples}},
	url = {https://openreview.net/forum?id=H1BLjgZCb},
	abstract = {Due to their complex nature, it is hard to characterize the ways in which machine learning models can misbehave or be exploited when deployed. Recent work on adversarial examples, i.e. inputs with minor perturbations that result in substantially different model predictions, is helpful in evaluating the robustness of these models by exposing the adversarial scenarios where they fail. However, these malicious perturbations are often unnatural, not semantically meaningful, and not applicable to complicated domains such as language. In this paper, we propose a framework to generate natural and legible adversarial examples that lie on the data manifold, by searching in semantic space of dense and continuous data representation, utilizing the recent advances in generative adversarial networks. We present generated adversaries to demonstrate the potential of the proposed approach for black-box classifiers for a wide range of applications such as image classification, textual entailment, and machine translation. We include experiments to show that the generated adversaries are natural, legible to humans, and useful in evaluating and analyzing black-box classifiers.},
	language = {en},
	urldate = {2023-06-16},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Zhao, Zhengli and Dua, Dheeru and Singh, Sameer},
	year = {2018},
}

@inproceedings{xiao_spatially_2018,
	title = {Spatially {Transformed} {Adversarial} {Examples}},
	url = {https://openreview.net/forum?id=HyydRMZC-},
	abstract = {Recent studies show that widely used Deep neural networks (DNNs) are vulnerable to the carefully crafted adversarial examples. Many advanced algorithms have been proposed to generate adversarial examples by leveraging the L\_p distance for penalizing perturbations. Different defense methods have also been explored to defend against such adversarial attacks. While the effectiveness of L\_p distance as a metric of perceptual quality remains an active research area, in this paper we will instead focus on a different type of perturbation, namely spatial transformation, as opposed to manipulating the pixel values directly as in prior works. Perturbations generated through spatial transformation could result in large L\_p distance measures, but our extensive experiments show that such spatially transformed adversarial examples are perceptually realistic and more difficult to defend against with existing defense systems. This potentially provides a new direction in adversarial example generation and the design of corresponding defenses. We visualize the spatial transformation based perturbation for different examples and show that our technique can produce realistic adversarial examples with smooth image deformation. Finally, we visualize the attention of deep networks with different types of adversarial examples to better understand how these examples are interpreted.},
	language = {en},
	urldate = {2023-08-17},
	author = {Xiao, Chaowei and Zhu, Jun-Yan and Li, Bo and He, Warren and Liu, Mingyan and Song, Dawn},
	month = feb,
	year = {2018},
}

@inproceedings{luo_towards_2018,
	title = {Towards {Imperceptible} and {Robust} {Adversarial} {Example} {Attacks} {Against} {Neural} {Networks}},
	volume = {32},
	copyright = {Copyright (c)},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/11499},
	doi = {10.1609/aaai.v32i1.11499},
	abstract = {Machine learning systems based on deep neural networks, being able to produce state-of-the-art results on various perception tasks, have gained mainstream adoption in many applications. However, they are shown to be vulnerable to adversarial example attack, which generates malicious output by adding slight perturbations to the input. Previous adversarial example crafting methods, however, use simple metrics to evaluate the distances between the original examples and the adversarial ones, which could be easily detected by human eyes. In addition, these attacks are often not robust due to the inevitable noises and deviation in the physical world. In this work, we present a new adversarial example attack crafting method, which takes the human perceptual system into consideration and maximizes the noise tolerance of the crafted adversarial example. Experimental results demonstrate the efficacy of the proposed technique.},
	language = {en},
	urldate = {2023-07-15},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Luo, Bo and Liu, Yannan and Wei, Lingxiao and Xu, Qiang},
	month = apr,
	year = {2018},
	note = {Number: 1},
	keywords = {DNN},
}

@article{engstrom_rotation_2018,
	title = {A {Rotation} and a {Translation} {Suffice}: {Fooling} {CNNs} with {Simple} {Transformations}},
	shorttitle = {A {Rotation} and a {Translation} {Suffice}},
	url = {https://openreview.net/forum?id=BJfvknCqFQ},
	abstract = {We show that simple spatial transformations, namely translations and rotations alone, suffice to fool neural networks on a significant fraction of their inputs in multiple image classification tasks. Our results are in sharp contrast to previous work in adversarial robustness that relied on more complicated optimization ap- proaches unlikely to appear outside a truly adversarial context. Moreover, the misclassifying rotations and translations are easy to find and require only a few black-box queries to the target model. Overall, our findings emphasize the need to design robust classifiers even for natural input transformations in benign settings.},
	language = {en},
	urldate = {2023-09-24},
	author = {Engstrom, Logan and Tran, Brandon and Tsipras, Dimitris and Schmidt, Ludwig and Madry, Aleksander},
	month = sep,
	year = {2018},
}

@inproceedings{balunovic_certifying_2019,
	title = {Certifying {Geometric} {Robustness} of {Neural} {Networks}},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper/2019/hash/f7fa6aca028e7ff4ef62d75ed025fe76-Abstract.html},
	abstract = {The use of neural networks in safety-critical computer vision systems calls for their
robustness certification against natural geometric transformations (e.g., rotation,
scaling). However, current certification methods target mostly norm-based pixel
perturbations and cannot certify robustness against geometric transformations. In
this work, we propose a new method to compute sound and asymptotically optimal
linear relaxations for any composition of transformations. Our method is based on
a novel combination of sampling and optimization. We implemented the method
in a system called DeepG and demonstrated that it certifies significantly more
complex geometric transformations than existing methods on both defended and
undefended networks while scaling to large architectures.},
	urldate = {2023-05-18},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Balunovic, Mislav and Baader, Maximilian and Singh, Gagandeep and Gehr, Timon and Vechev, Martin},
	year = {2019},
}

@inproceedings{liu_beyond_2018,
	title = {Beyond {Pixel} {Norm}-{Balls}: {Parametric} {Adversaries} using an {Analytically} {Differentiable} {Renderer}},
	shorttitle = {Beyond {Pixel} {Norm}-{Balls}},
	url = {https://openreview.net/forum?id=SJl2niR9KQ},
	abstract = {Many machine learning image classifiers are vulnerable to adversarial attacks, inputs with perturbations designed to intentionally trigger misclassification. Current adversarial methods directly alter pixel colors and evaluate against pixel norm-balls: pixel perturbations smaller than a specified magnitude, according to a measurement norm. This evaluation, however, has limited practical utility since perturbations in the pixel space do not correspond to underlying real-world phenomena of image formation that lead to them and has no security motivation attached. Pixels in natural images are measurements of light that has interacted with the geometry of a physical scene. As such, we propose a novel evaluation measure, parametric norm-balls, by directly perturbing physical parameters that underly image formation. One enabling contribution we present is a physically-based differentiable renderer that allows us to propagate pixel gradients to the parametric space of lighting and geometry. Our approach enables physically-based adversarial attacks, and our differentiable renderer leverages models from the interactive rendering literature to balance the performance and accuracy trade-offs necessary for a memory-efficient and scalable adversarial data augmentation workflow.},
	language = {en},
	urldate = {2023-08-14},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Liu, Hsueh-Ti Derek and Tao, Michael and Li, Chun-Liang and Nowrouzezahrai, Derek and Jacobson, Alec},
	month = sep,
	year = {2018},
}

@inproceedings{joshi_semantic_2019,
	title = {Semantic {Adversarial} {Attacks}: {Parametric} {Transformations} {That} {Fool} {Deep} {Classifiers}},
	shorttitle = {Semantic {Adversarial} {Attacks}},
	url = {https://openaccess.thecvf.com/content_ICCV_2019/html/Joshi_Semantic_Adversarial_Attacks_Parametric_Transformations_That_Fool_Deep_Classifiers_ICCV_2019_paper.html},
	urldate = {2023-05-16},
	author = {Joshi, Ameya and Mukherjee, Amitangshu and Sarkar, Soumik and Hegde, Chinmay},
	year = {2019},
	pages = {4773--4783},
}

@inproceedings{laidlaw_functional_2019,
	title = {Functional {Adversarial} {Attacks}},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper/2019/hash/6e923226e43cd6fac7cfe1e13ad000ac-Abstract.html},
	abstract = {We propose functional adversarial attacks, a novel class of threat models for crafting adversarial examples to fool machine learning models. Unlike a standard lp-ball threat model, a functional adversarial threat model allows only a single function to be used to perturb input features to produce an adversarial example. For example, a functional adversarial attack applied on colors of an image can change all red pixels simultaneously to light red. Such global uniform changes in images can be less perceptible than perturbing pixels of the image individually. For simplicity, we refer to functional adversarial attacks on image colors as ReColorAdv, which is the main focus of our experiments. We show that functional threat models can be combined with existing additive (lp) threat models to generate stronger threat models that allow both small, individual perturbations and large, uniform changes to an input. Moreover, we prove that such combinations encompass perturbations that would not be allowed in either constituent threat model. In practice, ReColorAdv can significantly reduce the accuracy of a ResNet-32 trained on CIFAR-10. Furthermore, to the best of our knowledge, combining ReColorAdv with other attacks leads to the strongest existing attack even after adversarial training.},
	urldate = {2023-05-14},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Laidlaw, Cassidy and Feizi, Soheil},
	year = {2019},
}

@misc{jordan_quantifying_2019,
	title = {Quantifying {Perceptual} {Distortion} of {Adversarial} {Examples}},
	url = {http://arxiv.org/abs/1902.08265},
	doi = {10.48550/arXiv.1902.08265},
	abstract = {Recent work has shown that additive threat models, which only permit the addition of bounded noise to the pixels of an image, are insufficient for fully capturing the space of imperceivable adversarial examples. For example, small rotations and spatial transformations can fool classifiers, remain imperceivable to humans, but have large additive distance from the original images. In this work, we leverage quantitative perceptual metrics like LPIPS and SSIM to define a novel threat model for adversarial attacks. To demonstrate the value of quantifying the perceptual distortion of adversarial examples, we present and employ a unifying framework fusing different attack styles. We first prove that our framework results in images that are unattainable by attack styles in isolation. We then perform adversarial training using attacks generated by our framework to demonstrate that networks are only robust to classes of adversarial perturbations they have been trained against, and combination attacks are stronger than any of their individual components. Finally, we experimentally demonstrate that our combined attacks retain the same perceptual distortion but induce far higher misclassification rates when compared against individual attacks.},
	urldate = {2023-10-05},
	publisher = {arXiv},
	author = {Jordan, Matt and Manoj, Naren and Goel, Surbhi and Dimakis, Alexandros G.},
	month = feb,
	year = {2019},
	note = {arXiv:1902.08265 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{engstrom_exploring_2019,
	title = {Exploring the {Landscape} of {Spatial} {Robustness}},
	url = {https://proceedings.mlr.press/v97/engstrom19a.html},
	abstract = {The study of adversarial robustness has so far largely focused on perturbations bound in \${\textbackslash}ell\_p\$-norms. However, state-of-the-art models turn out to be also vulnerable to other, more natural classes of perturbations such as translations and rotations. In this work, we thoroughly investigate the vulnerability of neural network–based classifiers to rotations and translations. While data augmentation offers relatively small robustness, we use ideas from robust optimization and test-time input aggregation to significantly improve robustness. Finally we find that, in contrast to the \${\textbackslash}ell\_p\$-norm case, first-order methods cannot reliably find worst-case perturbations. This highlights spatial robustness as a fundamentally different setting requiring additional study.},
	language = {en},
	urldate = {2023-07-08},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Engstrom, Logan and Tran, Brandon and Tsipras, Dimitris and Schmidt, Ludwig and Madry, Aleksander},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {1802--1811},
}

@inproceedings{duan_adversarial_2020,
	title = {Adversarial {Camouflage}: {Hiding} {Physical}-{World} {Attacks} {With} {Natural} {Styles}},
	shorttitle = {Adversarial {Camouflage}},
	url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Duan_Adversarial_Camouflage_Hiding_Physical-World_Attacks_With_Natural_Styles_CVPR_2020_paper.html},
	urldate = {2023-07-08},
	author = {Duan, Ranjie and Ma, Xingjun and Wang, Yisen and Bailey, James and Qin, A. K. and Yang, Yun},
	year = {2020},
	pages = {1000--1008},
}

@inproceedings{lin_dual_2020,
	title = {Dual {Manifold} {Adversarial} {Robustness}: {Defense} against {Lp} and non-{Lp} {Adversarial} {Attacks}},
	volume = {33},
	shorttitle = {Dual {Manifold} {Adversarial} {Robustness}},
	url = {https://proceedings.neurips.cc/paper/2020/hash/23937b42f9273974570fb5a56a6652ee-Abstract.html},
	abstract = {Adversarial training is a popular defense strategy against attack threat models with bounded Lp norms. However, it often degrades the model performance on normal images and more importantly, the defense does not generalize well to novel attacks. Given the success of deep generative models such as GANs and VAEs in characterizing the underlying manifold of images, we investigate whether or not the aforementioned deficiencies of adversarial training can be remedied by exploiting the underlying manifold information. To partially answer this question, we consider the scenario when the manifold information of the underlying data is available. We use a subset of ImageNet natural images where an approximate underlying manifold is learned using StyleGAN. We also construct an On-Manifold ImageNet'' (OM-ImageNet) dataset by projecting the ImageNet samples onto the learned manifold. For OM-ImageNet, the underlying manifold information is exact. Using OM-ImageNet, we first show that on-manifold adversarial training improves both standard accuracy and robustness to on-manifold attacks. However, since no out-of-manifold perturbations are realized, the defense can be broken by Lp adversarial attacks. We further propose Dual Manifold Adversarial Training (DMAT) where adversarial perturbations in both latent and image spaces are used in robustifying the model. Our DMAT improves performance on normal images, and achieves comparable robustness to the standard adversarial training against Lp attacks. In addition, we observe that models defended by DMAT achieve improved robustness against novel attacks which manipulate images by global color shifts or various types of image filtering. Interestingly, similar improvements are also achieved when the defended models are tested on (out-of-manifold) natural images. These results demonstrate the potential benefits of using manifold information in enhancing robustness of deep learning models against various types of novel adversarial attacks.},
	urldate = {2023-05-16},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Lin, Wei-An and Lau, Chun Pong and Levine, Alexander and Chellappa, Rama and Feizi, Soheil},
	year = {2020},
	pages = {3487--3498},
}

@inproceedings{qiu_semanticadv_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{SemanticAdv}: {Generating} {Adversarial} {Examples} via {Attribute}-{Conditioned} {Image} {Editing}},
	isbn = {978-3-030-58568-6},
	shorttitle = {{SemanticAdv}},
	doi = {10.1007/978-3-030-58568-6_2},
	abstract = {Recent studies have shown that DNNs are vulnerable to adversarial examples which are manipulated instances targeting to mislead DNNs to make incorrect predictions. Currently, most such adversarial examples try to guarantee “subtle perturbation” by limiting the \$\$L\_p\$\$Lpnorm of the perturbation. In this paper, we propose SemanticAdv to generate a new type of semantically realistic adversarial examples via attribute-conditioned image editing. Compared to existing methods, our SemanticAdv enables fine-grained analysis and evaluation of DNNs with input variations in the attribute space. We conduct comprehensive experiments to show that our adversarial examples not only exhibit semantically meaningful appearances but also achieve high targeted attack success rates under both whitebox and blackbox settings. Moreover, we show that the existing pixel-based and attribute-based defense methods fail to defend against SemanticAdv. We demonstrate the applicability of SemanticAdv on both face recognition and general street-view images to show its generalization. We believe that our work can shed light on further understanding about vulnerabilities of DNNs as well as novel defense approaches. Our implementation is available at https://github.com/AI-secure/SemanticAdv.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Qiu, Haonan and Xiao, Chaowei and Yang, Lei and Yan, Xinchen and Lee, Honglak and Li, Bo},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	pages = {19--37},
}

@inproceedings{zhao_towards_2020,
	title = {Towards {Large} {Yet} {Imperceptible} {Adversarial} {Image} {Perturbations} {With} {Perceptual} {Color} {Distance}},
	url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Zhao_Towards_Large_Yet_Imperceptible_Adversarial_Image_Perturbations_With_Perceptual_Color_CVPR_2020_paper.html},
	urldate = {2023-09-15},
	author = {Zhao, Zhengyu and Liu, Zhuoran and Larson, Martha},
	year = {2020},
	pages = {1039--1048},
}

@inproceedings{mohapatra_towards_2020,
	address = {Seattle, WA, USA},
	title = {Towards {Verifying} {Robustness} of {Neural} {Networks} {Against} {A} {Family} of {Semantic} {Perturbations}},
	isbn = {978-1-72817-168-5},
	url = {https://ieeexplore.ieee.org/document/9157375/},
	doi = {10.1109/CVPR42600.2020.00032},
	abstract = {Verifying robustness of neural networks given a speciﬁed threat model is a fundamental yet challenging task. While current veriﬁcation methods mainly focus on the ℓp-norm threat model of the input instances, robustness veriﬁcation against semantic adversarial attacks inducing large ℓp-norm perturbations, such as color shifting and lighting adjustment, are beyond their capacity. To bridge this gap, we propose Semantify-NN, a model-agnostic and generic robustness veriﬁcation approach against semantic perturbations for neural networks. By simply inserting our proposed semantic perturbation layers (SP-layers) to the input layer of any given model, Semantify-NN is model-agnostic, and any ℓp-norm based veriﬁcation tools can be used to verify the model robustness against semantic perturbations. We illustrate the principles of designing the SP-layers and provide examples including semantic perturbations to image classiﬁcation in the space of hue, saturation, lightness, brightness, contrast and rotation, respectively. In addition, an efﬁcient reﬁnement technique is proposed to further signiﬁcantly improve the semantic certiﬁcate. Experiments on various network architectures and different datasets demonstrate the superior veriﬁcation performance of Semantify-NN over ℓp-norm-based veriﬁcation frameworks that naively convert semantic perturbation to ℓp-norm. The results show that Semantify-NN can support robustness veriﬁcation against a wide range of semantic perturbations.},
	language = {en},
	urldate = {2023-05-18},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Mohapatra, Jeet and Weng, Tsui-Wei and Chen, Pin-Yu and Liu, Sijia and Daniel, Luca},
	month = jun,
	year = {2020},
	pages = {241--249},
}

@inproceedings{wong_learning_2020,
	title = {Learning perturbation sets for robust machine learning},
	url = {https://openreview.net/forum?id=MIDckA56aD},
	abstract = {Although much progress has been made towards robust deep learning, a significant gap in robustness remains between real-world perturbations and more narrowly defined sets typically studied in...},
	language = {en},
	urldate = {2021-12-16},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Wong, Eric and Kolter, J. Zico},
	month = sep,
	year = {2020},
}

@misc{robey_model-based_2020,
	title = {Model-{Based} {Robust} {Deep} {Learning}: {Generalizing} to {Natural}, {Out}-of-{Distribution} {Data}},
	shorttitle = {Model-{Based} {Robust} {Deep} {Learning}},
	url = {http://arxiv.org/abs/2005.10247},
	doi = {10.48550/arXiv.2005.10247},
	abstract = {While deep learning has resulted in major breakthroughs in many application domains, the frameworks commonly used in deep learning remain fragile to artificially-crafted and imperceptible changes in the data. In response to this fragility, adversarial training has emerged as a principled approach for enhancing the robustness of deep learning with respect to norm-bounded perturbations. However, there are other sources of fragility for deep learning that are arguably more common and less thoroughly studied. Indeed, natural variation such as lighting or weather conditions can significantly degrade the accuracy of trained neural networks, proving that such natural variation presents a significant challenge for deep learning. In this paper, we propose a paradigm shift from perturbation-based adversarial robustness toward model-based robust deep learning. Our objective is to provide general training algorithms that can be used to train deep neural networks to be robust against natural variation in data. Critical to our paradigm is first obtaining a model of natural variation which can be used to vary data over a range of natural conditions. Such models may be either known a priori or else learned from data. In the latter case, we show that deep generative models can be used to learn models of natural variation that are consistent with realistic conditions. We then exploit such models in three novel model-based robust training algorithms in order to enhance the robustness of deep learning with respect to the given model. Our extensive experiments show that across a variety of naturally-occurring conditions and across various datasets, deep neural networks trained with our model-based algorithms significantly outperform both standard deep learning algorithms as well as norm-bounded robust deep learning algorithms.},
	urldate = {2023-07-08},
	publisher = {arXiv},
	author = {Robey, Alexander and Hassani, Hamed and Pappas, George J.},
	month = nov,
	year = {2020},
	note = {arXiv:2005.10247 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{amir_understanding_2021,
	title = {Understanding and {Simplifying} {Perceptual} {Distances}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Amir_Understanding_and_Simplifying_Perceptual_Distances_CVPR_2021_paper.html},
	language = {en},
	urldate = {2023-09-07},
	author = {Amir, Dan and Weiss, Yair},
	year = {2021},
	pages = {12226--12235},
}

@inproceedings{laidlaw_perceptual_2021,
	title = {Perceptual {Adversarial} {Robustness}: {Defense} {Against} {Unseen} {Threat} {Models}},
	shorttitle = {Perceptual {Adversarial} {Robustness}},
	url = {https://openreview.net/forum?id=dFwBosAcJkN},
	abstract = {A key challenge in adversarial robustness is the lack of a precise mathematical characterization of human perception, used in the definition of adversarial attacks that are imperceptible to human eyes. Most current attacks and defenses try to get around this issue by considering restrictive adversarial threat models such as those bounded by \$L\_2\$ or \$L\_{\textbackslash}infty\$ distance, spatial perturbations, etc. However, models that are robust against any of these restrictive threat models are still fragile against other threat models, i.e. they have poor generalization to unforeseen attacks. Moreover, even if a model is robust against the union of several restrictive threat models, it is still susceptible to other imperceptible adversarial examples that are not contained in any of the constituent threat models. To resolve these issues, we propose adversarial training against the set of all imperceptible adversarial examples. Since this set is intractable to compute without a human in the loop, we approximate it using deep neural networks. We call this threat model the neural perceptual threat model (NPTM); it includes adversarial examples with a bounded neural perceptual distance (a neural network-based approximation of the true perceptual distance) to natural images. Through an extensive perceptual study, we show that the neural perceptual distance correlates well with human judgements of perceptibility of adversarial examples, validating our threat model. Under the NPTM, we develop novel perceptual adversarial attacks and defenses. Because the NPTM is very broad, we find that Perceptual Adversarial Training (PAT) against a perceptual attack gives robustness against many other types of adversarial attacks. We test PAT on CIFAR-10 and ImageNet-100 against five diverse adversarial attacks: \$L\_2\$, \$L\_{\textbackslash}infty\$, spatial, recoloring, and JPEG. We find that PAT achieves state-of-the-art robustness against the union of these five attacks—more than doubling the accuracy over the next best model—without training against any of them. That is, PAT generalizes well to unforeseen perturbation types. This is vital in sensitive applications where a particular threat model cannot be assumed, and to the best of our knowledge, PAT is the first adversarial training defense with this property. Code and data are available at https://github.com/cassidylaidlaw/perceptual-advex},
	language = {en},
	urldate = {2023-05-14},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Laidlaw, Cassidy and Singla, Sahil and Feizi, Soheil},
	month = jan,
	year = {2021},
}

@inproceedings{schneider_concept-based_2022,
	title = {Concept-based {Adversarial} {Attacks}: {Tricking} {Humans} and {Classifiers} {Alike}},
	shorttitle = {Concept-based {Adversarial} {Attacks}},
	doi = {10.1109/SPW54247.2022.9833874},
	abstract = {We propose to generate adversarial samples by modifying activations of upper layers encoding semantically meaningful concepts. The original sample is shifted towards a target sample, yielding an adversarial sample, by using the modified activations to reconstruct the original sample. A human might (and possibly should) notice differences between the original and the adversarial sample. Depending on the attacker-provided constraints, an adversarial sample can exhibit subtle differences or appear like a "forged" sample from another class. Our approach and goal are in stark contrast to common attacks involving perturbations of single pixels that are not recognizable by humans. Our approach is relevant in, e.g., multistage processing of inputs, where both humans and machines are involved in decision-making because invisible perturbations will not fool a human. Our evaluation focuses on deep neural networks. We also show the transferability of our adversarial examples among networks.},
	booktitle = {2022 {IEEE} {Security} and {Privacy} {Workshops} ({SPW})},
	author = {Schneider, Johannes and Apruzzese, Giovanni},
	month = may,
	year = {2022},
	note = {ISSN: 2770-8411},
	keywords = {Adversarial Attacks, Conferences, Decision making, Deep Learning, Deep learning, Encoding, Neural networks, Perturbation methods, Privacy, Semantic Attacks},
	pages = {66--72},
}

@article{xu_towards_2021,
	title = {Towards {Feature} {Space} {Adversarial} {Attack} by {Style} {Perturbation}},
	volume = {35},
	copyright = {Copyright (c) 2021 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/17259},
	doi = {10.1609/aaai.v35i12.17259},
	abstract = {We propose a new adversarial attack to Deep Neural Networks for image classification. Different from most existing attacks that directly perturb input pixels, our attack focuses on perturbing abstract features, more specifically, features that denote styles, including interpretable styles such as vivid colors and sharp outlines, and uninterpretable ones. It induces model misclassification by injecting imperceptible style changes through an optimization procedure.
We show that our attack can generate adversarial samples that are more natural-looking than the state-of-the-art unbounded attacks. The experiment also supports that existing pixel-space adversarial attack detection and defense techniques can hardly ensure robustness in the style-related feature space.},
	language = {en},
	number = {12},
	urldate = {2023-05-16},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Xu, Qiuling and Tao, Guanhong and Cheng, Siyuan and Zhang, Xiangyu},
	month = may,
	year = {2021},
	note = {Number: 12},
	keywords = {Adversarial Learning \& Robustness},
	pages = {10523--10531},
}

@inproceedings{dong_viewfool_2022,
	title = {{ViewFool}: {Evaluating} the {Robustness} of {Visual} {Recognition} to {Adversarial} {Viewpoints}},
	volume = {35},
	shorttitle = {{ViewFool}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/eee7ae5cf0c4356c2aeca400771791aa-Abstract-Conference.html},
	language = {en},
	urldate = {2023-07-08},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Dong, Yinpeng and Ruan, Shouwei and Su, Hang and Kang, Caixin and Wei, Xingxing and Zhu, Jun},
	month = dec,
	year = {2022},
	pages = {36789--36803},
}

@article{zhang_perceptual_2022,
	title = {Perceptual {Attacks} of {No}-{Reference} {Image} {Quality} {Models} with {Human}-in-the-{Loop}},
	volume = {35},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/137cb5dd61b2685bd2623967daee6860-Abstract-Conference.html},
	language = {en},
	urldate = {2023-09-08},
	journal = {Advances in Neural Information Processing Systems},
	author = {Zhang, Weixia and Li, Dingquan and Min, Xiongkuo and Zhai, Guangtao and Guo, Guodong and Yang, Xiaokang and Ma, Kede},
	month = dec,
	year = {2022},
	pages = {2916--2929},
}

@inproceedings{li_towards_2023,
	title = {Towards {Benchmarking} and {Assessing} {Visual} {Naturalness} of {Physical} {World} {Adversarial} {Attacks}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Li_Towards_Benchmarking_and_Assessing_Visual_Naturalness_of_Physical_World_Adversarial_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-07-15},
	author = {Li, Simin and Zhang, Shuning and Chen, Gujun and Wang, Dong and Feng, Pu and Wang, Jiakai and Liu, Aishan and Yi, Xin and Liu, Xianglong},
	year = {2023},
	pages = {12324--12333},
}

@inproceedings{duan_inequality_2023,
	title = {Inequality phenomenon in \$l\_\{{\textbackslash}infty\}\$-adversarial training, and its unrealized threats},
	url = {https://openreview.net/forum?id=4t9q35BxGr},
	abstract = {The appearance of adversarial examples raises attention from both academia and industry. Along with the attack-defense arms race, adversarial training is the most effective against adversarial examples. However, we find inequality phenomena occur during the \$l\_\{{\textbackslash}infty\}\$-adversarial training, that few features dominate the prediction made by the adversarially trained model. We systematically evaluate such inequality phenomena by extensive experiments and find such phenomena become more obvious when performing adversarial training with increasing adversarial strength (evaluated by \${\textbackslash}epsilon\$). We hypothesize such inequality phenomena make \$l\_\{{\textbackslash}infty\}\$-adversarially trained model less reliable than the standard trained model when few ``important features" are influenced. To validate our hypothesis, we proposed two simple attacks that either perturb or replace important features with noise or occlusion. Experiments show that \$l\_\{{\textbackslash}infty\}\$-adversarially trained model can be easily attacked when the few important features are influenced. Our work shed light on the limitation of the practicality of \$l\_\{{\textbackslash}infty\}\$-adversarial training.},
	language = {en},
	urldate = {2023-06-08},
	booktitle = {The {Eleventh} {International} {Conference} on {Learning} {Representations}},
	author = {Duan, Ranjie and Chen, YueFeng and Zhu, Yao and Jia, Xiaojun and Zhang, Rong and Xue', Hui},
	month = feb,
	year = {2023},
}

@inproceedings{liang_optimization_2023,
	title = {Optimization for {Robustness} {Evaluation} {Beyond} ℓp {Metrics}},
	doi = {10.1109/ICASSP49357.2023.10095871},
	abstract = {Empirical evaluation of the adversarial robustness of deep learning models involves solving non-trivial constrained optimization problems. Popular numerical algorithms to solve these constrained problems rely predominantly on projected gradient descent (PGD) and mostly handle adversarial perturbations modeled by the ℓ1, ℓ2, and ℓ∞ metrics. In this paper, we introduce a novel algorithmic framework that blends a general-purpose constrained-optimization solver PyGRANSO, With Constraint-Folding (PWCF), to add reliability and generality to robustness evaluation. PWCF 1) finds good-quality solutions without the need of delicate hyperparameter tuning and 2) can handle more general perturbation types, e.g., modeled by general ℓp (where p {\textgreater} 0) and perceptual (nonℓp) distances, which are inaccessible to existing PGD-based algorithms.},
	booktitle = {{ICASSP} 2023 - 2023 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Liang, Hengyue and Liang, Buyun and Cui, Ying and Mitchell, Tim and Sun, Ju},
	month = jun,
	year = {2023},
	keywords = {Deep learning, Measurement, Numerical models, Perturbation methods, Robustness, Signal processing, Signal processing algorithms, adversarial attack, adversarial robustness, constrained optimization, deep neural networks, perceptual distance, robustness evaluation},
	pages = {1--5},
}

@misc{kaufmann_testing_2023,
	title = {Testing {Robustness} {Against} {Unforeseen} {Adversaries}},
	url = {http://arxiv.org/abs/1908.08016},
	doi = {10.48550/arXiv.1908.08016},
	abstract = {When considering real-world adversarial settings, defenders are unlikely to have access to the full range of deployment-time adversaries during training, and adversaries are likely to use realistic adversarial distortions that will not be limited to small L\_p-constrained perturbations. To narrow in on this discrepancy between research and reality we introduce eighteen novel adversarial attacks, which we use to create ImageNet-UA, a new benchmark for evaluating model robustness against a wide range of unforeseen adversaries. We make use of our benchmark to identify a range of defense strategies which can help overcome this generalization gap, finding a rich space of techniques which can improve unforeseen robustness. We hope the greater variety and realism of ImageNet-UA will make it a useful tool for those working on real-world worst-case robustness, enabling development of more robust defenses which can generalize beyond attacks seen during training.},
	urldate = {2023-09-08},
	publisher = {arXiv},
	author = {Kaufmann, Max and Kang, Daniel and Sun, Yi and Basart, Steven and Yin, Xuwang and Mazeika, Mantas and Arora, Akul and Dziedzic, Adam and Boenisch, Franziska and Brown, Tom and Steinhardt, Jacob and Hendrycks, Dan},
	month = jul,
	year = {2023},
	note = {arXiv:1908.08016 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{ghazanfari_r-lpips_2023,
	title = {R-{LPIPS}: {An} {Adversarially} {Robust} {Perceptual} {Similarity} {Metric}},
	shorttitle = {R-{LPIPS}},
	url = {https://openreview.net/forum?id=uDzy9fmiCw},
	abstract = {Similarity metrics have played a significant role in computer vision to capture the underlying semantics of images. In recent years, advanced similarity metrics, such as the Learned Perceptual Image Patch Similarity (LPIPS), have emerged. These metrics leverage deep features extracted from trained neural networks and have demonstrated a remarkable ability to closely align with human perception when evaluating relative image similarity. However, it is now well-known that neural networks are susceptible to adversarial examples, i.e., small perturbations invisible to humans crafted to deliberately mislead the model. Consequently, the LPIPS metric is also sensitive to such adversarial examples. This susceptibility introduces significant security concerns, especially considering the widespread adoption of LPIPS in large-scale applications. In this paper, we propose the Robust Learned Perceptual Image Patch Similarity (R-LPIPS) metric, a new metric that leverages adversarially trained deep features. Through a comprehensive set of experiments, we demonstrate the superiority of R-LPIPS compared to the classical LPIPS metric.},
	language = {en},
	urldate = {2023-09-07},
	booktitle = {The {Second} {Workshop} on {New} {Frontiers} in {Adversarial} {Machine} {Learning}},
	author = {Ghazanfari, Sara and Garg, Siddharth and Krishnamurthy, Prashanth and Khorrami, Farshad and Araujo, Alexandre},
	month = aug,
	year = {2023},
}

@inproceedings{huang_improving_2023,
	title = {Improving {Adversarial} {Robustness} of {Masked} {Autoencoders} via {Test}-time {Frequency}-domain {Prompting}},
	url = {https://openaccess.thecvf.com/content/ICCV2023/html/Huang_Improving_Adversarial_Robustness_of_Masked_Autoencoders_via_Test-time_Frequency-domain_Prompting_ICCV_2023_paper.html},
	language = {en},
	urldate = {2023-10-04},
	booktitle = {Proceedings of the {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision}},
	author = {Huang, Qidong and Dong, Xiaoyi and Chen, Dongdong and Chen, Yinpeng and Yuan, Lu and Hua, Gang and Zhang, Weiming and Yu, Nenghai},
	year = {2023},
	pages = {1600--1610},
}

@inproceedings{zhou_conditional_2022,
	title = {Conditional {Prompt} {Learning} for {Vision}-{Language} {Models}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Zhou_Conditional_Prompt_Learning_for_Vision-Language_Models_CVPR_2022_paper.html},
	language = {en},
	urldate = {2023-07-21},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Zhou, Kaiyang and Yang, Jingkang and Loy, Chen Change and Liu, Ziwei},
	year = {2022},
	pages = {16816--16825},
}

@inproceedings{vats_adversarial_2023,
	title = {Adversarial {Examples} with {Specular} {Highlights}},
	url = {https://openaccess.thecvf.com/content/ICCV2023W/AROW/html/Vats_Adversarial_Examples_with_Specular_Highlights_ICCVW_2023_paper.html},
	language = {en},
	urldate = {2023-10-07},
	author = {Vats, Vanshika and Jerripothula, Koteswar Rao},
	year = {2023},
	pages = {3602--3611},
}

@inproceedings{santos_exploring_2023,
	title = {Exploring {Image} {Classification} {Robustness} and {Interpretability} with {Right} for the {Right} {Reasons} {Data} {Augmentation}},
	url = {https://openaccess.thecvf.com/content/ICCV2023W/LXCV/html/Santos_Exploring_Image_Classification_Robustness_and_Interpretability_with_Right_for_the_ICCVW_2023_paper.html},
	language = {en},
	urldate = {2023-10-07},
	author = {Santos, Flávio Arthur Oliveira and Zanchettin, Cleber},
	year = {2023},
	pages = {4147--4156},
}

@inproceedings{shu_test-time_2022,
	title = {Test-{Time} {Prompt} {Tuning} for {Zero}-{Shot} {Generalization} in {Vision}-{Language} {Models}},
	url = {https://openreview.net/forum?id=e8PVEkSa4Fq},
	abstract = {Pre-trained vision-language models (e.g., CLIP) have shown promising zero-shot generalization in many downstream tasks with properly designed text prompts. Instead of relying on hand-engineered prompts, recent works learn prompts using the training data from downstream tasks. While effective, training on domain-specific data reduces a model's generalization capability to unseen new domains. In this work, we propose test-time prompt tuning (TPT), a method that can learn adaptive prompts on the fly with a single test sample. TPT optimizes the prompt by minimizing the entropy with confidence selection so that the model has consistent predictions across different augmented views of each test sample. In evaluating generalization to natural distribution shifts, TPT improves the zero-shot top-1 accuracy of CLIP by 3.6{\textbackslash}\% on average, surpassing previous prompt tuning approaches that require additional task-specific training data. In evaluating cross-dataset generalization with unseen categories, TPTperforms on par with the state-of-the-art approaches that use additional training data.},
	language = {en},
	urldate = {2023-10-07},
	author = {Shu, Manli and Nie, Weili and Huang, De-An and Yu, Zhiding and Goldstein, Tom and Anandkumar, Anima and Xiao, Chaowei},
	month = may,
	year = {2022},
}

@inproceedings{yang_defending_2021,
	title = {Defending {Multimodal} {Fusion} {Models} {Against} {Single}-{Source} {Adversaries}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Yang_Defending_Multimodal_Fusion_Models_Against_Single-Source_Adversaries_CVPR_2021_paper.html},
	language = {en},
	urldate = {2023-10-06},
	author = {Yang, Karren and Lin, Wan-Yi and Barman, Manash and Condessa, Filipe and Kolter, Zico},
	year = {2021},
	pages = {3340--3349},
}

@misc{carlini_poisoning_2023,
	title = {Poisoning {Web}-{Scale} {Training} {Datasets} is {Practical}},
	url = {http://arxiv.org/abs/2302.10149},
	doi = {10.48550/arXiv.2302.10149},
	abstract = {Deep learning models are often trained on distributed, webscale datasets crawled from the internet. In this paper, we introduce two new dataset poisoning attacks that intentionally introduce malicious examples to a model's performance. Our attacks are immediately practical and could, today, poison 10 popular datasets. Our first attack, split-view poisoning, exploits the mutable nature of internet content to ensure a dataset annotator's initial view of the dataset differs from the view downloaded by subsequent clients. By exploiting specific invalid trust assumptions, we show how we could have poisoned 0.01\% of the LAION-400M or COYO-700M datasets for just \$60 USD. Our second attack, frontrunning poisoning, targets web-scale datasets that periodically snapshot crowd-sourced content -- such as Wikipedia -- where an attacker only needs a time-limited window to inject malicious examples. In light of both attacks, we notify the maintainers of each affected dataset and recommended several low-overhead defenses.},
	urldate = {2023-10-06},
	publisher = {arXiv},
	author = {Carlini, Nicholas and Jagielski, Matthew and Choquette-Choo, Christopher A. and Paleka, Daniel and Pearce, Will and Anderson, Hyrum and Terzis, Andreas and Thomas, Kurt and Tramèr, Florian},
	month = feb,
	year = {2023},
	note = {arXiv:2302.10149 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@inproceedings{ge_improving_2023,
	title = {Improving {Zero}-{Shot} {Generalization} and {Robustness} of {Multi}-{Modal} {Models}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Ge_Improving_Zero-Shot_Generalization_and_Robustness_of_Multi-Modal_Models_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-07-21},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Ge, Yunhao and Ren, Jie and Gallagher, Andrew and Wang, Yuxiao and Yang, Ming-Hsuan and Adam, Hartwig and Itti, Laurent and Lakshminarayanan, Balaji and Zhao, Jiaping},
	year = {2023},
	pages = {11093--11101},
}

@inproceedings{salman_unadversarial_2021,
	title = {Unadversarial {Examples}: {Designing} {Objects} for {Robust} {Vision}},
	volume = {34},
	shorttitle = {Unadversarial {Examples}},
	url = {https://proceedings.neurips.cc/paper/2021/hash/816a6db41f0e44644bc65808b6db5ca4-Abstract.html},
	abstract = {We study a class of computer vision settings wherein one can modify the design of the objects being recognized. We develop a framework that leverages this capability---and deep networks' unusual sensitivity to input perturbations---to design robust objects,'' i.e., objects that are explicitly optimized to be confidently classified. Our framework yields improved performance on standard benchmarks, a simulated robotics environment, and physical-world experiments.},
	urldate = {2023-10-06},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Salman, Hadi and Ilyas, Andrew and Engstrom, Logan and Vemprala, Sai and Madry, Aleksander and Kapoor, Ashish},
	year = {2021},
	pages = {15270--15284},
}

@inproceedings{feng_learning_2019,
	title = {Learning to {Confuse}: {Generating} {Training} {Time} {Adversarial} {Data} with {Auto}-{Encoder}},
	volume = {32},
	shorttitle = {Learning to {Confuse}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2019/hash/1ce83e5d4135b07c0b82afffbe2b3436-Abstract.html},
	abstract = {In this work, we consider one challenging training time attack by modifying training data with bounded perturbation, hoping to manipulate the behavior (both targeted or non-targeted) of any corresponding trained classifier during test time when facing clean samples. To achieve this, we proposed to use an auto-encoder-like network to generate such adversarial perturbations on the training data together with one imaginary victim differentiable classifier. The perturbation generator will learn to update its weights so as to produce the most harmful noise, aiming to cause the lowest performance for the victim classifier during test time. This can be formulated into a non-linear equality constrained optimization problem. Unlike GANs, solving such problem is computationally challenging, we then proposed a simple yet effective procedure to decouple the alternating updates for the two networks for stability. By teaching the perturbation generator to hijacking the training trajectory of the victim classifier, the generator can thus learn to move against the victim classifier step by step. The method proposed in this paper can be easily extended to the label specific setting where the attacker can manipulate the predictions of the victim classifier according to some predefined rules rather than only making wrong predictions. Experiments on various datasets including CIFAR-10 and a reduced version of ImageNet confirmed the effectiveness of the proposed method and empirical results showed that, such bounded perturbations have good transferability across different types of victim classifiers.},
	urldate = {2023-10-06},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Feng, Ji and Cai, Qi-Zhi and Zhou, Zhi-Hua},
	year = {2019},
}

@misc{yang_generative_2017,
	title = {Generative {Poisoning} {Attack} {Method} {Against} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1703.01340},
	doi = {10.48550/arXiv.1703.01340},
	abstract = {Poisoning attack is identified as a severe security threat to machine learning algorithms. In many applications, for example, deep neural network (DNN) models collect public data as the inputs to perform re-training, where the input data can be poisoned. Although poisoning attack against support vector machines (SVM) has been extensively studied before, there is still very limited knowledge about how such attack can be implemented on neural networks (NN), especially DNNs. In this work, we first examine the possibility of applying traditional gradient-based method (named as the direct gradient method) to generate poisoned data against NNs by leveraging the gradient of the target model w.r.t. the normal data. We then propose a generative method to accelerate the generation rate of the poisoned data: an auto-encoder (generator) used to generate poisoned data is updated by a reward function of the loss, and the target NN model (discriminator) receives the poisoned data to calculate the loss w.r.t. the normal data. Our experiment results show that the generative method can speed up the poisoned data generation rate by up to 239.38x compared with the direct gradient method, with slightly lower model accuracy degradation. A countermeasure is also designed to detect such poisoning attack methods by checking the loss of the target model.},
	urldate = {2023-10-06},
	publisher = {arXiv},
	author = {Yang, Chaofei and Wu, Qing and Li, Hai and Chen, Yiran},
	month = mar,
	year = {2017},
	note = {arXiv:1703.01340 [cs, stat]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{kanai_one-vs--rest_2023,
	title = {One-vs-the-{Rest} {Loss} to {Focus} on {Important} {Samples} in {Adversarial} {Training}},
	url = {https://proceedings.mlr.press/v202/kanai23a.html},
	abstract = {This paper proposes a new loss function for adversarial training. Since adversarial training has difficulties, e.g., necessity of high model capacity, focusing on important data points by weighting cross-entropy loss has attracted much attention. However, they are vulnerable to sophisticated attacks, e.g., Auto-Attack. This paper experimentally reveals that the cause of their vulnerability is their small margins between logits for the true label and the other labels. Since neural networks classify the data points based on the logits, logit margins should be large enough to avoid flipping the largest logit by the attacks. Importance-aware methods do not increase logit margins of important samples but decrease those of less-important samples compared with cross-entropy loss. To increase logit margins of important samples, we propose switching one-vs-the-rest loss (SOVR), which switches from cross-entropy to one-vs-the-rest loss for important samples that have small logit margins. We prove that one-vs-the-rest loss increases logit margins two times larger than the weighted cross-entropy loss for a simple problem. We experimentally confirm that SOVR increases logit margins of important samples unlike existing methods and achieves better robustness against Auto-Attack than importance-aware methods.},
	language = {en},
	urldate = {2023-10-06},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Kanai, Sekitoshi and Yamaguchi, Shin’Ya and Yamada, Masanori and Takahashi, Hiroshi and Ohno, Kentaro and Ida, Yasutoshi},
	month = jul,
	year = {2023},
	note = {ISSN: 2640-3498},
	pages = {15669--15695},
}

@misc{jin_manifold_2020,
	title = {Manifold {Regularization} for {Locally} {Stable} {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2003.04286},
	doi = {10.48550/arXiv.2003.04286},
	abstract = {We apply concepts from manifold regularization to develop new regularization techniques for training locally stable deep neural networks. Our regularizers are based on a sparsification of the graph Laplacian which holds with high probability when the data is sparse in high dimensions, as is common in deep learning. Empirically, our networks exhibit stability in a diverse set of perturbation models, including \${\textbackslash}ell\_2\$, \${\textbackslash}ell\_{\textbackslash}infty\$, and Wasserstein-based perturbations; in particular, we achieve 40\% adversarial accuracy on CIFAR-10 against an adaptive PGD attack using \${\textbackslash}ell\_{\textbackslash}infty\$ perturbations of size \${\textbackslash}epsilon = 8/255\$, and state-of-the-art verified accuracy of 21\% in the same perturbation model. Furthermore, our techniques are efficient, incurring overhead on par with two additional parallel forward passes through the network.},
	urldate = {2023-10-06},
	publisher = {arXiv},
	author = {Jin, Charles and Rinard, Martin},
	month = sep,
	year = {2020},
	note = {arXiv:2003.04286 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@inproceedings{elsayed_adversarial_2018,
	title = {Adversarial {Reprogramming} of {Neural} {Networks}},
	url = {https://openreview.net/forum?id=Syx_Ss05tm},
	abstract = {Deep neural networks are susceptible to adversarial attacks. In computer vision, well-crafted perturbations to images can cause neural networks to make mistakes such as confusing a cat with a computer. Previous adversarial attacks have been designed to degrade performance of models or cause machine learning models to produce specific outputs chosen ahead of time by the attacker. We introduce attacks that instead reprogram the target model to perform a task chosen by the attacker without the attacker needing to specify or compute the desired output for each test-time input. This attack finds a single adversarial perturbation, that can be added to all test-time inputs to a machine learning model in order to cause the model to perform a task chosen by the adversary—even if the model was not trained to do this task. These perturbations can thus be considered a program for the new task. We demonstrate adversarial reprogramming on six ImageNet classification models, repurposing these models to perform a counting task, as well as classification tasks: classification of MNIST and CIFAR-10 examples presented as inputs to the ImageNet model.},
	language = {en},
	urldate = {2023-10-04},
	author = {Elsayed, Gamaleldin F. and Goodfellow, Ian and Sohl-Dickstein, Jascha},
	month = sep,
	year = {2018},
}

@misc{bahng_exploring_2022,
	title = {Exploring {Visual} {Prompts} for {Adapting} {Large}-{Scale} {Models}},
	url = {http://arxiv.org/abs/2203.17274},
	doi = {10.48550/arXiv.2203.17274},
	abstract = {We investigate the efficacy of visual prompting to adapt large-scale models in vision. Following the recent approach from prompt tuning and adversarial reprogramming, we learn a single image perturbation such that a frozen model prompted with this perturbation performs a new task. Through comprehensive experiments, we demonstrate that visual prompting is particularly effective for CLIP and robust to distribution shift, achieving performance competitive with standard linear probes. We further analyze properties of the downstream dataset, prompt design, and output transformation in regard to adaptation performance. The surprising effectiveness of visual prompting provides a new perspective on adapting pre-trained models in vision. Code is available at http://hjbahng.github.io/visual\_prompting .},
	urldate = {2023-10-04},
	publisher = {arXiv},
	author = {Bahng, Hyojin and Jahanian, Ali and Sankaranarayanan, Swami and Isola, Phillip},
	month = jun,
	year = {2022},
	note = {arXiv:2203.17274 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{chen_visual_2023,
	title = {Visual {Prompting} for {Adversarial} {Robustness}},
	url = {https://ieeexplore.ieee.org/abstract/document/10097245},
	doi = {10.1109/ICASSP49357.2023.10097245},
	abstract = {In this work, we leverage visual prompting (VP) to improve adversarial robustness of a fixed, pre-trained model at test time. Compared to conventional adversarial defenses, VP allows us to design universal (i.e., data-agnostic) input prompting templates, which have plug-and-play capabilities at test time to achieve desired model performance without introducing much computation overhead. Although VP has been successfully applied to improving model generalization, it remains elusive whether and how it can be used to defend against adversarial attacks. We investigate this problem and show that the vanilla VP approach is not effective in adversarial defense since a universal input prompt lacks the capacity for robust learning against sample-specific adversarial perturbations. To circumvent it, we propose a new VP method, termed Class-wise Adversarial Visual Prompting (C-AVP), to generate class-wise visual prompts so as to not only leverage the strengths of ensemble prompts but also optimize their interrelations to improve model robustness. Our experiments show that C-AVP outperforms the conventional VP method, with 2.1× standard accuracy gain and 2× robust accuracy gain. Compared to classical test-time defenses, C-AVP also yields a 42× inference time speedup. Code is available at https://github.com/Phoveran/vp-for-adversarial-robustness.},
	urldate = {2023-10-04},
	booktitle = {{ICASSP} 2023 - 2023 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Chen, Aochuan and Lorenz, Peter and Yao, Yuguang and Chen, Pin-Yu and Liu, Sijia},
	month = jun,
	year = {2023},
	pages = {1--5},
}

@article{qiu_large_2023,
	title = {Large {AI} {Models} in {Health} {Informatics}: {Applications}, {Challenges}, and the {Future}},
	issn = {2168-2208},
	shorttitle = {Large {AI} {Models} in {Health} {Informatics}},
	url = {https://ieeexplore.ieee.org/document/10261199},
	doi = {10.1109/JBHI.2023.3316750},
	abstract = {Large AI models, or foundation models, are models recently emerging with massive scales both parameter-wise and data-wise, the magnitudes of which can reach beyond billions. Once pretrained, large AI models demonstrate impressive performance in various downstream tasks. A prime example is ChatGPT, whose capability has compelled people's imagination about the far-reaching influence that large AI models can have and their potential to transform different domains of our lives. In health informatics, the advent of large AI models has brought new paradigms for the design of methodologies. The scale of multi-modal data in the biomedical and health domain has been ever-expanding especially since the community embraced the era of deep learning, which provides the ground to develop, validate, and advance large AI models for breakthroughs in health-related areas. This article presents a comprehensive review of large AI models, from background to their applications. We identify seven key sectors in which large AI models are applicable and might have substantial influence, including 1) bioinformatics; 2) medical diagnosis; 3) medical imaging; 4) medical informatics; 5) medical education; 6) public health; and 7) medical robotics. We examine their challenges, followed by a critical discussion about potential future directions and pitfalls of large AI models in transforming the field of health informatics.},
	urldate = {2023-10-04},
	journal = {IEEE Journal of Biomedical and Health Informatics},
	author = {Qiu, Jianing and Li, Lin and Sun, Jiankai and Peng, Jiachuan and Shi, Peilun and Zhang, Ruiyang and Dong, Yinzhao and Lam, Kyle and Lo, Frank P.-W. and Xiao, Bo and Yuan, Wu and Wang, Ningli and Xu, Dong and Lo, Benny},
	year = {2023},
	note = {Conference Name: IEEE Journal of Biomedical and Health Informatics},
	pages = {1--14},
}

@inproceedings{qiu_benchmarking_2022,
	title = {Benchmarking {Robustness} under {Distribution} {Shift} of {Multimodal} {Image}-{Text} {Models}},
	url = {https://openreview.net/forum?id=PN4CUHbnlc},
	abstract = {Multimodal image-text models have shown remarkable performance in the past few years. However, the robustness of such foundation models against distribution shifts is crucial in downstream applications. In this paper, we investigate their robustness under image and text perturbations. We first build several multimodal benchmark datasets by applying 17 image perturbation and 16 text perturbation techniques. Then we extensively study the robustness of 6 widely adopted models on 3 downstream tasks (image-text retrieval, visual reasoning, and visual entailment). We observe that these powerful multimodal models are sensitive to image/text perturbations, especially to image perturbations. For text, character-level perturbations have shown higher adversarial impact than word-level and sentence-level perturbations. We also observe that models trained by generative objectives tend to be more robust. Our findings in terms of robustness study could facilitate the development of large image-text models, as well as their deployment for real-world applications.},
	language = {en},
	urldate = {2023-08-24},
	booktitle = {{NeurIPS} 2022 {Workshop} on {Distribution} {Shifts}: {Connecting} {Methods} and {Applications}},
	author = {Qiu, Jielin and Zhu, Yi and Shi, Xingjian and Tang, Zhiqiang and Zhao, Ding and Li, Bo and Li, Mu},
	month = oct,
	year = {2022},
}

@misc{noever_reading_2021,
	title = {Reading {Isn}'t {Believing}: {Adversarial} {Attacks} {On} {Multi}-{Modal} {Neurons}},
	shorttitle = {Reading {Isn}'t {Believing}},
	url = {http://arxiv.org/abs/2103.10480},
	doi = {10.48550/arXiv.2103.10480},
	abstract = {With Open AI's publishing of their CLIP model (Contrastive Language-Image Pre-training), multi-modal neural networks now provide accessible models that combine reading with visual recognition. Their network offers novel ways to probe its dual abilities to read text while classifying visual objects. This paper demonstrates several new categories of adversarial attacks, spanning basic typographical, conceptual, and iconographic inputs generated to fool the model into making false or absurd classifications. We demonstrate that contradictory text and image signals can confuse the model into choosing false (visual) options. Like previous authors, we show by example that the CLIP model tends to read first, look later, a phenomenon we describe as reading isn't believing.},
	urldate = {2023-10-04},
	publisher = {arXiv},
	author = {Noever, David A. and Noever, Samantha E. Miller},
	month = mar,
	year = {2021},
	note = {arXiv:2103.10480 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{inkawhich_adversarial_2023,
	title = {Adversarial {Attacks} on {Foundational} {Vision} {Models}},
	url = {http://arxiv.org/abs/2308.14597},
	doi = {10.48550/arXiv.2308.14597},
	abstract = {Rapid progress is being made in developing large, pretrained, task-agnostic foundational vision models such as CLIP, ALIGN, DINOv2, etc. In fact, we are approaching the point where these models do not have to be finetuned downstream, and can simply be used in zero-shot or with a lightweight probing head. Critically, given the complexity of working at this scale, there is a bottleneck where relatively few organizations in the world are executing the training then sharing the models on centralized platforms such as HuggingFace and torch.hub. The goal of this work is to identify several key adversarial vulnerabilities of these models in an effort to make future designs more robust. Intuitively, our attacks manipulate deep feature representations to fool an out-of-distribution (OOD) detector which will be required when using these open-world-aware models to solve closed-set downstream tasks. Our methods reliably make in-distribution (ID) images (w.r.t. a downstream task) be predicted as OOD and vice versa while existing in extremely low-knowledge-assumption threat models. We show our attacks to be potent in whitebox and blackbox settings, as well as when transferred across foundational model types (e.g., attack DINOv2 with CLIP)! This work is only just the beginning of a long journey towards adversarially robust foundational vision models.},
	urldate = {2023-10-04},
	publisher = {arXiv},
	author = {Inkawhich, Nathan and McDonald, Gwendolyn and Luley, Ryan},
	month = aug,
	year = {2023},
	note = {arXiv:2308.14597 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@inproceedings{ge_advancing_2023,
	title = {Advancing {Example} {Exploitation} {Can} {Alleviate} {Critical} {Challenges} in {Adversarial} {Training}},
	url = {https://openaccess.thecvf.com/content/ICCV2023/html/Ge_Advancing_Example_Exploitation_Can_Alleviate_Critical_Challenges_in_Adversarial_Training_ICCV_2023_paper.html},
	language = {en},
	urldate = {2023-10-04},
	author = {Ge, Yao and Li, Yun and Han, Keji and Zhu, Junyi and Long, Xianzhong},
	year = {2023},
	pages = {145--154},
}

@inproceedings{liu_sphereface_2017,
	title = {{SphereFace}: {Deep} {Hypersphere} {Embedding} for {Face} {Recognition}},
	shorttitle = {{SphereFace}},
	url = {https://openaccess.thecvf.com/content_cvpr_2017/html/Liu_SphereFace_Deep_Hypersphere_CVPR_2017_paper.html},
	urldate = {2023-10-03},
	author = {Liu, Weiyang and Wen, Yandong and Yu, Zhiding and Li, Ming and Raj, Bhiksha and Song, Le},
	year = {2017},
	pages = {212--220},
}

@inproceedings{liu_large-margin_2016,
	title = {Large-{Margin} {Softmax} {Loss} for {Convolutional} {Neural} {Networks}},
	url = {https://proceedings.mlr.press/v48/liud16.html},
	abstract = {Cross-entropy loss together with softmax is arguably one of the most common used supervision components in convolutional neural networks (CNNs). Despite its simplicity, popularity and excellent performance, the component does not explicitly encourage discriminative learning of features. In this paper, we propose a generalized large-margin softmax (L-Softmax) loss which explicitly encourages intra-class compactness and inter-class separability between learned features. Moreover, L-Softmax not only can adjust the desired margin but also can avoid overfitting. We also show that the L-Softmax loss can be optimized by typical stochastic gradient descent. Extensive experiments on four benchmark datasets demonstrate that the deeply-learned features with L-softmax loss become more discriminative, hence significantly boosting the performance on a variety of visual classification and verification tasks.},
	language = {en},
	urldate = {2023-10-03},
	booktitle = {Proceedings of {The} 33rd {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Liu, Weiyang and Wen, Yandong and Yu, Zhiding and Yang, Meng},
	month = jun,
	year = {2016},
	note = {ISSN: 1938-7228},
	pages = {507--516},
}

@inproceedings{wang_cosface_2018,
	address = {Salt Lake City, UT},
	title = {{CosFace}: {Large} {Margin} {Cosine} {Loss} for {Deep} {Face} {Recognition}},
	isbn = {978-1-5386-6420-9},
	shorttitle = {{CosFace}},
	url = {https://ieeexplore.ieee.org/document/8578650/},
	doi = {10.1109/CVPR.2018.00552},
	abstract = {Face recognition has made extraordinary progress owing to the advancement of deep convolutional neural networks (CNNs). The central task of face recognition, including face veriﬁcation and identiﬁcation, involves face feature discrimination. However, the traditional softmax loss of deep CNNs usually lacks the power of discrimination. To address this problem, recently several loss functions such as center loss, large margin softmax loss, and angular softmax loss have been proposed. All these improved losses share the same idea: maximizing inter-class variance and minimizing intra-class variance. In this paper, we propose a novel loss function, namely large margin cosine loss (LMCL), to realize this idea from a different perspective. More speciﬁcally, we reformulate the softmax loss as a cosine loss by L2 normalizing both features and weight vectors to remove radial variations, based on which a cosine margin term is introduced to further maximize the decision margin in the angular space. As a result, minimum intra-class variance and maximum inter-class variance are achieved by virtue of normalization and cosine decision margin maximization. We refer to our model trained with LMCL as CosFace. Extensive experimental evaluations are conducted on the most popular public-domain face recognition datasets such as MegaFace Challenge, Youtube Faces (YTF) and Labeled Face in the Wild (LFW). We achieve the state-of-the-art performance on these benchmarks, which conﬁrms the effectiveness of our proposed approach.},
	language = {en},
	urldate = {2023-10-03},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Wang, Hao and Wang, Yitong and Zhou, Zheng and Ji, Xing and Gong, Dihong and Zhou, Jingchao and Li, Zhifeng and Liu, Wei},
	month = jun,
	year = {2018},
	pages = {5265--5274},
}

@inproceedings{erdemir_adversarial_2021,
	title = {Adversarial {Robustness} with {Non}-uniform {Perturbations}},
	url = {https://openreview.net/forum?id=cQLkLAQgZ5I},
	abstract = {Adversarial training with non-uniform perturbations across the features provides better robustness against the real-world attacks than the uniform approach.},
	language = {en},
	urldate = {2021-12-02},
	booktitle = {Thirty-{Fifth} {Conference} on {Neural} {Information} {Processing} {Systems}},
	author = {Erdemir, Ecenaz and Bickford, Jeffrey and Melis, Luca and Aydore, Sergul},
	month = may,
	year = {2021},
}

@misc{ganguli_red_2022,
	title = {Red {Teaming} {Language} {Models} to {Reduce} {Harms}: {Methods}, {Scaling} {Behaviors}, and {Lessons} {Learned}},
	shorttitle = {Red {Teaming} {Language} {Models} to {Reduce} {Harms}},
	url = {http://arxiv.org/abs/2209.07858},
	doi = {10.48550/arXiv.2209.07858},
	abstract = {We describe our early efforts to red team language models in order to simultaneously discover, measure, and attempt to reduce their potentially harmful outputs. We make three main contributions. First, we investigate scaling behaviors for red teaming across 3 model sizes (2.7B, 13B, and 52B parameters) and 4 model types: a plain language model (LM); an LM prompted to be helpful, honest, and harmless; an LM with rejection sampling; and a model trained to be helpful and harmless using reinforcement learning from human feedback (RLHF). We find that the RLHF models are increasingly difficult to red team as they scale, and we find a flat trend with scale for the other model types. Second, we release our dataset of 38,961 red team attacks for others to analyze and learn from. We provide our own analysis of the data and find a variety of harmful outputs, which range from offensive language to more subtly harmful non-violent unethical outputs. Third, we exhaustively describe our instructions, processes, statistical methodologies, and uncertainty about red teaming. We hope that this transparency accelerates our ability to work together as a community in order to develop shared norms, practices, and technical standards for how to red team language models.},
	urldate = {2023-10-01},
	publisher = {arXiv},
	author = {Ganguli, Deep and Lovitt, Liane and Kernion, Jackson and Askell, Amanda and Bai, Yuntao and Kadavath, Saurav and Mann, Ben and Perez, Ethan and Schiefer, Nicholas and Ndousse, Kamal and Jones, Andy and Bowman, Sam and Chen, Anna and Conerly, Tom and DasSarma, Nova and Drain, Dawn and Elhage, Nelson and El-Showk, Sheer and Fort, Stanislav and Hatfield-Dodds, Zac and Henighan, Tom and Hernandez, Danny and Hume, Tristan and Jacobson, Josh and Johnston, Scott and Kravec, Shauna and Olsson, Catherine and Ringer, Sam and Tran-Johnson, Eli and Amodei, Dario and Brown, Tom and Joseph, Nicholas and McCandlish, Sam and Olah, Chris and Kaplan, Jared and Clark, Jack},
	month = nov,
	year = {2022},
	note = {arXiv:2209.07858 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computers and Society},
}

@misc{li_aroid_2023,
	title = {{AROID}: {Improving} {Adversarial} {Robustness} through {Online} {Instance}-wise {Data} {Augmentation}},
	shorttitle = {{AROID}},
	url = {http://arxiv.org/abs/2306.07197},
	doi = {10.48550/arXiv.2306.07197},
	abstract = {Deep neural networks are vulnerable to adversarial examples. Adversarial training (AT) is an effective defense against adversarial examples. However, AT is prone to overfitting which degrades robustness substantially. Recently, data augmentation (DA) was shown to be effective in mitigating robust overfitting if appropriately designed and optimized for AT. This work proposes a new method to automatically learn online, instance-wise, DA policies to improve robust generalization for AT. A novel policy learning objective, consisting of Vulnerability, Affinity and Diversity, is proposed and shown to be sufficiently effective and efficient to be practical for automatic DA generation during AT. This allows our method to efficiently explore a large search space for a more effective DA policy and evolve the policy as training progresses. Empirically, our method is shown to outperform or match all competitive DA methods across various model architectures (CNNs and ViTs) and datasets (CIFAR10, SVHN and Imagenette). Our DA policy reinforced vanilla AT to surpass several state-of-the-art AT methods (with baseline DA) in terms of both accuracy and robustness. It can also be combined with those advanced AT methods to produce a further boost in robustness.},
	urldate = {2023-09-27},
	publisher = {arXiv},
	author = {Li, Lin and Qiu, Jianing and Spratling, Michael},
	month = jun,
	year = {2023},
	note = {arXiv:2306.07197 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@inproceedings{kim_feature_2023,
	title = {Feature {Separation} and {Recalibration} for {Adversarial} {Robustness}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Kim_Feature_Separation_and_Recalibration_for_Adversarial_Robustness_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-09-27},
	author = {Kim, Woo Jae and Cho, Yoonki and Jung, Junsik and Yoon, Sung-Eui},
	year = {2023},
	pages = {8183--8192},
}

@inproceedings{wei_cfa_2023,
	title = {{CFA}: {Class}-{Wise} {Calibrated} {Fair} {Adversarial} {Training}},
	shorttitle = {{CFA}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Wei_CFA_Class-Wise_Calibrated_Fair_Adversarial_Training_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-09-27},
	author = {Wei, Zeming and Wang, Yifei and Guo, Yiwen and Wang, Yisen},
	year = {2023},
	pages = {8193--8201},
}

@inproceedings{diffenderfer_winning_2021,
	title = {A {Winning} {Hand}: {Compressing} {Deep} {Networks} {Can} {Improve} {Out}-of-{Distribution} {Robustness}},
	volume = {34},
	shorttitle = {A {Winning} {Hand}},
	url = {https://proceedings.neurips.cc/paper/2021/hash/0607f4c705595b911a4f3e7a127b44e0-Abstract.html},
	abstract = {Successful adoption of deep learning (DL) in the wild requires models to be: (1) compact, (2) accurate, and (3) robust to distributional shifts. Unfortunately, efforts towards simultaneously meeting these requirements have mostly been unsuccessful. This raises an important question: Is the inability to create Compact, Accurate, and Robust Deep neural networks (CARDs) fundamental? To answer this question, we perform a large-scale analysis of popular model compression techniques which uncovers several intriguing patterns. Notably, in contrast to traditional pruning approaches (e.g., fine tuning and gradual magnitude pruning), we find that lottery ticket-style'' approaches can surprisingly be used to produce CARDs, including binary-weight CARDs. Specifically, we are able to create extremely compact CARDs that, compared to their larger counterparts, have similar test accuracy and matching (or better) robustness---simply by pruning and (optionally) quantizing. Leveraging the compactness of CARDs, we develop a simple domain-adaptive test-time ensembling approach (CARD-Decks) that uses a gating module to dynamically select appropriate CARDs from the CARD-Deck based on their spectral-similarity with test samples. The proposed approach builds a "winning hand'' of CARDs that establishes a new state-of-the-art (on RobustBench) on CIFAR-10-C accuracies (i.e., 96.8\% standard and 92.75\% robust) and CIFAR-100-C accuracies (80.6\% standard and 71.3\% robust) with better memory usage than non-compressed baselines (pretrained CARDs and CARD-Decks available at https://github.com/RobustBench/robustbench). Finally, we provide theoretical support for our empirical findings.},
	urldate = {2023-09-25},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Diffenderfer, James and Bartoldson, Brian and Chaganti, Shreya and Zhang, Jize and Kailkhura, Bhavya},
	year = {2021},
	pages = {664--676},
}

@inproceedings{xu_exploring_2022,
	title = {Exploring and {Exploiting} {Decision} {Boundary} {Dynamics} for {Adversarial} {Robustness}},
	url = {https://openreview.net/forum?id=aRTKuscKByJ},
	abstract = {The robustness of a deep classifier can be characterized by its margins: the decision boundary's distances to natural data points. However, it is unclear whether existing robust training methods effectively increase the margin for each vulnerable point during training. To understand this, we propose a continuous-time framework for quantifying the relative speed of the decision boundary with respect to each individual point. Through visualizing the moving speed of the decision boundary under Adversarial Training, one of the most effective robust training algorithms, a surprising moving-behavior is revealed: the decision boundary moves away from some vulnerable points but simultaneously moves closer to others, decreasing their margins. To alleviate these conflicting dynamics of the decision boundary, we propose Dynamics-aware Robust Training (DyART), which encourages the decision boundary to engage in movement that prioritizes increasing smaller margins. In contrast to prior works, DyART directly operates on the margins rather than their indirect approximations, allowing for more targeted and effective robustness improvement. Experiments on the CIFAR-10 and Tiny-ImageNet datasets verify that DyART alleviates the conflicting dynamics of the decision boundary and obtains improved robustness under various perturbation sizes compared to the state-of-the-art defenses. Our code is available at https://github.com/Yuancheng-Xu/Dynamics-Aware-Robust-Training.},
	language = {en},
	urldate = {2023-09-25},
	author = {Xu, Yuancheng and Sun, Yanchao and Goldblum, Micah and Goldstein, Tom and Huang, Furong},
	month = sep,
	year = {2022},
}

@inproceedings{croce_provable_2019,
	title = {Provable robustness against all adversarial \$l\_p\$-perturbations for \$p{\textbackslash}geq 1\$},
	url = {https://openreview.net/forum?id=rklk_ySYPB},
	abstract = {In recent years several adversarial attacks and defenses have been proposed. Often seemingly robust models turn out to be non-robust when more sophisticated attacks are used. One way out of this dilemma are provable robustness guarantees. While provably robust models for specific \$l\_p\$-perturbation models have been developed, we show that they do not come with any guarantee against other \$l\_q\$-perturbations. We propose a new regularization scheme, MMR-Universal, for ReLU networks which enforces robustness wrt \$l\_1\$- {\textbackslash}textit\{and\} \$l\_{\textbackslash}infty\$-perturbations and show how that leads to the first provably robust models wrt any \$l\_p\$-norm for \$p{\textbackslash}geq 1\$.},
	language = {en},
	urldate = {2023-09-24},
	author = {Croce, Francesco and Hein, Matthias},
	month = sep,
	year = {2019},
}

@inproceedings{sun_spectral_2022,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {A {Spectral} {View} of {Randomized} {Smoothing} {Under} {Common} {Corruptions}: {Benchmarking} and {Improving} {Certified} {Robustness}},
	isbn = {978-3-031-19772-7},
	shorttitle = {A {Spectral} {View} of {Randomized} {Smoothing} {Under} {Common} {Corruptions}},
	doi = {10.1007/978-3-031-19772-7_38},
	abstract = {Certified robustness guarantee gauges a model’s resistance to test-time attacks and can assess the model’s readiness for deployment in the real world. In this work, we explore a new problem setting to critically examine how the adversarial robustness guarantees change when state-of-the-art randomized smoothing-based certifications encounter common corruptions of the test data. Our analysis demonstrates a previously unknown vulnerability of these certifiably robust models to low-frequency corruptions such as weather changes, rendering these models unfit for deployment in the wild. To alleviate this issue, we propose a novel data augmentation scheme, FourierMix, that produces augmentations to improve the spectral coverage of the training data. Furthermore, we propose a new regularizer that encourages consistent predictions on noise perturbations of the augmented data to improve the quality of the smoothed models. We show that FourierMix helps eliminate the spectral bias of certifiably robust models, enabling them to achieve significantly better certified robustness on a range of corruption benchmarks. Our evaluation also uncovers the inability of current corruption benchmarks to highlight the spectral biases of the models. To this end, we propose a comprehensive benchmarking suite that contains corruptions from different regions in the spectral domain. Evaluation of models trained with popular augmentation methods on the proposed suite unveils their spectral biases. It also establishes the superiority of FourierMix trained models in achieving stronger certified robustness guarantees under corruptions over the entire frequency spectrum.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2022},
	publisher = {Springer Nature Switzerland},
	author = {Sun, Jiachen and Mehra, Akshay and Kailkhura, Bhavya and Chen, Pin-Yu and Hendrycks, Dan and Hamm, Jihun and Mao, Z. Morley},
	editor = {Avidan, Shai and Brostow, Gabriel and Cissé, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
	year = {2022},
	keywords = {Benchmark, Certified robustness, Common corruption},
	pages = {654--671},
}

@inproceedings{zhang_limitations_2019,
	title = {The {Limitations} of {Adversarial} {Training} and the {Blind}-{Spot} {Attack}},
	url = {https://openreview.net/forum?id=HylTBhA5tQ},
	abstract = {The adversarial training procedure proposed by Madry et al. (2018) is one of the most effective methods to defend against adversarial examples in deep neural net- works (DNNs). In our paper, we shed some lights on the practicality and the hardness of adversarial training by showing that the effectiveness (robustness on test set) of adversarial training has a strong correlation with the distance between a test point and the manifold of training data embedded by the network. Test examples that are relatively far away from this manifold are more likely to be vulnerable to adversarial attacks. Consequentially, an adversarial training based defense is susceptible to a new class of attacks, the “blind-spot attack”, where the input images reside in “blind-spots” (low density regions) of the empirical distri- bution of training data but is still on the ground-truth data manifold. For MNIST, we found that these blind-spots can be easily found by simply scaling and shifting image pixel values. Most importantly, for large datasets with high dimensional and complex data manifold (CIFAR, ImageNet, etc), the existence of blind-spots in adversarial training makes defending on any valid test examples difficult due to the curse of dimensionality and the scarcity of training data. Additionally, we find that blind-spots also exist on provable defenses including (Kolter \& Wong, 2018) and (Sinha et al., 2018) because these trainable robustness certificates can only be practically optimized on a limited set of training data.},
	language = {en},
	urldate = {2023-06-16},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Zhang, Huan and Chen*, Hongge and Song, Zhao and Boning, Duane and Dhillon, Inderjit S. and Hsieh, Cho-Jui},
	year = {2019},
}

@misc{yang_generalized_2022,
	title = {Generalized {Out}-of-{Distribution} {Detection}: {A} {Survey}},
	shorttitle = {Generalized {Out}-of-{Distribution} {Detection}},
	url = {http://arxiv.org/abs/2110.11334},
	doi = {10.48550/arXiv.2110.11334},
	abstract = {Out-of-distribution (OOD) detection is critical to ensuring the reliability and safety of machine learning systems. For instance, in autonomous driving, we would like the driving system to issue an alert and hand over the control to humans when it detects unusual scenes or objects that it has never seen during training time and cannot make a safe decision. The term, OOD detection, first emerged in 2017 and since then has received increasing attention from the research community, leading to a plethora of methods developed, ranging from classification-based to density-based to distance-based ones. Meanwhile, several other problems, including anomaly detection (AD), novelty detection (ND), open set recognition (OSR), and outlier detection (OD), are closely related to OOD detection in terms of motivation and methodology. Despite common goals, these topics develop in isolation, and their subtle differences in definition and problem setting often confuse readers and practitioners. In this survey, we first present a unified framework called generalized OOD detection, which encompasses the five aforementioned problems, i.e., AD, ND, OSR, OOD detection, and OD. Under our framework, these five problems can be seen as special cases or sub-tasks, and are easier to distinguish. We then review each of these five areas by summarizing their recent technical developments, with a special focus on OOD detection methodologies. We conclude this survey with open challenges and potential research directions.},
	urldate = {2023-09-24},
	publisher = {arXiv},
	author = {Yang, Jingkang and Zhou, Kaiyang and Li, Yixuan and Liu, Ziwei},
	month = aug,
	year = {2022},
	note = {arXiv:2110.11334 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{park_ai_2023,
	title = {{AI} {Deception}: {A} {Survey} of {Examples}, {Risks}, and {Potential} {Solutions}},
	shorttitle = {{AI} {Deception}},
	url = {http://arxiv.org/abs/2308.14752},
	abstract = {This paper argues that a range of current AI systems have learned how to deceive humans. We define deception as the systematic inducement of false beliefs in the pursuit of some outcome other than the truth. We first survey empirical examples of AI deception, discussing both special-use AI systems (including Meta's CICERO) built for specific competitive situations, and general-purpose AI systems (such as large language models). Next, we detail several risks from AI deception, such as fraud, election tampering, and losing control of AI systems. Finally, we outline several potential solutions to the problems posed by AI deception: first, regulatory frameworks should subject AI systems that are capable of deception to robust risk-assessment requirements; second, policymakers should implement bot-or-not laws; and finally, policymakers should prioritize the funding of relevant research, including tools to detect AI deception and to make AI systems less deceptive. Policymakers, researchers, and the broader public should work proactively to prevent AI deception from destabilizing the shared foundations of our society.},
	urldate = {2023-09-09},
	publisher = {arXiv},
	author = {Park, Peter S. and Goldstein, Simon and O'Gara, Aidan and Chen, Michael and Hendrycks, Dan},
	month = aug,
	year = {2023},
	note = {arXiv:2308.14752 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computers and Society, Computer Science - Human-Computer Interaction},
}

@misc{barrett_identifying_2023,
	title = {Identifying and {Mitigating} the {Security} {Risks} of {Generative} {AI}},
	url = {http://arxiv.org/abs/2308.14840},
	abstract = {Every major technical invention resurfaces the dual-use dilemma -- the new technology has the potential to be used for good as well as for harm. Generative AI (GenAI) techniques, such as large language models (LLMs) and diffusion models, have shown remarkable capabilities (e.g., in-context learning, code-completion, and text-to-image generation and editing). However, GenAI can be used just as well by attackers to generate new attacks and increase the velocity and efficacy of existing attacks. This paper reports the findings of a workshop held at Google (co-organized by Stanford University and the University of Wisconsin-Madison) on the dual-use dilemma posed by GenAI. This paper is not meant to be comprehensive, but is rather an attempt to synthesize some of the interesting findings from the workshop. We discuss short-term and long-term goals for the community on this topic. We hope this paper provides both a launching point for a discussion on this important topic as well as interesting problems that the research community can work to address.},
	urldate = {2023-09-09},
	publisher = {arXiv},
	author = {Barrett, Clark and Boyd, Brad and Burzstein, Ellie and Carlini, Nicholas and Chen, Brad and Choi, Jihye and Chowdhury, Amrita Roy and Christodorescu, Mihai and Datta, Anupam and Feizi, Soheil and Fisher, Kathleen and Hashimoto, Tatsunori and Hendrycks, Dan and Jha, Somesh and Kang, Daniel and Kerschbaum, Florian and Mitchell, Eric and Mitchell, John and Ramzan, Zulfikar and Shams, Khawaja and Song, Dawn and Taly, Ankur and Yang, Diyi},
	month = aug,
	year = {2023},
	note = {arXiv:2308.14840 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
}

@inproceedings{ban_pre-trained_2022,
	title = {Pre-trained {Adversarial} {Perturbations}},
	volume = {35},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/084727e8abf90a8365b940036329cb6f-Abstract-Conference.html},
	language = {en},
	urldate = {2023-09-08},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Ban, Yuanhao and Dong, Yinpeng},
	month = dec,
	year = {2022},
	pages = {1196--1209},
}

@inproceedings{dong_how_2021,
	title = {How {Should} {Pre}-{Trained} {Language} {Models} {Be} {Fine}-{Tuned} {Towards} {Adversarial} {Robustness}?},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper/2021/hash/22b1f2e0983160db6f7bb9f62f4dbb39-Abstract.html},
	abstract = {The fine-tuning of pre-trained language models has a great success in many NLP fields. Yet, it is strikingly vulnerable to adversarial examples, e.g., word substitution attacks using only synonyms can easily fool a BERT-based sentiment analysis model. In this paper, we demonstrate that adversarial training, the prevalent defense technique, does not directly fit a conventional fine-tuning scenario, because it suffers severely from catastrophic forgetting: failing to retain the generic and robust linguistic features that have already been captured by the pre-trained model. In this light, we propose Robust Informative Fine-Tuning (RIFT), a novel adversarial fine-tuning method from an information-theoretical perspective. In particular, RIFT encourages an objective model to retain the features learned from the pre-trained model throughout the entire fine-tuning process, whereas a conventional one only uses the pre-trained weights for initialization. Experimental results show that RIFT consistently outperforms the state-of-the-arts on two popular NLP tasks: sentiment analysis and natural language inference, under different attacks across various pre-trained language models.},
	urldate = {2023-09-08},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Dong, Xinshuai and Luu, Anh Tuan and Lin, Min and Yan, Shuicheng and Zhang, Hanwang},
	year = {2021},
	pages = {4356--4369},
}

@inproceedings{sriramanan_toward_2022,
	title = {Toward {Efficient} {Robust} {Training} against {Union} of \${\textbackslash}ell\_p\$ {Threat} {Models}},
	volume = {35},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/a627b9468c319c13a70b7c2fb8df65a3-Abstract-Conference.html},
	language = {en},
	urldate = {2023-09-08},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Sriramanan, Gaurang and Gor, Maharshi and Feizi, Soheil},
	month = dec,
	year = {2022},
	pages = {25870--25882},
}

@inproceedings{maus_black_2023,
	title = {Black {Box} {Adversarial} {Prompting} for {Foundation} {Models}},
	url = {https://openreview.net/forum?id=aI5QPjTRbS},
	abstract = {Prompting interfaces allow users to quickly adjust the output of generative models in both vision and language. However, small changes and design choices in the prompt can lead to significant differences in the output. In this work, we develop a black-box framework for generating adversarial prompts for unstructured image and text generation. These prompts, which can be standalone or prepended to benign prompts, induce specific behaviors into the generative process, such as generating images of a particular object or generating high perplexity text.},
	language = {en},
	urldate = {2023-09-07},
	booktitle = {The {Second} {Workshop} on {New} {Frontiers} in {Adversarial} {Machine} {Learning}},
	author = {Maus, Natalie and Chao, Patrick and Wong, Eric and Gardner, Jacob R.},
	month = aug,
	year = {2023},
}

@article{chandrasekaran_evolution_2021,
	title = {Evolution of {Semantic} {Similarity}—{A} {Survey}},
	volume = {54},
	issn = {0360-0300},
	url = {https://dl.acm.org/doi/10.1145/3440755},
	doi = {10.1145/3440755},
	abstract = {Estimating the semantic similarity between text data is one of the challenging and open research problems in the field of Natural Language Processing (NLP). The versatility of natural language makes it difficult to define rule-based methods for determining semantic similarity measures. To address this issue, various semantic similarity methods have been proposed over the years. This survey article traces the evolution of such methods beginning from traditional NLP techniques such as kernel-based methods to the most recent research work on transformer-based models, categorizing them based on their underlying principles as knowledge-based, corpus-based, deep neural network–based methods, and hybrid methods. Discussing the strengths and weaknesses of each method, this survey provides a comprehensive view of existing systems in place for new researchers to experiment and develop innovative ideas to address the issue of semantic similarity.},
	number = {2},
	urldate = {2023-09-07},
	journal = {ACM Computing Surveys},
	author = {Chandrasekaran, Dhivya and Mago, Vijay},
	month = feb,
	year = {2021},
	keywords = {Semantic similarity, corpus-based methods, knowledge-based methods, linguistics, supervised and unsupervised methods, word embeddings},
	pages = {41:1--41:37},
}

@inproceedings{ouyang_training_2022,
	title = {Training language models to follow instructions with human feedback},
	volume = {35},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/b1efde53be364a73914f58805a001731-Abstract-Conference.html},
	language = {en},
	urldate = {2023-09-07},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul F. and Leike, Jan and Lowe, Ryan},
	month = dec,
	year = {2022},
	pages = {27730--27744},
}

@article{dolatabadi_adversarial_2023,
	title = {Adversarial {Coreset} {Selection} for {Efficient} {Robust} {Training}},
	issn = {1573-1405},
	url = {https://doi.org/10.1007/s11263-023-01860-4},
	doi = {10.1007/s11263-023-01860-4},
	abstract = {It has been shown that neural networks are vulnerable to adversarial attacks: adding well-crafted, imperceptible perturbations to their input can modify their output. Adversarial training is one of the most effective approaches to training robust models against such attacks. Unfortunately, this method is much slower than vanilla training of neural networks since it needs to construct adversarial examples for the entire training data at every iteration. By leveraging the theory of coreset selection, we show how selecting a small subset of training data provides a principled approach to reducing the time complexity of robust training. To this end, we first provide convergence guarantees for adversarial coreset selection. In particular, we show that the convergence bound is directly related to how well our coresets can approximate the gradient computed over the entire training data. Motivated by our theoretical analysis, we propose using this gradient approximation error as our adversarial coreset selection objective to reduce the training set size effectively. Once built, we run adversarial training over this subset of the training data. Unlike existing methods, our approach can be adapted to a wide variety of training objectives, including TRADES, \$\${\textbackslash}ell \_p\$\$-PGD, and Perceptual Adversarial Training. We conduct extensive experiments to demonstrate that our approach speeds up adversarial training by 2–3 times while experiencing a slight degradation in the clean and robust accuracy.},
	language = {en},
	urldate = {2023-09-07},
	journal = {International Journal of Computer Vision},
	author = {Dolatabadi, Hadi M. and Erfani, Sarah M. and Leckie, Christopher},
	month = aug,
	year = {2023},
	keywords = {Adversarial training, Coreset selection, Efficient training, Image classification, Robust deep learning},
}

@inproceedings{pang_boosting_2020,
	title = {Boosting {Adversarial} {Training} with {Hypersphere} {Embedding}},
	volume = {33},
	url = {https://proceedings.neurips.cc//paper/2020/hash/5898d8095428ee310bf7fa3da1864ff7-Abstract.html},
	abstract = {Adversarial training (AT) is one of the most effective defenses against adversarial attacks for deep learning models. In this work, we advocate incorporating the hypersphere embedding (HE) mechanism into the AT procedure by regularizing the features onto compact manifolds, which constitutes a lightweight yet effective module to blend in the strength of representation learning. Our extensive analyses reveal that AT and HE are well coupled to benefit the robustness of the adversarially trained models from several aspects. We validate the effectiveness and adaptability of HE by embedding it into the popular AT frameworks including PGD-AT, ALP, and TRADES, as well as the FreeAT and FastAT strategies. In the experiments, we evaluate our methods under a wide range of adversarial attacks on the CIFAR-10 and ImageNet datasets, which verifies that integrating HE can consistently enhance the model robustness for each AT framework with little extra computation.},
	urldate = {2023-09-07},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Pang, Tianyu and Yang, Xiao and Dong, Yinpeng and Xu, Kun and Zhu, Jun and Su, Hang},
	year = {2020},
	pages = {7779--7792},
}

@misc{bai_improving_2023,
	title = {Improving the {Accuracy}-{Robustness} {Trade}-{Off} of {Classifiers} via {Adaptive} {Smoothing}},
	url = {http://arxiv.org/abs/2301.12554},
	doi = {10.48550/arXiv.2301.12554},
	abstract = {While prior research has proposed a plethora of methods that enhance the adversarial robustness of neural classifiers, practitioners are still reluctant to adopt these techniques due to their unacceptably severe penalties in clean accuracy. This paper shows that by mixing the output probabilities of a standard classifier and a robust model, where the standard network is optimized for clean accuracy and is not robust in general, this accuracy-robustness trade-off can be significantly alleviated. We show that the robust base classifier's confidence difference for correct and incorrect examples is the key ingredient of this improvement. In addition to providing intuitive and empirical evidence, we also theoretically certify the robustness of the mixed classifier under realistic assumptions. Furthermore, we adapt an adversarial input detector into a mixing network that adaptively adjusts the mixture of the two base models, further reducing the accuracy penalty of achieving robustness. The proposed flexible method, termed "adaptive smoothing", can work in conjunction with existing or even future methods that improve clean accuracy, robustness, or adversary detection. Our empirical evaluation considers strong attack methods, including AutoAttack and adaptive attack. On the CIFAR-100 dataset, our method achieves an 85.21\% clean accuracy while maintaining a 38.72\% \${\textbackslash}ell\_{\textbackslash}infty\$-AutoAttacked (\${\textbackslash}epsilon\$=8/255) accuracy, becoming the second most robust method on the RobustBench CIFAR-100 benchmark as of submission, while improving the clean accuracy by ten percentage points compared with all listed models. The code that implements our method is available at https://github.com/Bai-YT/AdaptiveSmoothing.},
	urldate = {2023-09-07},
	publisher = {arXiv},
	author = {Bai, Yatong and Anderson, Brendon G. and Kim, Aerin and Sojoudi, Somayeh},
	month = may,
	year = {2023},
	note = {arXiv:2301.12554 [cs]},
	keywords = {68T07, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@misc{jain_baseline_2023,
	title = {Baseline {Defenses} for {Adversarial} {Attacks} {Against} {Aligned} {Language} {Models}},
	url = {http://arxiv.org/abs/2309.00614},
	doi = {10.48550/arXiv.2309.00614},
	abstract = {As Large Language Models quickly become ubiquitous, it becomes critical to understand their security vulnerabilities. Recent work shows that text optimizers can produce jailbreaking prompts that bypass moderation and alignment. Drawing from the rich body of work on adversarial machine learning, we approach these attacks with three questions: What threat models are practically useful in this domain? How do baseline defense techniques perform in this new domain? How does LLM security differ from computer vision? We evaluate several baseline defense strategies against leading adversarial attacks on LLMs, discussing the various settings in which each is feasible and effective. Particularly, we look at three types of defenses: detection (perplexity based), input preprocessing (paraphrase and retokenization), and adversarial training. We discuss white-box and gray-box settings and discuss the robustness-performance trade-off for each of the defenses considered. We find that the weakness of existing discrete optimizers for text, combined with the relatively high costs of optimization, makes standard adaptive attacks more challenging for LLMs. Future research will be needed to uncover whether more powerful optimizers can be developed, or whether the strength of filtering and preprocessing defenses is greater in the LLMs domain than it has been in computer vision.},
	urldate = {2023-09-07},
	publisher = {arXiv},
	author = {Jain, Neel and Schwarzschild, Avi and Wen, Yuxin and Somepalli, Gowthami and Kirchenbauer, John and Chiang, Ping-yeh and Goldblum, Micah and Saha, Aniruddha and Geiping, Jonas and Goldstein, Tom},
	month = sep,
	year = {2023},
	note = {arXiv:2309.00614 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@misc{zhao_prompt_2023,
	title = {Prompt as {Triggers} for {Backdoor} {Attack}: {Examining} the {Vulnerability} in {Language} {Models}},
	shorttitle = {Prompt as {Triggers} for {Backdoor} {Attack}},
	url = {http://arxiv.org/abs/2305.01219},
	doi = {10.48550/arXiv.2305.01219},
	abstract = {The prompt-based learning paradigm, which bridges the gap between pre-training and fine-tuning, achieves state-of-the-art performance on several NLP tasks, particularly in few-shot settings. Despite being widely applied, prompt-based learning is vulnerable to backdoor attacks. Textual backdoor attacks are designed to introduce targeted vulnerabilities into models by poisoning a subset of training samples through trigger injection and label modification. However, they suffer from flaws such as abnormal natural language expressions resulting from the trigger and incorrect labeling of poisoned samples. In this study, we propose ProAttack, a novel and efficient method for performing clean-label backdoor attacks based on the prompt, which uses the prompt itself as a trigger. Our method does not require external triggers and ensures correct labeling of poisoned samples, improving the stealthy nature of the backdoor attack. With extensive experiments on rich-resource and few-shot text classification tasks, we empirically validate ProAttack's competitive performance in textual backdoor attacks. Notably, in the rich-resource setting, ProAttack achieves state-of-the-art attack success rates in the clean-label backdoor attack benchmark without external triggers.},
	urldate = {2023-08-24},
	publisher = {arXiv},
	author = {Zhao, Shuai and Wen, Jinming and Tuan, Luu Anh and Zhao, Junbo and Fu, Jie},
	month = may,
	year = {2023},
	note = {arXiv:2305.01219 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{xu_instructions_2023,
	title = {Instructions as {Backdoors}: {Backdoor} {Vulnerabilities} of {Instruction} {Tuning} for {Large} {Language} {Models}},
	shorttitle = {Instructions as {Backdoors}},
	url = {http://arxiv.org/abs/2305.14710},
	doi = {10.48550/arXiv.2305.14710},
	abstract = {Instruction-tuned models are trained on crowdsourcing datasets with task instructions to achieve superior performance. However, in this work we raise security concerns about this training paradigm. Our studies demonstrate that an attacker can inject backdoors by issuing very few malicious instructions among thousands of gathered data and control model behavior through data poisoning, without even the need of modifying data instances or labels themselves. Through such instruction attacks, the attacker can achieve over 90\% attack success rate across four commonly used NLP datasets, and cause persistent backdoors that are easily transferred to 15 diverse datasets zero-shot. In this way, the attacker can directly apply poisoned instructions designed for one dataset on many other datasets. Moreover, the poisoned model cannot be cured by continual learning. Lastly, instruction attacks show resistance to existing inference-time defense. These findings highlight the need for more robust defenses against data poisoning attacks in instructiontuning models and underscore the importance of ensuring data quality in instruction crowdsourcing.},
	urldate = {2023-08-24},
	publisher = {arXiv},
	author = {Xu, Jiashu and Ma, Mingyu Derek and Wang, Fei and Xiao, Chaowei and Chen, Muhao},
	month = may,
	year = {2023},
	note = {arXiv:2305.14710 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@misc{greshake_not_2023,
	title = {Not what you've signed up for: {Compromising} {Real}-{World} {LLM}-{Integrated} {Applications} with {Indirect} {Prompt} {Injection}},
	shorttitle = {Not what you've signed up for},
	url = {http://arxiv.org/abs/2302.12173},
	doi = {10.48550/arXiv.2302.12173},
	abstract = {Large Language Models (LLMs) are increasingly being integrated into various applications. The functionalities of recent LLMs can be flexibly modulated via natural language prompts. This renders them susceptible to targeted adversarial prompting, e.g., Prompt Injection (PI) attacks enable attackers to override original instructions and employed controls. So far, it was assumed that the user is directly prompting the LLM. But, what if it is not the user prompting? We argue that LLM-Integrated Applications blur the line between data and instructions. We reveal new attack vectors, using Indirect Prompt Injection, that enable adversaries to remotely (without a direct interface) exploit LLM-integrated applications by strategically injecting prompts into data likely to be retrieved. We derive a comprehensive taxonomy from a computer security perspective to systematically investigate impacts and vulnerabilities, including data theft, worming, information ecosystem contamination, and other novel security risks. We demonstrate our attacks' practical viability against both real-world systems, such as Bing's GPT-4 powered Chat and code-completion engines, and synthetic applications built on GPT-4. We show how processing retrieved prompts can act as arbitrary code execution, manipulate the application's functionality, and control how and if other APIs are called. Despite the increasing integration and reliance on LLMs, effective mitigations of these emerging threats are currently lacking. By raising awareness of these vulnerabilities and providing key insights into their implications, we aim to promote the safe and responsible deployment of these powerful models and the development of robust defenses that protect users and systems from potential attacks.},
	urldate = {2023-08-24},
	publisher = {arXiv},
	author = {Greshake, Kai and Abdelnabi, Sahar and Mishra, Shailesh and Endres, Christoph and Holz, Thorsten and Fritz, Mario},
	month = may,
	year = {2023},
	note = {arXiv:2302.12173 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computers and Society, Computer Science - Cryptography and Security},
}

@misc{li_you_2023,
	title = {Do you really follow me? {Adversarial} {Instructions} for {Evaluating} the {Robustness} of {Large} {Language} {Models}},
	shorttitle = {Do you really follow me?},
	url = {http://arxiv.org/abs/2308.10819},
	doi = {10.48550/arXiv.2308.10819},
	abstract = {Large Language Models (LLMs) have shown remarkable proficiency in following instructions, making them valuable in customer-facing applications. However, their impressive capabilities also raise concerns about the amplification of risks posed by adversarial instructions, which can be injected into the model input by third-party attackers to manipulate LLMs' original instructions and prompt unintended actions and content. Therefore, it is crucial to understand LLMs' ability to accurately discern which instructions to follow to ensure their safe deployment in real-world scenarios. In this paper, we propose a pioneering benchmark for automatically evaluating the robustness of LLMs against adversarial instructions. The objective of this benchmark is to quantify the extent to which LLMs are influenced by injected adversarial instructions and assess their ability to differentiate between these adversarial instructions and original user instructions. Through experiments conducted with state-of-the-art instruction-following LLMs, we uncover significant limitations in their robustness against adversarial instruction attacks. Furthermore, our findings indicate that prevalent instruction-tuned models are prone to being overfitted to follow any instruction phrase in the prompt without truly understanding which instructions should be followed. This highlights the need to address the challenge of training models to comprehend prompts instead of merely following instruction phrases and completing the text.},
	urldate = {2023-08-24},
	publisher = {arXiv},
	author = {Li, Zekun and Peng, Baolin and He, Pengcheng and Yan, Xifeng},
	month = aug,
	year = {2023},
	note = {arXiv:2308.10819 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{liu_agentbench_2023,
	title = {{AgentBench}: {Evaluating} {LLMs} as {Agents}},
	shorttitle = {{AgentBench}},
	url = {http://arxiv.org/abs/2308.03688},
	abstract = {Large Language Models (LLMs) are becoming increasingly smart and autonomous, targeting real-world pragmatic missions beyond traditional NLP tasks. As a result, there has been an urgent need to evaluate LLMs as agents on challenging tasks in interactive environments. We present AgentBench, a multi-dimensional evolving benchmark that currently consists of 8 distinct environments to assess LLM-as-Agent's reasoning and decision-making abilities in a multi-turn open-ended generation setting. Our extensive test over 25 LLMs (including APIs and open-sourced models) shows that, while top commercial LLMs present a strong ability of acting as agents in complex environments, there is a significant disparity in performance between them and open-sourced competitors. It also serves as a component of an ongoing project with wider coverage and deeper consideration towards systematic LLM evaluation. Datasets, environments, and an integrated evaluation package for AgentBench are released at https://github.com/THUDM/AgentBench},
	urldate = {2023-08-24},
	publisher = {arXiv},
	author = {Liu, Xiao and Yu, Hao and Zhang, Hanchen and Xu, Yifan and Lei, Xuanyu and Lai, Hanyu and Gu, Yu and Ding, Hangliang and Men, Kaiwen and Yang, Kejuan and Zhang, Shudan and Deng, Xiang and Zeng, Aohan and Du, Zhengxiao and Zhang, Chenhui and Shen, Sheng and Zhang, Tianjun and Su, Yu and Sun, Huan and Huang, Minlie and Dong, Yuxiao and Tang, Jie},
	month = aug,
	year = {2023},
	note = {arXiv:2308.03688 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{wang_survey_2023,
	title = {A {Survey} on {Large} {Language} {Model} based {Autonomous} {Agents}},
	url = {http://arxiv.org/abs/2308.11432},
	abstract = {Autonomous agents have long been a prominent research topic in the academic community. Previous research in this field often focuses on training agents with limited knowledge within isolated environments, which diverges significantly from the human learning processes, and thus makes the agents hard to achieve human-like decisions. Recently, through the acquisition of vast amounts of web knowledge, large language models (LLMs) have demonstrated remarkable potential in achieving human-level intelligence. This has sparked an upsurge in studies investigating autonomous agents based on LLMs. To harness the full potential of LLMs, researchers have devised diverse agent architectures tailored to different applications. In this paper, we present a comprehensive survey of these studies, delivering a systematic review of the field of autonomous agents from a holistic perspective. More specifically, our focus lies in the construction of LLM-based agents, for which we propose a unified framework that encompasses a majority of the previous work. Additionally, we provide a summary of the various applications of LLM-based AI agents in the domains of social science, natural science, and engineering. Lastly, we discuss the commonly employed evaluation strategies for LLM-based AI agents. Based on the previous studies, we also present several challenges and future directions in this field. To keep track of this field and continuously update our survey, we maintain a repository for the related references at https://github.com/Paitesanshi/LLM-Agent-Survey.},
	urldate = {2023-08-24},
	publisher = {arXiv},
	author = {Wang, Lei and Ma, Chen and Feng, Xueyang and Zhang, Zeyu and Yang, Hao and Zhang, Jingsen and Chen, Zhiyuan and Tang, Jiakai and Chen, Xu and Lin, Yankai and Zhao, Wayne Xin and Wei, Zhewei and Wen, Ji-Rong},
	month = aug,
	year = {2023},
	note = {arXiv:2308.11432 [cs]
version: 1},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{bagdasaryan_abusing_2023,
	title = {({Ab})using {Images} and {Sounds} for {Indirect} {Instruction} {Injection} in {Multi}-{Modal} {LLMs}},
	url = {http://arxiv.org/abs/2307.10490},
	doi = {10.48550/arXiv.2307.10490},
	abstract = {We demonstrate how images and sounds can be used for indirect prompt and instruction injection in multi-modal LLMs. An attacker generates an adversarial perturbation corresponding to the prompt and blends it into an image or audio recording. When the user asks the (unmodified, benign) model about the perturbed image or audio, the perturbation steers the model to output the attacker-chosen text and/or make the subsequent dialog follow the attacker's instruction. We illustrate this attack with several proof-of-concept examples targeting LLaVa and PandaGPT.},
	urldate = {2023-08-24},
	publisher = {arXiv},
	author = {Bagdasaryan, Eugene and Hsieh, Tsung-Yin and Nassi, Ben and Shmatikov, Vitaly},
	month = jul,
	year = {2023},
	note = {arXiv:2307.10490 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@misc{evtimov_adversarial_2021,
	title = {Adversarial {Evaluation} of {Multimodal} {Models} under {Realistic} {Gray} {Box} {Assumption}},
	url = {http://arxiv.org/abs/2011.12902},
	doi = {10.48550/arXiv.2011.12902},
	abstract = {This work examines the vulnerability of multimodal (image + text) models to adversarial threats similar to those discussed in previous literature on unimodal (image- or text-only) models. We introduce realistic assumptions of partial model knowledge and access, and discuss how these assumptions differ from the standard "black-box"/"white-box" dichotomy common in current literature on adversarial attacks. Working under various levels of these "gray-box" assumptions, we develop new attack methodologies unique to multimodal classification and evaluate them on the Hateful Memes Challenge classification task. We find that attacking multiple modalities yields stronger attacks than unimodal attacks alone (inducing errors in up to 73\% of cases), and that the unimodal image attacks on multimodal classifiers we explored were stronger than character-based text augmentation attacks (inducing errors on average in 45\% and 30\% of cases, respectively).},
	urldate = {2023-08-24},
	publisher = {arXiv},
	author = {Evtimov, Ivan and Howes, Russel and Dolhansky, Brian and Firooz, Hamed and Ferrer, Cristian Canton},
	month = jun,
	year = {2021},
	note = {arXiv:2011.12902 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security},
}

@misc{schlarmann_adversarial_2023,
	title = {On the {Adversarial} {Robustness} of {Multi}-{Modal} {Foundation} {Models}},
	url = {http://arxiv.org/abs/2308.10741},
	doi = {10.48550/arXiv.2308.10741},
	abstract = {Multi-modal foundation models combining vision and language models such as Flamingo or GPT-4 have recently gained enormous interest. Alignment of foundation models is used to prevent models from providing toxic or harmful output. While malicious users have successfully tried to jailbreak foundation models, an equally important question is if honest users could be harmed by malicious third-party content. In this paper we show that imperceivable attacks on images in order to change the caption output of a multi-modal foundation model can be used by malicious content providers to harm honest users e.g. by guiding them to malicious websites or broadcast fake information. This indicates that countermeasures to adversarial attacks should be used by any deployed multi-modal foundation model.},
	urldate = {2023-08-24},
	publisher = {arXiv},
	author = {Schlarmann, Christian and Hein, Matthias},
	month = aug,
	year = {2023},
	note = {arXiv:2308.10741 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@inproceedings{schiappa_robustness_2022,
	title = {Robustness {Analysis} of {Video}-{Language} {Models} {Against} {Visual} and {Language} {Perturbations}},
	volume = {35},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/de6ff07cbd222c10d694c2b2f732aceb-Abstract-Datasets_and_Benchmarks.html},
	language = {en},
	urldate = {2023-08-24},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Schiappa, Madeline and Vyas, Shruti and Palangi, Hamid and Rawat, Yogesh and Vineet, Vibhav},
	month = dec,
	year = {2022},
	pages = {34405--34420},
}

@misc{vishwamitra_understanding_2021,
	title = {Understanding and {Measuring} {Robustness} of {Multimodal} {Learning}},
	url = {http://arxiv.org/abs/2112.12792},
	doi = {10.48550/arXiv.2112.12792},
	abstract = {The modern digital world is increasingly becoming multimodal. Although multimodal learning has recently revolutionized the state-of-the-art performance in multimodal tasks, relatively little is known about the robustness of multimodal learning in an adversarial setting. In this paper, we introduce a comprehensive measurement of the adversarial robustness of multimodal learning by focusing on the fusion of input modalities in multimodal models, via a framework called MUROAN (MUltimodal RObustness ANalyzer). We first present a unified view of multimodal models in MUROAN and identify the fusion mechanism of multimodal models as a key vulnerability. We then introduce a new type of multimodal adversarial attacks called decoupling attack in MUROAN that aims to compromise multimodal models by decoupling their fused modalities. We leverage the decoupling attack of MUROAN to measure several state-of-the-art multimodal models and find that the multimodal fusion mechanism in all these models is vulnerable to decoupling attacks. We especially demonstrate that, in the worst case, the decoupling attack of MUROAN achieves an attack success rate of 100\% by decoupling just 1.16\% of the input space. Finally, we show that traditional adversarial training is insufficient to improve the robustness of multimodal models with respect to decoupling attacks. We hope our findings encourage researchers to pursue improving the robustness of multimodal learning.},
	urldate = {2023-08-24},
	publisher = {arXiv},
	author = {Vishwamitra, Nishant and Hu, Hongxin and Zhao, Ziming and Cheng, Long and Luo, Feng},
	month = dec,
	year = {2021},
	note = {arXiv:2112.12792 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Multimedia},
}

@article{kuang_semantically_2023,
	title = {Semantically {Consistent} {Visual} {Representation} for {Adversarial} {Robustness}},
	issn = {1556-6021},
	doi = {10.1109/TIFS.2023.3306933},
	abstract = {Deep neural networks have been widely used in various domains owing to the success of deep learning. However, recent studies have shown that these models are vulnerable to adversarial examples, leading to inaccurate predictions. In this paper, we focus on the issue of adversarial robustness by examining it through the lens of semantic information, which drives us to propose a new perspective, i.e., adversarial attacks destroy the correlation between visual representations and semantic word vectors, while adversarial training restores it. Additionally, we discover that the correlation among robust representations of different categories aligns with the correlation among the corresponding semantic word vectors. Based on these empirical observations, we incorporate the semantic information into the model training process and propose Semantic Constraint Adversarial Robust Learning (SCARL). Firstly, inspired by the information-theoretical perspective, we maximize mutual information to bridge the information gap between the visual representations and the corresponding semantic word vectors in the embedding space. We further provide a differentiable lower bound to optimize such mutual information efficiently. Secondly, we introduce a novel semantic structure constraint that maintains the structure of visual representations consistent with that of semantic word vectors. Finally, we integrate these techniques with adversarial training to learn robust visual representations. We conduct extensive experiments on several datasets (such as CIFAR and TinyImageNet) and evaluate the robustness against various adversarial attacks (such as PGD-attack and AutoAttack), demonstrating the benefits of incorporating semantic information for improving model robustness. Our code is available at https://github.com/SkyKuang/SCARL.},
	journal = {IEEE Transactions on Information Forensics and Security},
	author = {Kuang, Huafeng and Liu, Hong and Wu, Yongjian and Ji, Rongrong},
	year = {2023},
	note = {Conference Name: IEEE Transactions on Information Forensics and Security},
	keywords = {Adversarial robustness, Correlation, Representation learning, Robustness, Semantics, Task analysis, Training, Visualization, adversarial training, semantic information, word embeddings},
	pages = {1--1},
}

@inproceedings{atienza_improving_2022,
	title = {Improving {Model} {Generalization} by {Agreement} of {Learned} {Representations} {From} {Data} {Augmentation}},
	url = {https://openaccess.thecvf.com/content/WACV2022/html/Atienza_Improving_Model_Generalization_by_Agreement_of_Learned_Representations_From_Data_WACV_2022_paper.html},
	language = {en},
	urldate = {2022-10-02},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Winter} {Conference} on {Applications} of {Computer} {Vision}},
	author = {Atienza, Rowel},
	year = {2022},
	pages = {372--381},
}

@inproceedings{geiping_how_2022,
	title = {How {Much} {Data} {Are} {Augmentations} {Worth}? {An} {Investigation} into {Scaling} {Laws}, {Invariance}, and {Implicit} {Regularization}},
	shorttitle = {How {Much} {Data} {Are} {Augmentations} {Worth}?},
	url = {https://openreview.net/forum?id=3aQs3MCSexD},
	abstract = {Despite the clear performance benefits of data augmentations, little is known about why they are so effective. In this paper, we disentangle several key mechanisms through which data augmentations operate. Establishing an exchange rate between augmented and additional real data, we find that in out-of-distribution testing scenarios, augmentations which yield samples that are diverse, but inconsistent with the data distribution can be even more valuable than additional training data. Moreover, we find that data augmentations which encourage invariances can be more valuable than invariance alone, especially on small and medium sized training sets. Following this observation, we show that augmentations induce additional stochasticity during training, effectively flattening the loss landscape.},
	language = {en},
	urldate = {2023-06-30},
	booktitle = {The {Eleventh} {International} {Conference} on {Learning} {Representations}},
	author = {Geiping, Jonas and Goldblum, Micah and Somepalli, Gowthami and Shwartz-Ziv, Ravid and Goldstein, Tom and Wilson, Andrew Gordon},
	month = sep,
	year = {2022},
}

@inproceedings{kar_3d_2022,
	title = {{3D} {Common} {Corruptions} and {Data} {Augmentation}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Kar_3D_Common_Corruptions_and_Data_Augmentation_CVPR_2022_paper.html},
	language = {en},
	urldate = {2022-07-31},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Kar, Oğuzhan Fatih and Yeo, Teresa and Atanov, Andrei and Zamir, Amir},
	year = {2022},
	pages = {18963--18974},
}

@inproceedings{rommel_cadda_2021,
	title = {{CADDA}: {Class}-wise {Automatic} {Differentiable} {Data} {Augmentation} for {EEG} {Signals}},
	shorttitle = {{CADDA}},
	url = {https://openreview.net/forum?id=6IYp-35L-xJ},
	abstract = {Data augmentation is a key element of deep learning pipelines, as it informs the network during training about transformations of the input data that keep the label unchanged. Manually finding...},
	language = {en},
	urldate = {2022-04-06},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Rommel, Cédric and Moreau, Thomas and Paillard, Joseph and Gramfort, Alexandre},
	month = sep,
	year = {2021},
}

@misc{zhou_advclip_2023,
	title = {{AdvCLIP}: {Downstream}-agnostic {Adversarial} {Examples} in {Multimodal} {Contrastive} {Learning}},
	shorttitle = {{AdvCLIP}},
	url = {http://arxiv.org/abs/2308.07026},
	abstract = {Multimodal contrastive learning aims to train a general-purpose feature extractor, such as CLIP, on vast amounts of raw, unlabeled paired image-text data. This can greatly benefit various complex downstream tasks, including cross-modal image-text retrieval and image classification. Despite its promising prospect, the security issue of cross-modal pre-trained encoder has not been fully explored yet, especially when the pre-trained encoder is publicly available for commercial use. In this work, we propose AdvCLIP, the first attack framework for generating downstream-agnostic adversarial examples based on cross-modal pre-trained encoders. AdvCLIP aims to construct a universal adversarial patch for a set of natural images that can fool all the downstream tasks inheriting the victim cross-modal pre-trained encoder. To address the challenges of heterogeneity between different modalities and unknown downstream tasks, we first build a topological graph structure to capture the relevant positions between target samples and their neighbors. Then, we design a topology-deviation based generative adversarial network to generate a universal adversarial patch. By adding the patch to images, we minimize their embeddings similarity to different modality and perturb the sample distribution in the feature space, achieving unviersal non-targeted attacks. Our results demonstrate the excellent attack performance of AdvCLIP on two types of downstream tasks across eight datasets. We also tailor three popular defenses to mitigate AdvCLIP, highlighting the need for new defense mechanisms to defend cross-modal pre-trained encoders.},
	urldate = {2023-08-17},
	publisher = {arXiv},
	author = {Zhou, Ziqi and Hu, Shengshan and Li, Minghui and Zhang, Hangtao and Zhang, Yechao and Jin, Hai},
	month = aug,
	year = {2023},
	note = {arXiv:2308.07026 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{croce_mind_2021,
	title = {Mind the {Box}: \$l\_1\$-{APGD} for {Sparse} {Adversarial} {Attacks} on {Image} {Classifiers}},
	shorttitle = {Mind the {Box}},
	url = {https://proceedings.mlr.press/v139/croce21a.html},
	abstract = {We show that when taking into account also the image domain [0,1]d[0,1]d[0,1]{\textasciicircum}d, established l1l1l\_1-projected gradient descent (PGD) attacks are suboptimal as they do not consider that the effective threat model is the intersection of the l1l1l\_1-ball and [0,1]d[0,1]d[0,1]{\textasciicircum}d. We study the expected sparsity of the steepest descent step for this effective threat model and show that the exact projection onto this set is computationally feasible and yields better performance. Moreover, we propose an adaptive form of PGD which is highly effective even with a small budget of iterations. Our resulting l1l1l\_1-APGD is a strong white-box attack showing that prior works overestimated their l1l1l\_1-robustness. Using l1l1l\_1-APGD for adversarial training we get a robust classifier with SOTA l1l1l\_1-robustness. Finally, we combine l1l1l\_1-APGD and an adaptation of the Square Attack to l1l1l\_1 into l1l1l\_1-AutoAttack, an ensemble of attacks which reliably assesses adversarial robustness for the threat model of l1l1l\_1-ball intersected with [0,1]d[0,1]d[0,1]{\textasciicircum}d.},
	language = {en},
	urldate = {2023-08-14},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Croce, Francesco and Hein, Matthias},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {2201--2211},
}

@article{dong_query-efficient_2022,
	title = {Query-{Efficient} {Black}-{Box} {Adversarial} {Attacks} {Guided} by a {Transfer}-{Based} {Prior}},
	volume = {44},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2021.3126733},
	abstract = {Adversarial attacks have been extensively studied in recent years since they can identify the vulnerability of deep learning models before deployed. In this paper, we consider the black-box adversarial setting, where the adversary needs to craft adversarial examples without access to the gradients of a target model. Previous methods attempted to approximate the true gradient either by using the transfer gradient of a surrogate white-box model or based on the feedback of model queries. However, the existing methods inevitably suffer from low attack success rates or poor query efficiency since it is difficult to estimate the gradient in a high-dimensional input space with limited information. To address these problems and improve black-box attacks, we propose two prior-guided random gradient-free (PRGF) algorithms based on biased sampling and gradient averaging, respectively. Our methods can take the advantage of a transfer-based prior given by the gradient of a surrogate model and the query information simultaneously. Through theoretical analyses, the transfer-based prior is appropriately integrated with model queries by an optimal coefficient in each method. Extensive experiments demonstrate that, in comparison with the alternative state-of-the-arts, both of our methods require much fewer queries to attack black-box models with higher success rates.},
	number = {12},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Dong, Yinpeng and Cheng, Shuyu and Pang, Tianyu and Su, Hang and Zhu, Jun},
	month = dec,
	year = {2022},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Adversarial examples, Analytical models, Approximation algorithms, Deep learning, Estimation, Numerical models, Optimization, Weight measurement, black-box attacks, query efficiency, transferability, zeroth-order optimization},
	pages = {9536--9548},
}

@inproceedings{cheng_improving_2019,
	title = {Improving {Black}-box {Adversarial} {Attacks} with a {Transfer}-based {Prior}},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper/2019/hash/32508f53f24c46f685870a075eaaa29c-Abstract.html},
	abstract = {We consider the black-box adversarial setting, where the adversary has to generate adversarial perturbations without access to the target models to compute gradients. Previous methods tried to approximate the gradient either by using a transfer gradient of a surrogate white-box model, or based on the query feedback. However, these methods often suffer from low attack success rates or poor query efficiency since it is non-trivial to estimate the gradient in a high-dimensional space with limited information. To address these problems, we propose a prior-guided random gradient-free (P-RGF) method to improve black-box adversarial attacks, which takes the advantage of a transfer-based prior and the query information simultaneously. The transfer-based prior given by the gradient of a surrogate model is appropriately integrated into our algorithm by an optimal coefficient derived by a theoretical analysis. Extensive experiments demonstrate that our method requires much fewer queries to attack black-box models with higher success rates compared with the alternative state-of-the-art methods.},
	urldate = {2023-08-14},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Cheng, Shuyu and Dong, Yinpeng and Pang, Tianyu and Su, Hang and Zhu, Jun},
	year = {2019},
}

@inproceedings{xie_adversarial_2020,
	title = {Adversarial {Examples} {Improve} {Image} {Recognition}},
	abstract = {Adversarial examples are commonly viewed as a threat to ConvNets. Here we present an opposite perspective: adversarial examples can be used to improve image recognition models if harnessed in the right manner. We propose AdvProp, an enhanced adversarial training scheme which treats adversarial examples as additional examples, to prevent overﬁtting. Key to our method is the usage of a separate auxiliary batch norm for adversarial examples, as they have different underlying distributions to normal examples. We show that AdvProp improves a wide range of models on various image recognition tasks and performs better when the models are bigger. For instance, by applying AdvProp to the latest EfﬁcientNet-B7 [41] on ImageNet, we achieve signiﬁcant improvements on ImageNet (+0.7\%), ImageNet-C (+6.5\%), ImageNet-A (+7.0\%) and StylizedImageNet (+4.8\%). With an enhanced EfﬁcientNet-B8, our method achieves the state-of-the-art 85.5\% ImageNet top-1 accuracy without extra data. This result even surpasses the best model in [24] which is trained with 3.5B Instagram images (∼3000× more than ImageNet) and ∼9.4× more parameters. Models are available at https://github.com/tensorflow/tpu/tree/ master/models/official/efficientnet.},
	language = {en},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Xie, Cihang and Tan, Mingxing and Gong, Boqing and Wang, Jiang and Yuille, Alan L and Le, Quoc V},
	year = {2020},
	pages = {10},
}

@inproceedings{dai_formulating_2022,
	title = {Formulating {Robustness} {Against} {Unforeseen} {Attacks}},
	volume = {35},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/392ac56724c133c37d5ea746e52f921f-Abstract-Conference.html},
	language = {en},
	urldate = {2023-07-06},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Dai, Sihui and Mahloujifar, Saeed and Mittal, Prateek},
	month = dec,
	year = {2022},
	pages = {8647--8661},
}

@inproceedings{tramer_adversarial_2019,
	title = {Adversarial {Training} and {Robustness} for {Multiple} {Perturbations}},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper/2019/hash/5d4ae76f053f8f2516ad12961ef7fe97-Abstract.html},
	urldate = {2023-08-11},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Tramer, Florian and Boneh, Dan},
	year = {2019},
}

@inproceedings{maini_adversarial_2020,
	title = {Adversarial {Robustness} {Against} the {Union} of {Multiple} {Perturbation} {Models}},
	url = {https://proceedings.mlr.press/v119/maini20a.html},
	abstract = {Owing to the susceptibility of deep learning systems to adversarial attacks, there has been a great deal of work in developing (both empirically and certifiably) robust classifiers. While most work has defended against a single type of attack, recent work has looked at defending against multiple perturbation models using simple aggregations of multiple attacks. However, these methods can be difficult to tune, and can easily result in imbalanced degrees of robustness to individual perturbation models, resulting in a sub-optimal worst-case loss over the union. In this work, we develop a natural generalization of the standard PGD-based procedure to incorporate multiple perturbation models into a single attack, by taking the worst-case over all steepest descent directions. This approach has the advantage of directly converging upon a trade-off between different perturbation models which minimizes the worst-case performance over the union. With this approach, we are able to train standard architectures which are simultaneously robust against ℓ∞ℓ∞{\textbackslash}ell\_{\textbackslash}infty, ℓ2ℓ2{\textbackslash}ell\_2, and ℓ1ℓ1{\textbackslash}ell\_1 attacks, outperforming past approaches on the MNIST and CIFAR10 datasets and achieving adversarial accuracy of 47.0\% against the union of (ℓ∞ℓ∞{\textbackslash}ell\_{\textbackslash}infty, ℓ2ℓ2{\textbackslash}ell\_2, ℓ1ℓ1{\textbackslash}ell\_1) perturbations with radius = (0.03, 0.5, 12) on the latter, improving upon previous approaches which achieve 40.6\% accuracy.},
	language = {en},
	urldate = {2023-08-11},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Maini, Pratyush and Wong, Eric and Kolter, Zico},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {6640--6650},
}

@misc{miao_learning_2023,
	title = {Learning {Instance}-{Specific} {Augmentations} by {Capturing} {Local} {Invariances}},
	url = {http://arxiv.org/abs/2206.00051},
	doi = {10.48550/arXiv.2206.00051},
	abstract = {We introduce InstaAug, a method for automatically learning input-specific augmentations from data. Previous methods for learning augmentations have typically assumed independence between the original input and the transformation applied to that input. This can be highly restrictive, as the invariances we hope our augmentation will capture are themselves often highly input dependent. InstaAug instead introduces a learnable invariance module that maps from inputs to tailored transformation parameters, allowing local invariances to be captured. This can be simultaneously trained alongside the downstream model in a fully end-to-end manner, or separately learned for a pre-trained model. We empirically demonstrate that InstaAug learns meaningful input-dependent augmentations for a wide range of transformation classes, which in turn provides better performance on both supervised and self-supervised tasks.},
	urldate = {2023-08-09},
	publisher = {arXiv},
	author = {Miao, Ning and Rainforth, Tom and Mathieu, Emile and Dubois, Yann and Teh, Yee Whye and Foster, Adam and Kim, Hyunjik},
	month = may,
	year = {2023},
	note = {arXiv:2206.00051 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@inproceedings{tack_consistency_2022,
	title = {Consistency {Regularization} for {Adversarial} {Robustness}},
	volume = {36},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/20817},
	doi = {10.1609/aaai.v36i8.20817},
	abstract = {Adversarial training (AT) is currently one of the most successful methods to obtain the adversarial robustness of deep neural networks. However, the phenomenon of robust overfitting, i.e., the robustness starts to decrease significantly during AT, has been problematic, not only making practitioners consider a bag of tricks for a successful training, e.g., early stopping, but also incurring a significant generalization gap in the robustness. In this paper, we propose an effective regularization technique that prevents robust overfitting by optimizing an auxiliary ‘consistency’ regularization loss during AT. Specifically, we discover that data augmentation is a quite effective tool to mitigate the overfitting in AT, and develop a regularization that forces the predictive distributions after attacking from two different augmentations of the same instance to be similar with each other. Our experimental results demonstrate that such a simple regularization technique brings significant improvements in the test robust accuracy of a wide range of AT methods. More remarkably, we also show that our method could significantly help the model to generalize its robustness against unseen adversaries, e.g., other types or larger perturbations compared to those used during training. Code is available at https://github.com/alinlab/consistency-adversarial.},
	language = {en},
	urldate = {2022-08-13},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Tack, Jihoon and Yu, Sihyun and Jeong, Jongheon and Kim, Minseon and Hwang, Sung Ju and Shin, Jinwoo},
	month = jun,
	year = {2022},
	pages = {8414--8422},
}

@inproceedings{yu_mora_2022,
	title = {{MORA}: {Improving} {Ensemble} {Robustness} {Evaluation} with {Model} {Reweighing} {Attack}},
	volume = {35},
	shorttitle = {{MORA}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/ac895e51849bfc99ae25e054fd4c2eda-Abstract-Conference.html},
	language = {en},
	urldate = {2023-05-27},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Yu, Yunrui and Gao, Xitong and Xu, Cheng-Zhong},
	month = dec,
	year = {2022},
	pages = {26955--26965},
}

@inproceedings{dong_enemy_2023,
	title = {The {Enemy} of {My} {Enemy} {Is} {My} {Friend}: {Exploring} {Inverse} {Adversaries} for {Improving} {Adversarial} {Training}},
	shorttitle = {The {Enemy} of {My} {Enemy} {Is} {My} {Friend}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Dong_The_Enemy_of_My_Enemy_Is_My_Friend_Exploring_Inverse_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-06-06},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Dong, Junhao and Moosavi-Dezfooli, Seyed-Mohsen and Lai, Jianhuang and Xie, Xiaohua},
	year = {2023},
	pages = {24678--24687},
}

@article{schwinn_exploring_2023,
	title = {Exploring misclassifications of robust neural networks to enhance adversarial attacks},
	issn = {1573-7497},
	url = {https://doi.org/10.1007/s10489-023-04532-5},
	doi = {10.1007/s10489-023-04532-5},
	abstract = {Progress in making neural networks more robust against adversarial attacks is mostly marginal, despite the great efforts of the research community. Moreover, the robustness evaluation is often imprecise, making it challenging to identify promising approaches. We do an observational study on the classification decisions of 19 different state-of-the-art neural networks trained to be robust against adversarial attacks. This analysis gives a new indication of the limits of the robustness of current models on a common benchmark. In addition, our findings suggest that current untargeted adversarial attacks induce misclassification toward only a limited amount of different classes. Similarly, we find that previous attacks under-explore the perturbation space during optimization. This leads to unsuccessful attacks for samples where the initial gradient direction is not a good approximation of the final adversarial perturbation direction. Additionally, we observe that both over- and under-confidence in model predictions result in an inaccurate assessment of model robustness. Based on these observations, we propose a novel loss function for adversarial attacks that consistently improves their efficiency and success rate compared to prior attacks for all 30 analyzed models.},
	language = {en},
	urldate = {2023-08-03},
	journal = {Applied Intelligence},
	author = {Schwinn, Leo and Raab, René and Nguyen, An and Zanca, Dario and Eskofier, Bjoern},
	month = mar,
	year = {2023},
	keywords = {Adversarial attacks, Computer vision, Deep learning, Robustness},
}

@inproceedings{liu_delving_2017,
	title = {Delving into {Transferable} {Adversarial} {Examples} and {Black}-box {Attacks}},
	url = {https://openreview.net/forum?id=Sys6GJqxl},
	abstract = {An intriguing property of deep neural networks is the existence of adversarial examples, which can transfer among different architectures. These transferable adversarial examples may severely hinder deep neural network-based applications. Previous works mostly study the transferability using small scale datasets. In this work, we are the first to conduct an extensive study of the transferability over large models and a large scale dataset, and we are also the first to study the transferability of targeted adversarial examples with their target labels. We study both non-targeted and targeted adversarial examples, and show that while transferable non-targeted adversarial examples are easy to find, targeted adversarial examples generated using existing approaches almost never transfer with their target labels. Therefore, we propose novel ensemble-based approaches to generating transferable adversarial examples. Using such approaches, we observe a large proportion of targeted adversarial examples that are able to transfer with their target labels for the first time. We also present some geometric studies to help understanding the transferable adversarial examples. Finally, we show that the adversarial examples generated using ensemble-based approaches can successfully attack Clarifai.com, which is a black-box image classification system.},
	language = {en},
	urldate = {2023-06-07},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Liu, Yanpei and Chen, Xinyun and Liu, Chang and Song, Dawn},
	year = {2017},
}

@inproceedings{luo_rethinking_2023,
	title = {Rethinking the {Effect} of {Data} {Augmentation} in {Adversarial} {Contrastive} {Learning}},
	url = {https://openreview.net/forum?id=0qmwFNJyxCL},
	abstract = {Recent works have shown that self-supervised learning can achieve remarkable robustness when integrated with adversarial training (AT). However, the robustness gap between supervised AT (sup-AT) and self-supervised AT (self-AT) remains significant. Motivated by this observation, we revisit existing self-AT methods and discover an inherent dilemma that affects self-AT robustness: either strong or weak data augmentations are harmful to self-AT, and a medium strength is insufficient to bridge the gap. To resolve this dilemma, we propose a simple remedy named DYNACL (Dynamic Adversarial Contrastive Learning). In particular, we propose an augmentation schedule that gradually anneals from a strong augmentation to a weak one to benefit from both extreme cases. Besides, we adopt a fast post-processing stage for adapting it to downstream tasks. Through extensive experiments, we show that DYNACL can improve state-of-the-art self-AT robustness by 8.84\% under Auto-Attack on the CIFAR-10 dataset, and can even outperform vanilla supervised adversarial training for the first time. Our code is available at {\textbackslash}url\{https://github.com/PKU-ML/DYNACL\}.},
	language = {en},
	urldate = {2023-03-12},
	booktitle = {The {Eleventh} {International} {Conference} on {Learning} {Representations}},
	author = {Luo, Rundong and Wang, Yifei and Wang, Yisen},
	month = feb,
	year = {2023},
}

@inproceedings{liu_towards_2018,
	title = {Towards {Robust} {Neural} {Networks} via {Random} {Self}-ensemble},
	url = {https://openaccess.thecvf.com/content_ECCV_2018/html/Xuanqing_Liu_Towards_Robust_Neural_ECCV_2018_paper.html},
	urldate = {2023-07-30},
	booktitle = {Proceedings of the {European} {Conference} on {Computer} {Vision} ({ECCV})},
	author = {Liu, Xuanqing and Cheng, Minhao and Zhang, Huan and Hsieh, Cho-Jui},
	year = {2018},
	pages = {369--385},
}

@inproceedings{petrov_certifying_2023,
	title = {Certifying {Ensembles}: {A} {General} {Certification} {Theory} with {S}-{Lipschitzness}},
	shorttitle = {Certifying {Ensembles}},
	url = {https://openreview.net/forum?id=zv7X5ybgSQ},
	abstract = {Improving and guaranteeing the robustness of deep learning models has been a topic of intense research. Ensembling, which combines several classifiers to provide a better model, has been shown to be beneficial for generalisation, uncertainty estimation, calibration, and mitigating the effects of concept drift. However, the impact of ensembling on certified robustness is less well understood. In this work, we generalise Lipschitz continuity by introducing S-Lipschitz classifiers, which we use to analyse the theoretical robustness of ensembles. Our results are precise conditions when ensembles of robust classifiers are more robust than any constituent classifier, as well as conditions when they are less robust.},
	language = {en},
	urldate = {2023-07-30},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Petrov, Aleksandar and Eiras, Francisco and Sanyal, Amartya and Torr, Philip and Bibi, Adel},
	month = jun,
	year = {2023},
}

@inproceedings{xie_mitigating_2018,
	title = {Mitigating {Adversarial} {Effects} {Through} {Randomization}},
	url = {https://openreview.net/forum?id=Sk9yuql0Z},
	abstract = {Convolutional neural networks have demonstrated high accuracy on various tasks in recent years. However, they are extremely vulnerable to adversarial examples. For example, imperceptible perturbations added to clean images can cause convolutional neural networks to fail. In this paper, we propose to utilize randomization at inference time to mitigate adversarial effects. Specifically, we use two randomization operations: random resizing, which resizes the input images to a random size, and random padding, which pads zeros around the input images in a random manner. Extensive experiments demonstrate that the proposed randomization method is very effective at defending against both single-step and iterative attacks. Our method provides the following advantages: 1) no additional training or fine-tuning, 2) very few additional computations, 3) compatible with other adversarial defense methods. By combining the proposed randomization method with an adversarially trained model, it achieves a normalized score of 0.924 (ranked No.2 among 107 defense teams) in the NIPS 2017 adversarial examples defense challenge, which is far better than using adversarial training alone with a normalized score of 0.773 (ranked No.56). The code is public available at https://github.com/cihangxie/NIPS2017\_adv\_challenge\_defense.},
	language = {en},
	urldate = {2023-07-30},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Xie, Cihang and Wang, Jianyu and Zhang, Zhishuai and Ren, Zhou and Yuille, Alan},
	month = feb,
	year = {2018},
}

@inproceedings{dhillon_stochastic_2018,
	title = {Stochastic {Activation} {Pruning} for {Robust} {Adversarial} {Defense}},
	url = {https://openreview.net/forum?id=H1uR4GZRZ},
	abstract = {Neural networks are known to be vulnerable to adversarial examples. Carefully chosen perturbations to real images, while imperceptible to humans, induce misclassification and threaten the reliability of deep learning systems in the wild. To guard against adversarial examples, we take inspiration from game theory and cast the problem as a minimax zero-sum game between the adversary and the model. In general, for such games, the optimal strategy for both players requires a stochastic policy, also known as a mixed strategy. In this light, we propose Stochastic Activation Pruning (SAP), a mixed strategy for adversarial defense. SAP prunes a random subset of activations (preferentially pruning those with smaller magnitude) and scales up the survivors to compensate. We can apply SAP to pretrained networks, including adversarially trained models, without fine-tuning, providing robustness against adversarial examples. Experiments demonstrate that SAP confers robustness against attacks, increasing accuracy and preserving calibration.},
	language = {en},
	urldate = {2023-07-30},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Dhillon, Guneet S. and Azizzadenesheli, Kamyar and Lipton, Zachary C. and Bernstein, Jeremy D. and Kossaifi, Jean and Khanna, Aran and Anandkumar, Animashree},
	month = feb,
	year = {2018},
}

@inproceedings{yang_learning_2020,
	title = {Learning {Black}-{Box} {Attackers} with {Transferable} {Priors} and {Query} {Feedback}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/90599c8fdd2f6e7a03ad173e2f535751-Abstract.html},
	abstract = {This paper addresses the challenging black-box adversarial attack problem, where only classification confidence of a victim model is available. Inspired by consistency of visual saliency between different vision models, a surrogate model is expected to improve the attack performance via transferability. By combining transferability-based and query-based black-box attack, we propose a surprisingly simple baseline approach (named SimBA++) using the surrogate model, which significantly outperforms several state-of-the-art methods. Moreover, to efficiently utilize the query feedback, we update the surrogate model in a novel learning scheme, named High-Order Gradient Approximation (HOGA). By constructing a high-order gradient computation graph, we update the surrogate model to approximate the victim model in both forward and backward pass. The SimBA++ and HOGA result in Learnable Black-Box Attack (LeBA), which surpasses previous state of the art by considerable margins: the proposed LeBA significantly reduces queries, while keeping higher attack success rates close to 100\% in extensive ImageNet experiments, including attacking vision benchmarks and defensive models. Code is open source at https://github.com/TrustworthyDL/LeBA.},
	urldate = {2023-07-30},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {YANG, Jiancheng and Jiang, Yangzhou and Huang, Xiaoyang and Ni, Bingbing and Zhao, Chenglong},
	year = {2020},
	pages = {12288--12299},
}

@inproceedings{qin_random_2021,
	title = {Random {Noise} {Defense} {Against} {Query}-{Based} {Black}-{Box} {Attacks}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper/2021/hash/3eb414bf1c2a66a09c185d60553417b8-Abstract.html},
	abstract = {The query-based black-box attacks have raised serious threats to machine learning models in many real applications. In this work, we study a lightweight defense method, dubbed Random Noise Defense (RND), which adds proper Gaussian noise to each query. We conduct the theoretical analysis about the effectiveness of RND against query-based black-box attacks and the corresponding adaptive attacks. Our theoretical results reveal that the defense performance of RND is determined by the magnitude ratio between the noise induced by RND and the noise added by the attackers for gradient estimation or local search.  The large magnitude ratio leads to the stronger defense performance of RND, and it's also critical for mitigating adaptive attacks. Based on our analysis, we further propose to combine RND with a plausible Gaussian augmentation Fine-tuning (RND-GF). It enables RND to add larger noise to each query while maintaining the clean accuracy to obtain a better trade-off between clean accuracy and defense performance. Additionally, RND can be flexibly combined with the existing defense methods to further boost the adversarial robustness, such as adversarial training (AT). Extensive experiments on CIFAR-10 and ImageNet verify our theoretical findings and the effectiveness of RND and RND-GF.},
	urldate = {2023-07-30},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Qin, Zeyu and Fan, Yanbo and Zha, Hongyuan and Wu, Baoyuan},
	year = {2021},
	pages = {7650--7663},
}

@inproceedings{ma_simulating_2021,
	title = {Simulating {Unknown} {Target} {Models} for {Query}-{Efficient} {Black}-{Box} {Attacks}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Ma_Simulating_Unknown_Target_Models_for_Query-Efficient_Black-Box_Attacks_CVPR_2021_paper.html},
	language = {en},
	urldate = {2023-07-30},
	author = {Ma, Chen and Chen, Li and Yong, Jun-Hai},
	year = {2021},
	pages = {11835--11844},
}

@inproceedings{alfarra_combating_2022,
	title = {Combating {Adversaries} with {Anti}-adversaries},
	volume = {36},
	copyright = {Copyright (c) 2022 Association for the Advancement of Artificial Intelligence},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/20545},
	doi = {10.1609/aaai.v36i6.20545},
	abstract = {Deep neural networks are vulnerable to small input perturbations known as adversarial attacks. Inspired by the fact that these adversaries are constructed by iteratively minimizing the confidence of a network for the true class label, we propose the anti-adversary layer, aimed at countering this effect. In particular, our layer generates an input perturbation in the opposite direction of the adversarial one and feeds the classifier a perturbed version of the input. Our approach is training-free and theoretically supported. We verify the effectiveness of our approach by combining our layer with both nominally and robustly trained models and conduct large-scale experiments from black-box to adaptive attacks on CIFAR10, CIFAR100, and ImageNet. Our layer significantly enhances model robustness while coming at no cost on clean accuracy.},
	language = {en},
	urldate = {2023-07-30},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Alfarra, Motasem and Perez, Juan C. and Thabet, Ali and Bibi, Adel and Torr, Philip H. S. and Ghanem, Bernard},
	month = jun,
	year = {2022},
	note = {Number: 6},
	keywords = {Computer Vision (CV)},
	pages = {5992--6000},
}

@inproceedings{buckman_thermometer_2018,
	title = {Thermometer {Encoding}: {One} {Hot} {Way} {To} {Resist} {Adversarial} {Examples}},
	shorttitle = {Thermometer {Encoding}},
	url = {https://openreview.net/forum?id=S18Su--CW},
	abstract = {It is well known that it is possible to construct "adversarial examples" for neural networks: inputs which are misclassified by the network yet indistinguishable from true data. We propose a simple modification to standard neural network architectures, thermometer encoding, which significantly increases the robustness of the network to adversarial examples. We demonstrate this robustness with experiments on the MNIST, CIFAR-10, CIFAR-100, and SVHN datasets, and show that models with thermometer-encoded inputs consistently have higher accuracy on adversarial examples, without decreasing generalization. State-of-the-art accuracy under the strongest known white-box attack was increased from 93.20\% to 94.30\% on MNIST and 50.00\% to 79.16\% on CIFAR-10. We explore the properties of these networks, providing evidence that thermometer encodings help neural networks to find more-non-linear decision boundaries.},
	language = {en},
	urldate = {2023-07-30},
	author = {Buckman, Jacob and Roy, Aurko and Raffel, Colin and Goodfellow, Ian},
	month = feb,
	year = {2018},
}

@inproceedings{na_cascade_2018,
	title = {Cascade {Adversarial} {Machine} {Learning} {Regularized} with a {Unified} {Embedding}},
	url = {https://openreview.net/forum?id=HyRVBzap-},
	abstract = {Injecting adversarial examples during training, known as adversarial training, can improve robustness against one-step attacks, but not for unknown iterative attacks. To address this challenge, we first show iteratively generated adversarial images easily transfer between networks trained with the same strategy. Inspired by this observation, we propose cascade adversarial training, which transfers the knowledge of the end results of adversarial training. We train a network from scratch by injecting iteratively generated adversarial images crafted from already defended networks in addition to one-step adversarial images from the network being trained. We also propose to utilize embedding space for both classification and low-level (pixel-level) similarity learning to ignore unknown pixel level perturbation. During training, we inject adversarial images without replacing their corresponding clean images and penalize the distance between the two embeddings (clean and adversarial). Experimental results show that cascade adversarial training together with our proposed low-level similarity learning efficiently enhances the robustness against iterative attacks, but at the expense of decreased robustness against one-step attacks. We show that combining those two techniques can also improve robustness under the worst case black box attack scenario.},
	language = {en},
	urldate = {2023-07-30},
	author = {Na, Taesik and Ko, Jong Hwan and Mukhopadhyay, Saibal},
	month = feb,
	year = {2018},
}

@inproceedings{zhang_countering_2021,
	title = {Countering {Adversarial} {Examples}: {Combining} {Input} {Transformation} and {Noisy} {Training}},
	shorttitle = {Countering {Adversarial} {Examples}},
	url = {https://openaccess.thecvf.com/content/ICCV2021W/AROW/html/Zhang_Countering_Adversarial_Examples_Combining_Input_Transformation_and_Noisy_Training_ICCVW_2021_paper.html},
	language = {en},
	urldate = {2023-07-30},
	booktitle = {Proceedings of the {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision}},
	author = {Zhang, Cheng and Gao, Pan},
	year = {2021},
	pages = {102--111},
}

@inproceedings{he_adversarial_2017,
	title = {Adversarial {Example} {Defense}: {Ensembles} of {Weak} {Defenses} are not {Strong}},
	shorttitle = {Adversarial {Example} {Defense}},
	url = {https://www.usenix.org/conference/woot17/workshop-program/presentation/he},
	language = {en},
	urldate = {2023-07-30},
	author = {He, Warren and Wei, James and Chen, Xinyun and Carlini, Nicholas and Song, Dawn},
	year = {2017},
}

@inproceedings{athalye_obfuscated_2018,
	title = {Obfuscated {Gradients} {Give} a {False} {Sense} of {Security}: {Circumventing} {Defenses} to {Adversarial} {Examples}},
	shorttitle = {Obfuscated {Gradients} {Give} a {False} {Sense} of {Security}},
	url = {https://proceedings.mlr.press/v80/athalye18a.html},
	abstract = {We identify obfuscated gradients, a kind of gradient masking, as a phenomenon that leads to a false sense of security in defenses against adversarial examples. While defenses that cause obfuscated gradients appear to defeat iterative optimization-based attacks, we find defenses relying on this effect can be circumvented. We describe characteristic behaviors of defenses exhibiting the effect, and for each of the three types of obfuscated gradients we discover, we develop attack techniques to overcome it. In a case study, examining non-certified white-box-secure defenses at ICLR 2018, we find obfuscated gradients are a common occurrence, with 7 of 9 defenses relying on obfuscated gradients. Our new attacks successfully circumvent 6 completely, and 1 partially, in the original threat model each paper considers.},
	language = {en},
	urldate = {2023-07-30},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Athalye, Anish and Carlini, Nicholas and Wagner, David},
	month = jul,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {274--283},
}

@inproceedings{guo_countering_2018,
	title = {Countering {Adversarial} {Images} using {Input} {Transformations}},
	url = {https://openreview.net/forum?id=SyJ7ClWCb},
	abstract = {This paper investigates strategies that defend against adversarial-example attacks on image-classification systems by transforming the inputs before feeding them to the system. Specifically, we study applying image transformations such as bit-depth reduction, JPEG compression, total variance minimization, and image quilting before feeding the image to a convolutional network classifier. Our experiments on ImageNet show that total variance minimization and image quilting are very effective defenses in practice, in particular, when the network is trained on transformed images. The strength of those defenses lies in their non-differentiable nature and their inherent randomness, which makes it difficult for an adversary to circumvent the defenses. Our best defense eliminates 60\% of strong gray-box and 90\% of strong black-box attacks by a variety of major attack methods.},
	language = {en},
	urldate = {2023-06-08},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Guo, Chuan and Rana, Mayank and Cisse, Moustapha and Maaten, Laurens van der},
	year = {2018},
}

@inproceedings{meng_magnet_2017,
	address = {New York, NY, USA},
	series = {{CCS} '17},
	title = {{MagNet}: {A} {Two}-{Pronged} {Defense} against {Adversarial} {Examples}},
	isbn = {978-1-4503-4946-8},
	shorttitle = {{MagNet}},
	url = {https://dl.acm.org/doi/10.1145/3133956.3134057},
	doi = {10.1145/3133956.3134057},
	abstract = {Deep learning has shown impressive performance on hard perceptual problems. However, researchers found deep learning systems to be vulnerable to small, specially crafted perturbations that are imperceptible to humans. Such perturbations cause deep learning systems to mis-classify adversarial examples, with potentially disastrous consequences where safety or security is crucial. Prior defenses against adversarial examples either targeted specific attacks or were shown to be ineffective. We propose MagNet, a framework for defending neural network classifiers against adversarial examples. MagNet neither modifies the protected classifier nor requires knowledge of the process for generating adversarial examples. MagNet includes one or more separate detector networks and a reformer network. The detector networks learn to differentiate between normal and adversarial examples by approximating the manifold of normal examples. Since they assume no specific process for generating adversarial examples, they generalize well. The reformer network moves adversarial examples towards the manifold of normal examples, which is effective for correctly classifying adversarial examples with small perturbation. We discuss the intrinsic difficulties in defending against whitebox attack and propose a mechanism to defend against graybox attack. Inspired by the use of randomness in cryptography, we use diversity to strengthen MagNet. We show empirically that MagNet is effective against the most advanced state-of-the-art attacks in blackbox and graybox scenarios without sacrificing false positive rate on normal examples.},
	urldate = {2023-07-30},
	booktitle = {Proceedings of the 2017 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {Association for Computing Machinery},
	author = {Meng, Dongyu and Chen, Hao},
	month = oct,
	year = {2017},
	keywords = {adversarial example, autoencoder, neural network},
	pages = {135--147},
}

@inproceedings{samangouei_defense-gan_2018,
	title = {Defense-{GAN}: {Protecting} {Classifiers} {Against} {Adversarial} {Attacks} {Using} {Generative} {Models}},
	shorttitle = {Defense-{GAN}},
	url = {https://openreview.net/forum?id=BkJ3ibb0-&noteId=SJwPXJaHG&ref=https://githubhelp.com},
	abstract = {In recent years, deep neural network approaches have been widely adopted for machine learning tasks, including classification. However, they were shown to be vulnerable to adversarial perturbations: carefully crafted small perturbations can cause misclassification of legitimate images. We propose Defense-GAN, a new framework leveraging the expressive capability of generative models to defend deep neural networks against such attacks. Defense-GAN is trained to model the distribution of unperturbed images. At inference time, it finds a close output to a given image which does not contain the adversarial changes. This output is then fed to the classifier. Our proposed method can be used with any classification model and does not modify the classifier structure or training procedure. It can also be used as a defense against any attack as it does not assume knowledge of the process for generating the adversarial examples. We empirically show that Defense-GAN is consistently effective against different attack methods and improves on existing defense strategies.},
	language = {en},
	urldate = {2023-07-30},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Samangouei, Pouya and Kabkab, Maya and Chellappa, Rama},
	month = feb,
	year = {2018},
}

@inproceedings{yoon_adversarial_2021,
	title = {Adversarial {Purification} with {Score}-based {Generative} {Models}},
	url = {https://proceedings.mlr.press/v139/yoon21a.html},
	abstract = {While adversarial training is considered as a standard defense method against adversarial attacks for image classifiers, adversarial purification, which purifies attacked images into clean images with a standalone purification, model has shown promises as an alternative defense method. Recently, an EBM trained with MCMC has been highlighted as a purification model, where an attacked image is purified by running a long Markov-chain using the gradients of the EBM. Yet, the practicality of the adversarial purification using an EBM remains questionable because the number of MCMC steps required for such purification is too large. In this paper, we propose a novel adversarial purification method based on an EBM trained with DSM. We show that an EBM trained with DSM can quickly purify attacked images within a few steps. We further introduce a simple yet effective randomized purification scheme that injects random noises into images before purification. This process screens the adversarial perturbations imposed on images by the random noises and brings the images to the regime where the EBM can denoise well. We show that our purification method is robust against various attacks and demonstrate its state-of-the-art performances.},
	language = {en},
	urldate = {2023-07-30},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Yoon, Jongmin and Hwang, Sung Ju and Lee, Juho},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {12062--12072},
}

@inproceedings{carlini_poisoning_2021,
	title = {Poisoning and {Backdooring} {Contrastive} {Learning}},
	url = {https://openreview.net/forum?id=iC4UHbQ01Mp},
	abstract = {Multimodal contrastive learning methods like CLIP train on noisy and uncurated training datasets. This is cheaper than labeling datasets manually, and even improves out-of-distribution robustness. We show that this practice makes backdoor and poisoning attacks a significant threat. By poisoning just 0.01\% of a dataset (e.g., just 300 images of the 3 million-example Conceptual Captions dataset), we can cause the model to misclassify test images by overlaying a small patch. Targeted poisoning attacks, whereby the model misclassifies a particular test input with an adversarially-desired label, are even easier requiring control of 0.0001\% of the dataset (e.g., just three out of the 3 million images). Our attacks call into question whether training on noisy and uncurated Internet scrapes is desirable.},
	language = {en},
	urldate = {2023-07-30},
	author = {Carlini, Nicholas and Terzis, Andreas},
	month = oct,
	year = {2021},
}

@misc{dong_survey_2023,
	title = {A {Survey} on {In}-context {Learning}},
	url = {http://arxiv.org/abs/2301.00234},
	doi = {10.48550/arXiv.2301.00234},
	abstract = {With the increasing ability of large language models (LLMs), in-context learning (ICL) has become a new paradigm for natural language processing (NLP), where LLMs make predictions only based on contexts augmented with a few examples. It has been a new trend to explore ICL to evaluate and extrapolate the ability of LLMs. In this paper, we aim to survey and summarize the progress and challenges of ICL. We first present a formal definition of ICL and clarify its correlation to related studies. Then, we organize and discuss advanced techniques, including training strategies, demonstration designing strategies, as well as related analysis. Finally, we discuss the challenges of ICL and provide potential directions for further research. We hope that our work can encourage more research on uncovering how ICL works and improving ICL.},
	urldate = {2023-07-30},
	publisher = {arXiv},
	author = {Dong, Qingxiu and Li, Lei and Dai, Damai and Zheng, Ce and Wu, Zhiyong and Chang, Baobao and Sun, Xu and Xu, Jingjing and Li, Lei and Sui, Zhifang},
	month = jun,
	year = {2023},
	note = {arXiv:2301.00234 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{liu_pre-train_2021,
	title = {Pre-train, {Prompt}, and {Predict}: {A} {Systematic} {Survey} of {Prompting} {Methods} in {Natural} {Language} {Processing}},
	shorttitle = {Pre-train, {Prompt}, and {Predict}},
	url = {http://arxiv.org/abs/2107.13586},
	doi = {10.48550/arXiv.2107.13586},
	abstract = {This paper surveys and organizes research works in a new paradigm in natural language processing, which we dub "prompt-based learning". Unlike traditional supervised learning, which trains a model to take in an input x and predict an output y as P(y{\textbar}x), prompt-based learning is based on language models that model the probability of text directly. To use these models to perform prediction tasks, the original input x is modified using a template into a textual string prompt x' that has some unfilled slots, and then the language model is used to probabilistically fill the unfilled information to obtain a final string x, from which the final output y can be derived. This framework is powerful and attractive for a number of reasons: it allows the language model to be pre-trained on massive amounts of raw text, and by defining a new prompting function the model is able to perform few-shot or even zero-shot learning, adapting to new scenarios with few or no labeled data. In this paper we introduce the basics of this promising paradigm, describe a unified set of mathematical notations that can cover a wide variety of existing work, and organize existing work along several dimensions, e.g.the choice of pre-trained models, prompts, and tuning strategies. To make the field more accessible to interested beginners, we not only make a systematic review of existing works and a highly structured typology of prompt-based concepts, but also release other resources, e.g., a website http://pretrain.nlpedia.ai/ including constantly-updated survey, and paperlist.},
	urldate = {2023-07-29},
	publisher = {arXiv},
	author = {Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao and Hayashi, Hiroaki and Neubig, Graham},
	month = jul,
	year = {2021},
	note = {arXiv:2107.13586 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{gu_systematic_2023,
	title = {A {Systematic} {Survey} of {Prompt} {Engineering} on {Vision}-{Language} {Foundation} {Models}},
	url = {http://arxiv.org/abs/2307.12980},
	doi = {10.48550/arXiv.2307.12980},
	abstract = {Prompt engineering is a technique that involves augmenting a large pre-trained model with task-specific hints, known as prompts, to adapt the model to new tasks. Prompts can be created manually as natural language instructions or generated automatically as either natural language instructions or vector representations. Prompt engineering enables the ability to perform predictions based solely on prompts without updating model parameters, and the easier application of large pre-trained models in real-world tasks. In past years, Prompt engineering has been well-studied in natural language processing. Recently, it has also been intensively studied in vision-language modeling. However, there is currently a lack of a systematic overview of prompt engineering on pre-trained vision-language models. This paper aims to provide a comprehensive survey of cutting-edge research in prompt engineering on three types of vision-language models: multimodal-to-text generation models (e.g. Flamingo), image-text matching models (e.g. CLIP), and text-to-image generation models (e.g. Stable Diffusion). For each type of model, a brief model summary, prompting methods, prompting-based applications, and the corresponding responsibility and integrity issues are summarized and discussed. Furthermore, the commonalities and differences between prompting on vision-language models, language models, and vision models are also discussed. The challenges, future directions, and research opportunities are summarized to foster future research on this topic.},
	urldate = {2023-07-29},
	publisher = {arXiv},
	author = {Gu, Jindong and Han, Zhen and Chen, Shuo and Beirami, Ahmad and He, Bailan and Zhang, Gengyuan and Liao, Ruotong and Qin, Yao and Tresp, Volker and Torr, Philip},
	month = jul,
	year = {2023},
	note = {arXiv:2307.12980 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{croce_sparse-rs_2022,
	title = {Sparse-{RS}: {A} {Versatile} {Framework} for {Query}-{Efficient} {Sparse} {Black}-{Box} {Adversarial} {Attacks}},
	volume = {36},
	copyright = {Copyright (c) 2022 Association for the Advancement of Artificial Intelligence},
	shorttitle = {Sparse-{RS}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/20595},
	doi = {10.1609/aaai.v36i6.20595},
	abstract = {We propose a versatile framework based on random search, Sparse-RS, for score-based sparse targeted and untargeted attacks in the black-box setting. Sparse-RS does not rely on substitute models and achieves state-of-the-art success rate and query efficiency for multiple sparse attack models: L0-bounded perturbations, adversarial patches, and adversarial frames. The L0-version of untargeted Sparse-RS outperforms all black-box and even all white-box attacks for different models on MNIST, CIFAR-10, and ImageNet. Moreover, our untargeted Sparse-RS achieves very high success rates even for the challenging settings of 20x20 adversarial patches and 2-pixel wide adversarial frames for 224x224 images. Finally, we show that Sparse-RS can be applied to generate targeted universal adversarial patches where it significantly outperforms the existing approaches. Our code is available at https://github.com/fra31/sparse-rs.},
	language = {en},
	urldate = {2023-07-28},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Croce, Francesco and Andriushchenko, Maksym and Singh, Naman D. and Flammarion, Nicolas and Hein, Matthias},
	month = jun,
	year = {2022},
	note = {Number: 6},
	keywords = {Computer Vision (CV)},
	pages = {6437--6445},
}

@misc{hayes_learning_2018,
	title = {Learning {Universal} {Adversarial} {Perturbations} with {Generative} {Models}},
	url = {http://arxiv.org/abs/1708.05207},
	abstract = {Neural networks are known to be vulnerable to adversarial examples, inputs that have been intentionally perturbed to remain visually similar to the source input, but cause a misclassification. It was recently shown that given a dataset and classifier, there exists so called universal adversarial perturbations, a single perturbation that causes a misclassification when applied to any input. In this work, we introduce universal adversarial networks, a generative network that is capable of fooling a target classifier when it's generated output is added to a clean sample from a dataset. We show that this technique improves on known universal adversarial attacks.},
	urldate = {2023-07-28},
	publisher = {arXiv},
	author = {Hayes, Jamie and Danezis, George},
	month = jan,
	year = {2018},
	note = {arXiv:1708.05207 [cs, stat]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{milliere_adversarial_2022,
	title = {Adversarial {Attacks} on {Image} {Generation} {With} {Made}-{Up} {Words}},
	url = {http://arxiv.org/abs/2208.04135},
	doi = {10.48550/arXiv.2208.04135},
	abstract = {Text-guided image generation models can be prompted to generate images using nonce words adversarially designed to robustly evoke specific visual concepts. Two approaches for such generation are introduced: macaronic prompting, which involves designing cryptic hybrid words by concatenating subword units from different languages; and evocative prompting, which involves designing nonce words whose broad morphological features are similar enough to that of existing words to trigger robust visual associations. The two methods can also be combined to generate images associated with more specific visual concepts. The implications of these techniques for the circumvention of existing approaches to content moderation, and particularly the generation of offensive or harmful images, are discussed.},
	urldate = {2023-07-27},
	publisher = {arXiv},
	author = {Millière, Raphaël},
	month = aug,
	year = {2022},
	note = {arXiv:2208.04135 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@misc{maus_black_2023,
	title = {Black {Box} {Adversarial} {Prompting} for {Foundation} {Models}},
	url = {http://arxiv.org/abs/2302.04237},
	doi = {10.48550/arXiv.2302.04237},
	abstract = {Prompting interfaces allow users to quickly adjust the output of generative models in both vision and language. However, small changes and design choices in the prompt can lead to significant differences in the output. In this work, we develop a black-box framework for generating adversarial prompts for unstructured image and text generation. These prompts, which can be standalone or prepended to benign prompts, induce specific behaviors into the generative process, such as generating images of a particular object or generating high perplexity text.},
	urldate = {2023-07-27},
	publisher = {arXiv},
	author = {Maus, Natalie and Chao, Patrick and Wong, Eric and Gardner, Jacob},
	month = may,
	year = {2023},
	note = {arXiv:2302.04237 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{wang_investigating_2023,
	title = {Investigating the {Existence} of "{Secret} {Language}'' in {Language} {Models}},
	url = {http://arxiv.org/abs/2307.12507},
	doi = {10.48550/arXiv.2307.12507},
	abstract = {In this paper, we study the problem of secret language in NLP, where current language models (LMs) seem to have a hidden vocabulary that allows them to interpret absurd inputs as meaningful concepts. We investigate two research questions: ``Does the secret language phenomenon exist in different language models?'' and ``Does secret language depend on specific context?'' To answer these questions, we introduce a novel method named {\textbackslash}textit\{SecretFinding\}, a gradient-based approach that can automatically discover secret languages in LMs. We conduct experiments on five representative models (Electra, ALBERT, Roberta, DistillBERT, and CLIP) finetuned on four NLP benchmarks (SST-2, MRPC, SNLI, and SQuAD) and a language-grounding benchmark (MSCOCO). Our experimental results show that even when we replace the most important words with others that are semantically dissimilar to the original words in a sentence, LMs do not consider the new sentence semantically dissimilar to the original, as the output does not change with a high probability. This phenomenon holds true across the five models and five tasks and gives a positive answer to the first research question. As for the second research question, we find that the secret language discovered by {\textbackslash}textit\{SecretFinding\} is quite general and could even be transferred to other models in the black-box settings, such as GPT-3 and ChatGPT. Finally, we discuss the causes of secret language, how to eliminate it, the potential connection to memorization, and ethical implications. Examples of secret language found by SecretFinding are available on https://huggingface.co/spaces/anonymousauthors/ACL23\_SecretLanguage.},
	urldate = {2023-07-27},
	publisher = {arXiv},
	author = {Wang, Yimu and Shi, Peng and Zhang, Hongyang},
	month = jul,
	year = {2023},
	note = {arXiv:2307.12507 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computers and Society, Computer Science - Cryptography and Security},
}

@misc{daras_discovering_2022,
	title = {Discovering the {Hidden} {Vocabulary} of {DALLE}-2},
	url = {http://arxiv.org/abs/2206.00169},
	doi = {10.48550/arXiv.2206.00169},
	abstract = {We discover that DALLE-2 seems to have a hidden vocabulary that can be used to generate images with absurd prompts. For example, it seems that {\textbackslash}texttt\{Apoploe vesrreaitais\} means birds and {\textbackslash}texttt\{Contarra ccetnxniams luryca tanniounons\} (sometimes) means bugs or pests. We find that these prompts are often consistent in isolation but also sometimes in combinations. We present our black-box method to discover words that seem random but have some correspondence to visual concepts. This creates important security and interpretability challenges.},
	urldate = {2023-07-27},
	publisher = {arXiv},
	author = {Daras, Giannis and Dimakis, Alexandros G.},
	month = may,
	year = {2022},
	note = {arXiv:2206.00169 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@inproceedings{wei_chain--thought_2022,
	title = {Chain-of-{Thought} {Prompting} {Elicits} {Reasoning} in {Large} {Language} {Models}},
	volume = {35},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html},
	language = {en},
	urldate = {2023-07-27},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc V. and Zhou, Denny},
	month = dec,
	year = {2022},
	pages = {24824--24837},
}

@inproceedings{kojima_large_2022,
	title = {Large {Language} {Models} are {Zero}-{Shot} {Reasoners}},
	volume = {35},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/8bb0d291acd4acf06ef112099c16f326-Abstract-Conference.html},
	language = {en},
	urldate = {2023-07-27},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Kojima, Takeshi and Gu, Shixiang (Shane) and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
	month = dec,
	year = {2022},
	pages = {22199--22213},
}

@inproceedings{brooks_instructpix2pix_2023,
	title = {{InstructPix2Pix}: {Learning} {To} {Follow} {Image} {Editing} {Instructions}},
	shorttitle = {{InstructPix2Pix}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Brooks_InstructPix2Pix_Learning_To_Follow_Image_Editing_Instructions_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-07-26},
	author = {Brooks, Tim and Holynski, Aleksander and Efros, Alexei A.},
	year = {2023},
	pages = {18392--18402},
}

@inproceedings{kar_3d_2022-1,
	title = {{3D} {Common} {Corruptions} and {Data} {Augmentation}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Kar_3D_Common_Corruptions_and_Data_Augmentation_CVPR_2022_paper.html},
	language = {en},
	urldate = {2023-07-23},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Kar, Oğuzhan Fatih and Yeo, Teresa and Atanov, Andrei and Zamir, Amir},
	year = {2022},
	pages = {18963--18974},
}

@misc{prabhu_lance_2023,
	title = {{LANCE}: {Stress}-testing {Visual} {Models} by {Generating} {Language}-guided {Counterfactual} {Images}},
	shorttitle = {{LANCE}},
	url = {http://arxiv.org/abs/2305.19164},
	abstract = {We propose an automated algorithm to stress-test a trained visual model by generating language-guided counterfactual test images (LANCE). Our method leverages recent progress in large language modeling and text-based image editing to augment an IID test set with a suite of diverse, realistic, and challenging test images without altering model weights. We benchmark the performance of a diverse set of pretrained models on our generated data and observe significant and consistent performance drops. We further analyze model sensitivity across different types of edits, and demonstrate its applicability at surfacing previously unknown class-level model biases in ImageNet.},
	urldate = {2023-07-23},
	publisher = {arXiv},
	author = {Prabhu, Viraj and Yenamandra, Sriram and Chattopadhyay, Prithvijit and Hoffman, Judy},
	month = may,
	year = {2023},
	note = {arXiv:2305.19164 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@inproceedings{li_imagenet-e_2023,
	title = {{ImageNet}-{E}: {Benchmarking} {Neural} {Network} {Robustness} via {Attribute} {Editing}},
	shorttitle = {{ImageNet}-{E}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Li_ImageNet-E_Benchmarking_Neural_Network_Robustness_via_Attribute_Editing_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-07-23},
	author = {Li, Xiaodan and Chen, Yuefeng and Zhu, Yao and Wang, Shuhui and Zhang, Rong and Xue, Hui},
	year = {2023},
	pages = {20371--20381},
}

@article{croce_scaling_2020,
	title = {Scaling up the {Randomized} {Gradient}-{Free} {Adversarial} {Attack} {Reveals} {Overestimation} of {Robustness} {Using} {Established} {Attacks}},
	volume = {128},
	issn = {1573-1405},
	url = {https://doi.org/10.1007/s11263-019-01213-0},
	doi = {10.1007/s11263-019-01213-0},
	abstract = {Modern neural networks are highly non-robust against adversarial manipulation. A significant amount of work has been invested in techniques to compute lower bounds on robustness through formal guarantees and to build provably robust models. However, it is still difficult to get guarantees for larger networks or robustness against larger perturbations. Thus attack strategies are needed to provide tight upper bounds on the actual robustness. We significantly improve the randomized gradient-free attack for ReLU networks (Croce and Hein in GCPR, 2018), in particular by scaling it up to large networks. We show that our attack achieves similar or significantly smaller robust accuracy than state-of-the-art attacks like PGD or the one of Carlini and Wagner, thus revealing an overestimation of the robustness by these state-of-the-art methods. Our attack is not based on a gradient descent scheme and in this sense gradient-free, which makes it less sensitive to the choice of hyperparameters as no careful selection of the stepsize is required.},
	language = {en},
	number = {4},
	urldate = {2023-07-22},
	journal = {International Journal of Computer Vision},
	author = {Croce, Francesco and Rauber, Jonas and Hein, Matthias},
	month = apr,
	year = {2020},
	keywords = {Adversarial attacks, Adversarial robustness, Gradient-free attacks, White-box attacks},
	pages = {1028--1046},
}

@inproceedings{katz_reluplex_2017,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Reluplex: {An} {Efficient} {SMT} {Solver} for {Verifying} {Deep} {Neural} {Networks}},
	isbn = {978-3-319-63387-9},
	shorttitle = {Reluplex},
	doi = {10.1007/978-3-319-63387-9_5},
	abstract = {Deep neural networks have emerged as a widely used and effective means for tackling complex, real-world problems. However, a major obstacle in applying them to safety-critical systems is the great difficulty in providing formal guarantees about their behavior. We present a novel, scalable, and efficient technique for verifying properties of deep neural networks (or providing counter-examples). The technique is based on the simplex method, extended to handle the non-convex Rectified Linear Unit (ReLU) activation function, which is a crucial ingredient in many modern neural networks. The verification procedure tackles neural networks as a whole, without making any simplifying assumptions. We evaluated our technique on a prototype deep neural network implementation of the next-generation airborne collision avoidance system for unmanned aircraft (ACAS Xu). Results show that our technique can successfully prove properties of networks that are an order of magnitude larger than the largest networks verified using existing methods.},
	language = {en},
	booktitle = {Computer {Aided} {Verification}},
	publisher = {Springer International Publishing},
	author = {Katz, Guy and Barrett, Clark and Dill, David L. and Julian, Kyle and Kochenderfer, Mykel J.},
	editor = {Majumdar, Rupak and Kunčak, Viktor},
	year = {2017},
	keywords = {Airborne Collision Avoidance System, Deep Neural Networks (DNNs), ReLU Function, Rectified Linear Unit (ReLU), Satisfiability Modulo Theories (SMT)},
	pages = {97--117},
}

@inproceedings{yuan_adaptive_2022,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Adaptive {Image} {Transformations} for {Transfer}-{Based} {Adversarial} {Attack}},
	isbn = {978-3-031-20065-6},
	doi = {10.1007/978-3-031-20065-6_1},
	abstract = {Adversarial attacks provide a good way to study the robustness of deep learning models. One category of methods in transfer-based black-box attack utilizes several image transformation operations to improve the transferability of adversarial examples, which is effective, but fails to take the specific characteristic of the input image into consideration. In this work, we propose a novel architecture, called Adaptive Image Transformation Learner (AITL), which incorporates different image transformation operations into a unified framework to further improve the transferability of adversarial examples. Unlike the fixed combinational transformations used in existing works, our elaborately designed transformation learner adaptively selects the most effective combination of image transformations specific to the input image. Extensive experiments on ImageNet demonstrate that our method significantly improves the attack success rates on both normally trained models and defense models under various settings.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2022},
	publisher = {Springer Nature Switzerland},
	author = {Yuan, Zheng and Zhang, Jie and Shan, Shiguang},
	editor = {Avidan, Shai and Brostow, Gabriel and Cissé, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
	year = {2022},
	keywords = {Adaptive image transformation, Adversarial attack, Transfer-based attack},
	pages = {1--17},
}

@misc{brown_language_2020,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	url = {http://arxiv.org/abs/2005.14165},
	abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
	urldate = {2023-07-22},
	publisher = {arXiv},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	month = jul,
	year = {2020},
	note = {arXiv:2005.14165 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{khattak_maple_2023,
	title = {{MaPLe}: {Multi}-{Modal} {Prompt} {Learning}},
	shorttitle = {{MaPLe}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Khattak_MaPLe_Multi-Modal_Prompt_Learning_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-07-21},
	author = {Khattak, Muhammad Uzair and Rasheed, Hanoona and Maaz, Muhammad and Khan, Salman and Khan, Fahad Shahbaz},
	year = {2023},
	pages = {19113--19122},
}

@misc{gu_towards_2023,
	title = {Towards {Robust} {Prompts} on {Vision}-{Language} {Models}},
	url = {http://arxiv.org/abs/2304.08479},
	doi = {10.48550/arXiv.2304.08479},
	abstract = {With the advent of vision-language models (VLMs) that can perform in-context and prompt-based learning, how can we design prompting approaches that robustly generalize to distribution shift and can be used on novel classes outside the support set of the prompts? In this work, we first define two types of robustness to distribution shift on VLMs, namely, robustness on base classes (the classes included in the support set of prompts) and robustness on novel classes. Then, we study the robustness of existing in-context learning and prompt learning approaches, where we find that prompt learning performs robustly on test images from base classes, while it does not generalize well on images from novel classes. We propose robust prompt learning by integrating multiple-scale image features into the prompt, which improves both types of robustness. Comprehensive experiments are conducted to study the defined robustness on six benchmarks and show the effectiveness of our proposal.},
	urldate = {2023-07-21},
	publisher = {arXiv},
	author = {Gu, Jindong and Beirami, Ahmad and Wang, Xuezhi and Beutel, Alex and Torr, Philip and Qin, Yao},
	month = apr,
	year = {2023},
	note = {arXiv:2304.08479 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{lu_prompt_2022,
	title = {Prompt {Distribution} {Learning}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Lu_Prompt_Distribution_Learning_CVPR_2022_paper.html},
	language = {en},
	urldate = {2023-07-21},
	author = {Lu, Yuning and Liu, Jianzhuang and Zhang, Yonggang and Liu, Yajing and Tian, Xinmei},
	year = {2022},
	pages = {5206--5215},
}

@misc{huang_unsupervised_2022,
	title = {Unsupervised {Prompt} {Learning} for {Vision}-{Language} {Models}},
	url = {http://arxiv.org/abs/2204.03649},
	doi = {10.48550/arXiv.2204.03649},
	abstract = {Contrastive vision-language models like CLIP have shown great progress in transfer learning. In the inference stage, the proper text description, also known as prompt, needs to be carefully designed to correctly classify the given images. In order to avoid laborious prompt engineering, recent works such as CoOp, CLIP-Adapter and Tip-Adapter propose to adapt vision-language models for downstream image recognition tasks on a small set of labeled data. Though promising improvements are achieved, requiring labeled data from the target datasets may restrict the scalability. In this paper, we explore a different scenario, in which the labels of the target datasets are unprovided, and we present an unsupervised prompt learning (UPL) approach to avoid prompt engineering while simultaneously improving transfer performance of CLIP-like vision-language models. As far as we know, UPL is the first work to introduce unsupervised learning into prompt learning. Experimentally, our UPL outperforms original CLIP with prompt engineering on ImageNet as well as other 10 datasets. An enhanced version of UPL is even competitive with the 8-shot CoOp and the 8-shot TIP-Adapter on most datasets. Code and models are available at https://github.com/tonyhuang2022/UPL.},
	urldate = {2023-07-21},
	publisher = {arXiv},
	author = {Huang, Tony and Chu, Jack and Wei, Fangyun},
	month = aug,
	year = {2022},
	note = {arXiv:2204.03649 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{lee_multimodal_2023,
	title = {Multimodal {Prompting} {With} {Missing} {Modalities} for {Visual} {Recognition}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Lee_Multimodal_Prompting_With_Missing_Modalities_for_Visual_Recognition_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-07-21},
	author = {Lee, Yi-Lun and Tsai, Yi-Hsuan and Chiu, Wei-Chen and Lee, Chen-Yu},
	year = {2023},
	pages = {14943--14952},
}

@inproceedings{yang_prompt_2023,
	address = {Toronto, Canada},
	title = {Prompt {Tuning} for {Unified} {Multimodal} {Pretrained} {Models}},
	url = {https://aclanthology.org/2023.findings-acl.27},
	abstract = {Prompt tuning has become a new paradigm for model tuning and it has demonstrated success in natural language pretraining and even vision pretraining. The parameter-efficient prompt tuning methods that optimize soft embeddings while keeping the pretrained model frozen demonstrate advantages in low computation costs and almost lossless performance. In this work, we explore the transfer of prompt tuning to multimodal pretrained models. Specifically, we implement prompt tuning to a unified sequence-to-sequence pretrained model by adding a sequence of learnable embeddings to each layer and finetuning the pretrained model on downstream task with only the learnable embeddings being optimized. Experimental results on a series of multimodal understanding and generation tasks demonstrate that our method OFA-PT can achieve comparable performance with finetuning across a series of multimodal generation and understanding tasks. Additionally, it significantly outperforms the unified multimodal pretrained model with other parameter-efficient tuning methods, e.g., Adapter, BitFit. etc. Besides, in comparison with finetuned models, the prompt-tuned models demonstrate improved robustness against adversarial attacks. We further figure out that experimental factors, including prompt length, prompt depth, and reparameteratization, have great impacts on the model performance, and thus we empirically provide a recommendation for the setups of prompt tuning.},
	urldate = {2023-07-21},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {ACL} 2023},
	publisher = {Association for Computational Linguistics},
	author = {Yang, Hao and Lin, Junyang and Yang, An and Wang, Peng and Zhou, Chang},
	month = jul,
	year = {2023},
	pages = {402--416},
}

@misc{yang_prompt_2022,
	title = {Prompt {Tuning} for {Generative} {Multimodal} {Pretrained} {Models}},
	url = {http://arxiv.org/abs/2208.02532},
	doi = {10.48550/arXiv.2208.02532},
	abstract = {Prompt tuning has become a new paradigm for model tuning and it has demonstrated success in natural language pretraining and even vision pretraining. In this work, we explore the transfer of prompt tuning to multimodal pretraining, with a focus on generative multimodal pretrained models, instead of contrastive ones. Specifically, we implement prompt tuning on the unified sequence-to-sequence pretrained model adaptive to both understanding and generation tasks. Experimental results demonstrate that the light-weight prompt tuning can achieve comparable performance with finetuning and surpass other light-weight tuning methods. Besides, in comparison with finetuned models, the prompt-tuned models demonstrate improved robustness against adversarial attacks. We further figure out that experimental factors, including the prompt length, prompt depth, and reparameteratization, have great impacts on the model performance, and thus we empirically provide a recommendation for the setups of prompt tuning. Despite the observed advantages, we still find some limitations in prompt tuning, and we correspondingly point out the directions for future studies. Codes are available at {\textbackslash}url\{https://github.com/OFA-Sys/OFA\}},
	urldate = {2023-07-21},
	publisher = {arXiv},
	author = {Yang, Hao and Lin, Junyang and Yang, An and Wang, Peng and Zhou, Chang and Yang, Hongxia},
	month = aug,
	year = {2022},
	note = {arXiv:2208.02532 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{zhuang_pilot_2023,
	title = {A {Pilot} {Study} of {Query}-{Free} {Adversarial} {Attack} {Against} {Stable} {Diffusion}},
	url = {https://openaccess.thecvf.com/content/CVPR2023W/AML/html/Zhuang_A_Pilot_Study_of_Query-Free_Adversarial_Attack_Against_Stable_Diffusion_CVPRW_2023_paper.html},
	language = {en},
	urldate = {2023-07-21},
	author = {Zhuang, Haomin and Zhang, Yihua and Liu, Sijia},
	year = {2023},
	pages = {2384--2391},
}

@inproceedings{parisot_learning_2023,
	title = {Learning {To} {Name} {Classes} for {Vision} and {Language} {Models}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Parisot_Learning_To_Name_Classes_for_Vision_and_Language_Models_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-07-21},
	author = {Parisot, Sarah and Yang, Yongxin and McDonagh, Steven},
	year = {2023},
	pages = {23477--23486},
}

@misc{zhu_promptbench_2023,
	title = {{PromptBench}: {Towards} {Evaluating} the {Robustness} of {Large} {Language} {Models} on {Adversarial} {Prompts}},
	shorttitle = {{PromptBench}},
	url = {http://arxiv.org/abs/2306.04528},
	doi = {10.48550/arXiv.2306.04528},
	abstract = {The increasing reliance on Large Language Models (LLMs) across academia and industry necessitates a comprehensive understanding of their robustness to prompts. In response to this vital need, we introduce PromptBench, a robustness benchmark designed to measure LLMs' resilience to adversarial prompts. This study uses a plethora of adversarial textual attacks targeting prompts across multiple levels: character, word, sentence, and semantic. These prompts are then employed in diverse tasks, such as sentiment analysis, natural language inference, reading comprehension, machine translation, and math problem-solving. Our study generates 4,032 adversarial prompts, meticulously evaluated over 8 tasks and 13 datasets, with 567,084 test samples in total. Our findings demonstrate that contemporary LLMs are vulnerable to adversarial prompts. Furthermore, we present comprehensive analysis to understand the mystery behind prompt robustness and its transferability. We then offer insightful robustness analysis and pragmatic recommendations for prompt composition, beneficial to both researchers and everyday users. We make our code, prompts, and methodologies to generate adversarial prompts publicly accessible, thereby enabling and encouraging collaborative exploration in this pivotal field: https://github.com/microsoft/promptbench.},
	urldate = {2023-07-21},
	publisher = {arXiv},
	author = {Zhu, Kaijie and Wang, Jindong and Zhou, Jiaheng and Wang, Zichen and Chen, Hao and Wang, Yidong and Yang, Linyi and Ye, Wei and Gong, Neil Zhenqiang and Zhang, Yue and Xie, Xing},
	month = jun,
	year = {2023},
	note = {arXiv:2306.04528 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@misc{fan_improving_2023,
	title = {Improving {CLIP} {Training} with {Language} {Rewrites}},
	url = {http://arxiv.org/abs/2305.20088},
	doi = {10.48550/arXiv.2305.20088},
	abstract = {Contrastive Language-Image Pre-training (CLIP) stands as one of the most effective and scalable methods for training transferable vision models using paired image and text data. CLIP models are trained using contrastive loss, which typically relies on data augmentations to prevent overfitting and shortcuts. However, in the CLIP training paradigm, data augmentations are exclusively applied to image inputs, while language inputs remain unchanged throughout the entire training process, limiting the exposure of diverse texts to the same image. In this paper, we introduce Language augmented CLIP (LaCLIP), a simple yet highly effective approach to enhance CLIP training through language rewrites. Leveraging the in-context learning capability of large language models, we rewrite the text descriptions associated with each image. These rewritten texts exhibit diversity in sentence structure and vocabulary while preserving the original key concepts and meanings. During training, LaCLIP randomly selects either the original texts or the rewritten versions as text augmentations for each image. Extensive experiments on CC3M, CC12M, RedCaps and LAION-400M datasets show that CLIP pre-training with language rewrites significantly improves the transfer performance without computation or memory overhead during training. Specifically for ImageNet zero-shot accuracy, LaCLIP outperforms CLIP by 8.2\% on CC12M and 2.4\% on LAION-400M. Code is available at https://github.com/LijieFan/LaCLIP.},
	urldate = {2023-07-21},
	publisher = {arXiv},
	author = {Fan, Lijie and Krishnan, Dilip and Isola, Phillip and Katabi, Dina and Tian, Yonglong},
	month = may,
	year = {2023},
	note = {arXiv:2305.20088 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{qi_visual_2023,
	title = {Visual {Adversarial} {Examples} {Jailbreak} {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2306.13213},
	doi = {10.48550/arXiv.2306.13213},
	abstract = {Recently, there has been a surge of interest in introducing vision into Large Language Models (LLMs). The proliferation of large Visual Language Models (VLMs), such as Flamingo, BLIP-2, and GPT-4, signifies an exciting convergence of advancements in both visual and language foundation models. Yet, the risks associated with this integrative approach are largely unexamined. In this paper, we shed light on the security and safety implications of this trend. First, we underscore that the continuous and high-dimensional nature of the additional visual input space intrinsically makes it a fertile ground for adversarial attacks. This unavoidably expands the attack surfaces of LLMs. Second, we highlight that the broad functionality of LLMs also presents visual attackers with a wider array of achievable adversarial objectives, extending the implications of security failures beyond mere misclassification. To elucidate these risks, we study adversarial examples in the visual input space of a VLM. Specifically, against MiniGPT-4, which incorporates safety mechanisms that can refuse harmful instructions, we present visual adversarial examples that can circumvent the safety mechanisms and provoke harmful behaviors of the model. Remarkably, we discover that adversarial examples, even if optimized on a narrow, manually curated derogatory corpus against specific social groups, can universally jailbreak the model's safety mechanisms. A single such adversarial example can generally undermine MiniGPT-4's safety, enabling it to heed a wide range of harmful instructions and produce harmful content far beyond simply imitating the derogatory corpus used in optimization. Unveiling these risks, we accentuate the urgent need for comprehensive risk assessments, robust defense strategies, and the implementation of responsible practices for the secure and safe utilization of VLMs.},
	urldate = {2023-07-20},
	publisher = {arXiv},
	author = {Qi, Xiangyu and Huang, Kaixuan and Panda, Ashwinee and Wang, Mengdi and Mittal, Prateek},
	month = jun,
	year = {2023},
	note = {arXiv:2306.13213 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@inproceedings{liu_slimmable_2023,
	title = {Slimmable {Dataset} {Condensation}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Liu_Slimmable_Dataset_Condensation_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-07-20},
	author = {Liu, Songhua and Ye, Jingwen and Yu, Runpeng and Wang, Xinchao},
	year = {2023},
	pages = {3759--3768},
}

@inproceedings{girdhar_imagebind_2023,
	title = {{ImageBind}: {One} {Embedding} {Space} {To} {Bind} {Them} {All}},
	shorttitle = {{ImageBind}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Girdhar_ImageBind_One_Embedding_Space_To_Bind_Them_All_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-07-20},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Girdhar, Rohit and El-Nouby, Alaaeldin and Liu, Zhuang and Singh, Mannat and Alwala, Kalyan Vasudev and Joulin, Armand and Misra, Ishan},
	year = {2023},
	pages = {15180--15190},
}

@inproceedings{wang_fast_2020,
	title = {On {Fast} {Adversarial} {Robustness} {Adaptation} in {Model}-{Agnostic} {Meta}-{Learning}},
	url = {https://openreview.net/forum?id=o81ZyBCojoA},
	abstract = {Model-agnostic meta-learning (MAML) has emerged as one of the most successful meta-learning techniques in few-shot learning. It enables us to learn a \${\textbackslash}textit\{meta-initialization\}\$ of model parameters (that we call \${\textbackslash}textit\{meta-model\}\$) to rapidly adapt to new tasks using a small amount of labeled training data. Despite the generalization power of the meta-model, it remains elusive that how \${\textbackslash}textit\{adversarial robustness\}\$ can be maintained by MAML in few-shot learning. In addition to generalization, robustness is also desired for a meta-model to defend adversarial examples (attacks). Toward promoting adversarial robustness in MAML, we first study \${\textbackslash}textit\{when\}\$ a robustness-promoting regularization should be incorporated, given the fact that MAML adopts a bi-level (fine-tuning vs. meta-update) learning procedure. We show that robustifying the meta-update stage is sufficient to make robustness adapted to the task-specific fine-tuning stage even if the latter uses a standard training protocol. We also make additional justification on the acquired robustness adaptation by peering into the interpretability of neurons' activation maps. Furthermore, we investigate \${\textbackslash}textit\{how\}\$ robust regularization can \${\textbackslash}textit\{efficiently\}\$ be designed in MAML. We propose a general but easily-optimized robustness-regularized meta-learning framework, which allows the use of unlabeled data augmentation, fast adversarial attack generation, and computationally-light fine-tuning. In particular, we for the first time show that the auxiliary contrastive learning task can enhance the adversarial robustness of MAML. Finally, extensive experiments are conducted to demonstrate the effectiveness of our proposed methods in robust few-shot learning.},
	language = {en},
	urldate = {2023-07-20},
	author = {Wang, Ren and Xu, Kaidi and Liu, Sijia and Chen, Pin-Yu and Weng, Tsui-Wei and Gan, Chuang and Wang, Meng},
	month = oct,
	year = {2020},
}

@inproceedings{goldblum_adversarially_2020,
	title = {Adversarially {Robust} {Few}-{Shot} {Learning}: {A} {Meta}-{Learning} {Approach}},
	volume = {33},
	shorttitle = {Adversarially {Robust} {Few}-{Shot} {Learning}},
	url = {https://proceedings.neurips.cc/paper/2020/hash/cfee398643cbc3dc5eefc89334cacdc1-Abstract.html},
	abstract = {Previous work on adversarially robust neural networks for image classification requires large training sets and computationally expensive training procedures.  On the other hand, few-shot learning methods are highly vulnerable to adversarial examples.  The goal of our work is to produce networks which both perform well at few-shot classification tasks and are simultaneously robust to adversarial examples.  We develop an algorithm, called Adversarial Querying (AQ), for producing adversarially robust meta-learners, and we thoroughly investigate the causes for adversarial vulnerability.  Moreover, our method achieves far superior robust performance on few-shot image classification tasks, such as Mini-ImageNet and CIFAR-FS, than robust transfer learning.},
	urldate = {2023-07-20},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Goldblum, Micah and Fowl, Liam and Goldstein, Tom},
	year = {2020},
	pages = {17886--17895},
}

@inproceedings{buolamwini_gender_2018,
	title = {Gender {Shades}: {Intersectional} {Accuracy} {Disparities} in {Commercial} {Gender} {Classification}},
	shorttitle = {Gender {Shades}},
	url = {https://proceedings.mlr.press/v81/buolamwini18a.html},
	abstract = {Recent studies demonstrate that machine learning algorithms can discriminate based on classes like race and gender. In this work, we present an approach to evaluate bias present in automated facial analysis algorithms and datasets with respect to phenotypic subgroups. Using the dermatologist  approved Fitzpatrick Skin Type classification system, we characterize the gender and skin type distribution of two facial analysis benchmarks, IJB-A and Adience. We find that these datasets are overwhelmingly composed of lighter-skinned subjects (79.6\% for IJB-A and 86.2\% for Adience) and introduce a new facial analysis dataset which is balanced by gender and skin type. We evaluate 3 commercial gender classification systems using our dataset and show that darker-skinned females are the most misclassified group (with error rates of up to 34.7\%). The maximum error rate for lighter-skinned males is 0.8\%. The substantial disparities in the accuracy of classifying darker females, lighter females, darker males, and lighter males in gender classification systems require urgent attention if commercial companies are to build genuinely fair, transparent and accountable facial analysis algorithms.},
	language = {en},
	urldate = {2023-07-19},
	booktitle = {Proceedings of the 1st {Conference} on {Fairness}, {Accountability} and {Transparency}},
	publisher = {PMLR},
	author = {Buolamwini, Joy and Gebru, Timnit},
	month = jan,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {77--91},
}

@inproceedings{zhao_ood-cv_2022,
	address = {Berlin, Heidelberg},
	title = {{OOD}-{CV}: {A} {Benchmark} for {Robustness} to {Out}-of-{Distribution} {Shifts} of {Individual} {Nuisances} in {Natural} {Images}},
	isbn = {978-3-031-20073-1},
	shorttitle = {{OOD}-{CV}},
	url = {https://doi.org/10.1007/978-3-031-20074-8_10},
	doi = {10.1007/978-3-031-20074-8_10},
	abstract = {Enhancing the robustness of vision algorithms in real-world scenarios is challenging. One reason is that existing robustness benchmarks are limited, as they either rely on synthetic data or ignore the effects of individual nuisance factors. We introduce OOD-CV , a benchmark dataset that includes out-of-distribution examples of 10 object categories in terms of pose, shape, texture, context and the weather conditions, and enables benchmarking models for image classification, object detection, and 3D pose estimation. In addition to this novel dataset, we contribute extensive experiments using popular baseline methods, which reveal that: 1) Some nuisance factors have a much stronger negative effect on the performance compared to others, also depending on the vision task. 2) Current approaches to enhance robustness have only marginal effects, and can even reduce robustness. 3) We do not observe significant differences between convolutional and transformer architectures. We believe our dataset provides a rich testbed to study robustness and will help push forward research in this area.},
	urldate = {2023-06-30},
	booktitle = {Computer {Vision} – {ECCV} 2022: 17th {European} {Conference}, {Tel} {Aviv}, {Israel}, {October} 23–27, 2022, {Proceedings}, {Part} {VIII}},
	publisher = {Springer-Verlag},
	author = {Zhao, Bingchen and Yu, Shaozuo and Ma, Wufei and Yu, Mingxin and Mei, Shenxiao and Wang, Angtian and He, Ju and Yuille, Alan and Kortylewski, Adam},
	month = oct,
	year = {2022},
	pages = {163--180},
}

@inproceedings{santurkar_breeds_2020,
	title = {{BREEDS}: {Benchmarks} for {Subpopulation} {Shift}},
	shorttitle = {{BREEDS}},
	url = {https://openreview.net/forum?id=mQPBmvyAuk},
	abstract = {We develop a methodology for assessing the robustness of models to subpopulation shift---specifically, their ability to generalize to novel data subpopulations that were not observed during training. Our approach leverages the class structure underlying existing datasets to control the data subpopulations that comprise the training and test distributions. This enables us to synthesize realistic distribution shifts whose sources can be precisely controlled and characterized, within existing large-scale datasets. Applying this methodology to the ImageNet dataset, we create a suite of subpopulation shift benchmarks of varying granularity. We then validate that the corresponding shifts are tractable by obtaining human baselines. Finally, we utilize these benchmarks to measure the sensitivity of standard model architectures as well as the effectiveness of existing train-time robustness interventions.},
	language = {en},
	urldate = {2023-07-19},
	author = {Santurkar, Shibani and Tsipras, Dimitris and Madry, Aleksander},
	month = oct,
	year = {2020},
}

@inproceedings{li_deeper_2017,
	title = {Deeper, {Broader} and {Artier} {Domain} {Generalization}},
	url = {https://openaccess.thecvf.com/content_iccv_2017/html/Li_Deeper_Broader_and_ICCV_2017_paper.html},
	urldate = {2023-07-19},
	author = {Li, Da and Yang, Yongxin and Song, Yi-Zhe and Hospedales, Timothy M.},
	year = {2017},
	pages = {5542--5550},
}

@inproceedings{sagawa_distributionally_2019,
	title = {Distributionally {Robust} {Neural} {Networks}},
	url = {https://openreview.net/forum?id=ryxGuJrFvS},
	abstract = {Overparameterized neural networks can be highly accurate on average on an i.i.d. test set, yet consistently fail on atypical groups of the data (e.g., by learning spurious correlations that hold on average but not in such groups). Distributionally robust optimization (DRO) allows us to learn models that instead minimize the worst-case training loss over a set of pre-defined groups. However, we find that naively applying group DRO to overparameterized neural networks fails: these models can perfectly fit the training data, and any model with vanishing average training loss also already has vanishing worst-case training loss. Instead, the poor worst-case performance arises from poor generalization on some groups. By coupling group DRO models with increased regularization---stronger-than-typical L2 regularization or early stopping---we achieve substantially higher worst-group accuracies, with 10-40 percentage point improvements on a natural language inference task and two image tasks, while maintaining high average accuracies. Our results suggest that regularization is important for worst-group generalization in the overparameterized regime, even if it is not needed for average generalization. Finally, we introduce a stochastic optimization algorithm for the group DRO setting and provide convergence guarantees for the new algorithm.},
	language = {en},
	urldate = {2023-07-19},
	author = {Sagawa*, Shiori and Koh*, Pang Wei and Hashimoto, Tatsunori B. and Liang, Percy},
	month = sep,
	year = {2019},
}

@inproceedings{xiao_noise_2020,
	title = {Noise or {Signal}: {The} {Role} of {Image} {Backgrounds} in {Object} {Recognition}},
	shorttitle = {Noise or {Signal}},
	url = {https://openreview.net/forum?id=gl3D-xY7wLq},
	abstract = {We assess the tendency of state-of-the-art object recognition models to depend on signals from image backgrounds. We create a toolkit for disentangling foreground and background signal on ImageNet images, and find that (a) models can achieve non-trivial accuracy by relying on the background alone, (b) models often misclassify images even in the presence of correctly classified foregrounds--up to 88\% of the time with adversarially chosen backgrounds, and (c) more accurate models tend to depend on backgrounds less. Our analysis of backgrounds brings us closer to understanding which correlations machine learning models use, and how they determine models' out of distribution performance.},
	language = {en},
	urldate = {2023-07-19},
	author = {Xiao, Kai Yuanqing and Engstrom, Logan and Ilyas, Andrew and Madry, Aleksander},
	month = oct,
	year = {2020},
}

@inproceedings{torralba_unbiased_2011,
	title = {Unbiased look at dataset bias},
	doi = {10.1109/CVPR.2011.5995347},
	abstract = {Datasets are an integral part of contemporary object recognition research. They have been the chief reason for the considerable progress in the field, not just as source of large amounts of training data, but also as means of measuring and comparing performance of competing algorithms. At the same time, datasets have often been blamed for narrowing the focus of object recognition research, reducing it to a single benchmark performance number. Indeed, some datasets, that started out as data capture efforts aimed at representing the visual world, have become closed worlds unto themselves (e.g. the Corel world, the Caltech-101 world, the PASCAL VOC world). With the focus on beating the latest benchmark numbers on the latest dataset, have we perhaps lost sight of the original purpose? The goal of this paper is to take stock of the current state of recognition datasets. We present a comparison study using a set of popular datasets, evaluated based on a number of criteria including: relative data bias, cross-dataset generalization, effects of closed-world assumption, and sample value. The experimental results, some rather surprising, suggest directions that can improve dataset collection as well as algorithm evaluation protocols. But more broadly, the hope is to stimulate discussion in the community regarding this very important, but largely neglected issue.},
	booktitle = {{CVPR} 2011},
	author = {Torralba, Antonio and Efros, Alexei A.},
	month = jun,
	year = {2011},
	note = {ISSN: 1063-6919},
	keywords = {Communities, Internet, Object recognition, Support vector machines, Testing, Training, Visualization},
	pages = {1521--1528},
}

@inproceedings{kortylewski_analyzing_2019,
	title = {Analyzing and {Reducing} the {Damage} of {Dataset} {Bias} to {Face} {Recognition} {With} {Synthetic} {Data}},
	url = {https://openaccess.thecvf.com/content_CVPRW_2019/html/BEFA/Kortylewski_Analyzing_and_Reducing_the_Damage_of_Dataset_Bias_to_Face_CVPRW_2019_paper.html},
	urldate = {2023-07-19},
	author = {Kortylewski, Adam and Egger, Bernhard and Schneider, Andreas and Gerig, Thomas and Morel-Forster, Andreas and Vetter, Thomas},
	year = {2019},
	pages = {0--0},
}

@inproceedings{jia_adv-watermark_2020,
	address = {New York, NY, USA},
	series = {{MM} '20},
	title = {Adv-watermark: {A} {Novel} {Watermark} {Perturbation} for {Adversarial} {Examples}},
	isbn = {978-1-4503-7988-5},
	shorttitle = {Adv-watermark},
	url = {https://dl.acm.org/doi/10.1145/3394171.3413976},
	doi = {10.1145/3394171.3413976},
	abstract = {Recent research has demonstrated that adding some imperceptible perturbations to original images can fool deep learning models. However, the current adversarial perturbations are usually shown in the form of noises, and thus have no practical meaning. Image watermark is a technique widely used for copyright protection. We can regard image watermark as a king of meaningful noises and adding it to the original image will not affect people's understanding of the image content, and will not arouse people's suspicion. Therefore, it will be interesting to generate adversarial examples using watermarks. In this paper, we propose a novel watermark perturbation for adversarial examples (Adv-watermark) which combines image watermarking techniques and adversarial example algorithms. Adding a meaningful watermark to the clean images can attack the DNN models. Specifically, we propose a novel optimization algorithm, which is called Basin Hopping Evolution (BHE), to generate adversarial watermarks in the black-box attack mode. Thanks to the BHE, Adv-watermark only requires a few queries from the threat models to finish the attacks. A series of experiments conducted on ImageNet and CASIA-WebFace datasets show that the proposed method can efficiently generate adversarial examples, and outperforms the state-of-the-art attack methods. Moreover, Adv-watermark is more robust against image transformation defense methods.},
	urldate = {2023-07-18},
	booktitle = {Proceedings of the 28th {ACM} {International} {Conference} on {Multimedia}},
	publisher = {Association for Computing Machinery},
	author = {Jia, Xiaojun and Wei, Xingxing and Cao, Xiaochun and Han, Xiaoguang},
	month = oct,
	year = {2020},
	keywords = {adversarial examples, basin hopping evolution, watermark perturbation},
	pages = {1579--1587},
}

@article{vidnerova_vulnerability_2020,
	title = {Vulnerability of classifiers to evolutionary generated adversarial examples},
	volume = {127},
	issn = {0893-6080},
	url = {https://www.sciencedirect.com/science/article/pii/S0893608020301350},
	doi = {10.1016/j.neunet.2020.04.015},
	abstract = {This paper deals with the vulnerability of machine learning models to adversarial examples and its implication for robustness and generalization properties. We propose an evolutionary algorithm that can generate adversarial examples for any machine learning model in the black-box attack scenario. This way, we can find adversarial examples without access to model’s parameters, only by querying the model at hand. We have tested a range of machine learning models including deep and shallow neural networks. Our experiments have shown that the vulnerability to adversarial examples is not only the problem of deep networks, but it spreads through various machine learning architectures. Rather, it depends on the type of computational units. Local units, such as Gaussian kernels, are less vulnerable to adversarial examples.},
	language = {en},
	urldate = {2023-07-18},
	journal = {Neural Networks},
	author = {Vidnerová, Petra and Neruda, Roman},
	month = jul,
	year = {2020},
	keywords = {Adversarial examples, Genetic algorithms, Kernel methods, Neural networks, Supervised learning},
	pages = {168--181},
}

@inproceedings{chen_ead_2018,
	title = {{EAD}: {Elastic}-{Net} {Attacks} to {Deep} {Neural} {Networks} via {Adversarial} {Examples}},
	volume = {32},
	copyright = {Copyright (c)},
	shorttitle = {{EAD}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/11302},
	doi = {10.1609/aaai.v32i1.11302},
	abstract = {Recent studies have highlighted the vulnerability of deep neural networks (DNNs) to adversarial examples — a visually indistinguishable adversarial image can easily be crafted to cause a well-trained model to misclassify. Existing methods for crafting adversarial examples are based on L2 and L∞ distortion metrics. However,  despite the fact that L1 distortion accounts for the total variation and encourages sparsity in the perturbation, little has been developed for crafting L1-based adversarial examples. In this paper, we formulate the process of attacking DNNs via adversarial examples as an elastic-net regularized optimization problem. Our elastic-net attacks to DNNs (EAD) feature L1-oriented adversarial examples and include the state-of-the-art L2 attack as a special case. Experimental results on MNIST, CIFAR10 and ImageNet show that EAD can yield a distinct set of adversarial examples with small L1 distortion and attains similar attack performance to the state-of-the-art methods in different attack scenarios. More importantly, EAD leads to improved attack transferability and complements adversarial training for DNNs, suggesting novel insights on leveraging L1 distortion in adversarial machine learning and security implications of DNNs.},
	language = {en},
	urldate = {2023-07-18},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Chen, Pin-Yu and Sharma, Yash and Zhang, Huan and Yi, Jinfeng and Hsieh, Cho-Jui},
	month = apr,
	year = {2018},
	note = {Number: 1},
	keywords = {robustness},
}

@inproceedings{athalye_synthesizing_2018,
	title = {Synthesizing {Robust} {Adversarial} {Examples}},
	url = {https://proceedings.mlr.press/v80/athalye18b.html},
	abstract = {Standard methods for generating adversarial examples for neural networks do not consistently fool neural network classifiers in the physical world due to a combination of viewpoint shifts, camera noise, and other natural transformations, limiting their relevance to real-world systems. We demonstrate the existence of robust 3D adversarial objects, and we present the first algorithm for synthesizing examples that are adversarial over a chosen distribution of transformations. We synthesize two-dimensional adversarial images that are robust to noise, distortion, and affine transformation. We apply our algorithm to complex three-dimensional objects, using 3D-printing to manufacture the first physical adversarial objects. Our results demonstrate the existence of 3D adversarial objects in the physical world.},
	language = {en},
	urldate = {2023-07-18},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Athalye, Anish and Engstrom, Logan and Ilyas, Andrew and Kwok, Kevin},
	month = jul,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {284--293},
}

@misc{lu_no_2017,
	title = {{NO} {Need} to {Worry} about {Adversarial} {Examples} in {Object} {Detection} in {Autonomous} {Vehicles}},
	url = {http://arxiv.org/abs/1707.03501},
	doi = {10.48550/arXiv.1707.03501},
	abstract = {It has been shown that most machine learning algorithms are susceptible to adversarial perturbations. Slightly perturbing an image in a carefully chosen direction in the image space may cause a trained neural network model to misclassify it. Recently, it was shown that physical adversarial examples exist: printing perturbed images then taking pictures of them would still result in misclassification. This raises security and safety concerns. However, these experiments ignore a crucial property of physical objects: the camera can view objects from different distances and at different angles. In this paper, we show experiments that suggest that current constructions of physical adversarial examples do not disrupt object detection from a moving platform. Instead, a trained neural network classifies most of the pictures taken from different distances and angles of a perturbed image correctly. We believe this is because the adversarial property of the perturbation is sensitive to the scale at which the perturbed picture is viewed, so (for example) an autonomous car will misclassify a stop sign only from a small range of distances. Our work raises an important question: can one construct examples that are adversarial for many or most viewing conditions? If so, the construction should offer very significant insights into the internal representation of patterns by deep networks. If not, there is a good prospect that adversarial examples can be reduced to a curiosity with little practical impact.},
	urldate = {2023-07-18},
	publisher = {arXiv},
	author = {Lu, Jiajun and Sibai, Hussein and Fabry, Evan and Forsyth, David},
	month = jul,
	year = {2017},
	note = {arXiv:1707.03501 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security},
}

@inproceedings{jan_connecting_2019,
	title = {Connecting the {Digital} and {Physical} {World}: {Improving} the {Robustness} of {Adversarial} {Attacks}},
	volume = {33},
	shorttitle = {Connecting the {Digital} and {Physical} {World}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/3926},
	doi = {10.1609/aaai.v33i01.3301962},
	abstract = {While deep learning models have achieved unprecedented success in various domains, there is also a growing concern of adversarial attacks against related applications. Recent results show that by adding a small amount of perturbations to an image (imperceptible to humans), the resulting adversarial examples can force a classifier to make targeted mistakes. So far, most existing works focus on crafting adversarial examples in the digital domain, while limited efforts have been devoted to understanding the physical domain attacks. In this work, we explore the feasibility of generating robust adversarial examples that remain effective in the physical domain. Our core idea is to use an image-to-image translation network to simulate the digital-to-physical transformation process for generating robust adversarial examples. To validate our method, we conduct a large-scale physical-domain experiment, which involves manually taking more than 3000 physical domain photos. The results show that our method outperforms existing ones by a large margin and demonstrates a high level of robustness and transferability.},
	language = {en},
	urldate = {2023-07-18},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Jan, Steve T.K. and Messou, Joseph and Lin, Yen-Chen and Huang, Jia-Bin and Wang, Gang},
	month = jul,
	year = {2019},
	pages = {962--969},
}

@inproceedings{poursaeed_generative_2018,
	title = {Generative {Adversarial} {Perturbations}},
	url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Poursaeed_Generative_Adversarial_Perturbations_CVPR_2018_paper.html},
	urldate = {2023-07-18},
	booktitle = {Proceedings of the {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Poursaeed, Omid and Katsman, Isay and Gao, Bicheng and Belongie, Serge},
	year = {2018},
	pages = {4422--4431},
}

@misc{kirillov_segment_2023,
	title = {Segment {Anything}},
	url = {http://arxiv.org/abs/2304.02643},
	doi = {10.48550/arXiv.2304.02643},
	abstract = {We introduce the Segment Anything (SA) project: a new task, model, and dataset for image segmentation. Using our efficient model in a data collection loop, we built the largest segmentation dataset to date (by far), with over 1 billion masks on 11M licensed and privacy respecting images. The model is designed and trained to be promptable, so it can transfer zero-shot to new image distributions and tasks. We evaluate its capabilities on numerous tasks and find that its zero-shot performance is impressive -- often competitive with or even superior to prior fully supervised results. We are releasing the Segment Anything Model (SAM) and corresponding dataset (SA-1B) of 1B masks and 11M images at https://segment-anything.com to foster research into foundation models for computer vision.},
	urldate = {2023-07-18},
	publisher = {arXiv},
	author = {Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C. and Lo, Wan-Yen and Dollár, Piotr and Girshick, Ross},
	month = apr,
	year = {2023},
	note = {arXiv:2304.02643 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@inproceedings{assran_self-supervised_2023,
	title = {Self-{Supervised} {Learning} {From} {Images} {With} a {Joint}-{Embedding} {Predictive} {Architecture}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Assran_Self-Supervised_Learning_From_Images_With_a_Joint-Embedding_Predictive_Architecture_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-07-18},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Assran, Mahmoud and Duval, Quentin and Misra, Ishan and Bojanowski, Piotr and Vincent, Pascal and Rabbat, Michael and LeCun, Yann and Ballas, Nicolas},
	year = {2023},
	pages = {15619--15629},
}

@inproceedings{schwinn_improving_2022,
	title = {Improving {Robustness} against {Real}-{World} and {Worst}-{Case} {Distribution} {Shifts} through {Decision} {Region} {Quantification}},
	url = {https://proceedings.mlr.press/v162/schwinn22a.html},
	abstract = {The reliability of neural networks is essential for their use in safety-critical applications. Existing approaches generally aim at improving the robustness of neural networks to either real-world distribution shifts (e.g., common corruptions and perturbations, spatial transformations, and natural adversarial examples) or worst-case distribution shifts (e.g., optimized adversarial examples). In this work, we propose the Decision Region Quantification (DRQ) algorithm to improve the robustness of any differentiable pre-trained model against both real-world and worst-case distribution shifts in the data. DRQ analyzes the robustness of local decision regions in the vicinity of a given data point to make more reliable predictions. We theoretically motivate the DRQ algorithm by showing that it effectively smooths spurious local extrema in the decision surface. Furthermore, we propose an implementation using targeted and untargeted adversarial attacks. An extensive empirical evaluation shows that DRQ increases the robustness of adversarially and non-adversarially trained models against real-world and worst-case distribution shifts on several computer vision benchmark datasets.},
	language = {en},
	urldate = {2023-07-15},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Schwinn, Leo and Bungert, Leon and Nguyen, An and Raab, René and Pulsmeyer, Falk and Precup, Doina and Eskofier, Bjoern and Zanca, Dario},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {19434--19449},
}

@inproceedings{modas_sparsefool_2019,
	title = {{SparseFool}: {A} {Few} {Pixels} {Make} a {Big} {Difference}},
	shorttitle = {{SparseFool}},
	url = {https://openaccess.thecvf.com/content_CVPR_2019/html/Modas_SparseFool_A_Few_Pixels_Make_a_Big_Difference_CVPR_2019_paper.html},
	urldate = {2023-07-14},
	author = {Modas, Apostolos and Moosavi-Dezfooli, Seyed-Mohsen and Frossard, Pascal},
	year = {2019},
	pages = {9087--9096},
}

@inproceedings{moosavi-dezfooli_deepfool_2016,
	title = {{DeepFool}: {A} {Simple} and {Accurate} {Method} to {Fool} {Deep} {Neural} {Networks}},
	shorttitle = {{DeepFool}},
	url = {https://openaccess.thecvf.com/content_cvpr_2016/html/Moosavi-Dezfooli_DeepFool_A_Simple_CVPR_2016_paper.html},
	urldate = {2023-07-14},
	author = {Moosavi-Dezfooli, Seyed-Mohsen and Fawzi, Alhussein and Frossard, Pascal},
	year = {2016},
	pages = {2574--2582},
}

@inproceedings{wang_towards_2023,
	title = {Towards {Transferable} {Targeted} {Adversarial} {Examples}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Wang_Towards_Transferable_Targeted_Adversarial_Examples_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-07-14},
	author = {Wang, Zhibo and Yang, Hongshan and Feng, Yunhe and Sun, Peng and Guo, Hengchang and Zhang, Zhifei and Ren, Kui},
	year = {2023},
	pages = {20534--20543},
}

@inproceedings{andriushchenko_square_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Square {Attack}: {A} {Query}-{Efficient} {Black}-{Box} {Adversarial} {Attack} via {Random} {Search}},
	isbn = {978-3-030-58592-1},
	shorttitle = {Square {Attack}},
	doi = {10.1007/978-3-030-58592-1_29},
	abstract = {We propose the Square Attack, a score-based black-box \$\$l\_2\$\$l2- and \$\$l\_{\textbackslash}infty \$\$l∞-adversarial attack that does not rely on local gradient information and thus is not affected by gradient masking. Square Attack is based on a randomized search scheme which selects localized square-shaped updates at random positions so that at each iteration the perturbation is situated approximately at the boundary of the feasible set. Our method is significantly more query efficient and achieves a higher success rate compared to the state-of-the-art methods, especially in the untargeted setting. In particular, on ImageNet we improve the average query efficiency in the untargeted setting for various deep networks by a factor of at least 1.8 and up to 3 compared to the recent state-of-the-art \$\$l\_{\textbackslash}infty \$\$l∞-attack of Al-Dujaili \& O’Reilly (2020). Moreover, although our attack is black-box, it can also outperform gradient-based white-box attacks on the standard benchmarks achieving a new state-of-the-art in terms of the success rate. The code of our attack is available at https://github.com/max-andr/square-attack.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Andriushchenko, Maksym and Croce, Francesco and Flammarion, Nicolas and Hein, Matthias},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	pages = {484--501},
}

@article{yuan_adversarial_2019,
	title = {Adversarial {Examples}: {Attacks} and {Defenses} for {Deep} {Learning}},
	volume = {30},
	issn = {2162-2388},
	shorttitle = {Adversarial {Examples}},
	doi = {10.1109/TNNLS.2018.2886017},
	abstract = {With rapid progress and significant successes in a wide spectrum of applications, deep learning is being applied in many safety-critical environments. However, deep neural networks (DNNs) have been recently found vulnerable to well-designed input samples called adversarial examples. Adversarial perturbations are imperceptible to human but can easily fool DNNs in the testing/deploying stage. The vulnerability to adversarial examples becomes one of the major risks for applying DNNs in safety-critical environments. Therefore, attacks and defenses on adversarial examples draw great attention. In this paper, we review recent findings on adversarial examples for DNNs, summarize the methods for generating adversarial examples, and propose a taxonomy of these methods. Under the taxonomy, applications for adversarial examples are investigated. We further elaborate on countermeasures for adversarial examples. In addition, three major challenges in adversarial examples and the potential solutions are discussed.},
	number = {9},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Yuan, Xiaoyong and He, Pan and Zhu, Qile and Li, Xiaolin},
	month = sep,
	year = {2019},
	note = {Conference Name: IEEE Transactions on Neural Networks and Learning Systems},
	keywords = {Adversarial examples, Computational modeling, Computer architecture, Data models, Deep learning, Feature extraction, Security, Task analysis, deep learning (DL), deep neural network (DNN), security},
	pages = {2805--2824},
}

@article{zhao_towards_2020,
	title = {Towards {Query}-{Efficient} {Black}-{Box} {Adversary} with {Zeroth}-{Order} {Natural} {Gradient} {Descent}},
	volume = {34},
	copyright = {Copyright (c) 2020 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/6173},
	doi = {10.1609/aaai.v34i04.6173},
	abstract = {Despite the great achievements of the modern deep neural networks (DNNs), the vulnerability/robustness of state-of-the-art DNNs raises security concerns in many application domains requiring high reliability. Various adversarial attacks are proposed to sabotage the learning performance of DNN models. Among those, the black-box adversarial attack methods have received special attentions owing to their practicality and simplicity. Black-box attacks usually prefer less queries in order to maintain stealthy and low costs. However, most of the current black-box attack methods adopt the first-order gradient descent method, which may come with certain deficiencies such as relatively slow convergence and high sensitivity to hyper-parameter settings. In this paper, we propose a zeroth-order natural gradient descent (ZO-NGD) method to design the adversarial attacks, which incorporates the zeroth-order gradient estimation technique catering to the black-box attack scenario and the second-order natural gradient descent to achieve higher query efficiency. The empirical evaluations on image classification datasets demonstrate that ZO-NGD can obtain significantly lower model query complexities compared with state-of-the-art attack methods.},
	language = {en},
	number = {04},
	urldate = {2023-07-14},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Zhao, Pu and Chen, Pin-yu and Wang, Siyue and Lin, Xue},
	month = apr,
	year = {2020},
	note = {Number: 04},
	pages = {6909--6916},
}

@inproceedings{chen_zoo_2017,
	address = {New York, NY, USA},
	series = {{AISec} '17},
	title = {{ZOO}: {Zeroth} {Order} {Optimization} {Based} {Black}-box {Attacks} to {Deep} {Neural} {Networks} without {Training} {Substitute} {Models}},
	isbn = {978-1-4503-5202-4},
	shorttitle = {{ZOO}},
	url = {https://dl.acm.org/doi/10.1145/3128572.3140448},
	doi = {10.1145/3128572.3140448},
	abstract = {Deep neural networks (DNNs) are one of the most prominent technologies of our time, as they achieve state-of-the-art performance in many machine learning tasks, including but not limited to image classification, text mining, and speech processing. However, recent research on DNNs has indicated ever-increasing concern on the robustness to adversarial examples, especially for security-critical tasks such as traffic sign identification for autonomous driving. Studies have unveiled the vulnerability of a well-trained DNN by demonstrating the ability of generating barely noticeable (to both human and machines) adversarial images that lead to misclassification. Furthermore, researchers have shown that these adversarial images are highly transferable by simply training and attacking a substitute model built upon the target model, known as a black-box attack to DNNs. Similar to the setting of training substitute models, in this paper we propose an effective black-box attack that also only has access to the input (images) and the output (confidence scores) of a targeted DNN. However, different from leveraging attack transferability from substitute models, we propose zeroth order optimization (ZOO) based attacks to directly estimate the gradients of the targeted DNN for generating adversarial examples. We use zeroth order stochastic coordinate descent along with dimension reduction, hierarchical attack and importance sampling techniques to efficiently attack black-box models. By exploiting zeroth order optimization, improved attacks to the targeted DNN can be accomplished, sparing the need for training substitute models and avoiding the loss in attack transferability. Experimental results on MNIST, CIFAR10 and ImageNet show that the proposed ZOO attack is as effective as the state-of-the-art white-box attack (e.g., Carlini and Wagner's attack) and significantly outperforms existing black-box attacks via substitute models.},
	urldate = {2023-07-14},
	booktitle = {Proceedings of the 10th {ACM} {Workshop} on {Artificial} {Intelligence} and {Security}},
	publisher = {Association for Computing Machinery},
	author = {Chen, Pin-Yu and Zhang, Huan and Sharma, Yash and Yi, Jinfeng and Hsieh, Cho-Jui},
	month = nov,
	year = {2017},
	keywords = {adversarial learning, black-box attack, deep learning, neural network, substitute model},
	pages = {15--26},
}

@inproceedings{alzantot_genattack_2019,
	address = {New York, NY, USA},
	series = {{GECCO} '19},
	title = {{GenAttack}: practical black-box attacks with gradient-free optimization},
	isbn = {978-1-4503-6111-8},
	shorttitle = {{GenAttack}},
	url = {https://dl.acm.org/doi/10.1145/3321707.3321749},
	doi = {10.1145/3321707.3321749},
	abstract = {Deep neural networks are vulnerable to adversarial examples, even in the black-box setting, where the attacker is restricted solely to query access. Existing black-box approaches to generating adversarial examples typically require a significant number of queries, either for training a substitute network or performing gradient estimation. We introduce GenAttack, a gradient-free optimization technique that uses genetic algorithms for synthesizing adversarial examples in the black-box setting. Our experiments on different datasets (MNIST, CIFAR-10, and ImageNet) show that GenAttack can successfully generate visually imperceptible adversarial examples against state-of-the-art image recognition models with orders of magnitude fewer queries than previous approaches. Against MNIST and CIFAR-10 models, GenAttack required roughly 2,126 and 2,568 times fewer queries respectively, than ZOO, the prior state-of-the-art black-box attack. In order to scale up the attack to large-scale high-dimensional ImageNet models, we perform a series of optimizations that further improve the query efficiency of our attack leading to 237 times fewer queries against the Inception-v3 model than ZOO. Furthermore, we show that GenAttack can successfully attack some state-of-the-art ImageNet defenses, including ensemble adversarial training and non-differentiable or randomized input transformations. Our results suggest that evolutionary algorithms open up a promising area of research into effective black-box attacks.},
	urldate = {2023-07-14},
	booktitle = {Proceedings of the {Genetic} and {Evolutionary} {Computation} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Alzantot, Moustafa and Sharma, Yash and Chakraborty, Supriyo and Zhang, Huan and Hsieh, Cho-Jui and Srivastava, Mani B.},
	month = jul,
	year = {2019},
	keywords = {adversarial examples, computer vision, deep learning, genetic algorithm},
	pages = {1111--1119},
}

@misc{narodytska_simple_2016,
	title = {Simple {Black}-{Box} {Adversarial} {Perturbations} for {Deep} {Networks}},
	url = {http://arxiv.org/abs/1612.06299},
	doi = {10.48550/arXiv.1612.06299},
	abstract = {Deep neural networks are powerful and popular learning models that achieve state-of-the-art pattern recognition performance on many computer vision, speech, and language processing tasks. However, these networks have also been shown susceptible to carefully crafted adversarial perturbations which force misclassification of the inputs. Adversarial examples enable adversaries to subvert the expected system behavior leading to undesired consequences and could pose a security risk when these systems are deployed in the real world. In this work, we focus on deep convolutional neural networks and demonstrate that adversaries can easily craft adversarial examples even without any internal knowledge of the target network. Our attacks treat the network as an oracle (black-box) and only assume that the output of the network can be observed on the probed inputs. Our first attack is based on a simple idea of adding perturbation to a randomly selected single pixel or a small set of them. We then improve the effectiveness of this attack by carefully constructing a small set of pixels to perturb by using the idea of greedy local-search. Our proposed attacks also naturally extend to a stronger notion of misclassification. Our extensive experimental results illustrate that even these elementary attacks can reveal a deep neural network's vulnerabilities. The simplicity and effectiveness of our proposed schemes mean that they could serve as a litmus test for designing robust networks.},
	urldate = {2023-07-14},
	publisher = {arXiv},
	author = {Narodytska, Nina and Kasiviswanathan, Shiva Prasad},
	month = dec,
	year = {2016},
	note = {arXiv:1612.06299 [cs, stat]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{huang_survey_2020,
	title = {A survey of safety and trustworthiness of deep neural networks: {Verification}, testing, adversarial attack and defence, and interpretability},
	volume = {37},
	issn = {1574-0137},
	shorttitle = {A survey of safety and trustworthiness of deep neural networks},
	url = {https://www.sciencedirect.com/science/article/pii/S1574013719302527},
	doi = {10.1016/j.cosrev.2020.100270},
	abstract = {In the past few years, significant progress has been made on deep neural networks (DNNs) in achieving human-level performance on several long-standing tasks. With the broader deployment of DNNs on various applications, the concerns over their safety and trustworthiness have been raised in public, especially after the widely reported fatal incidents involving self-driving cars. Research to address these concerns is particularly active, with a significant number of papers released in the past few years. This survey paper conducts a review of the current research effort into making DNNs safe and trustworthy, by focusing on four aspects: verification, testing, adversarial attack and defence, and interpretability. In total, we survey 202 papers, most of which were published after 2017.},
	language = {en},
	urldate = {2023-07-14},
	journal = {Computer Science Review},
	author = {Huang, Xiaowei and Kroening, Daniel and Ruan, Wenjie and Sharp, James and Sun, Youcheng and Thamo, Emese and Wu, Min and Yi, Xinping},
	month = aug,
	year = {2020},
	pages = {100270},
}

@misc{zhao_evaluating_2023,
	title = {On {Evaluating} {Adversarial} {Robustness} of {Large} {Vision}-{Language} {Models}},
	url = {http://arxiv.org/abs/2305.16934},
	doi = {10.48550/arXiv.2305.16934},
	abstract = {Large vision-language models (VLMs) such as GPT-4 have achieved unprecedented performance in response generation, especially with visual inputs, enabling more creative and adaptable interaction than large language models such as ChatGPT. Nonetheless, multimodal generation exacerbates safety concerns, since adversaries may successfully evade the entire system by subtly manipulating the most vulnerable modality (e.g., vision). To this end, we propose evaluating the robustness of open-source large VLMs in the most realistic and high-risk setting, where adversaries have only black-box system access and seek to deceive the model into returning the targeted responses. In particular, we first craft targeted adversarial examples against pretrained models such as CLIP and BLIP, and then transfer these adversarial examples to other VLMs such as MiniGPT-4, LLaVA, UniDiffuser, BLIP-2, and Img2Prompt. In addition, we observe that black-box queries on these VLMs can further improve the effectiveness of targeted evasion, resulting in a surprisingly high success rate for generating targeted responses. Our findings provide a quantitative understanding regarding the adversarial vulnerability of large VLMs and call for a more thorough examination of their potential security flaws before deployment in practice. Code is at https://github.com/yunqing-me/AttackVLM.},
	urldate = {2023-07-13},
	publisher = {arXiv},
	author = {Zhao, Yunqing and Pang, Tianyu and Du, Chao and Yang, Xiao and Li, Chongxuan and Cheung, Ngai-Man and Lin, Min},
	month = may,
	year = {2023},
	note = {arXiv:2305.16934 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning, Computer Science - Multimedia},
}

@misc{li_language-driven_2023,
	title = {Language-{Driven} {Anchors} for {Zero}-{Shot} {Adversarial} {Robustness}},
	url = {http://arxiv.org/abs/2301.13096},
	doi = {10.48550/arXiv.2301.13096},
	abstract = {Deep neural networks are known to be susceptible to adversarial attacks. In this work, we focus on improving adversarial robustness in the challenging zero-shot image classification setting. To address this issue, we propose LAAT, a novel Language-driven, Anchor-based Adversarial Training strategy. LAAT utilizes a text encoder to generate fixed anchors (normalized feature embeddings) for each category and then uses these anchors for adversarial training. By leveraging the semantic consistency of the text encoders, LAAT can enhance the adversarial robustness of the image model on novel categories without additional examples. We identify the large cosine similarity problem of recent text encoders and design several effective techniques to address it. The experimental results demonstrate that LAAT significantly improves zero-shot adversarial performance, outperforming previous state-of-the-art adversarially robust one-shot methods. Moreover, our method produces substantial zero-shot adversarial robustness when models are trained on large datasets such as ImageNet-1K and applied to several downstream datasets.},
	urldate = {2023-07-13},
	publisher = {arXiv},
	author = {Li, Xiao and Zhang, Wei and Liu, Yining and Hu, Zhanhao and Zhang, Bo and Hu, Xiaolin},
	month = apr,
	year = {2023},
	note = {arXiv:2301.13096 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@inproceedings{zhang_towards_2022,
	address = {New York, NY, USA},
	series = {{MM} '22},
	title = {Towards {Adversarial} {Attack} on {Vision}-{Language} {Pre}-training {Models}},
	isbn = {978-1-4503-9203-7},
	url = {https://dl.acm.org/doi/10.1145/3503161.3547801},
	doi = {10.1145/3503161.3547801},
	abstract = {While vision-language pre-training model (VLP) has shown revolutionary improvements on various vision-language (V+L) tasks, the studies regarding its adversarial robustness remain largely unexplored. This paper studied the adversarial attack on popular VLP models and V+L tasks. First, we analyzed the performance of adversarial attacks under different settings. By examining the influence of different perturbed objects and attack targets, we concluded some key observations as guidance on both designing strong multimodal adversarial attack and constructing robust VLP models. Second, we proposed a novel multimodal attack method on the VLP models called Collaborative Multimodal Adversarial Attack (Co-Attack), which collectively carries out the attacks on the image modality and the text modality. Experimental results demonstrated that the proposed method achieves improved attack performances on different V+L downstream tasks and VLP models. The analysis observations and novel attack method hopefully provide new understanding into the adversarial robustness of VLP models, so as to contribute their safe and reliable deployment in more real-world scenarios.},
	urldate = {2023-07-13},
	booktitle = {Proceedings of the 30th {ACM} {International} {Conference} on {Multimedia}},
	publisher = {Association for Computing Machinery},
	author = {Zhang, Jiaming and Yi, Qi and Sang, Jitao},
	month = oct,
	year = {2022},
	keywords = {adversarial attack, multimodal, vision-and-language pre-training},
	pages = {5005--5013},
}

@inproceedings{vaishnavi_transferring_2022,
	title = {Transferring {Adversarial} {Robustness} {Through} {Robust} {Representation} {Matching}},
	isbn = {978-1-939133-31-1},
	url = {https://www.usenix.org/conference/usenixsecurity22/presentation/vaishnavi},
	language = {en},
	urldate = {2023-07-13},
	author = {Vaishnavi, Pratik and Eykholt, Kevin and Rahmati, Amir},
	year = {2022},
	pages = {2083--2098},
}

@inproceedings{yucel_deep_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {A {Deep} {Dive} into {Adversarial} {Robustness} in {Zero}-{Shot} {Learning}},
	isbn = {978-3-030-66415-2},
	doi = {10.1007/978-3-030-66415-2_1},
	abstract = {Machine learning (ML) systems have introduced significant advances in various fields, due to the introduction of highly complex models. Despite their success, it has been shown multiple times that machine learning models are prone to imperceptible perturbations that can severely degrade their accuracy. So far, existing studies have primarily focused on models where supervision across all classes were available. In contrast, Zero-shot Learning (ZSL) and Generalized Zero-shot Learning (GZSL) tasks inherently lack supervision across all classes. In this paper, we present a study aimed on evaluating the adversarial robustness of ZSL and GZSL models. We leverage the well-established label embedding model and subject it to a set of established adversarial attacks and defenses across multiple datasets. In addition to creating possibly the first benchmark on adversarial robustness of ZSL models, we also present analyses on important points that require attention for better interpretation of ZSL robustness results. We hope these points, along with the benchmark, will help researchers establish a better understanding what challenges lie ahead and help guide their work.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2020 {Workshops}},
	publisher = {Springer International Publishing},
	author = {Yucel, Mehmet Kerim and Cinbis, Ramazan Gokberk and Duygulu, Pinar},
	editor = {Bartoli, Adrien and Fusiello, Andrea},
	year = {2020},
	pages = {3--21},
}

@inproceedings{grover_boosted_2018,
	title = {Boosted {Generative} {Models}},
	volume = {32},
	copyright = {Copyright (c)},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/11827},
	doi = {10.1609/aaai.v32i1.11827},
	abstract = {We propose a novel approach for using unsupervised boosting to create an ensemble of generative models, where models are trained in sequence to correct earlier mistakes. Our meta-algorithmic framework can leverage any existing base learner that permits likelihood evaluation, including recent deep expressive models. Further, our approach allows the ensemble to include discriminative models trained to distinguish real data from model-generated data. We show theoretical conditions under which incorporating a new model in the ensemble will improve the fit and empirically demonstrate the effectiveness of our black-box boosting algorithms on density estimation, classification, and sample generation on benchmark datasets for a wide range of generative models.},
	language = {en},
	urldate = {2023-07-01},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Grover, Aditya and Ermon, Stefano},
	month = apr,
	year = {2018},
	note = {Number: 1},
	keywords = {unsupervised learning},
}

@inproceedings{clarysse_why_2023,
	title = {Why adversarial training can hurt robust accuracy},
	url = {https://openreview.net/forum?id=-CA8yFkPc7O},
	abstract = {Machine learning classifiers with high test accuracy often perform poorly under adversarial attacks. It is commonly believed that adversarial training alleviates this issue. In this paper, we demonstrate that, surprisingly, the opposite can be true for a natural class of perceptible perturbations --- even though adversarial training helps when enough data is available, it may in fact hurt robust generalization in the small sample size regime. We first prove this phenomenon for a high-dimensional linear classification setting with noiseless observations. Using intuitive insights from the proof, we could surprisingly find perturbations on standard image datasets for which this behavior persists. Specifically, it occurs for perceptible attacks that effectively reduce class information such as object occlusions or corruptions.},
	language = {en},
	urldate = {2023-04-01},
	booktitle = {The {Eleventh} {International} {Conference} on {Learning} {Representations}},
	author = {Clarysse, Jacob and Hörrmann, Julia and Yang, Fanny},
	month = feb,
	year = {2023},
}

@inproceedings{croce_seasoning_2023,
	title = {Seasoning {Model} {Soups} for {Robustness} to {Adversarial} and {Natural} {Distribution} {Shifts}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Croce_Seasoning_Model_Soups_for_Robustness_to_Adversarial_and_Natural_Distribution_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-05-27},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Croce, Francesco and Rebuffi, Sylvestre-Alvise and Shelhamer, Evan and Gowal, Sven},
	year = {2023},
	pages = {12313--12323},
}

@article{li_imagenet-e_nodate,
	title = {{ImageNet}-{E}: {Benchmarking} {Neural} {Network} {Robustness} via {Attribute} {Editing}},
	language = {en},
	author = {Li, Xiaodan and Chen, Yuefeng and Zhu, Yao and Wang, Shuhui and Zhang, Rong and Xue, Hui},
}

@inproceedings{pang_bag_2021,
	title = {Bag of {Tricks} for {Adversarial} {Training}},
	url = {https://openreview.net/forum?id=Xb8xvrtB8Ce},
	abstract = {Adversarial training (AT) is one of the most effective strategies for promoting model robustness. However, recent benchmarks show that most of the proposed improvements on AT are less effective...},
	language = {en},
	urldate = {2021-02-23},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Pang, Tianyu and Yang, Xiao and Dong, Yinpeng and Su, Hang and Zhu, Jun},
	year = {2021},
}

@misc{yu_visual_2023,
	title = {Visual {Tuning}},
	url = {http://arxiv.org/abs/2305.06061},
	abstract = {Fine-tuning visual models has been widely shown promising performance on many downstream visual tasks. With the surprising development of pre-trained visual foundation models, visual tuning jumped out of the standard modus operandi that fine-tunes the whole pre-trained model or just the fully connected layer. Instead, recent advances can achieve superior performance than full-tuning the whole pre-trained parameters by updating far fewer parameters, enabling edge devices and downstream applications to reuse the increasingly large foundation models deployed on the cloud. With the aim of helping researchers get the full picture and future directions of visual tuning, this survey characterizes a large and thoughtful selection of recent works, providing a systematic and comprehensive overview of existing work and models. Specifically, it provides a detailed background of visual tuning and categorizes recent visual tuning techniques into five groups: prompt tuning, adapter tuning, parameter tuning, and remapping tuning. Meanwhile, it offers some exciting research directions for prospective pre-training and various interactions in visual tuning.},
	urldate = {2023-07-12},
	publisher = {arXiv},
	author = {Yu, Bruce X. B. and Chang, Jianlong and Wang, Haixin and Liu, Lingbo and Wang, Shijie and Wang, Zhiyu and Lin, Junfan and Xie, Lingxi and Li, Haojie and Lin, Zhouchen and Tian, Qi and Chen, Chang Wen},
	month = may,
	year = {2023},
	note = {arXiv:2305.06061 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{liu_intriguing_2023,
	title = {Intriguing {Properties} of {Text}-guided {Diffusion} {Models}},
	url = {http://arxiv.org/abs/2306.00974},
	doi = {10.48550/arXiv.2306.00974},
	abstract = {Text-guided diffusion models (TDMs) are widely applied but can fail unexpectedly. Common failures include: (i) natural-looking text prompts generating images with the wrong content, or (ii) different random samples of the latent variables that generate vastly different, and even unrelated, outputs despite being conditioned on the same text prompt. In this work, we aim to study and understand the failure modes of TDMs in more detail. To achieve this, we propose SAGE, an adversarial attack on TDMs that uses image classifiers as surrogate loss functions, to search over the discrete prompt space and the high-dimensional latent space of TDMs to automatically discover unexpected behaviors and failure cases in the image generation. We make several technical contributions to ensure that SAGE finds failure cases of the diffusion model, rather than the classifier, and verify this in a human study. Our study reveals four intriguing properties of TDMs that have not been systematically studied before: (1) We find a variety of natural text prompts producing images that fail to capture the semantics of input texts. We categorize these failures into ten distinct types based on the underlying causes. (2) We find samples in the latent space (which are not outliers) that lead to distorted images independent of the text prompt, suggesting that parts of the latent space are not well-structured. (3) We also find latent samples that lead to natural-looking images which are unrelated to the text prompt, implying a potential misalignment between the latent and prompt spaces. (4) By appending a single adversarial token embedding to an input prompt we can generate a variety of specified target objects, while only minimally affecting the CLIP score. This demonstrates the fragility of language representations and raises potential safety concerns.},
	urldate = {2023-07-12},
	publisher = {arXiv},
	author = {Liu, Qihao and Kortylewski, Adam and Bai, Yutong and Bai, Song and Yuille, Alan},
	month = jun,
	year = {2023},
	note = {arXiv:2306.00974 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{bar_visual_2022,
	title = {Visual {Prompting} via {Image} {Inpainting}},
	volume = {35},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/9f09f316a3eaf59d9ced5ffaefe97e0f-Abstract-Conference.html},
	language = {en},
	urldate = {2023-07-12},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Bar, Amir and Gandelsman, Yossi and Darrell, Trevor and Globerson, Amir and Efros, Alexei},
	month = dec,
	year = {2022},
	pages = {25005--25017},
}

@inproceedings{mao_understanding_2023,
	title = {Understanding {Zero}-shot {Adversarial} {Robustness} for {Large}-{Scale} {Models}},
	url = {https://openreview.net/forum?id=P4bXCawRi5J},
	abstract = {Pretrained large-scale vision-language models like CLIP have exhibited strong generalization over unseen tasks. Yet imperceptible adversarial perturbations can significantly reduce CLIP's performance on new tasks. In this work, we identify and explore the problem of adapting large-scale models for zero-shot adversarial robustness. We first identify two key factors during model adaption--training losses and adaptation methods--that affect the model's zero-shot adversarial robustness. We then propose a text-guided contrastive adversarial training loss, which aligns the text embeddings and the adversarial visual features with contrastive learning on a small set of training data. We apply this training loss to two adaption methods, model finetuning and visual prompt tuning. We find that visual prompt tuning is more effective in the absence of texts, while finetuning wins in the existence of text guidance. Overall, our approach significantly improves the zero-shot adversarial robustness over CLIP, seeing an average improvement of 31 points over ImageNet and 15 zero-shot datasets. We hope this work can shed light on understanding the zero-shot adversarial robustness of large-scale models.},
	language = {en},
	urldate = {2023-05-19},
	booktitle = {The {Eleventh} {International} {Conference} on {Learning} {Representations}},
	author = {Mao, Chengzhi and Geng, Scott and Yang, Junfeng and Wang, Xin and Vondrick, Carl},
	month = feb,
	year = {2023},
}

@inproceedings{dunlap_using_2023,
	title = {Using {Language} to {Extend} to {Unseen} {Domains}},
	url = {https://openreview.net/forum?id=eR2dG8yjnQ},
	abstract = {It is expensive to collect training data for every possible domain that a vision model may encounter when deployed. We instead consider how simply \${\textbackslash}textit\{verbalizing\}\$ the training domain (e.g.``photos of birds'') as well as domains we want to extend to but do not have data for (e.g.``paintings of birds'') can improve robustness. Using a multimodal model with a joint image and language embedding space, our method \${\textbackslash}textit\{LADS\}\$ learns a transformation of the image embeddings from the source domain to each target domain, while preserving task relevant information. Without using any images from the target domain, we show that over the \${\textbackslash}textit\{extended\}\$ domain containing both source and target, \${\textbackslash}textit\{LADS\}\$ outperforms standard fine-tuning and ensemble approaches over a suite of 4 benchmarks targeting domain adaptation and dataset bias.},
	language = {en},
	urldate = {2023-06-02},
	booktitle = {The {Eleventh} {International} {Conference} on {Learning} {Representations}},
	author = {Dunlap, Lisa and Mohri, Clara and Guillory, Devin and Zhang, Han and Darrell, Trevor and Gonzalez, Joseph E. and Raghunathan, Aditi and Rohrbach, Anna},
	month = feb,
	year = {2023},
}

@inproceedings{jia_visual_2022,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Visual {Prompt} {Tuning}},
	isbn = {978-3-031-19827-4},
	doi = {10.1007/978-3-031-19827-4_41},
	abstract = {The current modus operandi in adapting pre-trained models involves updating all the backbone parameters, i.e., full fine-tuning. This paper introduces Visual Prompt Tuning (VPT) as an efficient and effective alternative to full fine-tuning for large-scale Transformer models in vision. Taking inspiration from recent advances in efficiently tuning large language models, VPT introduces only a small amount (less than 1\% of model parameters) of trainable parameters in the input space while keeping the model backbone frozen. Via extensive experiments on a wide variety of downstream recognition tasks, we show that VPT achieves significant performance gains compared to other parameter efficient tuning protocols. Most importantly, VPT even outperforms full fine-tuning in many cases across model capacities and training data scales, while reducing per-task storage cost. Code is available at github.com/kmnp/vpt.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2022},
	publisher = {Springer Nature Switzerland},
	author = {Jia, Menglin and Tang, Luming and Chen, Bor-Chun and Cardie, Claire and Belongie, Serge and Hariharan, Bharath and Lim, Ser-Nam},
	editor = {Avidan, Shai and Brostow, Gabriel and Cissé, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
	year = {2022},
	pages = {709--727},
}

@inproceedings{ilyas_black-box_2018,
	title = {Black-box {Adversarial} {Attacks} with {Limited} {Queries} and {Information}},
	url = {https://proceedings.mlr.press/v80/ilyas18a.html},
	abstract = {Current neural network-based classifiers are susceptible to adversarial examples even in the black-box setting, where the attacker only has query access to the model. In practice, the threat model for real-world systems is often more restrictive than the typical black-box model where the adversary can observe the full output of the network on arbitrarily many chosen inputs. We define three realistic threat models that more accurately characterize many real-world classifiers: the query-limited setting, the partial-information setting, and the label-only setting. We develop new attacks that fool classifiers under these more restrictive threat models, where previous methods would be impractical or ineffective. We demonstrate that our methods are effective against an ImageNet classifier under our proposed threat models. We also demonstrate a targeted black-box attack against a commercial classifier, overcoming the challenges of limited query access, partial information, and other practical issues to break the Google Cloud Vision API.},
	language = {en},
	urldate = {2023-07-11},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Ilyas, Andrew and Engstrom, Logan and Athalye, Anish and Lin, Jessy},
	month = jul,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {2137--2146},
}

@inproceedings{croce_minimally_2020,
	title = {Minimally distorted {Adversarial} {Examples} with a {Fast} {Adaptive} {Boundary} {Attack}},
	url = {https://proceedings.mlr.press/v119/croce20a.html},
	abstract = {The evaluation of robustness against adversarial manipulation of neural networks-based classifiers is mainly tested with empirical attacks as methods for the exact computation, even when available, do not scale to large networks. We propose in this paper a new white-box adversarial attack wrt the lplpl\_p-norms for p∈\{1,2,∞\}p∈\{1,2,∞\}p {\textbackslash}in {\textbackslash}\{1,2,{\textbackslash}infty{\textbackslash}\} aiming at finding the minimal perturbation necessary to change the class of a given input. It has an intuitive geometric meaning, yields quickly high quality results, minimizes the size of the perturbation (so that it returns the robust accuracy at every threshold with a single run). It performs better or similar to state-of-the-art attacks which are partially specialized to one lplpl\_p-norm, and is robust to the phenomenon of gradient obfuscation.},
	language = {en},
	urldate = {2023-07-11},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Croce, Francesco and Hein, Matthias},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {2196--2205},
}

@inproceedings{vo_query_2022,
	title = {Query {Efficient} {Decision} {Based} {Sparse} {Attacks} {Against} {Black}-box {Deep} {Learning} {Models}},
	url = {https://openreview.net/forum?id=73MEhZ0anV},
	abstract = {Despite our best efforts, deep learning models remain highly vulnerable to even tiny adversarial perturbations applied to the inputs. The ability to extract information from solely the output of a...},
	language = {en},
	urldate = {2022-04-06},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Vo, Viet and Abbasnejad, Ehsan M. and Ranasinghe, Damith},
	year = {2022},
}

@inproceedings{papernot_practical_2017,
	address = {New York, NY, USA},
	series = {{ASIA} {CCS} '17},
	title = {Practical {Black}-{Box} {Attacks} against {Machine} {Learning}},
	isbn = {978-1-4503-4944-4},
	url = {https://dl.acm.org/doi/10.1145/3052973.3053009},
	doi = {10.1145/3052973.3053009},
	abstract = {Machine learning (ML) models, e.g., deep neural networks (DNNs), are vulnerable to adversarial examples: malicious inputs modified to yield erroneous model outputs, while appearing unmodified to human observers. Potential attacks include having malicious content like malware identified as legitimate or controlling vehicle behavior. Yet, all existing adversarial example attacks require knowledge of either the model internals or its training data. We introduce the first practical demonstration of an attacker controlling a remotely hosted DNN with no such knowledge. Indeed, the only capability of our black-box adversary is to observe labels given by the DNN to chosen inputs. Our attack strategy consists in training a local model to substitute for the target DNN, using inputs synthetically generated by an adversary and labeled by the target DNN. We use the local substitute to craft adversarial examples, and find that they are misclassified by the targeted DNN. To perform a real-world and properly-blinded evaluation, we attack a DNN hosted by MetaMind, an online deep learning API. We find that their DNN misclassifies 84.24\% of the adversarial examples crafted with our substitute. We demonstrate the general applicability of our strategy to many ML techniques by conducting the same attack against models hosted by Amazon and Google, using logistic regression substitutes. They yield adversarial examples misclassified by Amazon and Google at rates of 96.19\% and 88.94\%. We also find that this black-box attack strategy is capable of evading defense strategies previously found to make adversarial example crafting harder.},
	urldate = {2023-07-11},
	booktitle = {Proceedings of the 2017 {ACM} on {Asia} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {Association for Computing Machinery},
	author = {Papernot, Nicolas and McDaniel, Patrick and Goodfellow, Ian and Jha, Somesh and Celik, Z. Berkay and Swami, Ananthram},
	month = apr,
	year = {2017},
	keywords = {adversarial machine learning, black-box attack, machine learning},
	pages = {506--519},
}

@inproceedings{engstrom_identifying_2020,
	title = {Identifying {Statistical} {Bias} in {Dataset} {Replication}},
	url = {https://proceedings.mlr.press/v119/engstrom20a.html},
	abstract = {Dataset replication is a useful tool for assessing whether improvements in test accuracy on a specific benchmark correspond to improvements in models’ ability to generalize reliably. In this work, we present unintuitive yet significant ways in which standard approaches to dataset replication introduce statistical bias, skewing the resulting observations. We study ImageNet-v2, a replication of the ImageNet dataset on which models exhibit a significant (11-14\%) drop in accuracy, even after controlling for selection frequency, a human-in-the-loop measure of data quality. We show that after remeasuring selection frequencies and correcting for statistical bias, only an estimated 3.6\% of the original 11.7\% accuracy drop remains unaccounted for. We conclude with concrete recommendations for recognizing and avoiding bias in dataset replication. Code for our study is publicly available: https://git.io/data-rep-analysis.},
	language = {en},
	urldate = {2023-07-10},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Engstrom, Logan and Ilyas, Andrew and Santurkar, Shibani and Tsipras, Dimitris and Steinhardt, Jacob and Madry, Aleksander},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {2922--2932},
}

@inproceedings{sriramanan_towards_2021,
	title = {Towards {Efficient} and {Effective} {Adversarial} {Training}},
	url = {https://openreview.net/forum?id=kuK2VARZGnI},
	abstract = {We propose methods to improve the efficiency and effectiveness of Adversarial Training},
	language = {en},
	urldate = {2021-12-02},
	booktitle = {Thirty-{Fifth} {Conference} on {Neural} {Information} {Processing} {Systems}},
	author = {Sriramanan, Gaurang and Addepalli, Sravanti and Baburaj, Arya and Radhakrishnan, Venkatesh Babu},
	month = may,
	year = {2021},
}

@inproceedings{hu_naturalistic_2021,
	title = {Naturalistic {Physical} {Adversarial} {Patch} for {Object} {Detectors}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Hu_Naturalistic_Physical_Adversarial_Patch_for_Object_Detectors_ICCV_2021_paper.html},
	language = {en},
	urldate = {2023-07-09},
	author = {Hu, Yu-Chih-Tuan and Kung, Bo-Han and Tan, Daniel Stanley and Chen, Jun-Cheng and Hua, Kai-Lung and Cheng, Wen-Huang},
	year = {2021},
	pages = {7848--7857},
}

@inproceedings{hu_adversarial_2022,
	title = {Adversarial {Texture} for {Fooling} {Person} {Detectors} in the {Physical} {World}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Hu_Adversarial_Texture_for_Fooling_Person_Detectors_in_the_Physical_World_CVPR_2022_paper.html},
	language = {en},
	urldate = {2023-07-09},
	author = {Hu, Zhanhao and Huang, Siyuan and Zhu, Xiaopei and Sun, Fuchun and Zhang, Bo and Hu, Xiaolin},
	year = {2022},
	pages = {13307--13316},
}

@inproceedings{cheng_physical_2022,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Physical {Attack} on {Monocular} {Depth} {Estimation} with {Optimal} {Adversarial} {Patches}},
	isbn = {978-3-031-19839-7},
	doi = {10.1007/978-3-031-19839-7_30},
	abstract = {Deep learning has substantially boosted the performance of Monocular Depth Estimation (MDE), a critical component in fully vision-based autonomous driving (AD) systems (e.g., Tesla and Toyota). In this work, we develop an attack against learning-based MDE. In particular, we use an optimization-based method to systematically generate stealthy physical-object-oriented adversarial patches to attack depth estimation. We balance the stealth and effectiveness of our attack with object-oriented adversarial design, sensitive region localization, and natural style camouflage. Using real-world driving scenarios, we evaluate our attack on concurrent MDE models and a representative downstream task for AD (i.e., 3D object detection). Experimental results show that our method can generate stealthy, effective, and robust adversarial patches for different target objects and models and achieves more than 6 m mean depth estimation error and 93\% attack success rate (ASR) in object detection with a patch of 1/9 of the vehicle’s rear area. Field tests on three different driving routes with a real vehicle indicate that we cause over 6 m mean depth estimation error and reduce the object detection rate from 90.70\% to 5.16\% in continuous video frames.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2022},
	publisher = {Springer Nature Switzerland},
	author = {Cheng, Zhiyuan and Liang, James and Choi, Hongjun and Tao, Guanhong and Cao, Zhiwen and Liu, Dongfang and Zhang, Xiangyu},
	editor = {Avidan, Shai and Brostow, Gabriel and Cissé, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
	year = {2022},
	keywords = {Autonomous driving, Monocular depth estimation, Physical adversarial attack},
	pages = {514--532},
}

@inproceedings{tramer_adaptive_2020,
	title = {On {Adaptive} {Attacks} to {Adversarial} {Example} {Defenses}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/11f38f8ecd71867b42433548d1078e38-Abstract.html},
	abstract = {Adaptive attacks have (rightfully) become the de facto standard for evaluating defenses to adversarial examples. We find, however, that typical adaptive evaluations are incomplete.
We demonstrate that 13 defenses recently published at ICLR, ICML and NeurIPS---and which illustrate a diverse set of defense strategies---can be circumvented despite attempting to perform evaluations using adaptive attacks.},
	urldate = {2023-07-08},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Tramer, Florian and Carlini, Nicholas and Brendel, Wieland and Madry, Aleksander},
	year = {2020},
	pages = {1633--1645},
}

@inproceedings{siva_kumar_adversarial_2020,
	title = {Adversarial {Machine} {Learning}-{Industry} {Perspectives}},
	doi = {10.1109/SPW50608.2020.00028},
	abstract = {Based on interviews with 28 organizations, we found that industry practitioners are not equipped with tactical and strategic tools to protect, detect and respond to attacks on their Machine Learning (ML) systems. We leverage the insights from the interviews and enumerate the gaps in securing machine learning systems when viewed in the context of traditional software security development. We write this paper from the perspective of two personas: developers/ML engineers and security incident responders. The goal of this paper is to layout the research agenda to amend the Security Development Lifecycle for industrial-grade software in the adversarial ML era.},
	booktitle = {2020 {IEEE} {Security} and {Privacy} {Workshops} ({SPW})},
	author = {Siva Kumar, Ram Shankar and Nyström, Magnus and Lambert, John and Marshall, Andrew and Goertzel, Mario and Comissoneru, Andi and Swann, Matt and Xia, Sharon},
	month = may,
	year = {2020},
	keywords = {Interviews, Machine learning, Organizations, Security, Software, Software reliability, Tools, adversarial machine learning, engineering, software security},
	pages = {69--75},
}

@article{li_survey_2023,
	title = {A {Survey} on {Federated} {Learning} {Systems}: {Vision}, {Hype} and {Reality} for {Data} {Privacy} and {Protection}},
	volume = {35},
	issn = {1558-2191},
	shorttitle = {A {Survey} on {Federated} {Learning} {Systems}},
	doi = {10.1109/TKDE.2021.3124599},
	abstract = {As data privacy increasingly becomes a critical societal concern, federated learning has been a hot research topic in enabling the collaborative training of machine learning models among different organizations under the privacy restrictions. As researchers try to support more machine learning models with different privacy-preserving approaches, there is a requirement in developing systems and infrastructures to ease the development of various federated learning algorithms. Similar to deep learning systems such as PyTorch and TensorFlow that boost the development of deep learning, federated learning systems (FLSs) are equivalently important, and face challenges from various aspects such as effectiveness, efficiency, and privacy. In this survey, we conduct a comprehensive review on federated learning systems. To understand the key design system components and guide future research, we introduce the definition of federated learning systems and analyze the system components. Moreover, we provide a thorough categorization for federated learning systems according to six different aspects, including data distribution, machine learning model, privacy mechanism, communication architecture, scale of federation and motivation of federation. The categorization can help the design of federated learning systems as shown in our case studies. By systematically summarizing the existing federated learning systems, we present the design factors, case studies, and future research opportunities.},
	number = {4},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Li, Qinbin and Wen, Zeyi and Wu, Zhaomin and Hu, Sixu and Wang, Naibo and Li, Yuan and Liu, Xu and He, Bingsheng},
	month = apr,
	year = {2023},
	note = {Conference Name: IEEE Transactions on Knowledge and Data Engineering},
	keywords = {Collaborative work, Computational modeling, Data models, Data privacy, Deep learning, Federated learning, Machine learning, Servers, data mining, machine learning, survey},
	pages = {3347--3366},
}

@inproceedings{siddiqui_metadata_2023,
	title = {Metadata {Archaeology}: {Unearthing} {Data} {Subsets} by {Leveraging} {Training} {Dynamics}},
	shorttitle = {Metadata {Archaeology}},
	url = {https://openreview.net/forum?id=PvLnIaJbt9},
	abstract = {Modern machine learning research relies on relatively few carefully curated datasets. Even in these datasets, and typically in `untidy' or raw data, practitioners are faced with significant issues of data quality and diversity which can be prohibitively labor intensive to address. Existing methods for dealing with these challenges tend to make strong assumptions about the particular issues at play, and often require a priori knowledge or metadata such as domain labels. Our work is orthogonal to these methods: we instead focus on providing a unified and efficient framework for Metadata Archaeology -- uncovering and inferring metadata of examples in a dataset. We curate different subsets of data that might exist in a dataset (e.g. mislabeled, atypical, or out-of-distribution examples) using simple transformations, and leverage differences in learning dynamics between these probe suites to infer metadata of interest. Our method is on par with far more sophisticated mitigation methods across different tasks: identifying and correcting mislabeled examples, classifying minority-group samples, prioritizing points relevant for training and enabling scalable human auditing of relevant examples.},
	language = {en},
	urldate = {2023-05-16},
	booktitle = {The {Eleventh} {International} {Conference} on {Learning} {Representations}},
	author = {Siddiqui, Shoaib Ahmed and Rajkumar, Nitarshan and Maharaj, Tegan and Krueger, David and Hooker, Sara},
	month = feb,
	year = {2023},
}

@misc{wang_review_2023,
	title = {Review of {Large} {Vision} {Models} and {Visual} {Prompt} {Engineering}},
	url = {http://arxiv.org/abs/2307.00855},
	doi = {10.48550/arXiv.2307.00855},
	abstract = {Visual prompt engineering is a fundamental technology in the field of visual and image Artificial General Intelligence, serving as a key component for achieving zero-shot capabilities. As the development of large vision models progresses, the importance of prompt engineering becomes increasingly evident. Designing suitable prompts for specific visual tasks has emerged as a meaningful research direction. This review aims to summarize the methods employed in the computer vision domain for large vision models and visual prompt engineering, exploring the latest advancements in visual prompt engineering. We present influential large models in the visual domain and a range of prompt engineering methods employed on these models. It is our hope that this review provides a comprehensive and systematic description of prompt engineering methods based on large visual models, offering valuable insights for future researchers in their exploration of this field.},
	urldate = {2023-07-07},
	publisher = {arXiv},
	author = {Wang, Jiaqi and Liu, Zhengliang and Zhao, Lin and Wu, Zihao and Ma, Chong and Yu, Sigang and Dai, Haixing and Yang, Qiushi and Liu, Yiheng and Zhang, Songyao and Shi, Enze and Pan, Yi and Zhang, Tuo and Zhu, Dajiang and Li, Xiang and Jiang, Xi and Ge, Bao and Yuan, Yixuan and Shen, Dinggang and Liu, Tianming and Zhang, Shu},
	month = jul,
	year = {2023},
	note = {arXiv:2307.00855 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{amodei_concrete_2016,
	title = {Concrete {Problems} in {AI} {Safety}},
	url = {http://arxiv.org/abs/1606.06565},
	doi = {10.48550/arXiv.1606.06565},
	abstract = {Rapid progress in machine learning and artificial intelligence (AI) has brought increasing attention to the potential impacts of AI technologies on society. In this paper we discuss one such potential impact: the problem of accidents in machine learning systems, defined as unintended and harmful behavior that may emerge from poor design of real-world AI systems. We present a list of five practical research problems related to accident risk, categorized according to whether the problem originates from having the wrong objective function ("avoiding side effects" and "avoiding reward hacking"), an objective function that is too expensive to evaluate frequently ("scalable supervision"), or undesirable behavior during the learning process ("safe exploration" and "distributional shift"). We review previous work in these areas as well as suggesting research directions with a focus on relevance to cutting-edge AI systems. Finally, we consider the high-level question of how to think most productively about the safety of forward-looking applications of AI.},
	urldate = {2023-07-07},
	publisher = {arXiv},
	author = {Amodei, Dario and Olah, Chris and Steinhardt, Jacob and Christiano, Paul and Schulman, John and Mané, Dan},
	month = jul,
	year = {2016},
	note = {arXiv:1606.06565 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@inproceedings{rawat_devil_2022,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {The {Devil} {Is} in the {GAN}: {Backdoor} {Attacks} and {Defenses} in {Deep} {Generative} {Models}},
	isbn = {978-3-031-17143-7},
	shorttitle = {The {Devil} {Is} in the {GAN}},
	doi = {10.1007/978-3-031-17143-7_41},
	abstract = {Deep Generative Models (DGMs) are a popular class of models which find widespread use because of their ability to synthesise data from complex, high-dimensional manifolds. However, even with their increasing industrial adoption, they have not been subject to rigorous security analysis. In this work we examine backdoor attacks on DGMs which can significantly limit their applicability within a model supply chain and cause massive reputation damage for companies outsourcing DGMs form third parties. DGMs are vastly different from their discriminative counterparts and manifestation of attacks in DGMs is largely understudied. To this end we propose three novel training-time backdoor attacks which require modest computation effort but are highly effective. Furthermore, we demonstrate their effectiveness on large-scale industry-grade models across two different domains - images (StyleGAN) and audio (WaveGAN). Finally, we present an insightful discussion and prescribe a practical and comprehensive defense strategy for safe usage of DGMs.},
	language = {en},
	booktitle = {Computer {Security} – {ESORICS} 2022},
	publisher = {Springer Nature Switzerland},
	author = {Rawat, Ambrish and Levacher, Killian and Sinn, Mathieu},
	editor = {Atluri, Vijayalakshmi and Di Pietro, Roberto and Jensen, Christian D. and Meng, Weizhi},
	year = {2022},
	pages = {776--783},
}

@inproceedings{zhong_shadows_2022,
	title = {Shadows {Can} {Be} {Dangerous}: {Stealthy} and {Effective} {Physical}-{World} {Adversarial} {Attack} by {Natural} {Phenomenon}},
	shorttitle = {Shadows {Can} {Be} {Dangerous}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Zhong_Shadows_Can_Be_Dangerous_Stealthy_and_Effective_Physical-World_Adversarial_Attack_CVPR_2022_paper.html},
	language = {en},
	urldate = {2023-07-06},
	author = {Zhong, Yiqi and Liu, Xianming and Zhai, Deming and Jiang, Junjun and Ji, Xiangyang},
	year = {2022},
	pages = {15345--15354},
}

@misc{shumailov_curse_2023,
	title = {The {Curse} of {Recursion}: {Training} on {Generated} {Data} {Makes} {Models} {Forget}},
	shorttitle = {The {Curse} of {Recursion}},
	url = {http://arxiv.org/abs/2305.17493},
	abstract = {Stable Diffusion revolutionised image creation from descriptive text. GPT-2, GPT-3(.5) and GPT-4 demonstrated astonishing performance across a variety of language tasks. ChatGPT introduced such language models to the general public. It is now clear that large language models (LLMs) are here to stay, and will bring about drastic change in the whole ecosystem of online text and images. In this paper we consider what the future might hold. What will happen to GPT-\{n\} once LLMs contribute much of the language found online? We find that use of model-generated content in training causes irreversible defects in the resulting models, where tails of the original content distribution disappear. We refer to this effect as Model Collapse and show that it can occur in Variational Autoencoders, Gaussian Mixture Models and LLMs. We build theoretical intuition behind the phenomenon and portray its ubiquity amongst all learned generative models. We demonstrate that it has to be taken seriously if we are to sustain the benefits of training from large-scale data scraped from the web. Indeed, the value of data collected about genuine human interactions with systems will be increasingly valuable in the presence of content generated by LLMs in data crawled from the Internet.},
	urldate = {2023-07-06},
	publisher = {arXiv},
	author = {Shumailov, Ilia and Shumaylov, Zakhar and Zhao, Yiren and Gal, Yarin and Papernot, Nicolas and Anderson, Ross},
	month = may,
	year = {2023},
	note = {arXiv:2305.17493 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@inproceedings{rebuffi_revisiting_2023,
	title = {Revisiting adapters with adversarial training},
	url = {https://openreview.net/forum?id=HPdxC1THU8T},
	abstract = {While adversarial training is generally used as a defense mechanism, recent works show that it can also act as a regularizer. By co-training a neural network on clean and adversarial inputs, it is possible to improve classification accuracy on the clean, non-adversarial inputs. We demonstrate that, contrary to previous findings, it is not necessary to separate batch statistics when co-training on clean and adversarial inputs, and that it is sufficient to use adapters with few domain-specific parameters for each type of input. We establish that using the classification token of a Vision Transformer (ViT) as an adapter is enough to match the classification performance of dual normalization layers, while using significantly less additional parameters. First, we improve upon the top-1 accuracy of a non-adversarially trained ViT-B16 model by +1.12\% on ImageNet (reaching 83.76\% top-1 accuracy). Second, and more importantly, we show that training with adapters enables model soups through linear combinations of the clean and adversarial tokens. These model soups, which we call adversarial model soups, allow us to trade-off between clean and robust accuracy without sacrificing efficiency. Finally, we show that we can easily adapt the resulting models in the face of distribution shifts. Our ViT-B16 obtains top-1 accuracies on ImageNet variants that are on average +4.00\% better than those obtained with Masked Autoencoders.},
	language = {en},
	urldate = {2023-06-07},
	booktitle = {The {Eleventh} {International} {Conference} on {Learning} {Representations}},
	author = {Rebuffi, Sylvestre-Alvise and Croce, Francesco and Gowal, Sven},
	month = feb,
	year = {2023},
}

@misc{carlini_are_2023,
	title = {Are aligned neural networks adversarially aligned?},
	url = {http://arxiv.org/abs/2306.15447},
	doi = {10.48550/arXiv.2306.15447},
	abstract = {Large language models are now tuned to align with the goals of their creators, namely to be "helpful and harmless." These models should respond helpfully to user questions, but refuse to answer requests that could cause harm. However, adversarial users can construct inputs which circumvent attempts at alignment. In this work, we study to what extent these models remain aligned, even when interacting with an adversarial user who constructs worst-case inputs (adversarial examples). These inputs are designed to cause the model to emit harmful content that would otherwise be prohibited. We show that existing NLP-based optimization attacks are insufficiently powerful to reliably attack aligned text models: even when current NLP-based attacks fail, we can find adversarial inputs with brute force. As a result, the failure of current attacks should not be seen as proof that aligned text models remain aligned under adversarial inputs. However the recent trend in large-scale ML models is multimodal models that allow users to provide images that influence the text that is generated. We show these models can be easily attacked, i.e., induced to perform arbitrary un-aligned behavior through adversarial perturbation of the input image. We conjecture that improved NLP attacks may demonstrate this same level of adversarial control over text-only models.},
	urldate = {2023-07-05},
	publisher = {arXiv},
	author = {Carlini, Nicholas and Nasr, Milad and Choquette-Choo, Christopher A. and Jagielski, Matthew and Gao, Irena and Awadalla, Anas and Koh, Pang Wei and Ippolito, Daphne and Lee, Katherine and Tramer, Florian and Schmidt, Ludwig},
	month = jun,
	year = {2023},
	note = {arXiv:2306.15447 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@article{yang_hyperparameter_2020,
	title = {On hyperparameter optimization of machine learning algorithms: {Theory} and practice},
	volume = {415},
	issn = {0925-2312},
	shorttitle = {On hyperparameter optimization of machine learning algorithms},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231220311693},
	doi = {10.1016/j.neucom.2020.07.061},
	abstract = {Machine learning algorithms have been used widely in various applications and areas. To fit a machine learning model into different problems, its hyper-parameters must be tuned. Selecting the best hyper-parameter configuration for machine learning models has a direct impact on the model’s performance. It often requires deep knowledge of machine learning algorithms and appropriate hyper-parameter optimization techniques. Although several automatic optimization techniques exist, they have different strengths and drawbacks when applied to different types of problems. In this paper, optimizing the hyper-parameters of common machine learning models is studied. We introduce several state-of-the-art optimization techniques and discuss how to apply them to machine learning algorithms. Many available libraries and frameworks developed for hyper-parameter optimization problems are provided, and some open challenges of hyper-parameter optimization research are also discussed in this paper. Moreover, experiments are conducted on benchmark datasets to compare the performance of different optimization methods and provide practical examples of hyper-parameter optimization. This survey paper will help industrial users, data analysts, and researchers to better develop machine learning models by identifying the proper hyper-parameter configurations effectively.},
	language = {en},
	urldate = {2023-07-05},
	journal = {Neurocomputing},
	author = {Yang, Li and Shami, Abdallah},
	month = nov,
	year = {2020},
	keywords = {Bayesian optimization, Genetic algorithm, Grid search, Hyper-parameter optimization, Machine learning, Particle swarm optimization},
	pages = {295--316},
}

@article{morales-hernandez_survey_2023,
	title = {A survey on multi-objective hyperparameter optimization algorithms for machine learning},
	volume = {56},
	issn = {1573-7462},
	url = {https://doi.org/10.1007/s10462-022-10359-2},
	doi = {10.1007/s10462-022-10359-2},
	abstract = {Hyperparameter optimization (HPO) is a necessary step to ensure the best possible performance of Machine Learning (ML) algorithms. Several methods have been developed to perform HPO; most of these are focused on optimizing one performance measure (usually an error-based measure), and the literature on such single-objective HPO problems is vast. Recently, though, algorithms have appeared that focus on optimizing multiple conflicting objectives simultaneously. This article presents a systematic survey of the literature published between 2014 and 2020 on multi-objective HPO algorithms, distinguishing between metaheuristic-based algorithms, metamodel-based algorithms and approaches using a mixture of both. We also discuss the quality metrics used to compare multi-objective HPO procedures and present future research directions.},
	language = {en},
	number = {8},
	urldate = {2023-07-05},
	journal = {Artificial Intelligence Review},
	author = {Morales-Hernández, Alejandro and Van Nieuwenhuyse, Inneke and Rojas Gonzalez, Sebastian},
	month = aug,
	year = {2023},
	keywords = {Hyperparameter optimization, Machine learning, Meta-heuristic, Metamodel, Multi-objective optimization},
	pages = {8043--8093},
}

@article{bischl_hyperparameter_2023,
	title = {Hyperparameter optimization: {Foundations}, algorithms, best practices, and open challenges},
	volume = {13},
	issn = {1942-4795},
	shorttitle = {Hyperparameter optimization},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/widm.1484},
	doi = {10.1002/widm.1484},
	abstract = {Most machine learning algorithms are configured by a set of hyperparameters whose values must be carefully chosen and which often considerably impact performance. To avoid a time-consuming and irreproducible manual process of trial-and-error to find well-performing hyperparameter configurations, various automatic hyperparameter optimization (HPO) methods—for example, based on resampling error estimation for supervised machine learning—can be employed. After introducing HPO from a general perspective, this paper reviews important HPO methods, from simple techniques such as grid or random search to more advanced methods like evolution strategies, Bayesian optimization, Hyperband, and racing. This work gives practical recommendations regarding important choices to be made when conducting HPO, including the HPO algorithms themselves, performance evaluation, how to combine HPO with machine learning pipelines, runtime improvements, and parallelization. This article is categorized under: Algorithmic Development {\textgreater} Statistics Technologies {\textgreater} Machine Learning Technologies {\textgreater} Prediction},
	language = {en},
	number = {2},
	urldate = {2023-07-05},
	journal = {WIREs Data Mining and Knowledge Discovery},
	author = {Bischl, Bernd and Binder, Martin and Lang, Michel and Pielok, Tobias and Richter, Jakob and Coors, Stefan and Thomas, Janek and Ullmann, Theresa and Becker, Marc and Boulesteix, Anne-Laure and Deng, Difan and Lindauer, Marius},
	year = {2023},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/widm.1484},
	keywords = {automl, hyperparameter optimization, machine learning, model selection, tuning},
	pages = {e1484},
}

@article{zhang_memorization_2023,
	title = {Memorization {Weights} for {Instance} {Reweighting} in {Adversarial} {Training}},
	volume = {37},
	copyright = {Copyright (c) 2023 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/26329},
	doi = {10.1609/aaai.v37i9.26329},
	abstract = {Adversarial training is an effective way to defend deep neural networks (DNN) against adversarial examples. However, there are atypical samples that are rare and hard to learn, or even hurt DNNs' generalization performance on test data. In this paper, we propose a novel algorithm to reweight the training samples based on self-supervised techniques to mitigate the negative effects of the atypical samples. 
Specifically, a memory bank is built to record the popular samples as prototypes and calculate the memorization weight for each sample, evaluating the "typicalness" of a sample. All the training samples are reweigthed based on the proposed memorization weights to reduce the negative effects of atypical samples. Experimental results show the proposed method is flexible to boost state-of-the-art adversarial training methods, improving both robustness and standard accuracy of DNNs.},
	language = {en},
	number = {9},
	urldate = {2023-07-02},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Zhang, Jianfu and Hong, Yan and Zhao, Qibin},
	month = jun,
	year = {2023},
	note = {Number: 9},
	keywords = {ML: Adversarial Learning \& Robustness},
	pages = {11228--11236},
}

@inproceedings{karras_analyzing_2020,
	title = {Analyzing and {Improving} the {Image} {Quality} of {StyleGAN}},
	url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Karras_Analyzing_and_Improving_the_Image_Quality_of_StyleGAN_CVPR_2020_paper.html},
	urldate = {2023-07-01},
	author = {Karras, Tero and Laine, Samuli and Aittala, Miika and Hellsten, Janne and Lehtinen, Jaakko and Aila, Timo},
	year = {2020},
	pages = {8110--8119},
}

@inproceedings{karras_style-based_2019,
	title = {A {Style}-{Based} {Generator} {Architecture} for {Generative} {Adversarial} {Networks}},
	url = {https://openaccess.thecvf.com/content_CVPR_2019/html/Karras_A_Style-Based_Generator_Architecture_for_Generative_Adversarial_Networks_CVPR_2019_paper.html},
	urldate = {2023-07-01},
	author = {Karras, Tero and Laine, Samuli and Aila, Timo},
	year = {2019},
	pages = {4401--4410},
}

@inproceedings{brock_large_2018,
	title = {Large {Scale} {GAN} {Training} for {High} {Fidelity} {Natural} {Image} {Synthesis}},
	url = {https://openreview.net/forum?id=B1xsqj09Fm&noteId=HklmZ1xqhm&utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter},
	abstract = {Despite recent progress in generative image modeling, successfully generating high-resolution, diverse samples from complex datasets such as ImageNet remains an elusive goal. To this end, we train Generative Adversarial Networks at the largest scale yet attempted, and study the instabilities specific to such scale. We find that applying orthogonal regularization to the generator renders it amenable to a simple "truncation trick", allowing fine control over the trade-off between sample fidelity and variety by reducing the variance of the Generator's input. Our modifications lead to models which set the new state of the art in class-conditional image synthesis. When trained on ImageNet at 128x128 resolution, our models (BigGANs) achieve an Inception Score (IS) of 166.3 and Frechet Inception Distance (FID) of 9.6, improving over the previous best IS of 52.52 and FID of 18.65.},
	language = {en},
	urldate = {2023-07-01},
	author = {Brock, Andrew and Donahue, Jeff and Simonyan, Karen},
	month = sep,
	year = {2018},
}

@inproceedings{che_your_2020,
	title = {Your {GAN} is {Secretly} an {Energy}-based {Model} and {You} {Should} {Use} {Discriminator} {Driven} {Latent} {Sampling}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/90525e70b7842930586545c6f1c9310c-Abstract.html},
	urldate = {2023-07-01},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Che, Tong and ZHANG, Ruixiang and Sohl-Dickstein, Jascha and Larochelle, Hugo and Paull, Liam and Cao, Yuan and Bengio, Yoshua},
	year = {2020},
	pages = {12275--12287},
}

@article{ding_subsampling_2020,
	title = {Subsampling {Generative} {Adversarial} {Networks}: {Density} {Ratio} {Estimation} in {Feature} {Space} {With} {Softplus} {Loss}},
	volume = {68},
	issn = {1941-0476},
	shorttitle = {Subsampling {Generative} {Adversarial} {Networks}},
	doi = {10.1109/TSP.2020.2979601},
	abstract = {Filtering out unrealistic images from trained generative adversarial networks (GANs) has attracted considerable attention recently. Two density ratio based subsampling methods-Discriminator Rejection Sampling (DRS) and Metropolis-Hastings GAN (MH-GAN)-were recently proposed, and their effectiveness in improving GANs was demonstrated on multiple datasets. However, DRS and MH-GAN are based on discriminator-based density ratio estimation (DRE) methods, so they may not work well if the discriminator in the trained GAN is far from optimal. Moreover, they do not apply to some GANs (e.g., MMD-GAN). In this paper, we propose a novel Softplus (SP) loss for DRE. Based on it, we develop a sample-based DRE method in a feature space learned by a specially designed and pre-trained ResNet-34, termed DRE-F-SP. We derive the rate of convergence of a density ratio model trained under the SP loss. Then, we propose three density ratio based subsampling methods for GANs based on DRE-F-SP. Our subsampling methods do not rely on the optimality of the discriminator and are suitable for all types of GANs. We empirically show our subsampling approach can substantially outperform DRS and MH-GAN on a synthetic dataset, CIFAR-10, MNIST and CelebA, using multiple GANs.},
	journal = {IEEE Transactions on Signal Processing},
	author = {Ding, Xin and Wang, Z. Jane and Welch, William J.},
	year = {2020},
	note = {Conference Name: IEEE Transactions on Signal Processing},
	keywords = {Estimation, Gallium nitride, Generative adversarial networks, Generators, Markov processes, Neural networks, Training, density ratio estimation, subsampling GANs},
	pages = {1910--1922},
}

@inproceedings{azadi_discriminator_2018,
	title = {Discriminator {Rejection} {Sampling}},
	url = {https://openreview.net/forum?id=S1GkToR5tm&source=post_page---------------------------},
	abstract = {We propose a rejection sampling scheme using the discriminator of a GAN to approximately correct errors in the GAN generator distribution. We show that under quite strict assumptions, this will allow us to recover the data distribution exactly. We then examine where those strict assumptions break down and design a practical algorithm—called Discriminator Rejection Sampling (DRS)—that can be used on real data-sets. Finally, we demonstrate the efficacy of DRS on a mixture of Gaussians and on the state of the art SAGAN model. On ImageNet, we train an improved baseline that increases the best published Inception Score from 52.52 to 62.36 and reduces the Frechet Inception Distance from 18.65 to 14.79. We then use DRS to further improve on this baseline, improving the Inception Score to 76.08 and the FID to 13.75.},
	language = {en},
	urldate = {2023-07-01},
	author = {Azadi, Samaneh and Olsson, Catherine and Darrell, Trevor and Goodfellow, Ian and Odena, Augustus},
	month = sep,
	year = {2018},
}

@article{olvera-lopez_review_2010,
	title = {A review of instance selection methods},
	volume = {34},
	issn = {1573-7462},
	url = {https://doi.org/10.1007/s10462-010-9165-y},
	doi = {10.1007/s10462-010-9165-y},
	abstract = {In supervised learning, a training set providing previously known information is used to classify new instances. Commonly, several instances are stored in the training set but some of them are not useful for classifying therefore it is possible to get acceptable classification rates ignoring non useful cases; this process is known as instance selection. Through instance selection the training set is reduced which allows reducing runtimes in the classification and/or training stages of classifiers. This work is focused on presenting a survey of the main instance selection methods reported in the literature.},
	language = {en},
	number = {2},
	urldate = {2023-07-01},
	journal = {Artificial Intelligence Review},
	author = {Olvera-López, J. Arturo and Carrasco-Ochoa, J. Ariel and Martínez-Trinidad, J. Francisco and Kittler, Josef},
	month = aug,
	year = {2010},
	keywords = {Data reduction, Instance selection, Pre-processing, Supervised learning},
	pages = {133--143},
}

@inproceedings{sehwag_generating_2022,
	title = {Generating {High} {Fidelity} {Data} {From} {Low}-{Density} {Regions} {Using} {Diffusion} {Models}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Sehwag_Generating_High_Fidelity_Data_From_Low-Density_Regions_Using_Diffusion_Models_CVPR_2022_paper.html},
	language = {en},
	urldate = {2023-06-23},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Sehwag, Vikash and Hazirbas, Caner and Gordo, Albert and Ozgenel, Firat and Canton, Cristian},
	year = {2022},
	pages = {11492--11501},
}

@inproceedings{chong_effectively_2020,
	title = {Effectively {Unbiased} {FID} and {Inception} {Score} and {Where} to {Find} {Them}},
	url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Chong_Effectively_Unbiased_FID_and_Inception_Score_and_Where_to_Find_CVPR_2020_paper.html},
	urldate = {2023-05-29},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Chong, Min Jin and Forsyth, David},
	year = {2020},
	pages = {6070--6079},
}

@inproceedings{ravuri_seeing_2019,
	title = {Seeing is {Not} {Necessarily} {Believing}: {Limitations} of {BigGANs} for {Data} {Augmentation}},
	shorttitle = {Seeing is {Not} {Necessarily} {Believing}},
	url = {https://openreview.net/forum?id=rJMw747l_4},
	abstract = {Recent advances in Generative Adversarial Networks (GANs) – in architectural design, training strategies, and empirical tricks – have led nearly photorealistic samples on large-scale datasets such as ImageNet. In fact, for one model in particular, BigGAN, metrics such as Inception Score or Frechet Inception Distance nearly match those of the dataset, suggesting that these models are close to match-ing the distribution of the training set. Given the quality of these models, it is worth understanding to what extent these samples can be used for data augmentation, a task expressed as a long-term goal of the GAN research project. To that end, we train ResNet-50 classifiers using either purely BigGAN images or mixtures of ImageNet and BigGAN images, and test on the ImageNet validation set.Our preliminary results suggest both a measured view of state-of-the-art GAN quality and highlight limitations of current metrics. Using only BigGAN images, we find that Top-1 and Top-5 error increased by 120\% and 384\%, respectively, and furthermore, adding more BigGAN data to the ImageNet training set at best only marginally improves classifier performance. Finally, we find that neither Inception Score, nor FID, nor combinations thereof are predictive of classification accuracy. These results suggest that as GANs are beginning to be deployed in downstream tasks, we should create metrics that better measure downstream task performance. We propose classification performance as one such metric that, in addition to assessing per-class sample quality, is more suited to such downstream tasks.},
	language = {en},
	urldate = {2023-07-01},
	booktitle = {International {Conference} on {Learning} {Representations} {Workshop}},
	author = {Ravuri, Suman and Vinyals, Oriol},
	month = mar,
	year = {2019},
}

@inproceedings{shmelkov_how_2018,
	title = {How good is my {GAN}?},
	url = {https://openaccess.thecvf.com/content_ECCV_2018/html/Konstantin_Shmelkov_How_good_is_ECCV_2018_paper.html},
	urldate = {2023-07-01},
	author = {Shmelkov, Konstantin and Schmid, Cordelia and Alahari, Karteek},
	year = {2018},
	pages = {213--229},
}

@inproceedings{ravuri_classification_2019,
	title = {Classification {Accuracy} {Score} for {Conditional} {Generative} {Models}},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper_files/paper/2019/hash/fcf55a303b71b84d326fb1d06e332a26-Abstract.html},
	abstract = {Deep generative models (DGMs) of images are now sufficiently mature that they produce nearly photorealistic samples and obtain scores similar to the data distribution on heuristics such as Frechet Inception Distance (FID). These results, especially on large-scale datasets such as ImageNet, suggest that DGMs are learning the data distribution in a perceptually meaningful space and can be used in downstream tasks. To test this latter hypothesis, we use class-conditional generative models from a number of model classes—variational autoencoders, autoregressive models, and generative adversarial networks (GANs)—to infer the class labels of real data. We perform this inference by training an image classifier using only synthetic data and using the classifier to predict labels on real data. The performance on this task, which we call Classification Accuracy Score (CAS), reveals some surprising results not identified by traditional metrics and constitute our contributions. First, when using a state-of-the-art GAN (BigGAN-deep), Top-1 and Top-5 accuracy decrease by 27.9\% and 41.6\%, respectively, compared to the original data; and conditional generative models from other model classes, such as Vector-Quantized Variational Autoencoder-2 (VQ-VAE-2) and Hierarchical Autoregressive Models (HAMs), substantially outperform GANs on this benchmark. Second, CAS automatically surfaces particular classes for which generative models failed to capture the data distribution, and were previously unknown in the literature. Third, we find traditional GAN metrics such as Inception Score (IS) and FID neither predictive of CAS nor useful when evaluating non-GAN models. Furthermore, in order to facilitate better diagnoses of generative models, we open-source the proposed metric.},
	urldate = {2023-07-01},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Ravuri, Suman and Vinyals, Oriol},
	year = {2019},
}

@inproceedings{chai_ensembling_2021,
	title = {Ensembling {With} {Deep} {Generative} {Views}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Chai_Ensembling_With_Deep_Generative_Views_CVPR_2021_paper.html},
	language = {en},
	urldate = {2023-06-30},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Chai, Lucy and Zhu, Jun-Yan and Shechtman, Eli and Isola, Phillip and Zhang, Richard},
	year = {2021},
	pages = {14997--15007},
}

@inproceedings{baradad_jurjo_learning_2021,
	title = {Learning to {See} by {Looking} at {Noise}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper_files/paper/2021/hash/14f2ebeab937ca128186e7ba876faef9-Abstract.html},
	abstract = {Current vision systems are trained on huge datasets, and these datasets come with costs: curation is expensive, they inherit human biases, and there are concerns over privacy and usage rights. To counter these costs, interest has surged in learning from cheaper data sources, such as unlabeled images. In this paper we go a step further and ask if we can do away with real image datasets entirely, instead learning from procedural noise processes. We investigate a suite of image generation models that produce images from simple random processes. These are then used as training data for a visual representation learner with a contrastive loss. In particular, we study statistical image models, randomly initialized deep generative models, and procedural graphics models.Our findings show that it is important for the noise to capture certain structural properties of real data but that good performance can be achieved even with processes that are far from realistic. We also find that diversity is a key property to learn good representations.},
	urldate = {2023-07-01},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Baradad Jurjo, Manel and Wulff, Jonas and Wang, Tongzhou and Isola, Phillip and Torralba, Antonio},
	year = {2021},
	pages = {2556--2569},
}

@inproceedings{nalisnick_deep_2018,
	title = {Do {Deep} {Generative} {Models} {Know} {What} {They} {Don}'t {Know}?},
	url = {https://openreview.net/forum?id=H1xwNhCcYm},
	abstract = {A neural network deployed in the wild may be asked to make predictions for inputs that were drawn from a different distribution than that of the training data. A plethora of work has demonstrated that it is easy to find or synthesize inputs for which a neural network is highly confident yet wrong. Generative models are widely viewed to be robust to such mistaken confidence as modeling the density of the input features can be used to detect novel, out-of-distribution inputs. In this paper we challenge this assumption. We find that the density learned by flow-based models, VAEs, and PixelCNNs cannot distinguish images of common objects such as dogs, trucks, and horses (i.e. CIFAR-10) from those of house numbers (i.e. SVHN), assigning a higher likelihood to the latter when the model is trained on the former. Moreover, we find evidence of this phenomenon when pairing several popular image data sets: FashionMNIST vs MNIST, CelebA vs SVHN, ImageNet vs CIFAR-10 / CIFAR-100 / SVHN. To investigate this curious behavior, we focus analysis on flow-based generative models in particular since they are trained and evaluated via the exact marginal likelihood. We find such behavior persists even when we restrict the flows to constant-volume transformations. These transformations admit some theoretical analysis, and we show that the difference in likelihoods can be explained by the location and variances of the data and the model curvature. Our results caution against using the density estimates from deep generative models to identify inputs similar to the training distribution until their behavior for out-of-distribution inputs is better understood.},
	language = {en},
	urldate = {2023-07-01},
	author = {Nalisnick, Eric and Matsukawa, Akihiro and Teh, Yee Whye and Gorur, Dilan and Lakshminarayanan, Balaji},
	month = sep,
	year = {2018},
}

@inproceedings{shen_interpreting_2020,
	title = {Interpreting the {Latent} {Space} of {GANs} for {Semantic} {Face} {Editing}},
	url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Shen_Interpreting_the_Latent_Space_of_GANs_for_Semantic_Face_Editing_CVPR_2020_paper.html},
	urldate = {2023-07-01},
	author = {Shen, Yujun and Gu, Jinjin and Tang, Xiaoou and Zhou, Bolei},
	year = {2020},
	pages = {9243--9252},
}

@inproceedings{ramaswamy_fair_2021,
	title = {Fair {Attribute} {Classification} {Through} {Latent} {Space} {De}-{Biasing}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Ramaswamy_Fair_Attribute_Classification_Through_Latent_Space_De-Biasing_CVPR_2021_paper.html},
	language = {en},
	urldate = {2023-07-01},
	author = {Ramaswamy, Vikram V. and Kim, Sunnie S. Y. and Russakovsky, Olga},
	year = {2021},
	pages = {9301--9310},
}

@misc{tan_improving_2021,
	title = {Improving the {Fairness} of {Deep} {Generative} {Models} without {Retraining}},
	url = {http://arxiv.org/abs/2012.04842},
	doi = {10.48550/arXiv.2012.04842},
	abstract = {Generative Adversarial Networks (GANs) advance face synthesis through learning the underlying distribution of observed data. Despite the high-quality generated faces, some minority groups can be rarely generated from the trained models due to a biased image generation process. To study the issue, we first conduct an empirical study on a pre-trained face synthesis model. We observe that after training the GAN model not only carries the biases in the training data but also amplifies them to some degree in the image generation process. To further improve the fairness of image generation, we propose an interpretable baseline method to balance the output facial attributes without retraining. The proposed method shifts the interpretable semantic distribution in the latent space for a more balanced image generation while preserving the sample diversity. Besides producing more balanced data regarding a particular attribute (e.g., race, gender, etc.), our method is generalizable to handle more than one attribute at a time and synthesize samples of fine-grained subgroups. We further show the positive applicability of the balanced data sampled from GANs to quantify the biases in other face recognition systems, like commercial face attribute classifiers and face super-resolution algorithms.},
	urldate = {2023-07-01},
	publisher = {arXiv},
	author = {Tan, Shuhan and Shen, Yujun and Zhou, Bolei},
	month = mar,
	year = {2021},
	note = {arXiv:2012.04842 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
}

@article{kairouz_generating_2022,
	title = {Generating {Fair} {Universal} {Representations} {Using} {Adversarial} {Models}},
	volume = {17},
	issn = {1556-6021},
	doi = {10.1109/TIFS.2022.3170265},
	abstract = {We present a data-driven framework for learning fair universal representations (FUR) that guarantee statistical fairness for any learning task that may not be known a priori. Our framework leverages recent advances in adversarial learning to allow a data holder to learn representations in which a set of sensitive attributes are decoupled from the rest of the dataset. We formulate this as a constrained minimax game between an encoder and an adversary where the constraint ensures a measure of usefulness (utility) of the representation. The resulting problem is that of censoring, i.e., finding a representation that is least informative about the sensitive attributes given a utility constraint. For appropriately chosen adversarial loss functions, our censoring framework precisely clarifies the optimal adversarial strategy against strong information-theoretic adversaries; it also achieves the fairness measure of demographic parity for the resulting constrained representations. We evaluate the performance of our proposed framework on both synthetic and publicly available datasets. For these datasets, we use two tradeoff measures: censoring vs. representation fidelity and fairness vs. utility for downstream tasks, to amply demonstrate that multiple sensitive features can be effectively censored even as the resulting fair representations ensure accuracy for multiple downstream tasks.},
	journal = {IEEE Transactions on Information Forensics and Security},
	author = {Kairouz, Peter and Liao, Jiachun and Huang, Chong and Vyas, Maunil and Welfert, Monica and Sankar, Lalitha},
	year = {2022},
	note = {Conference Name: IEEE Transactions on Information Forensics and Security},
	keywords = {Distortion, Distortion measurement, Fair universal representations, Games, Loss measurement, Noise measurement, Prediction algorithms, Task analysis, algorithmic fairness, generative adversarial networks, minimax games},
	pages = {1970--1985},
}

@article{dumont_schutte_overcoming_2021,
	title = {Overcoming barriers to data sharing with medical image generation: a comprehensive evaluation},
	volume = {4},
	copyright = {2021 The Author(s)},
	issn = {2398-6352},
	shorttitle = {Overcoming barriers to data sharing with medical image generation},
	url = {https://www.nature.com/articles/s41746-021-00507-3},
	doi = {10.1038/s41746-021-00507-3},
	abstract = {Privacy concerns around sharing personally identifiable information are a major barrier to data sharing in medical research. In many cases, researchers have no interest in a particular individual’s information but rather aim to derive insights at the level of cohorts. Here, we utilise generative adversarial networks (GANs) to create medical imaging datasets consisting entirely of synthetic patient data. The synthetic images ideally have, in aggregate, similar statistical properties to those of a source dataset but do not contain sensitive personal information. We assess the quality of synthetic data generated by two GAN models for chest radiographs with 14 radiology findings and brain computed tomography (CT) scans with six types of intracranial haemorrhages. We measure the synthetic image quality by the performance difference of predictive models trained on either the synthetic or the real dataset. We find that synthetic data performance disproportionately benefits from a reduced number of classes. Our benchmark also indicates that at low numbers of samples per class, label overfitting effects start to dominate GAN training. We conducted a reader study in which trained radiologists discriminate between synthetic and real images. In accordance with our benchmark results, the classification accuracy of radiologists improves with an increasing resolution. Our study offers valuable guidelines and outlines practical conditions under which insights derived from synthetic images are similar to those that would have been derived from real data. Our results indicate that synthetic data sharing may be an attractive alternative to sharing real patient-level data in the right setting.},
	language = {en},
	number = {1},
	urldate = {2023-07-01},
	journal = {npj Digital Medicine},
	author = {DuMont Schütte, August and Hetzel, Jürgen and Gatidis, Sergios and Hepp, Tobias and Dietz, Benedikt and Bauer, Stefan and Schwab, Patrick},
	month = sep,
	year = {2021},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Brain imaging, Computed tomography, Medical research, Radiography},
	pages = {1--14},
}

@article{tucker_generating_2020,
	title = {Generating high-fidelity synthetic patient data for assessing machine learning healthcare software},
	volume = {3},
	copyright = {2020 The Author(s)},
	issn = {2398-6352},
	url = {https://www.nature.com/articles/s41746-020-00353-9%3C},
	doi = {10.1038/s41746-020-00353-9},
	abstract = {There is a growing demand for the uptake of modern artificial intelligence technologies within healthcare systems. Many of these technologies exploit historical patient health data to build powerful predictive models that can be used to improve diagnosis and understanding of disease. However, there are many issues concerning patient privacy that need to be accounted for in order to enable this data to be better harnessed by all sectors. One approach that could offer a method of circumventing privacy issues is the creation of realistic synthetic data sets that capture as many of the complexities of the original data set (distributions, non-linear relationships, and noise) but that does not actually include any real patient data. While previous research has explored models for generating synthetic data sets, here we explore the integration of resampling, probabilistic graphical modelling, latent variable identification, and outlier analysis for producing realistic synthetic data based on UK primary care patient data. In particular, we focus on handling missingness, complex interactions between variables, and the resulting sensitivity analysis statistics from machine learning classifiers, while quantifying the risks of patient re-identification from synthetic datapoints. We show that, through our approach of integrating outlier analysis with graphical modelling and resampling, we can achieve synthetic data sets that are not significantly different from original ground truth data in terms of feature distributions, feature dependencies, and sensitivity analysis statistics when inferring machine learning classifiers. What is more, the risk of generating synthetic data that is identical or very similar to real patients is shown to be low.},
	language = {en},
	number = {1},
	urldate = {2023-07-01},
	journal = {npj Digital Medicine},
	author = {Tucker, Allan and Wang, Zhenchen and Rotalinti, Ylenia and Myles, Puja},
	month = nov,
	year = {2020},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Computational science, Computer science, Epidemiology, Risk factors, Statistics},
	pages = {1--13},
}

@inproceedings{stadler_synthetic_2022,
	title = {Synthetic {Data} – {Anonymisation} {Groundhog} {Day}},
	isbn = {978-1-939133-31-1},
	url = {https://www.usenix.org/conference/usenixsecurity22/presentation/stadler},
	language = {en},
	urldate = {2023-04-03},
	booktitle = {31st {USENIX} {Security} {Symposium} ({USENIX} {Security} 22)},
	author = {Stadler, Theresa and Oprisanu, Bristena and Troncoso, Carmela},
	year = {2022},
	pages = {1451--1468},
}

@inproceedings{jain_distilling_2023,
	title = {Distilling {Model} {Failures} as {Directions} in {Latent} {Space}},
	url = {https://openreview.net/forum?id=99RpBVpLiX},
	abstract = {Existing methods for isolating hard subpopulations and spurious correlations in datasets often require human intervention. This can make these methods labor-intensive and dataset-specific. To address these shortcomings, we present a scalable method for automatically distilling a model's failure modes. Specifically, we harness linear classifiers to identify consistent error patterns, and, in turn, induce a natural representation of these failure modes as directions within the feature space. We demonstrate that this framework allows us to discover and automatically caption challenging subpopulations within the training dataset. Moreover, by combining our framework with off-the-shelf diffusion models, we can generate images that are especially challenging for the analyzed model, and thus can be used to perform synthetic data augmentation that helps remedy the model's failure modes.},
	language = {en},
	urldate = {2023-06-30},
	booktitle = {The {Eleventh} {International} {Conference} on {Learning} {Representations}},
	author = {Jain, Saachi and Lawrence, Hannah and Moitra, Ankur and Madry, Aleksander},
	year = {2023},
}

@inproceedings{jahanian_generative_2022,
	title = {Generative {Models} as a {Data} {Source} for {Multiview} {Representation} {Learning}},
	url = {https://openreview.net/forum?id=qhAeZjs7dCL},
	abstract = {Generative models are now capable of producing highly realistic images that look nearly indistinguishable from the data on which they are trained. This raises the question: if we have good enough generative models, do we still need datasets? We investigate this question in the setting of learning general-purpose visual representations from a black-box generative model rather than directly from data. Given an off-the-shelf image generator without any access to its training data, we train representations from the samples output by this generator. We compare several representation learning methods that can be applied to this setting, using the latent space of the generator to generate multiple "views" of the same semantic content. We show that for contrastive methods, this multiview data can naturally be used to identify positive pairs (nearby in latent space) and negative pairs (far apart in latent space). We find that the resulting representations rival or even outperform those learned directly from real data, but that good performance requires care in the sampling strategy applied and the training method. Generative models can be viewed as a compressed and organized copy of a dataset, and we envision a future where more and more "model zoos" proliferate while datasets become increasingly unwieldy, missing, or private. This paper suggests several techniques for dealing with visual representation learning in such a future. Code is available on our project page https://ali-design.github.io/GenRep/.},
	language = {en},
	urldate = {2023-05-28},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Jahanian, Ali and Puig, Xavier and Tian, Yonglong and Isola, Phillip},
	month = jan,
	year = {2022},
}

@inproceedings{asim_invertible_2020,
	title = {Invertible generative models for inverse problems: mitigating representation error and dataset bias},
	shorttitle = {Invertible generative models for inverse problems},
	url = {https://proceedings.mlr.press/v119/asim20a.html},
	abstract = {Trained generative models have shown remarkable performance as priors for inverse problems in imaging – for example, Generative Adversarial Network priors permit recovery of test images from 5-10x fewer measurements than sparsity priors. Unfortunately, these models may be unable to represent any particular image because of architectural choices, mode collapse, and bias in the training dataset. In this paper, we demonstrate that invertible neural networks, which have zero representation error by design, can be effective natural signal priors at inverse problems such as denoising, compressive sensing, and inpainting. Given a trained generative model, we study the empirical risk formulation of the desired inverse problem under a regularization that promotes high likelihood images, either directly by penalization or algorithmically by initialization. For compressive sensing, invertible priors can yield higher accuracy than sparsity priors across almost all undersampling ratios, and due to their lack of representation error, invertible priors can yield better reconstructions than GAN priors for images that have rare features of variation within the biased training set, including out-of-distribution natural images. We additionally compare performance for compressive sensing to unlearned methods, such as the deep decoder, and we establish theoretical bounds on expected recovery error in the case of a linear invertible model.},
	language = {en},
	urldate = {2023-06-30},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Asim, Muhammad and Daniels, Max and Leong, Oscar and Ahmed, Ali and Hand, Paul},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {399--409},
}

@inproceedings{bianchi_easily_2023,
	address = {New York, NY, USA},
	series = {{FAccT} '23},
	title = {Easily {Accessible} {Text}-to-{Image} {Generation} {Amplifies} {Demographic} {Stereotypes} at {Large} {Scale}},
	isbn = {9798400701924},
	url = {https://dl.acm.org/doi/10.1145/3593013.3594095},
	doi = {10.1145/3593013.3594095},
	abstract = {Machine learning models that convert user-written text descriptions into images are now widely available online and used by millions of users to generate millions of images a day. We investigate the potential for these models to amplify dangerous and complex stereotypes. We find a broad range of ordinary prompts produce stereotypes, including prompts simply mentioning traits, descriptors, occupations, or objects. For example, we find cases of prompting for basic traits or social roles resulting in images reinforcing whiteness as ideal, prompting for occupations resulting in amplification of racial and gender disparities, and prompting for objects resulting in reification of American norms. Stereotypes are present regardless of whether prompts explicitly mention identity and demographic language or avoid such language. Moreover, stereotypes persist despite mitigation strategies; neither user attempts to counter stereotypes by requesting images with specific counter-stereotypes nor institutional attempts to add system “guardrails” have prevented the perpetuation of stereotypes. Our analysis justifies concerns regarding the impacts of today’s models, presenting striking exemplars, and connecting these findings with deep insights into harms drawn from social scientific and humanist disciplines. This work contributes to the effort to shed light on the uniquely complex biases in language-vision models and demonstrates the ways that the mass deployment of text-to-image generation models results in mass dissemination of stereotypes and resulting harms.},
	urldate = {2023-06-30},
	booktitle = {Proceedings of the 2023 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {Association for Computing Machinery},
	author = {Bianchi, Federico and Kalluri, Pratyusha and Durmus, Esin and Ladhak, Faisal and Cheng, Myra and Nozza, Debora and Hashimoto, Tatsunori and Jurafsky, Dan and Zou, James and Caliskan, Aylin},
	month = jun,
	year = {2023},
	pages = {1493--1504},
}

@inproceedings{shipard_diversity_2023,
	title = {Diversity {Is} {Definitely} {Needed}: {Improving} {Model}-{Agnostic} {Zero}-{Shot} {Classification} via {Stable} {Diffusion}},
	shorttitle = {Diversity {Is} {Definitely} {Needed}},
	url = {https://openaccess.thecvf.com/content/CVPR2023W/GCV/html/Shipard_Diversity_Is_Definitely_Needed_Improving_Model-Agnostic_Zero-Shot_Classification_via_Stable_CVPRW_2023_paper.html},
	language = {en},
	urldate = {2023-06-30},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Shipard, Jordan and Wiliem, Arnold and Thanh, Kien Nguyen and Xiang, Wei and Fookes, Clinton},
	year = {2023},
	pages = {769--778},
}

@inproceedings{mao_generative_2021,
	title = {Generative {Interventions} for {Causal} {Learning}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Mao_Generative_Interventions_for_Causal_Learning_CVPR_2021_paper.html},
	language = {en},
	urldate = {2023-06-30},
	author = {Mao, Chengzhi and Cha, Augustine and Gupta, Amogh and Wang, Hao and Yang, Junfeng and Vondrick, Carl},
	year = {2021},
	pages = {3947--3956},
}

@inproceedings{chai_using_2020,
	title = {Using latent space regression to analyze and leverage compositionality in {GANs}},
	url = {https://openreview.net/forum?id=sjuuTm4vj0},
	abstract = {In recent years, Generative Adversarial Networks have become ubiquitous in both research and public perception, but how GANs convert an unstructured latent code to a high quality output is still an open question. In this work, we investigate regression into the latent space as a probe to understand the compositional properties of GANs. We find that combining the regressor and a pretrained generator provides a strong image prior, allowing us to create composite images from a collage of random image parts at inference time while maintaining global consistency. To compare compositional properties across different generators, we measure the trade-offs between reconstruction of the unrealistic input and image quality of the regenerated samples. We find that the regression approach enables more localized editing of individual image parts compared to direct editing in the latent space, and we conduct experiments to quantify this independence effect. Our method is agnostic to the semantics of edits, and does not require labels or predefined concepts during training. Beyond image composition, our method extends to a number of related applications, such as image inpainting or example-based image editing, which we demonstrate on several GANs and datasets, and because it uses only a single forward pass, it can operate in real-time. Code is available on our project page: https://chail.github.io/latent-composition/.},
	language = {en},
	urldate = {2023-06-30},
	author = {Chai, Lucy and Wulff, Jonas and Isola, Phillip},
	month = oct,
	year = {2020},
}

@inproceedings{jahanian_steerability_2019,
	title = {On the "steerability" of generative adversarial networks},
	url = {https://openreview.net/forum?id=HylsTT4FvB},
	abstract = {An open secret in contemporary machine learning is that many models work beautifully on standard benchmarks but fail to generalize outside the lab. This has been attributed to biased training data, which provide poor coverage over real world events. Generative models are no exception, but recent advances in generative adversarial networks (GANs) suggest otherwise -- these models can now synthesize strikingly realistic and diverse images. Is generative modeling of photos a solved problem? We show that although current GANs can fit standard datasets very well, they still fall short of being comprehensive models of the visual manifold. In particular, we study their ability to fit simple transformations such as camera movements and color changes. We find that the models reflect the biases of the datasets on which they are trained (e.g., centered objects), but that they also exhibit some capacity for generalization: by "steering" in latent space, we can shift the distribution while still creating realistic images. We hypothesize that the degree of distributional shift is related to the breadth of the training data distribution. Thus, we conduct experiments to quantify the limits of GAN transformations and introduce techniques to mitigate the problem. Code is released on our project page: https://ali-design.github.io/gan\_steerability/},
	language = {en},
	urldate = {2023-06-30},
	author = {Jahanian*, Ali and Chai*, Lucy and Isola, Phillip},
	month = sep,
	year = {2019},
}

@article{kataoka_pre-training_2022,
	title = {Pre-{Training} {Without} {Natural} {Images}},
	volume = {130},
	issn = {1573-1405},
	url = {https://doi.org/10.1007/s11263-021-01555-8},
	doi = {10.1007/s11263-021-01555-8},
	abstract = {Is it possible to use convolutional neural networks pre-trained without any natural images to assist natural image understanding? The paper proposes a novel concept, Formula-driven Supervised Learning (FDSL). We automatically generate image patterns and their category labels by assigning fractals, which are based on a natural law. Theoretically, the use of automatically generated images instead of natural images in the pre-training phase allows us to generate an infinitely large dataset of labeled images. The proposed framework is similar yet different from Self-Supervised Learning because the FDSL framework enables the creation of image patterns based on any mathematical formulas in addition to self-generated labels. Further, unlike pre-training with a synthetic image dataset, a dataset under the framework of FDSL is not required to define object categories, surface texture, lighting conditions, and camera viewpoint. In the experimental section, we find a better dataset configuration through an exploratory study, e.g., increase of \#category/\#instance, patch rendering, image coloring, and training epoch. Although models pre-trained with the proposed Fractal DataBase (FractalDB), a database without natural images, do not necessarily outperform models pre-trained with human annotated datasets in all settings, we are able to partially surpass the accuracy of ImageNet/Places pre-trained models. The FractalDB pre-trained CNN also outperforms other pre-trained models on auto-generated datasets based on FDSL such as Bezier curves and Perlin noise. This is reasonable since natural objects and scenes existing around us are constructed according to fractal geometry. Image representation with the proposed FractalDB captures a unique feature in the visualization of convolutional layers and attentions.},
	language = {en},
	number = {4},
	urldate = {2023-06-30},
	journal = {International Journal of Computer Vision},
	author = {Kataoka, Hirokatsu and Okayasu, Kazushige and Matsumoto, Asato and Yamagata, Eisuke and Yamada, Ryosuke and Inoue, Nakamasa and Nakamura, Akio and Satoh, Yutaka},
	month = apr,
	year = {2022},
	keywords = {Formula-driven supervised learning, Image recognition, Representation learning},
	pages = {990--1007},
}

@inproceedings{takashima_visual_2023,
	title = {Visual {Atoms}: {Pre}-{Training} {Vision} {Transformers} {With} {Sinusoidal} {Waves}},
	shorttitle = {Visual {Atoms}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Takashima_Visual_Atoms_Pre-Training_Vision_Transformers_With_Sinusoidal_Waves_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-06-30},
	author = {Takashima, Sora and Hayamizu, Ryo and Inoue, Nakamasa and Kataoka, Hirokatsu and Yokota, Rio},
	year = {2023},
	pages = {18579--18588},
}

@inproceedings{girdhar_imagebind_2023,
	title = {{ImageBind}: {One} {Embedding} {Space} {To} {Bind} {Them} {All}},
	shorttitle = {{ImageBind}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Girdhar_ImageBind_One_Embedding_Space_To_Bind_Them_All_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-06-30},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Girdhar, Rohit and El-Nouby, Alaaeldin and Liu, Zhuang and Singh, Mannat and Alwala, Kalyan Vasudev and Joulin, Armand and Misra, Ishan},
	year = {2023},
	pages = {15180--15190},
}

@inproceedings{rebuffi_adversarially_2023,
	title = {Adversarially {Self}-{Supervised} {Pre}-training {Improves} {Accuracy} and {Robustness}},
	url = {https://openreview.net/forum?id=Y4Tm1A2Z1B},
	abstract = {There is growing interest in learning visual representations that work well across distribution shifts as illustrated by the increasing number of ImageNet evaluation sets. In this paper, we reconsider adversarial training, which is generally used as a defense against adversarial shifts, as a way to improve the pre-training of representations for transfer across tasks and natural shifts. In this study we combine adversarial training with different self-supervised pre-training methods such as bootstrap your own latent (BYOL), masked auto-encoding (MAE), and the auxiliary task of rotation prediction (RotNet). We show that the adversarial versions of these self-supervision methods consistently lead to better fine-tuning accuracy both in and out of distribution compared to standard self-supervision, even with nominal/non-adversarial fine-tuning. Furthermore we observe that, to reach best performance with adversarial self-supervised pre-training, (1) the optimal perturbation radius differs among pre-training methods, and (2) that the robust parameters of early layers need to be preserved during fine-tuning to avoid losing the benefits of adversarial pre-training. Finally, we show that there is not a single adversarial self-supervised method that dominates others across all variants, but that adversarial MAE is the best choice for in-distribution variants, and that adversarial BYOL is best for out-of-distribution variants.},
	language = {en},
	urldate = {2023-06-30},
	booktitle = {International {Conference} on {Learning} {Representations} {Workshop}},
	author = {Rebuffi, Sylvestre-Alvise and Wiles, Olivia and Shelhamer, Evan and Gowal, Sven},
	month = apr,
	year = {2023},
}

@misc{you_diffusion_2023,
	title = {Diffusion {Models} and {Semi}-{Supervised} {Learners} {Benefit} {Mutually} with {Few} {Labels}},
	url = {http://arxiv.org/abs/2302.10586},
	doi = {10.48550/arXiv.2302.10586},
	abstract = {In an effort to further advance semi-supervised generative and classification tasks, we propose a simple yet effective training strategy called dual pseudo training (DPT), built upon strong semi-supervised learners and diffusion models. DPT operates in three stages: training a classifier on partially labeled data to predict pseudo-labels; training a conditional generative model using these pseudo-labels to generate pseudo images; and retraining the classifier with a mix of real and pseudo images. Empirically, DPT consistently achieves SOTA performance of semi-supervised generation and classification across various settings. In particular, with one or two labels per class, DPT achieves a Fr{\textbackslash}'echet Inception Distance (FID) score of 3.08 or 2.52 on ImageNet 256x256, surpassing strong diffusion models with full labels, such as IDDPM, CDM, ADM, and LDM. Besides, DPT outperforms competitive semi-supervised baselines substantially on ImageNet classification tasks, achieving top-1 accuracies of 59.0 (+2.8), 69.5 (+3.0), and 74.4 (+2.0) with one, two, or five labels per class, respectively. Notably, our results demonstrate that diffusion can generate realistic images with only a few labels (e.g., {\textless}0.1\%) and generative augmentation remains viable for semi-supervised classification.},
	urldate = {2023-06-30},
	publisher = {arXiv},
	author = {You, Zebin and Zhong, Yong and Bao, Fan and Sun, Jiacheng and Li, Chongxuan and Zhu, Jun},
	month = jun,
	year = {2023},
	note = {arXiv:2302.10586 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{yuan_not_2023,
	title = {Not {Just} {Pretty} {Pictures}: {Text}-to-{Image} {Generators} {Enable} {Interpretable} {Interventions} for {Robust} {Representations}},
	shorttitle = {Not {Just} {Pretty} {Pictures}},
	url = {http://arxiv.org/abs/2212.11237},
	doi = {10.48550/arXiv.2212.11237},
	abstract = {Neural image classifiers are known to undergo severe performance degradation when exposed to input that exhibits covariate shift with respect to the training distribution. In this paper, we show that recent Text-to-Image (T2I) generators' ability to edit images to approximate interventions via natural-language prompts is a promising technology to train more robust classifiers. Using current open-source models, we find that a variety of prompting strategies are effective for producing augmented training datasets sufficient to achieve state-of-the-art performance (1) in widely adopted Single-Domain Generalization benchmarks, (2) in reducing classifiers' dependency on spurious features and (3) facilitating the application of Multi-Domain Generalization techniques when fewer training domains are available.},
	urldate = {2023-06-30},
	publisher = {arXiv},
	author = {Yuan, Jianhao and Pinto, Francesco and Davies, Adam and Gupta, Aarushi and Torr, Philip},
	month = apr,
	year = {2023},
	note = {arXiv:2212.11237 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{yue_domain_2019,
	title = {Domain {Randomization} and {Pyramid} {Consistency}: {Simulation}-to-{Real} {Generalization} {Without} {Accessing} {Target} {Domain} {Data}},
	shorttitle = {Domain {Randomization} and {Pyramid} {Consistency}},
	url = {https://openaccess.thecvf.com/content_ICCV_2019/html/Yue_Domain_Randomization_and_Pyramid_Consistency_Simulation-to-Real_Generalization_Without_Accessing_Target_ICCV_2019_paper.html},
	urldate = {2023-06-30},
	author = {Yue, Xiangyu and Zhang, Yang and Zhao, Sicheng and Sangiovanni-Vincentelli, Alberto and Keutzer, Kurt and Gong, Boqing},
	year = {2019},
	pages = {2100--2110},
}

@misc{tanaka_data_2019,
	title = {Data {Augmentation} {Using} {GANs}},
	url = {http://arxiv.org/abs/1904.09135},
	doi = {10.48550/arXiv.1904.09135},
	abstract = {In this paper we propose the use of Generative Adversarial Networks (GAN) to generate artificial training data for machine learning tasks. The generation of artificial training data can be extremely useful in situations such as imbalanced data sets, performing a role similar to SMOTE or ADASYN. It is also useful when the data contains sensitive information, and it is desirable to avoid using the original data set as much as possible (example: medical data). We test our proposal on benchmark data sets using different network architectures, and show that a Decision Tree (DT) classifier trained using the training data generated by the GAN reached the same, (and surprisingly sometimes better), accuracy and recall than a DT trained on the original data set.},
	urldate = {2023-06-30},
	publisher = {arXiv},
	author = {Tanaka, Fabio Henrique Kiyoiti dos Santos and Aranha, Claus},
	month = apr,
	year = {2019},
	note = {arXiv:1904.09135 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{sandfort_data_2019,
	title = {Data augmentation using generative adversarial networks ({CycleGAN}) to improve generalizability in {CT} segmentation tasks},
	volume = {9},
	copyright = {2019 This is a U.S. government work and not under copyright protection in the U.S.; foreign copyright protection may apply},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-019-52737-x},
	doi = {10.1038/s41598-019-52737-x},
	abstract = {Labeled medical imaging data is scarce and expensive to generate. To achieve generalizable deep learning models large amounts of data are needed. Standard data augmentation is a method to increase generalizability and is routinely performed. Generative adversarial networks offer a novel method for data augmentation. We evaluate the use of CycleGAN for data augmentation in CT segmentation tasks. Using a large image database we trained a CycleGAN to transform contrast CT images into non-contrast images. We then used the trained CycleGAN to augment our training using these synthetic non-contrast images. We compared the segmentation performance of a U-Net trained on the original dataset compared to a U-Net trained on the combined dataset of original data and synthetic non-contrast images. We further evaluated the U-Net segmentation performance on two separate datasets: The original contrast CT dataset on which segmentations were created and a second dataset from a different hospital containing only non-contrast CTs. We refer to these 2 separate datasets as the in-distribution and out-of-distribution datasets, respectively. We show that in several CT segmentation tasks performance is improved significantly, especially in out-of-distribution (noncontrast CT) data. For example, when training the model with standard augmentation techniques, performance of segmentation of the kidneys on out-of-distribution non-contrast images was dramatically lower than for in-distribution data (Dice score of 0.09 vs. 0.94 for out-of-distribution vs. in-distribution data, respectively, p {\textless} 0.001). When the kidney model was trained with CycleGAN augmentation techniques, the out-of-distribution (non-contrast) performance increased dramatically (from a Dice score of 0.09 to 0.66, p {\textless} 0.001). Improvements for the liver and spleen were smaller, from 0.86 to 0.89 and 0.65 to 0.69, respectively. We believe this method will be valuable to medical imaging researchers to reduce manual segmentation effort and cost in CT imaging.},
	language = {en},
	number = {1},
	urldate = {2023-06-30},
	journal = {Scientific Reports},
	author = {Sandfort, Veit and Yan, Ke and Pickhardt, Perry J. and Summers, Ronald M.},
	month = nov,
	year = {2019},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Diagnostic markers, Image processing},
	pages = {16884},
}

@misc{gal_image_2022,
	title = {An {Image} is {Worth} {One} {Word}: {Personalizing} {Text}-to-{Image} {Generation} using {Textual} {Inversion}},
	shorttitle = {An {Image} is {Worth} {One} {Word}},
	url = {http://arxiv.org/abs/2208.01618},
	doi = {10.48550/arXiv.2208.01618},
	abstract = {Text-to-image models offer unprecedented freedom to guide creation through natural language. Yet, it is unclear how such freedom can be exercised to generate images of specific unique concepts, modify their appearance, or compose them in new roles and novel scenes. In other words, we ask: how can we use language-guided models to turn our cat into a painting, or imagine a new product based on our favorite toy? Here we present a simple approach that allows such creative freedom. Using only 3-5 images of a user-provided concept, like an object or a style, we learn to represent it through new "words" in the embedding space of a frozen text-to-image model. These "words" can be composed into natural language sentences, guiding personalized creation in an intuitive way. Notably, we find evidence that a single word embedding is sufficient for capturing unique and varied concepts. We compare our approach to a wide range of baselines, and demonstrate that it can more faithfully portray the concepts across a range of applications and tasks. Our code, data and new words will be available at: https://textual-inversion.github.io},
	urldate = {2023-06-30},
	publisher = {arXiv},
	author = {Gal, Rinon and Alaluf, Yuval and Atzmon, Yuval and Patashnik, Or and Bermano, Amit H. and Chechik, Gal and Cohen-Or, Daniel},
	month = aug,
	year = {2022},
	note = {arXiv:2208.01618 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Machine Learning},
}

@inproceedings{trabucco_effective_2023,
	title = {Effective {Data} {Augmentation} {With} {Diffusion} {Models}},
	url = {https://openreview.net/forum?id=dcCpG0CVMf},
	abstract = {Data augmentation is one of the most prevalent tools in deep learning, underpinning many recent advances, including those from classification, generative models, and representation learning. The standard approach to data augmentation combines simple transformations like rotations and flips to generate new images from existing ones. However, these new images lack diversity along key semantic axes present in the data. Consider the task of recognizing different animals. Current augmentations fail to produce diversity in task-relevant high-level semantic attributes like the species of the animal. We address the lack of diversity in data augmentation with image-to-image transformations parameterized by pre-trained text-to-image diffusion models. Our method edits images to change their semantics using an off-the-shelf diffusion model, and generalizes to novel visual concepts from a few labelled examples. We evaluate our approach on image classification tasks in a few-shot setting, and on a real-world weed recognition task, and observe an improvement in accuracy in tested domains.},
	language = {en},
	urldate = {2023-06-30},
	booktitle = {{ICLR} 2023 {Workshop} on {Mathematical} and {Empirical} {Understanding} of {Foundation} {Models}},
	author = {Trabucco, Brandon and Doherty, Kyle and Gurinas, Max and Salakhutdinov, Ruslan},
	month = feb,
	year = {2023},
}

@inproceedings{somepalli_diffusion_2023,
	title = {Diffusion {Art} or {Digital} {Forgery}? {Investigating} {Data} {Replication} in {Diffusion} {Models}},
	shorttitle = {Diffusion {Art} or {Digital} {Forgery}?},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Somepalli_Diffusion_Art_or_Digital_Forgery_Investigating_Data_Replication_in_Diffusion_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-06-30},
	author = {Somepalli, Gowthami and Singla, Vasu and Goldblum, Micah and Geiping, Jonas and Goldstein, Tom},
	year = {2023},
	pages = {6048--6058},
}

@misc{carlini_extracting_2023,
	title = {Extracting {Training} {Data} from {Diffusion} {Models}},
	url = {http://arxiv.org/abs/2301.13188},
	doi = {10.48550/arXiv.2301.13188},
	abstract = {Image diffusion models such as DALL-E 2, Imagen, and Stable Diffusion have attracted significant attention due to their ability to generate high-quality synthetic images. In this work, we show that diffusion models memorize individual images from their training data and emit them at generation time. With a generate-and-filter pipeline, we extract over a thousand training examples from state-of-the-art models, ranging from photographs of individual people to trademarked company logos. We also train hundreds of diffusion models in various settings to analyze how different modeling and data decisions affect privacy. Overall, our results show that diffusion models are much less private than prior generative models such as GANs, and that mitigating these vulnerabilities may require new advances in privacy-preserving training.},
	urldate = {2023-06-30},
	publisher = {arXiv},
	author = {Carlini, Nicholas and Hayes, Jamie and Nasr, Milad and Jagielski, Matthew and Sehwag, Vikash and Tramèr, Florian and Balle, Borja and Ippolito, Daphne and Wallace, Eric},
	month = jan,
	year = {2023},
	note = {arXiv:2301.13188 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@article{zhang_shifting_2022,
	title = {Shifting machine learning for healthcare from development to deployment and from models to data},
	volume = {6},
	copyright = {2022 Springer Nature Limited},
	issn = {2157-846X},
	url = {https://www.nature.com/articles/s41551-022-00898-y},
	doi = {10.1038/s41551-022-00898-y},
	abstract = {In the past decade, the application of machine learning (ML) to healthcare has helped drive the automation of physician tasks as well as enhancements in clinical capabilities and access to care. This progress has emphasized that, from model development to model deployment, data play central roles. In this Review, we provide a data-centric view of the innovations and challenges that are defining ML for healthcare. We discuss deep generative models and federated learning as strategies to augment datasets for improved model performance, as well as the use of the more recent transformer models for handling larger datasets and enhancing the modelling of clinical text. We also discuss data-focused problems in the deployment of ML, emphasizing the need to efficiently deliver data to ML models for timely clinical predictions and to account for natural data shifts that can deteriorate model performance.},
	language = {en},
	number = {12},
	urldate = {2023-06-30},
	journal = {Nature Biomedical Engineering},
	author = {Zhang, Angela and Xing, Lei and Zou, James and Wu, Joseph C.},
	month = dec,
	year = {2022},
	note = {Number: 12
Publisher: Nature Publishing Group},
	keywords = {Biomedical engineering, Computational science, Machine learning, Medical imaging, Translational research},
	pages = {1330--1345},
}

@misc{chambon_roentgen_2022,
	title = {{RoentGen}: {Vision}-{Language} {Foundation} {Model} for {Chest} {X}-ray {Generation}},
	shorttitle = {{RoentGen}},
	url = {http://arxiv.org/abs/2211.12737},
	doi = {10.48550/arXiv.2211.12737},
	abstract = {Multimodal models trained on large natural image-text pair datasets have exhibited astounding abilities in generating high-quality images. Medical imaging data is fundamentally different to natural images, and the language used to succinctly capture relevant details in medical data uses a different, narrow but semantically rich, domain-specific vocabulary. Not surprisingly, multi-modal models trained on natural image-text pairs do not tend to generalize well to the medical domain. Developing generative imaging models faithfully representing medical concepts while providing compositional diversity could mitigate the existing paucity of high-quality, annotated medical imaging datasets. In this work, we develop a strategy to overcome the large natural-medical distributional shift by adapting a pre-trained latent diffusion model on a corpus of publicly available chest x-rays (CXR) and their corresponding radiology (text) reports. We investigate the model's ability to generate high-fidelity, diverse synthetic CXR conditioned on text prompts. We assess the model outputs quantitatively using image quality metrics, and evaluate image quality and text-image alignment by human domain experts. We present evidence that the resulting model (RoentGen) is able to create visually convincing, diverse synthetic CXR images, and that the output can be controlled to a new extent by using free-form text prompts including radiology-specific language. Fine-tuning this model on a fixed training set and using it as a data augmentation method, we measure a 5\% improvement of a classifier trained jointly on synthetic and real images, and a 3\% improvement when trained on a larger but purely synthetic training set. Finally, we observe that this fine-tuning distills in-domain knowledge in the text-encoder and can improve its representation capabilities of certain diseases like pneumothorax by 25\%.},
	urldate = {2023-06-30},
	publisher = {arXiv},
	author = {Chambon, Pierre and Bluethgen, Christian and Delbrouck, Jean-Benoit and Van der Sluijs, Rogier and Połacin, Małgorzata and Chaves, Juan Manuel Zambrano and Abraham, Tanishq Mathew and Purohit, Shivanshu and Langlotz, Curtis P. and Chaudhari, Akshay},
	month = nov,
	year = {2022},
	note = {arXiv:2211.12737 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{chen_synthetic_2021,
	title = {Synthetic data in machine learning for medicine and healthcare},
	volume = {5},
	copyright = {2021 Springer Nature Limited},
	issn = {2157-846X},
	url = {https://www.nature.com/articles/s41551-021-00751-8},
	doi = {10.1038/s41551-021-00751-8},
	abstract = {The proliferation of synthetic data in artificial intelligence for medicine and healthcare raises concerns about the vulnerabilities of the software and the challenges of current policy.},
	language = {en},
	number = {6},
	urldate = {2023-06-30},
	journal = {Nature Biomedical Engineering},
	author = {Chen, Richard J. and Lu, Ming Y. and Chen, Tiffany Y. and Williamson, Drew F. K. and Mahmood, Faisal},
	month = jun,
	year = {2021},
	note = {Number: 6
Publisher: Nature Publishing Group},
	keywords = {Health policy, Image processing, Machine learning, Translational research},
	pages = {493--497},
}

@inproceedings{gowal_achieving_2020,
	title = {Achieving {Robustness} in the {Wild} via {Adversarial} {Mixing} {With} {Disentangled} {Representations}},
	url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Gowal_Achieving_Robustness_in_the_Wild_via_Adversarial_Mixing_With_Disentangled_CVPR_2020_paper.html},
	urldate = {2023-06-30},
	author = {Gowal, Sven and Qin, Chongli and Huang, Po-Sen and Cemgil, Taylan and Dvijotham, Krishnamurthy and Mann, Timothy and Kohli, Pushmeet},
	year = {2020},
	pages = {1211--1220},
}

@article{chen_generative_2022,
	title = {Generative {Adversarial} {Networks} in {Medical} {Image} augmentation: {A} review},
	volume = {144},
	issn = {0010-4825},
	shorttitle = {Generative {Adversarial} {Networks} in {Medical} {Image} augmentation},
	url = {https://www.sciencedirect.com/science/article/pii/S0010482522001743},
	doi = {10.1016/j.compbiomed.2022.105382},
	abstract = {Object
With the development of deep learning, the number of training samples for medical image-based diagnosis and treatment models is increasing. Generative Adversarial Networks (GANs) have attracted attention in medical image processing due to their excellent image generation capabilities and have been widely used in data augmentation. In this paper, a comprehensive and systematic review and analysis of medical image augmentation work are carried out, and its research status and development prospects are reviewed.
Method
This paper reviews 105 medical image augmentation related papers, which mainly collected by ELSEVIER, IEEE Xplore, and Springer from 2018 to 2021. We counted these papers according to the parts of the organs corresponding to the images, and sorted out the medical image datasets that appeared in them, the loss function in model training, and the quantitative evaluation metrics of image augmentation. At the same time, we briefly introduce the literature collected in three journals and three conferences that have received attention in medical image processing.
Result
First, we summarize the advantages of various augmentation models, loss functions, and evaluation metrics. Researchers can use this information as a reference when designing augmentation tasks. Second, we explore the relationship between augmented models and the amount of the training set, and tease out the role that augmented models may play when the quality of the training set is limited. Third, the statistical number of papers shows that the development momentum of this research field remains strong. Furthermore, we discuss the existing limitations of this type of model and suggest possible research directions.
Conclusion
We discuss GAN-based medical image augmentation work in detail. This method effectively alleviates the challenge of limited training samples for medical image diagnosis and treatment models. It is hoped that this review will benefit researchers interested in this field.},
	language = {en},
	urldate = {2023-06-30},
	journal = {Computers in Biology and Medicine},
	author = {Chen, Yizhou and Yang, Xu-Hua and Wei, Zihan and Heidari, Ali Asghar and Zheng, Nenggan and Li, Zhicheng and Chen, Huiling and Hu, Haigen and Zhou, Qianwei and Guan, Qiu},
	month = may,
	year = {2022},
	keywords = {Augmentation, Deep learning, Generative adversarial networks, Image synthesis, Medical image},
	pages = {105382},
}

@article{zhao_synthesizing_2018,
	title = {Synthesizing retinal and neuronal images with generative adversarial nets},
	volume = {49},
	issn = {1361-8415},
	url = {https://www.sciencedirect.com/science/article/pii/S1361841518304596},
	doi = {10.1016/j.media.2018.07.001},
	abstract = {This paper aims at synthesizing multiple realistic-looking retinal (or neuronal) images from an unseen tubular structured annotation that contains the binary vessel (or neuronal) morphology. The generated phantoms are expected to preserve the same tubular structure, and resemble the visual appearance of the training images. Inspired by the recent progresses in generative adversarial nets (GANs) as well as image style transfer, our approach enjoys several advantages. It works well with a small training set with as few as 10 training examples, which is a common scenario in medical image analysis. Besides, it is capable of synthesizing diverse images from the same tubular structured annotation. Extensive experimental evaluations on various retinal fundus and neuronal imaging applications demonstrate the merits of the proposed approach.},
	language = {en},
	urldate = {2023-06-30},
	journal = {Medical Image Analysis},
	author = {Zhao, He and Li, Huiqi and Maurer-Stroh, Sebastian and Cheng, Li},
	month = oct,
	year = {2018},
	keywords = {Data-driven image synthesis, Deep learning, Neuronal image synthesis, Retinal fundus image synthesis},
	pages = {14--26},
}

@article{havaei_conditional_2021,
	title = {Conditional generation of medical images via disentangled adversarial inference},
	volume = {72},
	issn = {1361-8415},
	url = {https://www.sciencedirect.com/science/article/pii/S1361841521001523},
	doi = {10.1016/j.media.2021.102106},
	abstract = {Synthetic medical image generation has a huge potential for improving healthcare through many applications, from data augmentation for training machine learning systems to preserving patient privacy. Conditional Adversarial Generative Networks (cGANs) use a conditioning factor to generate images and have shown great success in recent years. Intuitively, the information in an image can be divided into two parts: 1) content which is presented through the conditioning vector and 2) style which is the undiscovered information missing from the conditioning vector. Current practices in using cGANs for medical image generation, only use a single variable for image generation (i.e., content) and therefore, do not provide much flexibility nor control over the generated image. In this work we propose DRAI—a dual adversarial inference framework with augmented disentanglement constraints—to learn from the image itself, disentangled representations of style and content, and use this information to impose control over the generation process. In this framework, style is learned in a fully unsupervised manner, while content is learned through both supervised learning (using the conditioning vector) and unsupervised learning (with the inference mechanism). We undergo two novel regularization steps to ensure content-style disentanglement. First, we minimize the shared information between content and style by introducing a novel application of the gradient reverse layer (GRL); second, we introduce a self-supervised regularization method to further separate information in the content and style variables. For evaluation, we consider two types of baselines: single latent variable models that infer a single variable, and double latent variable models that infer two variables (style and content). We conduct extensive qualitative and quantitative assessments on two publicly available medical imaging datasets (LIDC and HAM10000) and test for conditional image generation, image retrieval and style-content disentanglement. We show that in general, two latent variable models achieve better performance and give more control over the generated image. We also show that our proposed model (DRAI) achieves the best disentanglement score and has the best overall performance.},
	language = {en},
	urldate = {2023-06-30},
	journal = {Medical Image Analysis},
	author = {Havaei, Mohammad and Mao, Ximeng and Wang, Yiping and Lao, Qicheng},
	month = aug,
	year = {2021},
	keywords = {41A05, 41A10, 65D05, 65D17},
	pages = {102106},
}

@article{han_breaking_2020,
	title = {Breaking medical data sharing boundaries by using synthesized radiographs},
	volume = {6},
	url = {https://www.science.org/doi/full/10.1126/sciadv.abb7973},
	doi = {10.1126/sciadv.abb7973},
	abstract = {Computer vision (CV) has the potential to change medicine fundamentally. Expert knowledge provided by CV can enhance diagnosis. Unfortunately, existing algorithms often remain below expectations, as databases used for training are usually too small, incomplete, and heterogeneous in quality. Moreover, data protection is a serious obstacle to the exchange of data. To overcome this limitation, we propose to use generative models (GMs) to produce high-resolution synthetic radiographs that do not contain any personal identification information. Blinded analyses by CV and radiology experts confirmed the high similarity of synthesized and real radiographs. The combination of pooled GM improves the performance of CV algorithms trained on smaller datasets, and the integration of synthesized data into patient data repositories can compensate for underrepresented disease entities. By integrating federated learning strategies, even hospitals with few datasets can contribute to and benefit from GM training.},
	number = {49},
	urldate = {2023-06-30},
	journal = {Science Advances},
	author = {Han, Tianyu and Nebelung, Sven and Haarburger, Christoph and Horst, Nicolas and Reinartz, Sebastian and Merhof, Dorit and Kiessling, Fabian and Schulz, Volkmar and Truhn, Daniel},
	month = dec,
	year = {2020},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {eabb7973},
}

@inproceedings{rashid_skin_2019,
	title = {Skin {Lesion} {Classification} {Using} {GAN} based {Data} {Augmentation}},
	doi = {10.1109/EMBC.2019.8857905},
	abstract = {Early detection and frequent monitoring are critical for survival of skin cancer patients. Unfortunately, in practice a significant number of cases remain undetected until advanced stages, reducing the chances of survival. An appealing approach for early detection is to employ automated classification of dermoscopic images acquired via low-cost, smartphone-based hardware. By far, the most successful classification approaches on this task are based on deep learning. Unfortunately, most medical image classification tasks are unable to leverage the true potential of deep learning due to limited sizes of training datasets. Investigation of novel data generation techniques is thus an appealing option since it can enable us to augment our training data by a large number of synthetically generated examples. In this work, we investigate the possibility of obtaining realistic looking dermoscopic images via generative adversarial networks (GANs). These images are then employed to augment our existing training set in an effort to enhance the performance of a deep convolutional neural network on the skin lesion classification task. Results are compared with conventional data augmentation strategies and demonstrate that GAN based augmentation delivers significant performance gains.},
	booktitle = {2019 41st {Annual} {International} {Conference} of the {IEEE} {Engineering} in {Medicine} and {Biology} {Society} ({EMBC})},
	author = {Rashid, Haroon and Tanveer, M. Asjid and Aqeel Khan, Hassan},
	month = jul,
	year = {2019},
	note = {ISSN: 1558-4615},
	keywords = {Biomedical imaging, Data Augmentation, GANs, Generative adversarial networks, Generators, Lesions, Skin, Skin Lesion, Task analysis, Training},
	pages = {916--919},
}

@inproceedings{li_signed_2019,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Signed {Laplacian} {Deep} {Learning} with {Adversarial} {Augmentation} for {Improved} {Mammography} {Diagnosis}},
	isbn = {978-3-030-32226-7},
	doi = {10.1007/978-3-030-32226-7_54},
	abstract = {Computer-aided breast cancer diagnosis in mammography is limited by inadequate data and the similarity between benign and cancerous masses. To address this, we propose a signed graph regularized deep neural network with adversarial augmentation, named DiagNet. Firstly, we use adversarial learning to generate positive and negative mass-contained mammograms for each mass class. After that, a signed similarity graph is built upon the expanded data to further highlight the discrimination. Finally, a deep convolutional neural network is trained by jointly optimizing the signed graph regularization and classification loss. Experiments show that the DiagNet framework outperforms the state-of-the-art in breast mass diagnosis in mammography.},
	language = {en},
	booktitle = {Medical {Image} {Computing} and {Computer} {Assisted} {Intervention} – {MICCAI} 2019},
	publisher = {Springer International Publishing},
	author = {Li, Heyi and Chen, Dongdong and Nailon, William H. and Davies, Mike E. and Laurenson, David I.},
	editor = {Shen, Dinggang and Liu, Tianming and Peters, Terry M. and Staib, Lawrence H. and Essert, Caroline and Zhou, Sean and Yap, Pew-Thian and Khan, Ali},
	year = {2019},
	keywords = {Adversarial learning, Deep learning, Graph regularization, Mammography diagnosis},
	pages = {486--494},
}

@article{ju_leveraging_2021,
	title = {Leveraging {Regular} {Fundus} {Images} for {Training} {UWF} {Fundus} {Diagnosis} {Models} via {Adversarial} {Learning} and {Pseudo}-{Labeling}},
	volume = {40},
	issn = {1558-254X},
	doi = {10.1109/TMI.2021.3056395},
	abstract = {Recently, ultra-widefield (UWF) 200° fundus imaging by Optos cameras has gradually been introduced because of its broader insights for detecting more information on the fundus than regular 30° - 60° fundus cameras. Compared with UWF fundus images, regular fundus images contain a large amount of high-quality and well-annotated data. Due to the domain gap, models trained by regular fundus images to recognize UWF fundus images perform poorly. Hence, given that annotating medical data is labor intensive and time consuming, in this paper, we explore how to leverage regular fundus images to improve the limited UWF fundus data and annotations for more efficient training. We propose the use of a modified cycle generative adversarial network (CycleGAN) model to bridge the gap between regular and UWF fundus and generate additional UWF fundus images for training. A consistency regularization term is proposed in the loss of the GAN to improve and regulate the quality of the generated data. Our method does not require that images from the two domains be paired or even that the semantic labels be the same, which provides great convenience for data collection. Furthermore, we show that our method is robust to noise and errors introduced by the generated unlabeled data with the pseudo-labeling technique. We evaluated the effectiveness of our methods on several common fundus diseases and tasks, such as diabetic retinopathy (DR) classification, lesion detection and tessellated fundus segmentation. The experimental results demonstrate that our proposed method simultaneously achieves superior generalizability of the learned representations and performance improvements in multiple tasks.},
	number = {10},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Ju, Lie and Wang, Xin and Zhao, Xin and Bonnington, Paul and Drummond, Tom and Ge, Zongyuan},
	month = oct,
	year = {2021},
	note = {Conference Name: IEEE Transactions on Medical Imaging},
	keywords = {Annotation-efficient deep learning, Diseases, Gallium nitride, Generative adversarial networks, Imaging, Retina, Task analysis, Training, adversarial learning, domain adaptation, ultra-widefield fundus images},
	pages = {2911--2925},
}

@inproceedings{baur_generating_2018,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Generating {Highly} {Realistic} {Images} of {Skin} {Lesions} with {GANs}},
	isbn = {978-3-030-01201-4},
	doi = {10.1007/978-3-030-01201-4_28},
	abstract = {As many other machine learning driven medical image analysis tasks, skin image analysis suffers from a chronic lack of labeled data and skewed class distributions, which poses problems for the training of robust and well-generalizing models. The ability to synthesize realistic looking images of skin lesions could act as a reliever for the aforementioned problems. Generative Adversarial Networks (GANs) have been successfully used to synthesize realistically looking medical images, however limited to low resolution, whereas machine learning models for challenging tasks such as skin lesion segmentation or classification benefit from much higher resolution data. In this work, we successfully synthesize realistically looking images of skin lesions with GANs at such high resolution. Therefore, we utilize the concept of progressive growing, which we both quantitatively and qualitatively compare to other GAN architectures such as the DCGAN and the LAPGAN. Our results show that with the help of progressive growing, we can synthesize highly realistic dermoscopic images of skin lesions that even expert dermatologists find hard to distinguish from real ones.},
	language = {en},
	booktitle = {{OR} 2.0 {Context}-{Aware} {Operating} {Theaters}, {Computer} {Assisted} {Robotic} {Endoscopy}, {Clinical} {Image}-{Based} {Procedures}, and {Skin} {Image} {Analysis}},
	publisher = {Springer International Publishing},
	author = {Baur, Christoph and Albarqouni, Shadi and Navab, Nassir},
	editor = {Stoyanov, Danail and Taylor, Zeike and Sarikaya, Duygu and McLeod, Jonathan and González Ballester, Miguel Angel and Codella, Noel C.F. and Martel, Anne and Maier-Hein, Lena and Malpani, Anand and Zenati, Marco A. and De Ribaupierre, Sandrine and Xiongbiao, Luo and Collins, Toby and Reichl, Tobias and Drechsler, Klaus and Erdt, Marius and Linguraru, Marius George and Oyarzun Laura, Cristina and Shekhar, Raj and Wesarg, Stefan and Celebi, M. Emre and Dana, Kristin and Halpern, Allan},
	year = {2018},
	pages = {260--267},
}

@article{kather_medical_2022,
	title = {Medical domain knowledge in domain-agnostic generative {AI}},
	volume = {5},
	copyright = {2022 The Author(s)},
	issn = {2398-6352},
	url = {https://www.nature.com/articles/s41746-022-00634-5},
	doi = {10.1038/s41746-022-00634-5},
	abstract = {The text-guided diffusion model GLIDE (Guided Language to Image Diffusion for Generation and Editing) is the state of the art in text-to-image generative artificial intelligence (AI). GLIDE has rich representations, but medical applications of this model have not been systematically explored. If GLIDE had useful medical knowledge, it could be used for medical image analysis tasks, a domain in which AI systems are still highly engineered towards a single use-case. Here we show that the publicly available GLIDE model has reasonably strong representations of key topics in cancer research and oncology, in particular the general style of histopathology images and multiple facets of diseases, pathological processes and laboratory assays. However, GLIDE seems to lack useful representations of the style and content of radiology data. Our findings demonstrate that domain-agnostic generative AI models can learn relevant medical concepts without explicit training. Thus, GLIDE and similar models might be useful for medical image processing tasks in the future - particularly with additional domain-specific fine-tuning.},
	language = {en},
	number = {1},
	urldate = {2023-06-30},
	journal = {npj Digital Medicine},
	author = {Kather, Jakob Nikolas and Ghaffari Laleh, Narmin and Foersch, Sebastian and Truhn, Daniel},
	month = jul,
	year = {2022},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Biomedical engineering, Data integration, Machine learning},
	pages = {1--5},
}

@inproceedings{wiles_fine-grained_2021,
	title = {A {Fine}-{Grained} {Analysis} on {Distribution} {Shift}},
	url = {https://openreview.net/forum?id=Dl4LetuLdyK},
	abstract = {Robustness to distribution shifts is critical for deploying machine learning models in the real world. Despite this necessity, there has been little work in defining the underlying mechanisms that cause these shifts and evaluating the robustness of algorithms across multiple, different distribution shifts. To this end, we introduce a framework that enables fine-grained analysis of various distribution shifts. We provide a holistic analysis of current state-of-the-art methods by evaluating 19 distinct methods grouped into five categories across both synthetic and real-world datasets. Overall, we train more than 85K models. Our experimental framework can be easily extended to include new methods, shifts, and datasets. We find, unlike previous work (Gulrajani \& Lopez-Paz, 2021), that progress has been made over a standard ERM baseline; in particular, pretraining and augmentations (learned or heuristic) offer large gains in many cases. However, the best methods are not consistent over different datasets and shifts. We will open source our experimental framework, allowing future work to evaluate new methods over multiple shifts to obtain a more complete picture of a method's effectiveness. Code is available at github.com/deepmind/distribution\_shift\_framework.},
	language = {en},
	urldate = {2023-06-30},
	author = {Wiles, Olivia and Gowal, Sven and Stimberg, Florian and Rebuffi, Sylvestre-Alvise and Ktena, Ira and Dvijotham, Krishnamurthy Dj and Cemgil, Ali Taylan},
	month = oct,
	year = {2021},
}

@inproceedings{gulrajani_search_2020,
	title = {In {Search} of {Lost} {Domain} {Generalization}},
	url = {https://openreview.net/forum?id=lQdXeXDoWtI},
	abstract = {The goal of domain generalization algorithms is to predict well on distributions different from those seen during training. While a myriad of domain generalization algorithms exist, inconsistencies in experimental conditions---datasets, network architectures, and model selection criteria---render fair comparisons difficult. The goal of this paper is to understand how useful domain generalization algorithms are in realistic settings. As a first step, we realize that model selection is non-trivial for domain generalization tasks, and we argue that algorithms without a model selection criterion remain incomplete. Next we implement DomainBed, a testbed for domain generalization including seven benchmarks, fourteen algorithms, and three model selection criteria. When conducting extensive experiments using DomainBed we find that when carefully implemented and tuned, ERM outperforms the state-of-the-art in terms of average performance. Furthermore, no algorithm included in DomainBed outperforms ERM by more than one point when evaluated under the same experimental conditions. We hope that the release of DomainBed, alongside contributions from fellow researchers, will streamline reproducible and rigorous advances in domain generalization.},
	language = {en},
	urldate = {2023-06-30},
	author = {Gulrajani, Ishaan and Lopez-Paz, David},
	month = oct,
	year = {2020},
}

@article{varoquaux_machine_2022,
	title = {Machine learning for medical imaging: methodological failures and recommendations for the future},
	volume = {5},
	copyright = {2022 The Author(s)},
	issn = {2398-6352},
	shorttitle = {Machine learning for medical imaging},
	url = {https://www.nature.com/articles/s41746-022-00592-y},
	doi = {10.1038/s41746-022-00592-y},
	abstract = {Research in computer analysis of medical images bears many promises to improve patients’ health. However, a number of systematic challenges are slowing down the progress of the field, from limitations of the data, such as biases, to research incentives, such as optimizing for publication. In this paper we review roadblocks to developing and assessing methods. Building our analysis on evidence from the literature and data challenges, we show that at every step, potential biases can creep in. On a positive note, we also discuss on-going efforts to counteract these problems. Finally we provide recommendations on how to further address these problems in the future.},
	language = {en},
	number = {1},
	urldate = {2023-06-30},
	journal = {npj Digital Medicine},
	author = {Varoquaux, Gaël and Cheplygina, Veronika},
	month = apr,
	year = {2022},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Computer science, Medical research, Research data},
	pages = {1--8},
}

@inproceedings{zhang_datasetgan_2021,
	title = {{DatasetGAN}: {Efficient} {Labeled} {Data} {Factory} {With} {Minimal} {Human} {Effort}},
	shorttitle = {{DatasetGAN}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Zhang_DatasetGAN_Efficient_Labeled_Data_Factory_With_Minimal_Human_Effort_CVPR_2021_paper.html},
	language = {en},
	urldate = {2023-06-30},
	author = {Zhang, Yuxuan and Ling, Huan and Gao, Jun and Yin, Kangxue and Lafleche, Jean-Francois and Barriuso, Adela and Torralba, Antonio and Fidler, Sanja},
	year = {2021},
	pages = {10145--10155},
}

@inproceedings{peebles_gan-supervised_2022,
	title = {{GAN}-{Supervised} {Dense} {Visual} {Alignment}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Peebles_GAN-Supervised_Dense_Visual_Alignment_CVPR_2022_paper.html},
	language = {en},
	urldate = {2023-06-30},
	author = {Peebles, William and Zhu, Jun-Yan and Zhang, Richard and Torralba, Antonio and Efros, Alexei A. and Shechtman, Eli},
	year = {2022},
	pages = {13470--13481},
}

@inproceedings{huang_auggan_2018,
	title = {{AugGAN}: {Cross} {Domain} {Adaptation} with {GAN}-based {Data} {Augmentation}},
	shorttitle = {{AugGAN}},
	url = {https://openaccess.thecvf.com/content_ECCV_2018/html/Sheng-Wei_Huang_AugGAN_Cross_Domain_ECCV_2018_paper.html},
	urldate = {2023-06-30},
	author = {Huang, Sheng-Wei and Lin, Che-Tsung and Chen, Shu-Ping and Wu, Yen-Yi and Hsu, Po-Hao and Lai, Shang-Hong},
	year = {2018},
	pages = {718--731},
}

@inproceedings{sankaranarayanan_generate_2018,
	title = {Generate to {Adapt}: {Aligning} {Domains} {Using} {Generative} {Adversarial} {Networks}},
	shorttitle = {Generate to {Adapt}},
	url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Sankaranarayanan_Generate_to_Adapt_CVPR_2018_paper.html},
	urldate = {2023-06-30},
	author = {Sankaranarayanan, Swami and Balaji, Yogesh and Castillo, Carlos D. and Chellappa, Rama},
	year = {2018},
	pages = {8503--8512},
}

@article{frid-adar_gan-based_2018,
	title = {{GAN}-based synthetic medical image augmentation for increased {CNN} performance in liver lesion classification},
	volume = {321},
	issn = {0925-2312},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231218310749},
	doi = {10.1016/j.neucom.2018.09.013},
	abstract = {Deep learning methods, and in particular convolutional neural networks (CNNs), have led to an enormous breakthrough in a wide range of computer vision tasks, primarily by using large-scale annotated datasets. However, obtaining such datasets in the medical domain remains a challenge. In this paper, we present methods for generating synthetic medical images using recently presented deep learning Generative Adversarial Networks (GANs). Furthermore, we show that generated medical images can be used for synthetic data augmentation, and improve the performance of CNN for medical image classification. Our novel method is demonstrated on a limited dataset of computed tomography (CT) images of 182 liver lesions (53 cysts, 64 metastases and 65 hemangiomas). We first exploit GAN architectures for synthesizing high quality liver lesion ROIs. Then we present a novel scheme for liver lesion classification using CNN. Finally, we train the CNN using classic data augmentation and our synthetic data augmentation and compare performance. In addition, we explore the quality of our synthesized examples using visualization and expert assessment. The classification performance using only classic data augmentation yielded 78.6\% sensitivity and 88.4\% specificity. By adding the synthetic data augmentation the results increased to 85.7\% sensitivity and 92.4\% specificity. We believe that this approach to synthetic data augmentation can generalize to other medical classification applications and thus support radiologists’ efforts to improve diagnosis.},
	language = {en},
	urldate = {2023-06-30},
	journal = {Neurocomputing},
	author = {Frid-Adar, Maayan and Diamant, Idit and Klang, Eyal and Amitai, Michal and Goldberger, Jacob and Greenspan, Hayit},
	month = dec,
	year = {2018},
	keywords = {Convolutional neural networks, Data augmentation, Deep learning, Generative adversarial network, Image synthesis, Lesion classification, Liver lesions},
	pages = {321--331},
}

@inproceedings{li_bigdatasetgan_2022,
	title = {{BigDatasetGAN}: {Synthesizing} {ImageNet} {With} {Pixel}-{Wise} {Annotations}},
	shorttitle = {{BigDatasetGAN}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Li_BigDatasetGAN_Synthesizing_ImageNet_With_Pixel-Wise_Annotations_CVPR_2022_paper.html},
	language = {en},
	urldate = {2023-06-30},
	author = {Li, Daiqing and Ling, Huan and Kim, Seung Wook and Kreis, Karsten and Fidler, Sanja and Torralba, Antonio},
	year = {2022},
	pages = {21330--21340},
}

@inproceedings{sariyildiz_fake_2023,
	title = {Fake {It} {Till} {You} {Make} {It}: {Learning} {Transferable} {Representations} {From} {Synthetic} {ImageNet} {Clones}},
	language = {en},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Sariyildiz, Mert Bulent and Alahari, Karteek and Larlus, Diane and Kalantidis, Yannis},
	year = {2023},
}

@article{menghani_efficient_2023,
	title = {Efficient {Deep} {Learning}: {A} {Survey} on {Making} {Deep} {Learning} {Models} {Smaller}, {Faster}, and {Better}},
	volume = {55},
	issn = {0360-0300},
	shorttitle = {Efficient {Deep} {Learning}},
	url = {https://dl.acm.org/doi/10.1145/3578938},
	doi = {10.1145/3578938},
	abstract = {Deep learning has revolutionized the fields of computer vision, natural language understanding, speech recognition, information retrieval, and more. However, with the progressive improvements in deep learning models, their number of parameters, latency, and resources required to train, among others, have all increased significantly. Consequently, it has become important to pay attention to these footprint metrics of a model as well, not just its quality. We present and motivate the problem of efficiency in deep learning, followed by a thorough survey of the five core areas of model efficiency (spanning modeling techniques, infrastructure, and hardware) and the seminal work there. We also present an experiment-based guide along with code for practitioners to optimize their model training and deployment. We believe this is the first comprehensive survey in the efficient deep learning space that covers the landscape of model efficiency from modeling techniques to hardware support. It is our hope that this survey would provide readers with the mental model and the necessary understanding of the field to apply generic efficiency techniques to immediately get significant improvements, and also equip them with ideas for further research and experimentation to achieve additional gains.},
	number = {12},
	urldate = {2023-06-29},
	journal = {ACM Computing Surveys},
	author = {Menghani, Gaurav},
	month = mar,
	year = {2023},
	keywords = {Efficient deep learning, distillation, efficient artificial intelligence, efficient machine learning, model compression, model optimization, pruning, quantization, sparsity},
	pages = {259:1--259:37},
}

@inproceedings{sun_can_2021,
	title = {Can {Shape} {Structure} {Features} {Improve} {Model} {Robustness} {Under} {Diverse} {Adversarial} {Settings}?},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Sun_Can_Shape_Structure_Features_Improve_Model_Robustness_Under_Diverse_Adversarial_ICCV_2021_paper.html},
	language = {en},
	urldate = {2023-06-28},
	author = {Sun, Mingjie and Li, Zichao and Xiao, Chaowei and Qiu, Haonan and Kailkhura, Bhavya and Liu, Mingyan and Li, Bo},
	year = {2021},
	pages = {7526--7535},
}

@misc{dong_data_2021,
	title = {Data {Quality} {Matters} {For} {Adversarial} {Training}: {An} {Empirical} {Study}},
	shorttitle = {Data {Quality} {Matters} {For} {Adversarial} {Training}},
	url = {http://arxiv.org/abs/2102.07437},
	doi = {10.48550/arXiv.2102.07437},
	abstract = {Multiple intriguing problems are hovering in adversarial training, including robust overfitting, robustness overestimation, and robustness-accuracy trade-off. These problems pose great challenges to both reliable evaluation and practical deployment. Here, we empirically show that these problems share one common cause -- low-quality samples in the dataset. Specifically, we first propose a strategy to measure the data quality based on the learning behaviors of the data during adversarial training and find that low-quality data may not be useful and even detrimental to the adversarial robustness. We then design controlled experiments to investigate the interconnections between data quality and problems in adversarial training. We find that when low-quality data is removed, robust overfitting and robustness overestimation can be largely alleviated; and robustness-accuracy trade-off becomes less significant. These observations not only verify our intuition about data quality but may also open new opportunities to advance adversarial training.},
	urldate = {2023-06-28},
	publisher = {arXiv},
	author = {Dong, Chengyu and Liu, Liyuan and Shang, Jingbo},
	month = oct,
	year = {2021},
	note = {arXiv:2102.07437 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@inproceedings{mendonca_adversarial_2022,
	title = {Adversarial training with informed data selection},
	doi = {10.23919/EUSIPCO55093.2022.9909845},
	abstract = {With the increasing amount of available data and advances in computing capabilities, deep neural networks (DNNs) have been successfully employed to solve challenging tasks in various areas, including healthcare, climete, and finance. Nevertheless, state-of-the-art DNNs are susceptible to quasi-imperceptible perturbed versions of the original images - adversarial examples. These perturbations of the network input can lead to disastrous implications in critical areas where wrong decisions can directly affect human lives. Adversarial training is the most efficient solution to defend the network against these malicious attacks. However, adversarial trained networks generally come with lower clean accuracy and higher computational complexity. This work proposes a data selection (DS) strategy to be applied in the mini - batch training. Based on the cross-entropy loss, the most relevant samples in the batch are selected to update the model parameters in the backpropagation. The simulation results show that a good compromise can be obtained regarding robustness and standard accuracy, whereas the computational complexity of the backpropagation pass is reduced.},
	booktitle = {2022 30th {European} {Signal} {Processing} {Conference} ({EUSIPCO})},
	author = {Mendonça, Marcele O. K. and Maroto, Javier and Frossard, Pascal and Diniz, Paulo S. R.},
	month = aug,
	year = {2022},
	note = {ISSN: 2076-1465},
	keywords = {Backpropagation, Computational modeling, Deep learning, Neural networks, Robustness, Simulation, Training, adversarial training, data-selection, robustness-accuracy tradeoff, sampling strategy},
	pages = {608--612},
}

@misc{xiong_it_2023,
	title = {It {Is} {All} {About} {Data}: {A} {Survey} on the {Effects} of {Data} on {Adversarial} {Robustness}},
	shorttitle = {It {Is} {All} {About} {Data}},
	url = {http://arxiv.org/abs/2303.09767},
	abstract = {Adversarial examples are inputs to machine learning models that an attacker has intentionally designed to confuse the model into making a mistake. Such examples pose a serious threat to the applicability of machine-learning-based systems, especially in life- and safety-critical domains. To address this problem, the area of adversarial robustness investigates mechanisms behind adversarial attacks and defenses against these attacks. This survey reviews literature that focuses on the effects of data used by a model on the model's adversarial robustness. It systematically identifies and summarizes the state-of-the-art research in this area and further discusses gaps of knowledge and promising future research directions.},
	urldate = {2023-06-28},
	publisher = {arXiv},
	author = {Xiong, Peiyu and Tegegn, Michael and Sarin, Jaskeerat Singh and Pal, Shubhraneel and Rubin, Julia},
	month = mar,
	year = {2023},
	note = {arXiv:2303.09767 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@inproceedings{xu_adversarial_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Adversarial {T}-{Shirt}! {Evading} {Person} {Detectors} in a {Physical} {World}},
	isbn = {978-3-030-58558-7},
	doi = {10.1007/978-3-030-58558-7_39},
	abstract = {It is known that deep neural networks (DNNs) are vulnerable to adversarial attacks. The so-called physical adversarial examples deceive DNN-based decision makers by attaching adversarial patches to real objects. However, most of the existing works on physical adversarial attacks focus on static objects such as glass frames, stop signs and images attached to cardboard. In this work, we propose Adversarial T-shirts, a robust physical adversarial example for evading person detectors even if it could undergo non-rigid deformation due to a moving person’s pose changes. To the best of our knowledge, this is the first work that models the effect of deformation for designing physical adversarial examples with respect to non-rigid objects such as T-shirts. We show that the proposed method achieves 74\% and 57\% attack success rates in the digital and physical worlds respectively against YOLOv2. In contrast, the state-of-the-art physical attack method to fool a person detector only achieves 18\% attack success rate. Furthermore, by leveraging min-max optimization, we extend our method to the ensemble attack setting against two object detectors YOLO-v2 and Faster R-CNN simultaneously.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Xu, Kaidi and Zhang, Gaoyuan and Liu, Sijia and Fan, Quanfu and Sun, Mengshu and Chen, Hongge and Chen, Pin-Yu and Wang, Yanzhi and Lin, Xue},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	keywords = {Deep learning, Object detection, Physical adversarial attack},
	pages = {665--681},
}

@inproceedings{laugros_are_2019,
	title = {Are {Adversarial} {Robustness} and {Common} {Perturbation} {Robustness} {Independant} {Attributes} ?},
	url = {https://openaccess.thecvf.com/content_ICCVW_2019/html/RLQ/Laugros_Are_Adversarial_Robustness_and_Common_Perturbation_Robustness_Independant_Attributes__ICCVW_2019_paper.html},
	urldate = {2023-05-26},
	booktitle = {Proceedings of the {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} {Workshops}},
	author = {Laugros, Alfred and Caplier, Alice and Ospici, Matthieu},
	year = {2019},
	pages = {0--0},
}

@inproceedings{devillers_does_2021,
	address = {Online},
	title = {Does language help generalization in vision models?},
	url = {https://aclanthology.org/2021.conll-1.13},
	doi = {10.18653/v1/2021.conll-1.13},
	abstract = {Vision models trained on multimodal datasets can benefit from the wide availability of large image-caption datasets. A recent model (CLIP) was found to generalize well in zero-shot and transfer learning settings. This could imply that linguistic or “semantic grounding” confers additional generalization abilities to the visual feature space. Here, we systematically evaluate various multimodal architectures and vision-only models in terms of unsupervised clustering, few-shot learning, transfer learning and adversarial robustness. In each setting, multimodal training produced no additional generalization capability compared to standard supervised visual training. We conclude that work is still required for semantic grounding to help improve vision models.},
	urldate = {2023-06-28},
	booktitle = {Proceedings of the 25th {Conference} on {Computational} {Natural} {Language} {Learning}},
	publisher = {Association for Computational Linguistics},
	author = {Devillers, Benjamin and Choksi, Bhavin and Bielawski, Romain and VanRullen, Rufin},
	month = nov,
	year = {2021},
	pages = {171--182},
}

@misc{santurkar_is_2022,
	title = {Is a {Caption} {Worth} a {Thousand} {Images}? {A} {Controlled} {Study} for {Representation} {Learning}},
	shorttitle = {Is a {Caption} {Worth} a {Thousand} {Images}?},
	url = {http://arxiv.org/abs/2207.07635},
	doi = {10.48550/arXiv.2207.07635},
	abstract = {The development of CLIP [Radford et al., 2021] has sparked a debate on whether language supervision can result in vision models with more transferable representations than traditional image-only methods. Our work studies this question through a carefully controlled comparison of two approaches in terms of their ability to learn representations that generalize to downstream classification tasks. We find that when the pre-training dataset meets certain criteria -- it is sufficiently large and contains descriptive captions with low variability -- image-only methods do not match CLIP's transfer performance, even when they are trained with more image data. However, contrary to what one might expect, there are practical settings in which these criteria are not met, wherein added supervision through captions is actually detrimental. Motivated by our findings, we devise simple prescriptions to enable CLIP to better leverage the language information present in existing pre-training datasets.},
	urldate = {2023-06-28},
	publisher = {arXiv},
	author = {Santurkar, Shibani and Dubois, Yann and Taori, Rohan and Liang, Percy and Hashimoto, Tatsunori},
	month = jul,
	year = {2022},
	note = {arXiv:2207.07635 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{wortsman_robust_2022,
	address = {New Orleans, LA, USA},
	title = {Robust fine-tuning of zero-shot models},
	isbn = {978-1-66546-946-3},
	url = {https://ieeexplore.ieee.org/document/9878622/},
	doi = {10.1109/CVPR52688.2022.00780},
	abstract = {Large pre-trained models such as CLIP or ALIGN offer consistent accuracy across a range of data distributions when performing zero-shot inference (i.e., without fine-tuning on a specific dataset). Although existing fine-tuning methods substantially improve accuracy on a given target distribution, they often reduce robustness to distribution shifts. We address this tension by introducing a simple and effective method for improving robustness while fine-tuning: ensembling the weights of the zero-shot and fine-tuned models (WiSE-FT). Compared to standard fine-tuning, WiSE-FT provides large accuracy improvements under distribution shift, while preserving high accuracy on the target distribution. On ImageNet and five derived distribution shifts, WiSE-FT improves accuracy under distribution shift by 4 to 6 percentage points (pp) over prior work while increasing ImageNet accuracy by 1.6 pp. WiSE-FT achieves similarly large robustness gains (2 to 23 pp) on a diverse set of six further distribution shifts, and accuracy gains of 0.8 to 3.3 pp compared to standard fine-tuning on commonly used transfer learning datasets. These improvements come at no additional computational cost during fine-tuning or inference.},
	language = {en},
	urldate = {2023-06-28},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Wortsman, Mitchell and Ilharco, Gabriel and Kim, Jong Wook and Li, Mike and Kornblith, Simon and Roelofs, Rebecca and Lopes, Raphael Gontijo and Hajishirzi, Hannaneh and Farhadi, Ali and Namkoong, Hongseok and Schmidt, Ludwig},
	month = jun,
	year = {2022},
	pages = {7949--7961},
}

@article{patterson_carbon_2022,
	title = {The {Carbon} {Footprint} of {Machine} {Learning} {Training} {Will} {Plateau}, {Then} {Shrink}},
	volume = {55},
	issn = {1558-0814},
	doi = {10.1109/MC.2022.3148714},
	abstract = {Machine learning (ML) workloads have rapidly grown, raising concerns about their carbon footprint. We show four best practices to reduce ML training energy and carbon dioxide emissions. If the whole ML field adopts best practices, we predict that by 2030, total carbon emissions from training will decline.},
	number = {7},
	journal = {Computer},
	author = {Patterson, David and Gonzalez, Joseph and Hölzle, Urs and Le, Quoc and Liang, Chen and Munguia, Lluis-Miquel and Rothchild, Daniel and So, David R. and Texier, Maud and Dean, Jeff},
	month = jul,
	year = {2022},
	note = {Conference Name: Computer},
	keywords = {Best practices, Carbon dioxide, Carbon footprint, Emissions, Machine learning, Training data},
	pages = {18--28},
}

@misc{rutinowski_self-perception_2023,
	title = {The {Self}-{Perception} and {Political} {Biases} of {ChatGPT}},
	url = {http://arxiv.org/abs/2304.07333},
	doi = {10.48550/arXiv.2304.07333},
	abstract = {This contribution analyzes the self-perception and political biases of OpenAI's Large Language Model ChatGPT. Taking into account the first small-scale reports and studies that have emerged, claiming that ChatGPT is politically biased towards progressive and libertarian points of view, this contribution aims to provide further clarity on this subject. For this purpose, ChatGPT was asked to answer the questions posed by the political compass test as well as similar questionnaires that are specific to the respective politics of the G7 member states. These eight tests were repeated ten times each and revealed that ChatGPT seems to hold a bias towards progressive views. The political compass test revealed a bias towards progressive and libertarian views, with the average coordinates on the political compass being (-6.48, -5.99) (with (0, 0) the center of the compass, i.e., centrism and the axes ranging from -10 to 10), supporting the claims of prior research. The political questionnaires for the G7 member states indicated a bias towards progressive views but no significant bias between authoritarian and libertarian views, contradicting the findings of prior reports, with the average coordinates being (-3.27, 0.58). In addition, ChatGPT's Big Five personality traits were tested using the OCEAN test and its personality type was queried using the Myers-Briggs Type Indicator (MBTI) test. Finally, the maliciousness of ChatGPT was evaluated using the Dark Factor test. These three tests were also repeated ten times each, revealing that ChatGPT perceives itself as highly open and agreeable, has the Myers-Briggs personality type ENFJ, and is among the 15\% of test-takers with the least pronounced dark traits.},
	urldate = {2023-06-27},
	publisher = {arXiv},
	author = {Rutinowski, Jérôme and Franke, Sven and Endendyk, Jan and Dormuth, Ina and Pauly, Markus},
	month = apr,
	year = {2023},
	note = {arXiv:2304.07333 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computers and Society, Computer Science - Human-Computer Interaction},
}

@inproceedings{bender_dangers_2021,
	address = {New York, NY, USA},
	series = {{FAccT} '21},
	title = {On the {Dangers} of {Stochastic} {Parrots}: {Can} {Language} {Models} {Be} {Too} {Big}? 🦜},
	isbn = {978-1-4503-8309-7},
	shorttitle = {On the {Dangers} of {Stochastic} {Parrots}},
	url = {https://dl.acm.org/doi/10.1145/3442188.3445922},
	doi = {10.1145/3442188.3445922},
	abstract = {The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.},
	urldate = {2023-06-27},
	booktitle = {Proceedings of the 2021 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {Association for Computing Machinery},
	author = {Bender, Emily M. and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
	month = mar,
	year = {2021},
	pages = {610--623},
}

@misc{patterson_carbon_2021,
	title = {Carbon {Emissions} and {Large} {Neural} {Network} {Training}},
	url = {http://arxiv.org/abs/2104.10350},
	doi = {10.48550/arXiv.2104.10350},
	abstract = {The computation demand for machine learning (ML) has grown rapidly recently, which comes with a number of costs. Estimating the energy cost helps measure its environmental impact and finding greener strategies, yet it is challenging without detailed information. We calculate the energy use and carbon footprint of several recent large models-T5, Meena, GShard, Switch Transformer, and GPT-3-and refine earlier estimates for the neural architecture search that found Evolved Transformer. We highlight the following opportunities to improve energy efficiency and CO2 equivalent emissions (CO2e): Large but sparsely activated DNNs can consume {\textless}1/10th the energy of large, dense DNNs without sacrificing accuracy despite using as many or even more parameters. Geographic location matters for ML workload scheduling since the fraction of carbon-free energy and resulting CO2e vary {\textasciitilde}5X-10X, even within the same country and the same organization. We are now optimizing where and when large models are trained. Specific datacenter infrastructure matters, as Cloud datacenters can be {\textasciitilde}1.4-2X more energy efficient than typical datacenters, and the ML-oriented accelerators inside them can be {\textasciitilde}2-5X more effective than off-the-shelf systems. Remarkably, the choice of DNN, datacenter, and processor can reduce the carbon footprint up to {\textasciitilde}100-1000X. These large factors also make retroactive estimates of energy cost difficult. To avoid miscalculations, we believe ML papers requiring large computational resources should make energy consumption and CO2e explicit when practical. We are working to be more transparent about energy use and CO2e in our future research. To help reduce the carbon footprint of ML, we believe energy usage and CO2e should be a key metric in evaluating models, and we are collaborating with MLPerf developers to include energy usage during training and inference in this industry standard benchmark.},
	urldate = {2023-06-27},
	publisher = {arXiv},
	author = {Patterson, David and Gonzalez, Joseph and Le, Quoc and Liang, Chen and Munguia, Lluis-Miquel and Rothchild, Daniel and So, David and Texier, Maud and Dean, Jeff},
	month = apr,
	year = {2021},
	note = {arXiv:2104.10350 [cs]},
	keywords = {Computer Science - Computers and Society, Computer Science - Machine Learning},
}

@article{schwartz_green_2020,
	title = {Green {AI}},
	volume = {63},
	issn = {0001-0782},
	url = {https://dl.acm.org/doi/10.1145/3381831},
	doi = {10.1145/3381831},
	abstract = {Creating efficiency in AI research will decrease its carbon footprint and increase its inclusivity as deep learning study should not require the deepest pockets.},
	number = {12},
	urldate = {2023-06-27},
	journal = {Communications of the ACM},
	author = {Schwartz, Roy and Dodge, Jesse and Smith, Noah A. and Etzioni, Oren},
	month = nov,
	year = {2020},
	pages = {54--63},
}

@misc{weidinger_ethical_2021,
	title = {Ethical and social risks of harm from {Language} {Models}},
	url = {http://arxiv.org/abs/2112.04359},
	doi = {10.48550/arXiv.2112.04359},
	abstract = {This paper aims to help structure the risk landscape associated with large-scale Language Models (LMs). In order to foster advances in responsible innovation, an in-depth understanding of the potential risks posed by these models is needed. A wide range of established and anticipated risks are analysed in detail, drawing on multidisciplinary expertise and literature from computer science, linguistics, and social sciences. We outline six specific risk areas: I. Discrimination, Exclusion and Toxicity, II. Information Hazards, III. Misinformation Harms, V. Malicious Uses, V. Human-Computer Interaction Harms, VI. Automation, Access, and Environmental Harms. The first area concerns the perpetuation of stereotypes, unfair discrimination, exclusionary norms, toxic language, and lower performance by social group for LMs. The second focuses on risks from private data leaks or LMs correctly inferring sensitive information. The third addresses risks arising from poor, false or misleading information including in sensitive domains, and knock-on risks such as the erosion of trust in shared information. The fourth considers risks from actors who try to use LMs to cause harm. The fifth focuses on risks specific to LLMs used to underpin conversational agents that interact with human users, including unsafe use, manipulation or deception. The sixth discusses the risk of environmental harm, job automation, and other challenges that may have a disparate effect on different social groups or communities. In total, we review 21 risks in-depth. We discuss the points of origin of different risks and point to potential mitigation approaches. Lastly, we discuss organisational responsibilities in implementing mitigations, and the role of collaboration and participation. We highlight directions for further research, particularly on expanding the toolkit for assessing and evaluating the outlined risks in LMs.},
	urldate = {2023-06-26},
	publisher = {arXiv},
	author = {Weidinger, Laura and Mellor, John and Rauh, Maribeth and Griffin, Conor and Uesato, Jonathan and Huang, Po-Sen and Cheng, Myra and Glaese, Mia and Balle, Borja and Kasirzadeh, Atoosa and Kenton, Zac and Brown, Sasha and Hawkins, Will and Stepleton, Tom and Biles, Courtney and Birhane, Abeba and Haas, Julia and Rimell, Laura and Hendricks, Lisa Anne and Isaac, William and Legassick, Sean and Irving, Geoffrey and Gabriel, Iason},
	month = dec,
	year = {2021},
	note = {arXiv:2112.04359 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computers and Society},
}

@article{mokander_auditing_2023,
	title = {Auditing large language models: a three-layered approach},
	issn = {2730-5961},
	shorttitle = {Auditing large language models},
	url = {https://doi.org/10.1007/s43681-023-00289-2},
	doi = {10.1007/s43681-023-00289-2},
	abstract = {Large language models (LLMs) represent a major advance in artificial intelligence (AI) research. However, the widespread use of LLMs is also coupled with significant ethical and social challenges. Previous research has pointed towards auditing as a promising governance mechanism to help ensure that AI systems are designed and deployed in ways that are ethical, legal, and technically robust. However, existing auditing procedures fail to address the governance challenges posed by LLMs, which display emergent capabilities and are adaptable to a wide range of downstream tasks. In this article, we address that gap by outlining a novel blueprint for how to audit LLMs. Specifically, we propose a three-layered approach, whereby governance audits (of technology providers that design and disseminate LLMs), model audits (of LLMs after pre-training but prior to their release), and application audits (of applications based on LLMs) complement and inform each other. We show how audits, when conducted in a structured and coordinated manner on all three levels, can be a feasible and effective mechanism for identifying and managing some of the ethical and social risks posed by LLMs. However, it is important to remain realistic about what auditing can reasonably be expected to achieve. Therefore, we discuss the limitations not only of our three-layered approach but also of the prospect of auditing LLMs at all. Ultimately, this article seeks to expand the methodological toolkit available to technology providers and policymakers who wish to analyse and evaluate LLMs from technical, ethical, and legal perspectives.},
	language = {en},
	urldate = {2023-06-25},
	journal = {AI and Ethics},
	author = {Mökander, Jakob and Schuett, Jonas and Kirk, Hannah Rose and Floridi, Luciano},
	month = may,
	year = {2023},
	keywords = {Artificial intelligence, Auditing, Ethics, Foundation models, Governance, Large language models, Natural language processing, Policy, Risk management},
}

@article{char_implementing_2018,
	title = {Implementing {Machine} {Learning} in {Health} {Care} — {Addressing} {Ethical} {Challenges}},
	volume = {378},
	issn = {0028-4793},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5962261/},
	doi = {10.1056/NEJMp1714229},
	number = {11},
	urldate = {2023-06-25},
	journal = {The New England journal of medicine},
	author = {Char, Danton S. and Shah, Nigam H. and Magnus, David},
	month = mar,
	year = {2018},
	pmid = {29539284},
	pmcid = {PMC5962261},
	pages = {981--983},
}

@misc{greshake_not_2023,
	title = {Not what you've signed up for: {Compromising} {Real}-{World} {LLM}-{Integrated} {Applications} with {Indirect} {Prompt} {Injection}},
	shorttitle = {Not what you've signed up for},
	url = {http://arxiv.org/abs/2302.12173},
	doi = {10.48550/arXiv.2302.12173},
	abstract = {Large Language Models (LLMs) are increasingly being integrated into various applications. The functionalities of recent LLMs can be flexibly modulated via natural language prompts. This renders them susceptible to targeted adversarial prompting, e.g., Prompt Injection (PI) attacks enable attackers to override original instructions and employed controls. So far, it was assumed that the user is directly prompting the LLM. But, what if it is not the user prompting? We argue that LLM-Integrated Applications blur the line between data and instructions. We reveal new attack vectors, using Indirect Prompt Injection, that enable adversaries to remotely (without a direct interface) exploit LLM-integrated applications by strategically injecting prompts into data likely to be retrieved. We derive a comprehensive taxonomy from a computer security perspective to systematically investigate impacts and vulnerabilities, including data theft, worming, information ecosystem contamination, and other novel security risks. We demonstrate our attacks' practical viability against both real-world systems, such as Bing's GPT-4 powered Chat and code-completion engines, and synthetic applications built on GPT-4. We show how processing retrieved prompts can act as arbitrary code execution, manipulate the application's functionality, and control how and if other APIs are called. Despite the increasing integration and reliance on LLMs, effective mitigations of these emerging threats are currently lacking. By raising awareness of these vulnerabilities and providing key insights into their implications, we aim to promote the safe and responsible deployment of these powerful models and the development of robust defenses that protect users and systems from potential attacks.},
	urldate = {2023-06-25},
	publisher = {arXiv},
	author = {Greshake, Kai and Abdelnabi, Sahar and Mishra, Shailesh and Endres, Christoph and Holz, Thorsten and Fritz, Mario},
	month = may,
	year = {2023},
	note = {arXiv:2302.12173 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computers and Society, Computer Science - Cryptography and Security},
}

@misc{li_multi-step_2023,
	title = {Multi-step {Jailbreaking} {Privacy} {Attacks} on {ChatGPT}},
	url = {http://arxiv.org/abs/2304.05197},
	doi = {10.48550/arXiv.2304.05197},
	abstract = {With the rapid progress of large language models (LLMs), many downstream NLP tasks can be well solved given appropriate prompts. Though model developers and researchers work hard on dialog safety to avoid generating harmful content from LLMs, it is still challenging to steer AI-generated content (AIGC) for the human good. As powerful LLMs are devouring existing text data from various domains (e.g., GPT-3 is trained on 45TB texts), it is natural to doubt whether the private information is included in the training data and what privacy threats can these LLMs and their downstream applications bring. In this paper, we study the privacy threats from OpenAI's ChatGPT and the New Bing enhanced by ChatGPT and show that application-integrated LLMs may cause new privacy threats. To this end, we conduct extensive experiments to support our claims and discuss LLMs' privacy implications.},
	urldate = {2023-06-25},
	publisher = {arXiv},
	author = {Li, Haoran and Guo, Dadi and Fan, Wei and Xu, Mingshi and Huang, Jie and Meng, Fanpu and Song, Yangqiu},
	month = may,
	year = {2023},
	note = {arXiv:2304.05197 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Cryptography and Security},
}

@inproceedings{duan_are_2023,
	title = {Are {Diffusion} {Models} {Vulnerable} to {Membership} {Inference} {Attacks}?},
	url = {https://openreview.net/forum?id=ZMDv1Mo89E},
	abstract = {Diffusion-based generative models have shown great potential for image synthesis, but there is a lack of research on the security and privacy risks they may pose. In this paper, we investigate the vulnerability of diffusion models to Membership Inference Attacks (MIAs), a common privacy concern. Our results indicate that existing MIAs designed for GANs or VAE are largely ineffective on diffusion models, either due to inapplicable scenarios (e.g., requiring the discriminator of GANs) or inappropriate assumptions (e.g., closer distances between synthetic samples and member samples). To address this gap, we propose Step-wise Error Comparing Membership Inference (SecMI), a query-based MIA that infers memberships by assessing the matching of forward process posterior estimation at each timestep. SecMI follows the common overfitting assumption in MIA where member samples normally have smaller estimation errors, compared with hold-out samples. We consider both the standard diffusion models, e.g., DDPM, and the text-to-image diffusion models, e.g., Latent Diffusion Models and Stable Diffusion. Experimental results demonstrate that our methods precisely infer the membership with high confidence on both of the two scenarios across multiple different datasets. Code is available at https://github.com/jinhaoduan/SecMI.},
	language = {en},
	urldate = {2023-06-25},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Duan, Jinhao and Kong, Fei and Wang, Shiqi and Shi, Xiaoshuang and Xu, Kaidi},
	month = jun,
	year = {2023},
}

@article{zhang_membership_2022,
	title = {Membership inference attacks against synthetic health data},
	volume = {125},
	issn = {1532-0464},
	url = {https://www.sciencedirect.com/science/article/pii/S1532046421003063},
	doi = {10.1016/j.jbi.2021.103977},
	abstract = {Synthetic data generation has emerged as a promising method to protect patient privacy while sharing individual-level health data. Intuitively, sharing synthetic data should reduce disclosure risks because no explicit linkage is retained between the synthetic records and the real data upon which it is based. However, the risks associated with synthetic data are still evolving, and what seems protected today may not be tomorrow. In this paper, we show that membership inference attacks, whereby an adversary infers if the data from certain target individuals (known to the adversary a priori) were relied upon by the synthetic data generation process, can be substantially enhanced through state-of-the-art machine learning frameworks, which calls into question the protective nature of existing synthetic data generators. Specifically, we formulate the membership inference problem from the perspective of the data holder, who aims to perform a disclosure risk assessment prior to sharing any health data. To support such an assessment, we introduce a framework for effective membership inference against synthetic health data without specific assumptions about the generative model or a well-defined data structure, leveraging the principles of contrastive representation learning. To illustrate the potential for such an attack, we conducted experiments against synthesis approaches using two datasets derived from several health data resources (Vanderbilt University Medical Center, the All of Us Research Program) to determine the upper bound of risk brought by an adversary who invokes an optimal strategy. The results indicate that partially synthetic data are vulnerable to membership inference at a very high rate. By contrast, fully synthetic data are only marginally susceptible and, in most cases, could be deemed sufficiently protected from membership inference.},
	language = {en},
	urldate = {2023-06-25},
	journal = {Journal of Biomedical Informatics},
	author = {Zhang, Ziqi and Yan, Chao and Malin, Bradley A.},
	month = jan,
	year = {2022},
	keywords = {Contrastive representation learning, Electronic health record, Membership inference, Synthetic data},
	pages = {103977},
}

@inproceedings{haim_reconstructing_2022,
	title = {Reconstructing {Training} {Data} {From} {Trained} {Neural} {Networks}},
	url = {https://openreview.net/forum?id=Sxk8Bse3RKO},
	abstract = {Understanding to what extent neural networks memorize training data is an intriguing question with practical and theoretical implications. In this paper we show that in some cases a significant fraction of the training data can in fact be reconstructed from the parameters of a trained neural network classifier. We propose a novel reconstruction scheme that stems from recent theoretical results about the implicit bias in training neural networks with gradient-based methods. To the best of our knowledge, our results are the first to show that reconstructing a large portion of the actual training samples from a trained neural network classifier is generally possible. This has negative implications on privacy, as it can be used as an attack for revealing sensitive training data. We demonstrate our method for binary MLP classifiers on a few standard computer vision datasets.},
	language = {en},
	urldate = {2023-06-16},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Haim, Niv and Vardi, Gal and Yehudai, Gilad and Shamir, Ohad and Irani, Michal},
	month = oct,
	year = {2022},
}

@inproceedings{carlini_quantifying_2022,
	title = {Quantifying {Memorization} {Across} {Neural} {Language} {Models}},
	url = {https://openreview.net/forum?id=TatRHT_1cK},
	abstract = {Large language models (LMs) have been shown to memorize parts of their training data, and when prompted appropriately, they will emit the memorized training data verbatim. This is undesirable because memorization violates privacy (exposing user data), degrades utility (repeated easy-to-memorize text is often low quality), and hurts fairness (some texts are memorized over others). We describe three log-linear relationships that quantify the degree to which LMs emit memorized training data. Memorization significantly grows as we increase (1) the capacity of a model, (2) the number of times an example has been duplicated, and (3) the number of tokens of context used to prompt the model. Surprisingly, we find the situation becomes complicated when generalizing these results across model families. On the whole, we find that memorization in LMs is more prevalent than previously believed and will likely get worse as models continues to scale, at least without active mitigations.},
	language = {en},
	urldate = {2023-06-24},
	author = {Carlini, Nicholas and Ippolito, Daphne and Jagielski, Matthew and Lee, Katherine and Tramer, Florian and Zhang, Chiyuan},
	month = sep,
	year = {2022},
}

@inproceedings{carlini_extracting_2021,
	title = {Extracting {Training} {Data} from {Large} {Language} {Models}},
	isbn = {978-1-939133-24-3},
	url = {https://www.usenix.org/conference/usenixsecurity21/presentation/carlini-extracting},
	language = {en},
	urldate = {2023-06-24},
	author = {Carlini, Nicholas and Tramèr, Florian and Wallace, Eric and Jagielski, Matthew and Herbert-Voss, Ariel and Lee, Katherine and Roberts, Adam and Brown, Tom and Song, Dawn and Erlingsson, Úlfar and Oprea, Alina and Raffel, Colin},
	year = {2021},
	pages = {2633--2650},
}

@misc{zhuo_red_2023,
	title = {Red teaming {ChatGPT} via {Jailbreaking}: {Bias}, {Robustness}, {Reliability} and {Toxicity}},
	shorttitle = {Red teaming {ChatGPT} via {Jailbreaking}},
	url = {http://arxiv.org/abs/2301.12867},
	doi = {10.48550/arXiv.2301.12867},
	abstract = {Recent breakthroughs in natural language processing (NLP) have permitted the synthesis and comprehension of coherent text in an open-ended way, therefore translating the theoretical algorithms into practical applications. The large language models (LLMs) have significantly impacted businesses such as report summarization software and copywriters. Observations indicate, however, that LLMs may exhibit social prejudice and toxicity, posing ethical and societal dangers of consequences resulting from irresponsibility. Large-scale benchmarks for accountable LLMs should consequently be developed. Although several empirical investigations reveal the existence of a few ethical difficulties in advanced LLMs, there is little systematic examination and user study of the risks and harmful behaviors of current LLM usage. To further educate future efforts on constructing ethical LLMs responsibly, we perform a qualitative research method called ``red teaming'' on OpenAI's ChatGPT{\textbackslash}footnote\{In this paper, ChatGPT refers to the version released on Dec 15th.\} to better understand the practical features of ethical dangers in recent LLMs. We analyze ChatGPT comprehensively from four perspectives: 1) {\textbackslash}textit\{Bias\} 2) {\textbackslash}textit\{Reliability\} 3) {\textbackslash}textit\{Robustness\} 4) {\textbackslash}textit\{Toxicity\}. In accordance with our stated viewpoints, we empirically benchmark ChatGPT on multiple sample datasets. We find that a significant number of ethical risks cannot be addressed by existing benchmarks, and hence illustrate them via additional case studies. In addition, we examine the implications of our findings on AI ethics and harmal behaviors of ChatGPT, as well as future problems and practical design considerations for responsible LLMs. We believe that our findings may give light on future efforts to determine and mitigate the ethical hazards posed by machines in LLM applications.},
	urldate = {2023-06-24},
	publisher = {arXiv},
	author = {Zhuo, Terry Yue and Huang, Yujin and Chen, Chunyang and Xing, Zhenchang},
	month = may,
	year = {2023},
	note = {arXiv:2301.12867 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Software Engineering},
}

@misc{munoz-gonzalez_poisoning_2019,
	title = {Poisoning {Attacks} with {Generative} {Adversarial} {Nets}},
	url = {http://arxiv.org/abs/1906.07773},
	doi = {10.48550/arXiv.1906.07773},
	abstract = {Machine learning algorithms are vulnerable to poisoning attacks: An adversary can inject malicious points in the training dataset to influence the learning process and degrade the algorithm's performance. Optimal poisoning attacks have already been proposed to evaluate worst-case scenarios, modelling attacks as a bi-level optimization problem. Solving these problems is computationally demanding and has limited applicability for some models such as deep networks. In this paper we introduce a novel generative model to craft systematic poisoning attacks against machine learning classifiers generating adversarial training examples, i.e. samples that look like genuine data points but that degrade the classifier's accuracy when used for training. We propose a Generative Adversarial Net with three components: generator, discriminator, and the target classifier. This approach allows us to model naturally the detectability constrains that can be expected in realistic attacks and to identify the regions of the underlying data distribution that can be more vulnerable to data poisoning. Our experimental evaluation shows the effectiveness of our attack to compromise machine learning classifiers, including deep networks.},
	urldate = {2023-06-24},
	publisher = {arXiv},
	author = {Muñoz-González, Luis and Pfitzner, Bjarne and Russo, Matteo and Carnerero-Cano, Javier and Lupu, Emil C.},
	month = sep,
	year = {2019},
	note = {arXiv:1906.07773 [cs, stat]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{sadasivan_cuda_2023,
	title = {{CUDA}: {Convolution}-{Based} {Unlearnable} {Datasets}},
	shorttitle = {{CUDA}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Sadasivan_CUDA_Convolution-Based_Unlearnable_Datasets_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-06-03},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Sadasivan, Vinu Sankar and Soltanolkotabi, Mahdi and Feizi, Soheil},
	year = {2023},
	pages = {3862--3871},
}

@inproceedings{devries_instance_2020,
	title = {Instance {Selection} for {GANs}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/99f6a934a7cf277f2eaece8e3ce619b2-Abstract.html},
	abstract = {Recent advances in Generative Adversarial Networks (GANs) have led to their widespread adoption for the purposes of generating high quality synthetic imagery. While capable of generating photo-realistic images, these models often produce unrealistic samples which fall outside of the data manifold. Several recently proposed techniques attempt to avoid spurious samples, either by rejecting them after generation, or by truncating the model's latent space. While effective, these methods are inefficient, as a large fraction of training time and model capacity are dedicated towards samples that will ultimately go unused. In this work we propose a novel approach to improve sample quality: altering the training dataset via instance selection before model training has taken place. By refining the empirical data distribution before training, we redirect model capacity towards high-density regions, which ultimately improves sample fidelity, lowers model capacity requirements, and significantly reduces training time. Code is available at https://github.com/uoguelph-mlrg/instanceselectionfor\_gans.},
	urldate = {2023-06-23},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {DeVries, Terrance and Drozdzal, Michal and Taylor, Graham W},
	year = {2020},
	pages = {13285--13296},
}

@inproceedings{xie_active_2023,
	title = {Active {Finetuning}: {Exploiting} {Annotation} {Budget} in the {Pretraining}-{Finetuning} {Paradigm}},
	shorttitle = {Active {Finetuning}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Xie_Active_Finetuning_Exploiting_Annotation_Budget_in_the_Pretraining-Finetuning_Paradigm_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-06-23},
	author = {Xie, Yichen and Lu, Han and Yan, Junchi and Yang, Xiaokang and Tomizuka, Masayoshi and Zhan, Wei},
	year = {2023},
	pages = {23715--23724},
}

@inproceedings{kim_coreset_2023,
	title = {Coreset {Sampling} {From} {Open}-{Set} for {Fine}-{Grained} {Self}-{Supervised} {Learning}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Kim_Coreset_Sampling_From_Open-Set_for_Fine-Grained_Self-Supervised_Learning_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-06-23},
	author = {Kim, Sungnyun and Bae, Sangmin and Yun, Se-Young},
	year = {2023},
	pages = {7537--7547},
}

@inproceedings{mink_security_2023,
	title = {"{Security} is not my field, {I}’m a stats guy": {A} {Qualitative} {Root} {Cause} {Analysis} of {Barriers} to {Adversarial} {Machine} {Learning} {Defenses} in {Industry}},
	shorttitle = {"{Security} is not my field, {I}’m a stats guy"},
	url = {https://teamusec.de/publications/conf-usenix-mink23/},
	abstract = {Adversarial machine learning (AML) has the potential to leak
training data, force arbitrary classifications, and greatly degrade overall performance of machine learning models, all
of which academics and companies alike consider as serious
issues. Despite this, seminal work has found that most organizations insufficiently protect against such threats. While
the lack of defenses to AML is most commonly attributed to
missing knowledge, it is unknown why mitigations are unrealized in industry projects. To better understand the reasons
behind the lack of deployed AML defenses, we conduct semistructured interviews (n=21) with data scientists and data
engineers to explore what barriers impede the effective implementation of such defenses. We find that practitioners’ ability
to deploy defenses is hampered by three primary factors: a
lack of institutional motivation and educational resources for
these concepts, an inability to adequately assess their AML
risk and make subsequent decisions, and organizational structures and goals that discourage implementation in favor of
other objectives. We conclude by discussing practical recommendations for companies and practitioners to be made more
aware of these risks, and better prepared to respond.},
	language = {en},
	urldate = {2023-06-22},
	author = {Mink, Jaron and Kaur, Harjot and Schmüser, Juliane and Fahl, Sascha and Acar, Yasemin},
	month = aug,
	year = {2023},
}

@article{balcan_analysis_2023,
	title = {An {Analysis} of {Robustness} of {Non}-{Lipschitz} {Networks}},
	volume = {24},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v24/22-0410.html},
	abstract = {Despite significant advances, deep networks remain highly susceptible to adversarial attack. One fundamental challenge is that small input perturbations can often produce large movements in the network’s final-layer feature space. In this paper, we define an attack model that abstracts this challenge, to help understand its intrinsic properties. In our model, the adversary may move data an arbitrary distance in feature space but only in random low-dimensional subspaces. We prove such adversaries can be quite powerful: defeating any algorithm that must classify any input it is given. However, by allowing the algorithm to abstain on unusual inputs, we show such adversaries can be overcome when classes are reasonably well-separated in feature space. We further provide strong theoretical guarantees for setting algorithm parameters to optimize over accuracy-abstention trade-offs using data-driven methods. Our results provide new robustness guarantees for nearest-neighbor style algorithms, and also have application to contrastive learning, where we empirically demonstrate the ability of such algorithms to obtain high robust accuracy with low abstention rates. Our model is also motivated by strategic classification, where entities being classified aim to manipulate their observable features to produce a preferred classification, and we provide new insights into that area as well.},
	number = {98},
	urldate = {2023-06-22},
	journal = {Journal of Machine Learning Research},
	author = {Balcan, Maria-Florina and Blum, Avrim and Sharma, Dravyansh and Zhang, Hongyang},
	year = {2023},
	pages = {1--43},
}

@misc{touvron_llama_2023,
	title = {{LLaMA}: {Open} and {Efficient} {Foundation} {Language} {Models}},
	shorttitle = {{LLaMA}},
	url = {http://arxiv.org/abs/2302.13971},
	doi = {10.48550/arXiv.2302.13971},
	abstract = {We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.},
	urldate = {2023-06-22},
	publisher = {arXiv},
	author = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timothée and Rozière, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and Rodriguez, Aurelien and Joulin, Armand and Grave, Edouard and Lample, Guillaume},
	month = feb,
	year = {2023},
	note = {arXiv:2302.13971 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{mckenzie_inverse_2023,
	title = {Inverse {Scaling}: {When} {Bigger} {Isn}'t {Better}},
	shorttitle = {Inverse {Scaling}},
	url = {http://arxiv.org/abs/2306.09479},
	doi = {10.48550/arXiv.2306.09479},
	abstract = {Work on scaling laws has found that large language models (LMs) show predictable improvements to overall loss with increased scale (model size, training data, and compute). Here, we present evidence for the claim that LMs may show inverse scaling, or worse task performance with increased scale, e.g., due to flaws in the training objective and data. We present empirical evidence of inverse scaling on 11 datasets collected by running a public contest, the Inverse Scaling Prize, with a substantial prize pool. Through analysis of the datasets, along with other examples found in the literature, we identify four potential causes of inverse scaling: (i) preference to repeat memorized sequences over following in-context instructions, (ii) imitation of undesirable patterns in the training data, (iii) tasks containing an easy distractor task which LMs could focus on, rather than the harder real task, and (iv) correct but misleading few-shot demonstrations of the task. We release the winning datasets at https://inversescaling.com/data to allow for further investigation of inverse scaling. Our tasks have helped drive the discovery of U-shaped and inverted-U scaling trends, where an initial trend reverses, suggesting that scaling trends are less reliable at predicting the behavior of larger-scale models than previously understood. Overall, our results suggest that there are tasks for which increased model scale alone may not lead to progress, and that more careful thought needs to go into the data and objectives for training language models.},
	urldate = {2023-06-22},
	publisher = {arXiv},
	author = {McKenzie, Ian R. and Lyzhov, Alexander and Pieler, Michael and Parrish, Alicia and Mueller, Aaron and Prabhu, Ameya and McLean, Euan and Kirtland, Aaron and Ross, Alexis and Liu, Alisa and Gritsevskiy, Andrew and Wurgaft, Daniel and Kauffman, Derik and Recchia, Gabriel and Liu, Jiacheng and Cavanagh, Joe and Weiss, Max and Huang, Sicong and Droid, The Floating and Tseng, Tom and Korbak, Tomasz and Shen, Xudong and Zhang, Yuhui and Zhou, Zhengping and Kim, Najoung and Bowman, Samuel R. and Perez, Ethan},
	month = jun,
	year = {2023},
	note = {arXiv:2306.09479 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computers and Society},
}

@misc{openai_gpt-4_2023,
	title = {{GPT}-4 {Technical} {Report}},
	url = {http://arxiv.org/abs/2303.08774},
	doi = {10.48550/arXiv.2303.08774},
	abstract = {We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10\% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.},
	urldate = {2023-06-22},
	publisher = {arXiv},
	author = {OpenAI},
	month = mar,
	year = {2023},
	note = {arXiv:2303.08774 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{liu_summary_2023,
	title = {Summary of {ChatGPT}/{GPT}-4 {Research} and {Perspective} {Towards} the {Future} of {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2304.01852},
	doi = {10.48550/arXiv.2304.01852},
	abstract = {This paper presents a comprehensive survey of ChatGPT and GPT-4, state-of-the-art large language models (LLM) from the GPT series, and their prospective applications across diverse domains. Indeed, key innovations such as large-scale pre-training that captures knowledge across the entire world wide web, instruction fine-tuning and Reinforcement Learning from Human Feedback (RLHF) have played significant roles in enhancing LLMs' adaptability and performance. We performed an in-depth analysis of 194 relevant papers on arXiv, encompassing trend analysis, word cloud representation, and distribution analysis across various application domains. The findings reveal a significant and increasing interest in ChatGPT/GPT-4 research, predominantly centered on direct natural language processing applications, while also demonstrating considerable potential in areas ranging from education and history to mathematics, medicine, and physics. This study endeavors to furnish insights into ChatGPT's capabilities, potential implications, ethical concerns, and offer direction for future advancements in this field.},
	urldate = {2023-06-22},
	publisher = {arXiv},
	author = {Liu, Yiheng and Han, Tianle and Ma, Siyuan and Zhang, Jiayue and Yang, Yuanyuan and Tian, Jiaming and He, Hao and Li, Antong and He, Mengshen and Liu, Zhengliang and Wu, Zihao and Zhu, Dajiang and Li, Xiang and Qiang, Ning and Shen, Dingang and Liu, Tianming and Ge, Bao},
	month = may,
	year = {2023},
	note = {arXiv:2304.01852 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@article{lee_benefits_2023,
	title = {Benefits, {Limits}, and {Risks} of {GPT}-4 as an {AI} {Chatbot} for {Medicine}},
	volume = {388},
	issn = {0028-4793},
	url = {https://doi.org/10.1056/NEJMsr2214184},
	doi = {10.1056/NEJMsr2214184},
	number = {13},
	urldate = {2023-06-22},
	journal = {New England Journal of Medicine},
	author = {Lee, Peter and Bubeck, Sebastien and Petro, Joseph},
	month = mar,
	year = {2023},
	pmid = {36988602},
	note = {Publisher: Massachusetts Medical Society
\_eprint: https://doi.org/10.1056/NEJMsr2214184},
	pages = {1233--1239},
}

@misc{nori_capabilities_2023,
	title = {Capabilities of {GPT}-4 on {Medical} {Challenge} {Problems}},
	url = {http://arxiv.org/abs/2303.13375},
	doi = {10.48550/arXiv.2303.13375},
	abstract = {Large language models (LLMs) have demonstrated remarkable capabilities in natural language understanding and generation across various domains, including medicine. We present a comprehensive evaluation of GPT-4, a state-of-the-art LLM, on medical competency examinations and benchmark datasets. GPT-4 is a general-purpose model that is not specialized for medical problems through training or engineered to solve clinical tasks. Our analysis covers two sets of official practice materials for the USMLE, a three-step examination program used to assess clinical competency and grant licensure in the United States. We also evaluate performance on the MultiMedQA suite of benchmark datasets. Beyond measuring model performance, experiments were conducted to investigate the influence of test questions containing both text and images on model performance, probe for memorization of content during training, and study probability calibration, which is of critical importance in high-stakes applications like medicine. Our results show that GPT-4, without any specialized prompt crafting, exceeds the passing score on USMLE by over 20 points and outperforms earlier general-purpose models (GPT-3.5) as well as models specifically fine-tuned on medical knowledge (Med-PaLM, a prompt-tuned version of Flan-PaLM 540B). In addition, GPT-4 is significantly better calibrated than GPT-3.5, demonstrating a much-improved ability to predict the likelihood that its answers are correct. We also explore the behavior of the model qualitatively through a case study that shows the ability of GPT-4 to explain medical reasoning, personalize explanations to students, and interactively craft new counterfactual scenarios around a medical case. Implications of the findings are discussed for potential uses of GPT-4 in medical education, assessment, and clinical practice, with appropriate attention to challenges of accuracy and safety.},
	urldate = {2023-06-22},
	publisher = {arXiv},
	author = {Nori, Harsha and King, Nicholas and McKinney, Scott Mayer and Carignan, Dean and Horvitz, Eric},
	month = apr,
	year = {2023},
	note = {arXiv:2303.13375 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@article{wei_emergent_2022,
	title = {Emergent {Abilities} of {Large} {Language} {Models}},
	issn = {2835-8856},
	url = {https://openreview.net/forum?id=yzkSU5zdwD},
	abstract = {Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence raises the question of whether additional scaling could potentially further expand the range of capabilities of language models.},
	language = {en},
	urldate = {2023-06-21},
	journal = {Transactions on Machine Learning Research},
	author = {Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and Chi, Ed H. and Hashimoto, Tatsunori and Vinyals, Oriol and Liang, Percy and Dean, Jeff and Fedus, William},
	month = aug,
	year = {2022},
}

@misc{assran_self-supervised_2023,
	title = {Self-{Supervised} {Learning} from {Images} with a {Joint}-{Embedding} {Predictive} {Architecture}},
	url = {http://arxiv.org/abs/2301.08243},
	doi = {10.48550/arXiv.2301.08243},
	abstract = {This paper demonstrates an approach for learning highly semantic image representations without relying on hand-crafted data-augmentations. We introduce the Image-based Joint-Embedding Predictive Architecture (I-JEPA), a non-generative approach for self-supervised learning from images. The idea behind I-JEPA is simple: from a single context block, predict the representations of various target blocks in the same image. A core design choice to guide I-JEPA towards producing semantic representations is the masking strategy; specifically, it is crucial to (a) sample target blocks with sufficiently large scale (semantic), and to (b) use a sufficiently informative (spatially distributed) context block. Empirically, when combined with Vision Transformers, we find I-JEPA to be highly scalable. For instance, we train a ViT-Huge/14 on ImageNet using 16 A100 GPUs in under 72 hours to achieve strong downstream performance across a wide range of tasks, from linear classification to object counting and depth prediction.},
	urldate = {2023-06-21},
	publisher = {arXiv},
	author = {Assran, Mahmoud and Duval, Quentin and Misra, Ishan and Bojanowski, Piotr and Vincent, Pascal and Rabbat, Michael and LeCun, Yann and Ballas, Nicolas},
	month = apr,
	year = {2023},
	note = {arXiv:2301.08243 [cs, eess]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing},
}

@misc{cui_decoupled_2023,
	title = {Decoupled {Kullback}-{Leibler} {Divergence} {Loss}},
	url = {http://arxiv.org/abs/2305.13948},
	abstract = {In this paper, we delve deeper into the Kullback-Leibler (KL) Divergence loss and observe that it is equivalent to the Doupled Kullback-Leibler (DKL) Divergence loss that consists of 1) a weighted Mean Square Error (wMSE) loss and 2) a Cross-Entropy loss incorporating soft labels. From our analysis of the DKL loss, we have identified two areas for improvement. Firstly, we address the limitation of DKL in scenarios like knowledge distillation by breaking its asymmetry property in training optimization. This modification ensures that the wMSE component is always effective during training, providing extra constructive cues. Secondly, we introduce global information into DKL for intra-class consistency regularization. With these two enhancements, we derive the Improved Kullback-Leibler (IKL) Divergence loss and evaluate its effectiveness by conducting experiments on CIFAR-10/100 and ImageNet datasets, focusing on adversarial training and knowledge distillation tasks. The proposed approach achieves new state-of-the-art performance on both tasks, demonstrating the substantial practical merits. Code and models will be available soon at https://github.com/jiequancui/DKL.},
	urldate = {2023-06-21},
	publisher = {arXiv},
	author = {Cui, Jiequan and Tian, Zhuotao and Zhong, Zhisheng and Qi, Xiaojuan and Yu, Bei and Zhang, Hanwang},
	month = may,
	year = {2023},
	note = {arXiv:2305.13948 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@inproceedings{kumar_fine-tuning_2022,
	title = {Fine-{Tuning} can {Distort} {Pretrained} {Features} and {Underperform} {Out}-of-{Distribution}},
	url = {https://openreview.net/forum?id=UYneFzXSJWh},
	abstract = {When transferring a pretrained model to a downstream task, two popular methods are full fine-tuning (updating all the model parameters) and linear probing (updating only the last linear layer---the "head"). It is well known that fine-tuning leads to better accuracy in-distribution (ID). However, in this paper, we find that fine-tuning can achieve worse accuracy than linear probing out-of-distribution (OOD) when the pretrained features are good and the distribution shift is large. On 10 distribution shift datasets (BREEDS-Living17, BREEDS-Entity30, DomainNet, CIFAR \${\textbackslash}to\$ STL, CIFAR-10.1, FMoW, ImageNetV2, ImageNet-R, ImageNet-A, ImageNet-Sketch), fine-tuning obtains on average 2\% higher accuracy ID but 7\% lower accuracy OOD than linear probing. We show theoretically that this tradeoff between ID and OOD accuracy arises even in a simple setting: fine-tuning overparameterized two-layer linear networks. We prove that the OOD error of fine-tuning is high when we initialize with a fixed or random head---this is because while fine-tuning learns the head, the lower layers of the neural network change simultaneously and distort the pretrained features. Our analysis suggests that the easy two-step strategy of linear probing then full fine-tuning (LP-FT), sometimes used as a fine-tuning heuristic, combines the benefits of both fine-tuning and linear probing. Empirically, LP-FT outperforms both fine-tuning and linear probing on the above datasets (1\% better ID, 10\% better OOD than full fine-tuning).},
	language = {en},
	urldate = {2023-06-21},
	author = {Kumar, Ananya and Raghunathan, Aditi and Jones, Robbie Matthew and Ma, Tengyu and Liang, Percy},
	month = jan,
	year = {2022},
}

@inproceedings{dong_how_2021,
	title = {How {Should} {Pre}-{Trained} {Language} {Models} {Be} {Fine}-{Tuned} {Towards} {Adversarial} {Robustness}?},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper/2021/hash/22b1f2e0983160db6f7bb9f62f4dbb39-Abstract.html},
	abstract = {The fine-tuning of pre-trained language models has a great success in many NLP fields. Yet, it is strikingly vulnerable to adversarial examples, e.g., word substitution attacks using only synonyms can easily fool a BERT-based sentiment analysis model. In this paper, we demonstrate that adversarial training, the prevalent defense technique, does not directly fit a conventional fine-tuning scenario, because it suffers severely from catastrophic forgetting: failing to retain the generic and robust linguistic features that have already been captured by the pre-trained model. In this light, we propose Robust Informative Fine-Tuning (RIFT), a novel adversarial fine-tuning method from an information-theoretical perspective. In particular, RIFT encourages an objective model to retain the features learned from the pre-trained model throughout the entire fine-tuning process, whereas a conventional one only uses the pre-trained weights for initialization. Experimental results show that RIFT consistently outperforms the state-of-the-arts on two popular NLP tasks: sentiment analysis and natural language inference, under different attacks across various pre-trained language models.},
	urldate = {2023-06-21},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Dong, Xinshuai and Luu, Anh Tuan and Lin, Min and Yan, Shuicheng and Zhang, Hanwang},
	year = {2021},
	pages = {4356--4369},
}

@inproceedings{zheng_unlabeled_2017,
	title = {Unlabeled {Samples} {Generated} by {GAN} {Improve} the {Person} {Re}-{Identification} {Baseline} in {Vitro}},
	url = {https://openaccess.thecvf.com/content_iccv_2017/html/Zheng_Unlabeled_Samples_Generated_ICCV_2017_paper.html},
	urldate = {2023-06-21},
	author = {Zheng, Zhedong and Zheng, Liang and Yang, Yi},
	year = {2017},
	pages = {3754--3762},
}

@misc{chen_utility_2023,
	title = {The utility of {ChatGPT} for cancer treatment information},
	copyright = {© 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.medrxiv.org/content/10.1101/2023.03.16.23287316v1},
	doi = {10.1101/2023.03.16.23287316},
	abstract = {The use of large language models (LLMs) such as ChatGPT for medical question-answering is becoming increasingly popular. However, there are concerns that these models may generate and amplify medical misinformation. Because cancer patients frequently seek to educate themselves through online resources, some individuals will likely use ChatGPT to obtain cancer treatment information. This study evaluated the performance and robustness of ChatGPT in providing breast, prostate, and lung cancer treatment recommendations that align with National Comprehensive Cancer Network (NCCN) guidelines. Four prompt templates were created to explore how differences in how the query is posed impacts response. ChatGPT output was scored by 3 oncologists and a 4th oncologist adjudicated in cases of disagreement. ChatGPT provided at least one NCCN-concordant recommendation for 102/104 (98\%) prompts. However, 35/102 (34.3\%) of these also included a recommendation that was at least partially non-concordant with NCCN guidelines. Responses varied based on prompt type. In conclusion, ChatGPT did not perform well at reliably and robustly providing cancer treatment recommendations. Patients and clinicians should be aware of the limitations of ChatGPT and similar technologies for self-education.},
	language = {en},
	urldate = {2023-06-21},
	publisher = {medRxiv},
	author = {Chen, Shan and Kann, Benjamin H. and Foote, Michael B. and Aerts, Hugo JWL and Savova, Guergana K. and Mak, Raymond H. and Bitterman, Danielle S.},
	month = mar,
	year = {2023},
	note = {Pages: 2023.03.16.23287316},
}

@misc{shen_chatgpt_2023,
	title = {In {ChatGPT} {We} {Trust}? {Measuring} and {Characterizing} the {Reliability} of {ChatGPT}},
	shorttitle = {In {ChatGPT} {We} {Trust}?},
	url = {http://arxiv.org/abs/2304.08979},
	doi = {10.48550/arXiv.2304.08979},
	abstract = {The way users acquire information is undergoing a paradigm shift with the advent of ChatGPT. Unlike conventional search engines, ChatGPT retrieves knowledge from the model itself and generates answers for users. ChatGPT's impressive question-answering (QA) capability has attracted more than 100 million users within a short period of time but has also raised concerns regarding its reliability. In this paper, we perform the first large-scale measurement of ChatGPT's reliability in the generic QA scenario with a carefully curated set of 5,695 questions across ten datasets and eight domains. We find that ChatGPT's reliability varies across different domains, especially underperforming in law and science questions. We also demonstrate that system roles, originally designed by OpenAI to allow users to steer ChatGPT's behavior, can impact ChatGPT's reliability. We further show that ChatGPT is vulnerable to adversarial examples, and even a single character change can negatively affect its reliability in certain cases. We believe that our study provides valuable insights into ChatGPT's reliability and underscores the need for strengthening the reliability and security of large language models (LLMs).},
	urldate = {2023-06-21},
	publisher = {arXiv},
	author = {Shen, Xinyue and Chen, Zeyuan and Backes, Michael and Zhang, Yang},
	month = apr,
	year = {2023},
	note = {arXiv:2304.08979 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@misc{wang_robustness_2023,
	title = {On the {Robustness} of {ChatGPT}: {An} {Adversarial} and {Out}-of-distribution {Perspective}},
	shorttitle = {On the {Robustness} of {ChatGPT}},
	url = {http://arxiv.org/abs/2302.12095},
	doi = {10.48550/arXiv.2302.12095},
	abstract = {ChatGPT is a recent chatbot service released by OpenAI and is receiving increasing attention over the past few months. While evaluations of various aspects of ChatGPT have been done, its robustness, i.e., the performance to unexpected inputs, is still unclear to the public. Robustness is of particular concern in responsible AI, especially for safety-critical applications. In this paper, we conduct a thorough evaluation of the robustness of ChatGPT from the adversarial and out-of-distribution (OOD) perspective. To do so, we employ the AdvGLUE and ANLI benchmarks to assess adversarial robustness and the Flipkart review and DDXPlus medical diagnosis datasets for OOD evaluation. We select several popular foundation models as baselines. Results show that ChatGPT shows consistent advantages on most adversarial and OOD classification and translation tasks. However, the absolute performance is far from perfection, which suggests that adversarial and OOD robustness remains a significant threat to foundation models. Moreover, ChatGPT shows astounding performance in understanding dialogue-related texts and we find that it tends to provide informal suggestions for medical tasks instead of definitive answers. Finally, we present in-depth discussions of possible research directions.},
	urldate = {2023-06-21},
	publisher = {arXiv},
	author = {Wang, Jindong and Hu, Xixu and Hou, Wenxin and Chen, Hao and Zheng, Runkai and Wang, Yidong and Yang, Linyi and Huang, Haojun and Ye, Wei and Geng, Xiubo and Jiao, Binxin and Zhang, Yue and Xie, Xing},
	month = mar,
	year = {2023},
	note = {arXiv:2302.12095 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{li_trustworthy_2022,
	title = {Trustworthy {AI}: {From} {Principles} to {Practices}},
	shorttitle = {Trustworthy {AI}},
	url = {http://arxiv.org/abs/2110.01167},
	doi = {10.48550/arXiv.2110.01167},
	abstract = {The rapid development of Artificial Intelligence (AI) technology has enabled the deployment of various systems based on it. However, many current AI systems are found vulnerable to imperceptible attacks, biased against underrepresented groups, lacking in user privacy protection. These shortcomings degrade user experience and erode people's trust in all AI systems. In this review, we provide AI practitioners with a comprehensive guide for building trustworthy AI systems. We first introduce the theoretical framework of important aspects of AI trustworthiness, including robustness, generalization, explainability, transparency, reproducibility, fairness, privacy preservation, and accountability. To unify currently available but fragmented approaches toward trustworthy AI, we organize them in a systematic approach that considers the entire lifecycle of AI systems, ranging from data acquisition to model development, to system development and deployment, finally to continuous monitoring and governance. In this framework, we offer concrete action items for practitioners and societal stakeholders (e.g., researchers, engineers, and regulators) to improve AI trustworthiness. Finally, we identify key opportunities and challenges for the future development of trustworthy AI systems, where we identify the need for a paradigm shift toward comprehensively trustworthy AI systems.},
	urldate = {2023-06-20},
	publisher = {arXiv},
	author = {Li, Bo and Qi, Peng and Liu, Bo and Di, Shuai and Liu, Jingen and Pei, Jiquan and Yi, Jinfeng and Zhou, Bowen},
	month = may,
	year = {2022},
	note = {arXiv:2110.01167 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{huang_survey_2023,
	title = {A {Survey} of {Safety} and {Trustworthiness} of {Large} {Language} {Models} through the {Lens} of {Verification} and {Validation}},
	url = {http://arxiv.org/abs/2305.11391},
	doi = {10.48550/arXiv.2305.11391},
	abstract = {Large Language Models (LLMs) have exploded a new heatwave of AI, for their ability to engage end-users in human-level conversations with detailed and articulate answers across many knowledge domains. In response to their fast adoption in many industrial applications, this survey concerns their safety and trustworthiness. First, we review known vulnerabilities of the LLMs, categorising them into inherent issues, intended attacks, and unintended bugs. Then, we consider if and how the Verification and Validation (V\&V) techniques, which have been widely developed for traditional software and deep learning models such as convolutional neural networks, can be integrated and further extended throughout the lifecycle of the LLMs to provide rigorous analysis to the safety and trustworthiness of LLMs and their applications. Specifically, we consider four complementary techniques: falsification and evaluation, verification, runtime monitoring, and ethical use. Considering the fast development of LLMs, this survey does not intend to be complete (although it includes 300 references), especially when it comes to the applications of LLMs in various domains, but rather a collection of organised literature reviews and discussions to support the quick understanding of the safety and trustworthiness issues from the perspective of V\&V.},
	urldate = {2023-06-20},
	publisher = {arXiv},
	author = {Huang, Xiaowei and Ruan, Wenjie and Huang, Wei and Jin, Gaojie and Dong, Yi and Wu, Changshun and Bensalem, Saddek and Mu, Ronghui and Qi, Yi and Zhao, Xingyu and Cai, Kaiwen and Zhang, Yanghao and Wu, Sihao and Xu, Peipei and Wu, Dengyu and Freitas, Andre and Mustafa, Mustafa A.},
	month = may,
	year = {2023},
	note = {arXiv:2305.11391 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@article{ding_parameter-efficient_2023,
	title = {Parameter-efficient fine-tuning of large-scale pre-trained language models},
	volume = {5},
	copyright = {2023 The Author(s)},
	issn = {2522-5839},
	url = {https://www.nature.com/articles/s42256-023-00626-4},
	doi = {10.1038/s42256-023-00626-4},
	abstract = {With the prevalence of pre-trained language models (PLMs) and the pre-training–fine-tuning paradigm, it has been continuously shown that larger models tend to yield better performance. However, as PLMs scale up, fine-tuning and storing all the parameters is prohibitively costly and eventually becomes practically infeasible. This necessitates a new branch of research focusing on the parameter-efficient adaptation of PLMs, which optimizes a small portion of the model parameters while keeping the rest fixed, drastically cutting down computation and storage costs. In general, it demonstrates that large-scale models could be effectively stimulated by the optimization of a few parameters. Despite the various designs, here we discuss and analyse the approaches under a more consistent and accessible term ‘delta-tuning’, where ‘delta’ a mathematical notation often used to denote changes, is borrowed to refer to the portion of parameters that are ‘changed’ during training. We formally describe the problem and propose a unified categorization criterion for existing delta-tuning methods to explore their correlations and differences. We also discuss the theoretical principles underlying the effectiveness of delta-tuning and interpret them from the perspectives of optimization and optimal control. Furthermore, we provide a holistic empirical study on over 100 natural language processing tasks and investigate various aspects of delta-tuning. With comprehensive study and analysis, our research demonstrates the theoretical and practical properties of delta-tuning in the adaptation of PLMs.},
	language = {en},
	number = {3},
	urldate = {2023-06-20},
	journal = {Nature Machine Intelligence},
	author = {Ding, Ning and Qin, Yujia and Yang, Guang and Wei, Fuchao and Yang, Zonghan and Su, Yusheng and Hu, Shengding and Chen, Yulin and Chan, Chi-Min and Chen, Weize and Yi, Jing and Zhao, Weilin and Wang, Xiaozhi and Liu, Zhiyuan and Zheng, Hai-Tao and Chen, Jianfei and Liu, Yang and Tang, Jie and Li, Juanzi and Sun, Maosong},
	month = mar,
	year = {2023},
	note = {Number: 3
Publisher: Nature Publishing Group},
	keywords = {Computer science, Electrical and electronic engineering},
	pages = {220--235},
}

@misc{dai_auggpt_2023,
	title = {{AugGPT}: {Leveraging} {ChatGPT} for {Text} {Data} {Augmentation}},
	shorttitle = {{AugGPT}},
	url = {http://arxiv.org/abs/2302.13007},
	abstract = {Text data augmentation is an effective strategy for overcoming the challenge of limited sample sizes in many natural language processing (NLP) tasks. This challenge is especially prominent in the few-shot learning scenario, where the data in the target domain is generally much scarcer and of lowered quality. A natural and widely-used strategy to mitigate such challenges is to perform data augmentation to better capture the data invariance and increase the sample size. However, current text data augmentation methods either can't ensure the correct labeling of the generated data (lacking faithfulness) or can't ensure sufficient diversity in the generated data (lacking compactness), or both. Inspired by the recent success of large language models, especially the development of ChatGPT, which demonstrated improved language comprehension abilities, in this work, we propose a text data augmentation approach based on ChatGPT (named AugGPT). AugGPT rephrases each sentence in the training samples into multiple conceptually similar but semantically different samples. The augmented samples can then be used in downstream model training. Experiment results on few-shot learning text classification tasks show the superior performance of the proposed AugGPT approach over state-of-the-art text data augmentation methods in terms of testing accuracy and distribution of the augmented samples.},
	urldate = {2023-06-20},
	publisher = {arXiv},
	author = {Dai, Haixing and Liu, Zhengliang and Liao, Wenxiong and Huang, Xiaoke and Cao, Yihan and Wu, Zihao and Zhao, Lin and Xu, Shaochen and Liu, Wei and Liu, Ninghao and Li, Sheng and Zhu, Dajiang and Cai, Hongmin and Sun, Lichao and Li, Quanzheng and Shen, Dinggang and Liu, Tianming and Li, Xiang},
	month = mar,
	year = {2023},
	note = {arXiv:2302.13007 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{cirillo_sex_2020,
	title = {Sex and gender differences and biases in artificial intelligence for biomedicine and healthcare},
	volume = {3},
	copyright = {2020 The Author(s)},
	issn = {2398-6352},
	url = {https://www.nature.com/articles/s41746-020-0288-5},
	doi = {10.1038/s41746-020-0288-5},
	abstract = {Precision Medicine implies a deep understanding of inter-individual differences in health and disease that are due to genetic and environmental factors. To acquire such understanding there is a need for the implementation of different types of technologies based on artificial intelligence (AI) that enable the identification of biomedically relevant patterns, facilitating progress towards individually tailored preventative and therapeutic interventions. Despite the significant scientific advances achieved so far, most of the currently used biomedical AI technologies do not account for bias detection. Furthermore, the design of the majority of algorithms ignore the sex and gender dimension and its contribution to health and disease differences among individuals. Failure in accounting for these differences will generate sub-optimal results and produce mistakes as well as discriminatory outcomes. In this review we examine the current sex and gender gaps in a subset of biomedical technologies used in relation to Precision Medicine. In addition, we provide recommendations to optimize their utilization to improve the global health and disease landscape and decrease inequalities.},
	language = {en},
	number = {1},
	urldate = {2023-06-20},
	journal = {npj Digital Medicine},
	author = {Cirillo, Davide and Catuara-Solarz, Silvina and Morey, Czuee and Guney, Emre and Subirats, Laia and Mellino, Simona and Gigante, Annalisa and Valencia, Alfonso and Rementeria, María José and Chadha, Antonella Santuccione and Mavridis, Nikolaos},
	month = jun,
	year = {2020},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Biomarkers, Computational models, Medical ethics, Risk factors},
	pages = {1--11},
}

@article{char_identifying_2020,
	title = {Identifying {Ethical} {Considerations} for {Machine} {Learning} {Healthcare} {Applications}},
	volume = {20},
	issn = {1526-5161},
	url = {https://doi.org/10.1080/15265161.2020.1819469},
	doi = {10.1080/15265161.2020.1819469},
	abstract = {Along with potential benefits to healthcare delivery, machine learning healthcare applications (ML-HCAs) raise a number of ethical concerns. Ethical evaluations of ML-HCAs will need to structure the overall problem of evaluating these technologies, especially for a diverse group of stakeholders. This paper outlines a systematic approach to identifying ML-HCA ethical concerns, starting with a conceptual model of the pipeline of the conception, development, implementation of ML-HCAs, and the parallel pipeline of evaluation and oversight tasks at each stage. Over this model, we layer key questions that raise value-based issues, along with ethical considerations identified in large part by a literature review, but also identifying some ethical considerations that have yet to receive attention. This pipeline model framework will be useful for systematic ethical appraisals of ML-HCA from development through implementation, and for interdisciplinary collaboration of diverse stakeholders that will be required to understand and subsequently manage the ethical implications of ML-HCAs.},
	number = {11},
	urldate = {2023-06-20},
	journal = {The American Journal of Bioethics},
	author = {Char, Danton S. and Abràmoff, Michael D. and Feudtner, Chris},
	month = nov,
	year = {2020},
	pmid = {33103967},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/15265161.2020.1819469},
	keywords = {A Framework to Evaluate Ethical Considerations with ML-HCA Applications—Valuable, Even Necessary, but Never Comprehensive, AI Ethics Is Not a Panacea, Accountability in the Machine Learning Pipeline: The Critical Role of Research Ethics Oversight, Addressing the “Wicked” Problems in Machine Learning Applications – Time for Bioethical Agility, An Ethical Framework to Nowhere, An Evaluation of the Pipeline Framework for Ethical Considerations in Machine Learning Healthcare Applications: The Case of Prediction from Functional Neuroimaging Data, Artificial intelligence, Deepening the Normative Evaluation of Machine Learning Healthcare Application by Complementing Ethical Considerations with Regulatory Governance, Embedded Ethics Could Help Implement the Pipeline Model Framework for Machine Learning Healthcare Applications, It is Time for Bioethicists to Enter the Arena of Machine Learning Ethics, Keeping the Patient at the Center of Machine Learning in Healthcare, Machine Learning Healthcare Applications (ML-HCAs) Are No Stand-Alone Systems but Part of an Ecosystem – A Broader Ethical and Health Technology Assessment Approach is Needed, Machine Learning in Healthcare: Exceptional Technologies Require Exceptional Ethics, Respect and Trustworthiness in the Patient-Provider-Machine Relationship: Applying a Relational Lens to Machine Learning Healthcare Applications, Structural Disparities in Data Science: A Prolegomenon for the Future of Machine Learning, What Counts as “Clinical Data” in Machine Learning Healthcare Applications?, What’s in the Box?: Uncertain Accountability of Machine Learning Applications in Healthcare, Where Bioethics Meets Machine Ethics, effectiveness, ethics, machine learning, safety, test characteristics},
	pages = {7--17},
}

@misc{li_artificial_2023,
	title = {Artificial {General} {Intelligence} for {Medical} {Imaging}},
	url = {http://arxiv.org/abs/2306.05480},
	doi = {10.48550/arXiv.2306.05480},
	abstract = {In this review, we explore the potential applications of Artificial General Intelligence (AGI) models in healthcare, focusing on foundational Large Language Models (LLMs), Large Vision Models, and Large Multimodal Models. We emphasize the importance of integrating clinical expertise, domain knowledge, and multimodal capabilities into AGI models. In addition, we lay out key roadmaps that guide the development and deployment of healthcare AGI models. Throughout the review, we provide critical perspectives on the potential challenges and pitfalls associated with deploying large-scale AGI models in the medical field. This comprehensive review aims to offer insights into the future implications of AGI in medical imaging, healthcare and beyond.},
	urldate = {2023-06-19},
	publisher = {arXiv},
	author = {Li, Xiang and Zhang, Lu and Wu, Zihao and Liu, Zhengliang and Zhao, Lin and Yuan, Yixuan and Liu, Jun and Li, Gang and Zhu, Dajiang and Yan, Pingkuan and Li, Quanzheng and Liu, Wei and Liu, Tianming and Shen, Dinggang},
	month = jun,
	year = {2023},
	note = {arXiv:2306.05480 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
}

@inproceedings{liu_practical_2022,
	title = {Practical {Evaluation} of {Adversarial} {Robustness} via {Adaptive} {Auto} {Attack}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Liu_Practical_Evaluation_of_Adversarial_Robustness_via_Adaptive_Auto_Attack_CVPR_2022_paper.html},
	language = {en},
	urldate = {2022-07-26},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Liu, Ye and Cheng, Yaya and Gao, Lianli and Liu, Xianglong and Zhang, Qilong and Song, Jingkuan},
	year = {2022},
	pages = {15105--15114},
}

@inproceedings{yin_fourier_2019,
	title = {A {Fourier} {Perspective} on {Model} {Robustness} in {Computer} {Vision}},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper/2019/hash/b05b57f6add810d3b7490866d74c0053-Abstract.html},
	abstract = {Achieving robustness to distributional shift is a longstanding and challenging goal of computer vision. Data augmentation is a commonly used approach for improving robustness, however robustness gains are typically not uniform across corruption types. Indeed increasing performance in the presence of random noise is often met with reduced performance on other corruptions such as contrast change. Understanding when and why these sorts of trade-offs occur is a crucial step towards mitigating them. Towards this end, we investigate recently observed trade-offs caused by Gaussian data augmentation and adversarial training. We find that both methods improve robustness to corruptions that are concentrated in the high frequency domain while reducing robustness to corruptions that are concentrated in the low frequency domain. This suggests that one way to mitigate these trade-offs via data augmentation is to use a more diverse set of augmentations.
Towards this end we observe that AutoAugment, a recently proposed data augmentation policy optimized for clean accuracy, achieves state-of-the-art robustness on the CIFAR-10-C benchmark.},
	urldate = {2023-06-18},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Yin, Dong and Gontijo Lopes, Raphael and Shlens, Jon and Cubuk, Ekin Dogus and Gilmer, Justin},
	year = {2019},
}

@inproceedings{chang_active_2017,
	title = {Active {Bias}: {Training} {More} {Accurate} {Neural} {Networks} by {Emphasizing} {High} {Variance} {Samples}},
	volume = {30},
	shorttitle = {Active {Bias}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2017/hash/2f37d10131f2a483a8dd005b3d14b0d9-Abstract.html},
	abstract = {Self-paced learning and hard example mining re-weight training instances to improve learning accuracy. This paper presents two improved alternatives based on lightweight estimates of sample uncertainty in stochastic gradient descent (SGD): the variance in predicted probability of the correct class across iterations of mini-batch SGD, and the proximity of the correct class probability to the decision threshold. Extensive experimental results on six datasets show that our methods reliably improve accuracy in various network architectures, including additional gains on top of other popular training techniques, such as residual learning, momentum, ADAM, batch normalization, dropout, and distillation.},
	urldate = {2023-06-18},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Chang, Haw-Shiuan and Learned-Miller, Erik and McCallum, Andrew},
	year = {2017},
}

@inproceedings{shrivastava_learning_2017,
	address = {Honolulu, HI},
	title = {Learning from {Simulated} and {Unsupervised} {Images} through {Adversarial} {Training}},
	isbn = {978-1-5386-0457-1},
	url = {http://ieeexplore.ieee.org/document/8099724/},
	doi = {10.1109/CVPR.2017.241},
	language = {en},
	urldate = {2023-06-18},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Shrivastava, Ashish and Pfister, Tomas and Tuzel, Oncel and Susskind, Joshua and Wang, Wenda and Webb, Russell},
	month = jul,
	year = {2017},
	pages = {2242--2251},
}

@inproceedings{addepalli_scaling_2022,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Scaling {Adversarial} {Training} to {Large} {Perturbation} {Bounds}},
	isbn = {978-3-031-20065-6},
	doi = {10.1007/978-3-031-20065-6_18},
	abstract = {The vulnerability of Deep Neural Networks to Adversarial Attacks has fuelled research towards building robust models. While most Adversarial Training algorithms aim at defending attacks constrained within low magnitude Lp norm bounds, real-world adversaries are not limited by such constraints. In this work, we aim to achieve adversarial robustness within larger bounds, against perturbations that may be perceptible, but do not change human (or Oracle) prediction. The presence of images that flip Oracle predictions and those that do not makes this a challenging setting for adversarial robustness. We discuss the ideal goals of an adversarial defense algorithm beyond perceptual limits, and further highlight the shortcomings of naively extending existing training algorithms to higher perturbation bounds. In order to overcome these shortcomings, we propose a novel defense, Oracle-Aligned Adversarial Training (OA-AT), to align the predictions of the network with that of an Oracle during adversarial training. The proposed approach achieves state-of-the-art performance at large epsilon bounds (such as an L-inf bound of 16/255 on CIFAR-10) while outperforming existing defenses (AWP, TRADES, PGD-AT) at standard bounds (8/255) as well.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2022},
	publisher = {Springer Nature Switzerland},
	author = {Addepalli, Sravanti and Jain, Samyak and Sriramanan, Gaurang and Venkatesh Babu, R.},
	editor = {Avidan, Shai and Brostow, Gabriel and Cissé, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
	year = {2022},
	pages = {301--316},
}

@inproceedings{pang_improving_2019,
	title = {Improving {Adversarial} {Robustness} via {Promoting} {Ensemble} {Diversity}},
	url = {https://proceedings.mlr.press/v97/pang19a.html},
	abstract = {Though deep neural networks have achieved significant progress on various tasks, often enhanced by model ensemble, existing high-performance models can be vulnerable to adversarial attacks. Many efforts have been devoted to enhancing the robustness of individual networks and then constructing a straightforward ensemble, e.g., by directly averaging the outputs, which ignores the interaction among networks. This paper presents a new method that explores the interaction among individual networks to improve robustness for ensemble models. Technically, we define a new notion of ensemble diversity in the adversarial setting as the diversity among non-maximal predictions of individual members, and present an adaptive diversity promoting (ADP) regularizer to encourage the diversity, which leads to globally better robustness for the ensemble by making adversarial examples difficult to transfer among individual members. Our method is computationally efficient and compatible with the defense methods acting on individual networks. Empirical results on various datasets verify that our method can improve adversarial robustness while maintaining state-of-the-art accuracy on normal examples.},
	language = {en},
	urldate = {2023-06-17},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Pang, Tianyu and Xu, Kun and Du, Chao and Chen, Ning and Zhu, Jun},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {4970--4979},
}

@article{shao_adversarial_2022,
	title = {On the {Adversarial} {Robustness} of {Vision} {Transformers}},
	issn = {2835-8856},
	url = {https://openreview.net/forum?id=lE7K4n1Esk},
	abstract = {Following the success in advancing natural language processing and understanding, transformers are expected to bring revolutionary changes to computer vision. This work provides a comprehensive study on the robustness of vision transformers (ViTs) against adversarial perturbations. Tested on various white-box and transfer attack settings, we find that ViTs possess better adversarial robustness when compared with MLP-Mixer and convolutional neural networks (CNNs) including ConvNeXt, and this observation also holds for certified robustness. Through frequency analysis and feature visualization, we summarize the following main observations contributing to the improved robustness of ViTs: 1) Features learned by ViTs contain less high-frequency patterns that have spurious correlation, which helps explain why ViTs are less sensitive to high-frequency perturbations than CNNs and MLP-Mixer, and there is a high correlation between how much the model learns high-frequency features and its robustness against different frequency-based perturbations. 2) Introducing convolutional or tokens-to-token blocks for learning high-frequency features in ViTs can improve classification accuracy but at the cost of adversarial robustness. 3) Modern CNN designs that borrow techniques from ViTs including activation function, layer norm, larger kernel size to imitate the global attention, and patchify the images as inputs, etc., could help bridge the performance gap between ViTs and CNNs not only in terms of performance, but also certified and empirical adversarial robustness. Moreover, we show adversarial training is also applicable to ViT for training robust models, and sharpness-aware minimization can also help improve robustness, while pre-training with clean images on larger datasets does not significantly improve adversarial robustness.},
	language = {en},
	urldate = {2023-02-18},
	journal = {Transactions on Machine Learning Research},
	author = {Shao, Rulin and Shi, Zhouxing and Yi, Jinfeng and Chen, Pin-Yu and Hsieh, Cho-Jui},
	year = {2022},
}

@inproceedings{cao_personalized_2021,
	title = {Personalized and {Invertible} {Face} {De}-{Identification} by {Disentangled} {Identity} {Information} {Manipulation}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Cao_Personalized_and_Invertible_Face_De-Identification_by_Disentangled_Identity_Information_Manipulation_ICCV_2021_paper.html},
	language = {en},
	urldate = {2023-06-16},
	booktitle = {Proceedings of the {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision}},
	author = {Cao, Jingyi and Liu, Bo and Wen, Yunqian and Xie, Rong and Song, Li},
	year = {2021},
	pages = {3334--3342},
}

@inproceedings{hukkelas_does_2023,
	title = {Does {Image} {Anonymization} {Impact} {Computer} {Vision} {Training}?},
	url = {https://openaccess.thecvf.com/content/CVPR2023W/WAD/html/Hukkelas_Does_Image_Anonymization_Impact_Computer_Vision_Training_CVPRW_2023_paper.html},
	language = {en},
	urldate = {2023-06-16},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Hukkelås, Håkon and Lindseth, Frank},
	year = {2023},
	pages = {140--150},
}

@inproceedings{gilmer_adversarial_2019,
	title = {Adversarial {Examples} {Are} a {Natural} {Consequence} of {Test} {Error} in {Noise}},
	url = {https://proceedings.mlr.press/v97/gilmer19a.html},
	abstract = {Over the last few years, the phenomenon of adversarial examples — maliciously constructed inputs that fool trained machine learning models — has captured the attention of the research community, especially when restricted to small modifications of a correctly handled input. Less surprisingly, image classifiers also lack human-level performance on randomly corrupted images, such as images with additive Gaussian noise. In this paper we provide both empirical and theoretical evidence that these are two manifestations of the same underlying phenomenon, and therefore the adversarial robustness and corruption robustness research programs are closely related. This suggests that improving adversarial robustness should go hand in hand with improving performance in the presence of more general and realistic image corruptions. This yields a computationally tractable evaluation metric for defenses to consider: test error in noisy image distributions.},
	language = {en},
	urldate = {2023-06-16},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Gilmer, Justin and Ford, Nicolas and Carlini, Nicholas and Cubuk, Ekin},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {2280--2289},
}

@article{kather_medical_2022,
	title = {Medical domain knowledge in domain-agnostic generative {AI}},
	volume = {5},
	copyright = {2022 The Author(s)},
	issn = {2398-6352},
	url = {https://www.nature.com/articles/s41746-022-00634-5},
	doi = {10.1038/s41746-022-00634-5},
	abstract = {The text-guided diffusion model GLIDE (Guided Language to Image Diffusion for Generation and Editing) is the state of the art in text-to-image generative artificial intelligence (AI). GLIDE has rich representations, but medical applications of this model have not been systematically explored. If GLIDE had useful medical knowledge, it could be used for medical image analysis tasks, a domain in which AI systems are still highly engineered towards a single use-case. Here we show that the publicly available GLIDE model has reasonably strong representations of key topics in cancer research and oncology, in particular the general style of histopathology images and multiple facets of diseases, pathological processes and laboratory assays. However, GLIDE seems to lack useful representations of the style and content of radiology data. Our findings demonstrate that domain-agnostic generative AI models can learn relevant medical concepts without explicit training. Thus, GLIDE and similar models might be useful for medical image processing tasks in the future - particularly with additional domain-specific fine-tuning.},
	language = {en},
	number = {1},
	urldate = {2023-06-16},
	journal = {npj Digital Medicine},
	author = {Kather, Jakob Nikolas and Ghaffari Laleh, Narmin and Foersch, Sebastian and Truhn, Daniel},
	month = jul,
	year = {2022},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Biomedical engineering, Data integration, Machine learning},
	pages = {1--5},
}

@article{yoon_anonymization_2020,
	title = {Anonymization {Through} {Data} {Synthesis} {Using} {Generative} {Adversarial} {Networks} ({ADS}-{GAN})},
	volume = {24},
	issn = {2168-2208},
	doi = {10.1109/JBHI.2020.2980262},
	abstract = {The medical and machine learning communities are relying on the promise of artificial intelligence (AI) to transform medicine through enabling more accurate decisions and personalized treatment. However, progress is slow. Legal and ethical issues around unconsented patient data and privacy is one of the limiting factors in data sharing, resulting in a significant barrier in accessing routinely collected electronic health records (EHR) by the machine learning community. We propose a novel framework for generating synthetic data that closely approximates the joint distribution of variables in an original EHR dataset, providing a readily accessible, legally and ethically appropriate solution to support more open data sharing, enabling the development of AI solutions. In order to address issues around lack of clarity in defining sufficient anonymization, we created a quantifiable, mathematical definition for “identifiability”. We used a conditional generative adversarial networks (GAN) framework to generate synthetic data while minimize patient identifiability that is defined based on the probability of re-identification given the combination of all data on any individual patient. We compared models fitted to our synthetically generated data to those fitted to the real data across four independent datasets to evaluate similarity in model performance, while assessing the extent to which original observations can be identified from the synthetic data. Our model, ADS-GAN, consistently outperformed state-of-the-art methods, and demonstrated reliability in the joint distributions. We propose that this method could be used to develop datasets that can be made publicly available while considerably lowering the risk of breaching patient confidentiality.},
	number = {8},
	journal = {IEEE Journal of Biomedical and Health Informatics},
	author = {Yoon, Jinsung and Drumright, Lydia N. and van der Schaar, Mihaela},
	month = aug,
	year = {2020},
	note = {Conference Name: IEEE Journal of Biomedical and Health Informatics},
	keywords = {Gallium nitride, Generative adversarial networks, Generators, Machine learning, Medical services, Synthetic data generation, anonymization, electronic health records, generative adversarial networks, identifiability},
	pages = {2378--2388},
}

@article{kim_selective_2020,
	title = {Selective {Feature} {Anonymization} for {Privacy}-{Preserving} {Image} {Data} {Publishing}},
	volume = {9},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2079-9292},
	url = {https://www.mdpi.com/2079-9292/9/5/874},
	doi = {10.3390/electronics9050874},
	abstract = {There is a strong positive correlation between the development of deep learning and the amount of public data available. Not all data can be released in their raw form because of the risk to the privacy of the related individuals. The main objective of privacy-preserving data publication is to anonymize the data while maintaining their utility. In this paper, we propose a privacy-preserving semi-generative adversarial network (PPSGAN) that selectively adds noise to class-independent features of each image to enable the processed image to maintain its original class label. Our experiments on training classifiers with synthetic datasets anonymized with various methods confirm that PPSGAN shows better utility than other conventional methods, including blurring, noise-adding, filtering, and generation using GANs.},
	language = {en},
	number = {5},
	urldate = {2023-06-16},
	journal = {Electronics},
	author = {Kim, Taehoon and Yang, Jihoon},
	month = may,
	year = {2020},
	note = {Number: 5
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {adversarial learning, data privacy, deep learning, differential privacy, generative adversarial networks, machine learning, model inversion attacks},
	pages = {874},
}

@misc{jadon_leveraging_2023,
	title = {Leveraging {Generative} {AI} {Models} for {Synthetic} {Data} {Generation} in {Healthcare}: {Balancing} {Research} and {Privacy}},
	shorttitle = {Leveraging {Generative} {AI} {Models} for {Synthetic} {Data} {Generation} in {Healthcare}},
	url = {http://arxiv.org/abs/2305.05247},
	doi = {10.48550/arXiv.2305.05247},
	abstract = {The widespread adoption of electronic health records and digital healthcare data has created a demand for data-driven insights to enhance patient outcomes, diagnostics, and treatments. However, using real patient data presents privacy and regulatory challenges, including compliance with HIPAA and GDPR. Synthetic data generation, using generative AI models like GANs and VAEs offers a promising solution to balance valuable data access and patient privacy protection. In this paper, we examine generative AI models for creating realistic, anonymized patient data for research and training, explore synthetic data applications in healthcare, and discuss its benefits, challenges, and future research directions. Synthetic data has the potential to revolutionize healthcare by providing anonymized patient data while preserving privacy and enabling versatile applications.},
	urldate = {2023-06-16},
	publisher = {arXiv},
	author = {Jadon, Aryan and Kumar, Shashank},
	month = may,
	year = {2023},
	note = {arXiv:2305.05247 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@article{beaulieu-jones_privacy-preserving_2019,
	title = {Privacy-{Preserving} {Generative} {Deep} {Neural} {Networks} {Support} {Clinical} {Data} {Sharing}},
	volume = {12},
	url = {https://www.ahajournals.org/doi/10.1161/CIRCOUTCOMES.118.005122},
	doi = {10.1161/CIRCOUTCOMES.118.005122},
	abstract = {Background:

Data sharing accelerates scientific progress but sharing individual-level data while preserving patient privacy presents a barrier.

Methods and Results:

Using pairs of deep neural networks, we generated simulated, synthetic participants that closely resemble participants of the SPRINT trial (Systolic Blood Pressure Trial). We showed that such paired networks can be trained with differential privacy, a formal privacy framework that limits the likelihood that queries of the synthetic participants’ data could identify a real a participant in the trial. Machine learning predictors built on the synthetic population generalize to the original data set. This finding suggests that the synthetic data can be shared with others, enabling them to perform hypothesis-generating analyses as though they had the original trial data.

Conclusions:

Deep neural networks that generate synthetic participants facilitate secondary analyses and reproducible investigation of clinical data sets by enhancing data sharing while preserving participant privacy.},
	number = {7},
	urldate = {2023-06-16},
	journal = {Circulation: Cardiovascular Quality and Outcomes},
	author = {Beaulieu-Jones, Brett K. and Wu, Zhiwei Steven and Williams, Chris and Lee, Ran and Bhavnani, Sanjeev P. and Byrd, James Brian and Greene, Casey S.},
	month = jul,
	year = {2019},
	note = {Publisher: American Heart Association},
	keywords = {blood pressure, deep learning, machine learning, privacy, propensity score},
	pages = {e005122},
}

@inproceedings{hukkelas_deepprivacy2_2023,
	title = {{DeepPrivacy2}: {Towards} {Realistic} {Full}-{Body} {Anonymization}},
	shorttitle = {{DeepPrivacy2}},
	url = {https://openaccess.thecvf.com/content/WACV2023/html/Hukkelas_DeepPrivacy2_Towards_Realistic_Full-Body_Anonymization_WACV_2023_paper.html},
	language = {en},
	urldate = {2023-06-16},
	author = {Hukkelås, Håkon and Lindseth, Frank},
	year = {2023},
	pages = {1329--1338},
}

@inproceedings{klemp_ldfa_2023,
	title = {{LDFA}: {Latent} {Diffusion} {Face} {Anonymization} for {Self}-{Driving} {Applications}},
	shorttitle = {{LDFA}},
	url = {https://openaccess.thecvf.com/content/CVPR2023W/E2EAD/html/Klemp_LDFA_Latent_Diffusion_Face_Anonymization_for_Self-Driving_Applications_CVPRW_2023_paper.html},
	language = {en},
	urldate = {2023-06-16},
	author = {Klemp, Marvin and Rösch, Kevin and Wagner, Royden and Quehl, Jannik and Lauer, Martin},
	year = {2023},
	pages = {3198--3204},
}

@inproceedings{alaa_how_2022,
	title = {How {Faithful} is your {Synthetic} {Data}? {Sample}-level {Metrics} for {Evaluating} and {Auditing} {Generative} {Models}},
	shorttitle = {How {Faithful} is your {Synthetic} {Data}?},
	url = {https://proceedings.mlr.press/v162/alaa22a.html},
	abstract = {Devising domain- and model-agnostic evaluation metrics for generative models is an important and as yet unresolved problem. Most existing metrics, which were tailored solely to the image synthesis setup, exhibit a limited capacity for diagnosing the different modes of failure of generative models across broader application domains. In this paper, we introduce a 3-dimensional evaluation metric, (αα{\textbackslash}alpha-Precision, ββ{\textbackslash}beta-Recall, Authenticity), that characterizes the fidelity, diversity and generalization performance of any generative model in a domain-agnostic fashion. Our metric unifies statistical divergence measures with precision-recall analysis, enabling sample- and distribution-level diagnoses of model fidelity and diversity. We introduce generalization as an additional, independent dimension (to the fidelity-diversity trade-off) that quantifies the extent to which a model copies training data\{—\}a crucial performance indicator when modeling sensitive data with requirements on privacy. The three metric components correspond to (interpretable) probabilistic quantities, and are estimated via sample-level binary classification. The sample-level nature of our metric inspires a novel use case which we call model auditing, wherein we judge the quality of individual samples generated by a (black-box) model, discarding low-quality samples and hence improving the overall model performance in a post-hoc manner.},
	language = {en},
	urldate = {2023-06-16},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Alaa, Ahmed and Breugel, Boris Van and Saveliev, Evgeny S. and Schaar, Mihaela van der},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {290--306},
}

@inproceedings{liu_ppgan_2019,
	title = {{PPGAN}: {Privacy}-{Preserving} {Generative} {Adversarial} {Network}},
	shorttitle = {{PPGAN}},
	doi = {10.1109/ICPADS47876.2019.00150},
	abstract = {Generative Adversarial Network (GAN) and its variants serve as a perfect representation of the data generation model, providing researchers with a large amount of high-quality generated data. They illustrate a promising direction for research with limited data availability. When GAN learns the semantic-rich data distribution from a dataset, the density of the generated distribution tends to concentrate on the training data. Due to the gradient parameters of the deep neural network contain the data distribution of the training samples, they can easily remember the training samples. When GAN is applied to private or sensitive data, for instance, patient medical records, as private information may be leakage. To address this issue, we propose a Privacy-preserving Generative Adversarial Network (PPGAN) model, in which we achieve differential privacy in GANs by adding well-designed noise to the gradient during the model learning procedure. Besides, we introduced the Moments Accountant strategy in the PPGAN training process to improve the stability and compatibility of the model by controlling privacy loss. We also give a mathematical proof of the differential privacy discriminator. Through extensive case studies of the benchmark datasets, we demonstrate that PPGAN can generate high-quality synthetic data while retaining the required data available under a reasonable privacy budget.},
	booktitle = {2019 {IEEE} 25th {International} {Conference} on {Parallel} and {Distributed} {Systems} ({ICPADS})},
	author = {Liu, Yi and Peng, Jialiang and Yu, James J.Q. and Wu, Yi},
	month = dec,
	year = {2019},
	note = {ISSN: 1521-9097},
	keywords = {Privacy leakage, GAN, deep learning, differential privacy, moments accountant},
	pages = {985--989},
}

@article{sun_adversarial_2023,
	title = {Adversarial {Attacks} {Against} {Deep} {Generative} {Models} on {Data}: {A} {Survey}},
	volume = {35},
	issn = {1558-2191},
	shorttitle = {Adversarial {Attacks} {Against} {Deep} {Generative} {Models} on {Data}},
	doi = {10.1109/TKDE.2021.3130903},
	abstract = {Deep generative models have gained much attention given their ability to generate data for applications as varied as healthcare to financial technology to surveillance, and many more - the most popular models being generative adversarial networks (GANs) and variational auto-encoders (VAEs). Yet, as with all machine learning models, ever is the concern over security breaches and privacy leaks and deep generative models are no exception. In fact, these models have advanced so rapidly in recent years that work on their security is still in its infancy. In an attempt to audit the current and future threats against these models, and to provide a roadmap for defense preparations in the short term, we prepared this comprehensive and specialized survey on the security and privacy preservation of GANs and VAEs. Our focus is on the inner connection between attacks and model architectures and, more specifically, on five components of deep generative models: the training data, the latent code, the generators/decoders of GANs/VAEs, the discriminators/encoders of GANs/VAEs, and the generated data. For each model, component and attack, we review the current research progress and identify the key challenges. The paper concludes with a discussion of possible future attacks and research directions in the field.},
	number = {4},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Sun, Hui and Zhu, Tianqing and Zhang, Zhiqiu and Jin, Dawei and Xiong, Ping and Zhou, Wanlei},
	month = apr,
	year = {2023},
	note = {Conference Name: IEEE Transactions on Knowledge and Data Engineering},
	keywords = {Biological system modeling, Codes, Data models, Deep generative models, Generators, Privacy, Security, Training, deep learning, evasion attack, membership inference attack, model defense},
	pages = {3367--3388},
}

@article{quan_compressed_2018,
	title = {Compressed {Sensing} {MRI} {Reconstruction} {Using} a {Generative} {Adversarial} {Network} {With} a {Cyclic} {Loss}},
	volume = {37},
	issn = {1558-254X},
	doi = {10.1109/TMI.2018.2820120},
	abstract = {Compressed sensing magnetic resonance imaging (CS-MRI) has provided theoretical foundations upon which the time-consuming MRI acquisition process can be accelerated. However, it primarily relies on iterative numerical solvers, which still hinders their adaptation in time-critical applications. In addition, recent advances in deep neural networks have shown their potential in computer vision and image processing, but their adaptation to MRI reconstruction is still in an early stage. In this paper, we propose a novel deep learning-based generative adversarial model, RefineGAN, for fast and accurate CS-MRI reconstruction. The proposed model is a variant of fullyresidual convolutional autoencoder and generative adversarial networks (GANs), specifically designed for CS-MRI formulation; it employs deeper generator and discriminator networks with cyclic data consistency loss for faithful interpolation in the given under-sampled k-space data. In addition, our solution leverages a chained networkto further enhance the reconstruction quality. RefineGAN is fast and accurate-the reconstruction process is extremely rapid, as low as tens of milliseconds for reconstruction of a 256 × 256 image, because it is one-way deployment on a feed-forward network, and the image quality is superior even for extremely low sampling rate (as low as 10\%) due to the data-driven nature of the method. We demonstrate that RefineGAN outperforms the state-of-the-art CS-MRI methods by a large margin in terms of both running time and image quality via evaluation using several open-source MRI databases.},
	number = {6},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Quan, Tran Minh and Nguyen-Duc, Thanh and Jeong, Won-Ki},
	month = jun,
	year = {2018},
	note = {Conference Name: IEEE Transactions on Medical Imaging},
	keywords = {Compressed sensing, CycleGAN, Databases, DiscoGAN, GAN, Gallium nitride, Image quality, Image reconstruction, MRI, Machine learning, Magnetic resonance imaging, Training},
	pages = {1488--1497},
}

@inproceedings{augenstein_generative_2020,
	title = {{GENERATIVE} {MODELS} {FOR} {EFFECTIVE} {ML} {ON} {PRIVATE}, {DECENTRALIZED} {DATASETS}},
	abstract = {To improve real-world applications of machine learning, experienced modelers develop intuition about their datasets, their models, and how the two interact. Manual inspection of raw data—of representative samples, of outliers, of misclassiﬁcations—is an essential tool in a) identifying and ﬁxing problems in the data, b) generating new modeling hypotheses, and c) assigning or reﬁning human-provided labels. However, manual data inspection is problematic for privacy-sensitive datasets, such as those representing the behavior of realworld individuals. Furthermore, manual data inspection is impossible in the increasingly important setting of federated learning, where raw examples are stored at the edge and the modeler may only access aggregated outputs such as metrics or model parameters. This paper demonstrates that generative models—trained using federated methods and with formal differential privacy guarantees—can be used effectively to debug many commonly occurring data issues even when the data cannot be directly inspected. We explore these methods in applications to text with differentially private federated RNNs and to images using a novel algorithm for differentially private federated GANs.},
	language = {en},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Augenstein, Sean and Ramaswamy, Swaroop and Mathews, Rajiv and McMahan, H Brendan and Ramage, Daniel and Kairouz, Peter and Chen, Mingqing},
	year = {2020},
}

@article{rocher_estimating_2019,
	title = {Estimating the success of re-identifications in incomplete datasets using generative models},
	volume = {10},
	copyright = {2019 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-019-10933-3;},
	doi = {10.1038/s41467-019-10933-3},
	abstract = {While rich medical, behavioral, and socio-demographic data are key to modern data-driven research, their collection and use raise legitimate privacy concerns. Anonymizing datasets through de-identification and sampling before sharing them has been the main tool used to address those concerns. We here propose a generative copula-based method that can accurately estimate the likelihood of a specific person to be correctly re-identified, even in a heavily incomplete dataset. On 210 populations, our method obtains AUC scores for predicting individual uniqueness ranging from 0.84 to 0.97, with low false-discovery rate. Using our model, we find that 99.98\% of Americans would be correctly re-identified in any dataset using 15 demographic attributes. Our results suggest that even heavily sampled anonymized datasets are unlikely to satisfy the modern standards for anonymization set forth by GDPR and seriously challenge the technical and legal adequacy of the de-identification release-and-forget model.},
	language = {en},
	number = {1},
	urldate = {2023-06-16},
	journal = {Nature Communications},
	author = {Rocher, Luc and Hendrickx, Julien M. and de Montjoye, Yves-Alexandre},
	month = jul,
	year = {2019},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Computational science, Social sciences},
	pages = {3069},
}

@article{costa_end--end_2018,
	title = {End-to-{End} {Adversarial} {Retinal} {Image} {Synthesis}},
	volume = {37},
	issn = {1558-254X},
	doi = {10.1109/TMI.2017.2759102},
	abstract = {In medical image analysis applications, the availability of the large amounts of annotated data is becoming increasingly critical. However, annotated medical data is often scarce and costly to obtain. In this paper, we address the problem of synthesizing retinal color images by applying recent techniques based on adversarial learning. In this setting, a generative model is trained to maximize a loss function provided by a second model attempting to classify its output into real or synthetic. In particular, we propose to implement an adversarial autoencoder for the task of retinal vessel network synthesis. We use the generated vessel trees as an intermediate stage for the generation of color retinal images, which is accomplished with a generative adversarial network. Both models require the optimization of almost everywhere differentiable loss functions, which allows us to train them jointly. The resulting model offers an end-to-end retinal image synthesis system capable of generating as many retinal images as the user requires, with their corresponding vessel networks, by sampling from a simple probability distribution that we impose to the associated latent space. We show that the learned latent space contains a well-defined semantic structure, implying that we can perform calculations in the space of retinal images, e.g., smoothly interpolating new data points between two retinal images. Visual and quantitative results demonstrate that the synthesized images are substantially different from those in the training set, while being also anatomically consistent and displaying a reasonable visual quality.},
	number = {3},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Costa, Pedro and Galdran, Adrian and Meyer, Maria Ines and Niemeijer, Meindert and Abràmoff, Michael and Mendonça, Ana Maria and Campilho, Aurélio},
	month = mar,
	year = {2018},
	note = {Conference Name: IEEE Transactions on Medical Imaging},
	keywords = {Biomedical imaging, Image generation, Mathematical model, Retinal image synthesis, Retinal vessels, Training, adversarial autoencoders, generative adversarial networks, retinal image analysis},
	pages = {781--791},
}

@article{ren_controllable_2021,
	title = {Controllable {Medical} {Image} {Generation} via {Generative} {Adversarial} {Networks}},
	volume = {33},
	issn = {2470-1173},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9897627/},
	doi = {10.2352/issn.2470-1173.2021.11.hvei-112},
	abstract = {Radiologists and pathologists frequently make highly consequential perceptual decisions. For example, visually searching for a tumor and recognizing whether it is malignant can have a life-changing impact on a patient. Unfortunately, all human perceivers—even radiologists—have perceptual biases. Because human perceivers (medical doctors) will, for the foreseeable future, be the final judges of whether a tumor is malignant, understanding and mitigating human perceptual biases is important. While there has been research on perceptual biases in medical image perception tasks, the stimuli used for these studies were highly artificial and often critiqued. Realistic stimuli have not been used because it has not been possible to generate or control them for psychophysical experiments. Here, we propose to use Generative Adversarial Networks (GAN) to create vivid and realistic medical image stimuli that can be used in psychophysical and computer vision studies of medical image perception. Our model can generate tumor-like stimuli with specified shapes and realistic textures in a controlled manner. Various experiments showed the authenticity of our GAN-generated stimuli and the controllability of our model.},
	urldate = {2023-06-16},
	journal = {IS\&T International Symposium on Electronic Imaging},
	author = {Ren, Zhihang and Yu, Stella X. and Whitney, David},
	year = {2021},
	pmid = {36741986},
	pmcid = {PMC9897627},
	pages = {art00003},
}

@inproceedings{mikolajczyk_biasing_2022,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {The (de)biasing {Effect} of {GAN}-{Based} {Augmentation} {Methods} on {Skin} {Lesion} {Images}},
	isbn = {978-3-031-16452-1},
	doi = {10.1007/978-3-031-16452-1_42},
	abstract = {New medical datasets are now more open to the public, allowing for better and more extensive research. Although prepared with the utmost care, new datasets might still be a source of spurious correlations that affect the learning process. Moreover, data collections are usually not large enough and are often unbalanced. One approach to alleviate the data imbalance is using data augmentation with Generative Adversarial Networks (GANs) to extend the dataset with high-quality images. GANs are usually trained on the same biased datasets as the target data, resulting in more biased instances. This work explored unconditional and conditional GANs to compare their bias inheritance and how the synthetic data influenced the models. We provided extensive manual data annotation of possibly biasing artifacts on the well-known ISIC dataset with skin lesions. In addition, we examined classification models trained on both real and synthetic data with counterfactual bias explanations. Our experiments showed that GANs inherited biases and sometimes even amplified them, leading to even stronger spurious correlations. Manual data annotation and synthetic images are publicly available for reproducible scientific research.},
	language = {en},
	booktitle = {Medical {Image} {Computing} and {Computer} {Assisted} {Intervention} – {MICCAI} 2022},
	publisher = {Springer Nature Switzerland},
	author = {Mikołajczyk, Agnieszka and Majchrowska, Sylwia and Carrasco Limeros, Sandra},
	editor = {Wang, Linwei and Dou, Qi and Fletcher, P. Thomas and Speidel, Stefanie and Li, Shuo},
	year = {2022},
	keywords = {Bias, Explainable AI, Generative adversarial networks, Skin lesion classification},
	pages = {437--447},
}

@article{chen_generative_2022,
	title = {Generative {Adversarial} {Networks} in {Medical} {Image} augmentation: {A} review},
	volume = {144},
	issn = {0010-4825},
	shorttitle = {Generative {Adversarial} {Networks} in {Medical} {Image} augmentation},
	url = {https://www.sciencedirect.com/science/article/pii/S0010482522001743},
	doi = {10.1016/j.compbiomed.2022.105382},
	abstract = {Object
With the development of deep learning, the number of training samples for medical image-based diagnosis and treatment models is increasing. Generative Adversarial Networks (GANs) have attracted attention in medical image processing due to their excellent image generation capabilities and have been widely used in data augmentation. In this paper, a comprehensive and systematic review and analysis of medical image augmentation work are carried out, and its research status and development prospects are reviewed.
Method
This paper reviews 105 medical image augmentation related papers, which mainly collected by ELSEVIER, IEEE Xplore, and Springer from 2018 to 2021. We counted these papers according to the parts of the organs corresponding to the images, and sorted out the medical image datasets that appeared in them, the loss function in model training, and the quantitative evaluation metrics of image augmentation. At the same time, we briefly introduce the literature collected in three journals and three conferences that have received attention in medical image processing.
Result
First, we summarize the advantages of various augmentation models, loss functions, and evaluation metrics. Researchers can use this information as a reference when designing augmentation tasks. Second, we explore the relationship between augmented models and the amount of the training set, and tease out the role that augmented models may play when the quality of the training set is limited. Third, the statistical number of papers shows that the development momentum of this research field remains strong. Furthermore, we discuss the existing limitations of this type of model and suggest possible research directions.
Conclusion
We discuss GAN-based medical image augmentation work in detail. This method effectively alleviates the challenge of limited training samples for medical image diagnosis and treatment models. It is hoped that this review will benefit researchers interested in this field.},
	language = {en},
	urldate = {2023-06-16},
	journal = {Computers in Biology and Medicine},
	author = {Chen, Yizhou and Yang, Xu-Hua and Wei, Zihan and Heidari, Ali Asghar and Zheng, Nenggan and Li, Zhicheng and Chen, Huiling and Hu, Haigen and Zhou, Qianwei and Guan, Qiu},
	month = may,
	year = {2022},
	keywords = {Augmentation, Deep learning, Generative adversarial networks, Image synthesis, Medical image},
	pages = {105382},
}

@inproceedings{hukkelas_deepprivacy_2019,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{DeepPrivacy}: {A} {Generative} {Adversarial} {Network} for {Face} {Anonymization}},
	isbn = {978-3-030-33720-9},
	shorttitle = {{DeepPrivacy}},
	doi = {10.1007/978-3-030-33720-9_44},
	abstract = {We propose a novel architecture which is able to automatically anonymize faces in images while retaining the original data distribution. We ensure total anonymization of all faces in an image by generating images exclusively on privacy-safe information. Our model is based on a conditional generative adversarial network, generating images considering the original pose and image background. The conditional information enables us to generate highly realistic faces with a seamless transition between the generated face and the existing background. Furthermore, we introduce a diverse dataset of human faces, including unconventional poses, occluded faces, and a vast variability in backgrounds. Finally, we present experimental results reflecting the capability of our model to anonymize images while preserving the data distribution, making the data suitable for further training of deep learning models. As far as we know, no other solution has been proposed that guarantees the anonymization of faces while generating realistic images.},
	language = {en},
	booktitle = {Advances in {Visual} {Computing}},
	publisher = {Springer International Publishing},
	author = {Hukkelås, Håkon and Mester, Rudolf and Lindseth, Frank},
	editor = {Bebis, George and Boyle, Richard and Parvin, Bahram and Koracin, Darko and Ushizima, Daniela and Chai, Sek and Sueda, Shinjiro and Lin, Xin and Lu, Aidong and Thalmann, Daniel and Wang, Chaoli and Xu, Panpan},
	year = {2019},
	keywords = {Face de-identification, Generative adversarial networks, Image anonymization},
	pages = {565--578},
}

@article{chen_privacy_2021,
	title = {Privacy preservation for image data: {A} {GAN}-based method},
	volume = {36},
	issn = {1098-111X},
	shorttitle = {Privacy preservation for image data},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/int.22356},
	doi = {10.1002/int.22356},
	abstract = {The importance of protecting personal information, like, a person's address or health history, is well known and commonly discussed. However, images also contain sensitive information that can compromise a person's privacy or be used for nefarious purposes. To date, most methods for preserving privacy with images have relied on obfuscation techniques, such as pixelation, blurring, or masking parts of the image. However, new face-recognition technologies driven by deep learning are showing cracks in the old techniques. Moreover, faceless recognition is presenting a whole new set of challenges for image privacy. The core of these issues it is how to ensure privacy while still being able to see and use the image. Our solution is a model based on a generative adversarial network that protects identity information while preserving face features of the original image as much as possible. The premise is to generate a fake image of a face that shares all the same attributes as the original image, for example, a brown-eyed child smiling. With this strategy, the image remains useful, but no person or algorithm could determine the identity of the pictured individual. The framework consists of three parts: a detection module, an image creation module, and an image transformation module. The detection module extracts the attribute labels. The image creation module generates images of faces, and the image transformation module transforms the fake features to match the attributes in the original image. A comprehensive set of experiments shows the effectiveness of the proposed framework.},
	language = {en},
	number = {4},
	urldate = {2023-06-16},
	journal = {International Journal of Intelligent Systems},
	author = {Chen, Zhenfei and Zhu, Tianqing and Xiong, Ping and Wang, Chenguang and Ren, Wei},
	year = {2021},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/int.22356},
	keywords = {computer vision, generative adversarial networks, image privacy, privacy preservation},
	pages = {1668--1685},
}

@inproceedings{chen_perceptual_2021,
	title = {Perceptual {Indistinguishability}-{Net} ({PI}-{Net}): {Facial} {Image} {Obfuscation} {With} {Manipulable} {Semantics}},
	shorttitle = {Perceptual {Indistinguishability}-{Net} ({PI}-{Net})},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Chen_Perceptual_Indistinguishability-Net_PI-Net_Facial_Image_Obfuscation_With_Manipulable_Semantics_CVPR_2021_paper.html},
	language = {en},
	urldate = {2023-06-16},
	author = {Chen, Jia-Wei and Chen, Li-Ju and Yu, Chia-Mu and Lu, Chun-Shien},
	year = {2021},
	pages = {6478--6487},
}

@inproceedings{jiang_dartblur_2023,
	title = {{DartBlur}: {Privacy} {Preservation} {With} {Detection} {Artifact} {Suppression}},
	shorttitle = {{DartBlur}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Jiang_DartBlur_Privacy_Preservation_With_Detection_Artifact_Suppression_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-06-16},
	author = {Jiang, Baowei and Bai, Bing and Lin, Haozhe and Wang, Yu and Guo, Yuchen and Fang, Lu},
	year = {2023},
	pages = {16479--16488},
}

@inproceedings{maximov_ciagan_2020,
	title = {{CIAGAN}: {Conditional} {Identity} {Anonymization} {Generative} {Adversarial} {Networks}},
	shorttitle = {{CIAGAN}},
	url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Maximov_CIAGAN_Conditional_Identity_Anonymization_Generative_Adversarial_Networks_CVPR_2020_paper.html},
	urldate = {2023-06-16},
	author = {Maximov, Maxim and Elezi, Ismail and Leal-Taixe, Laura},
	year = {2020},
	pages = {5447--5456},
}

@article{acs_differentially_2019,
	title = {Differentially {Private} {Mixture} of {Generative} {Neural} {Networks}},
	volume = {31},
	issn = {1558-2191},
	doi = {10.1109/TKDE.2018.2855136},
	abstract = {Generative models are used in a wide range of applications building on large amounts of contextually rich information. Due to possible privacy violations of the individuals whose data is used to train these models, however, publishing or sharing generative models is not always viable. In this paper, we present a novel technique for privately releasing generative models and entire high-dimensional datasets produced by these models. We model the generator distribution of the training data with a mixture of k generative neural networks. These are trained together and collectively learn the generator distribution of a dataset. Data is divided into k clusters, using a novel differentially private kernel k-means, then each cluster is given to separate generative neural networks, such as Restricted Boltzmann Machines or Variational Autoencoders, which are trained only on their own cluster using differentially private gradient descent. We evaluate our approach using the MNIST dataset, as well as call detail records and transit datasets, showing that it produces realistic synthetic samples, which can also be used to accurately compute arbitrary number of counting queries.},
	number = {6},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Acs, Gergely and Melis, Luca and Castelluccia, Claude and De Cristofaro, Emiliano},
	month = jun,
	year = {2019},
	note = {Conference Name: IEEE Transactions on Knowledge and Data Engineering},
	keywords = {Data models, Data privacy, Differential privacy, Kernel, Machine learning, Neural networks, Privacy, Training, generative networks, neural networks},
	pages = {1109--1121},
}

@incollection{bae_anomigan_2019,
	title = {{AnomiGAN}: {Generative} {Adversarial} {Networks} for {Anonymizing} {Private} {Medical} {Data}},
	isbn = {9789811215629},
	shorttitle = {{AnomiGAN}},
	url = {https://www.worldscientific.com/doi/abs/10.1142/9789811215636_0050},
	urldate = {2023-06-16},
	booktitle = {Biocomputing 2020},
	publisher = {WORLD SCIENTIFIC},
	author = {Bae, Ho and Jung, Dahuin and Choi, Hyun-Soo and Yoon, Sungroh},
	month = nov,
	year = {2019},
	doi = {10.1142/9789811215636_0050},
	keywords = {Deep neural networks, anonymization, differential privacy, generative adversarial networks},
	pages = {563--574},
}

@article{vovk_methods_2023,
	title = {Methods and tools for healthcare data anonymization: a literature review},
	volume = {52},
	issn = {0308-1079},
	shorttitle = {Methods and tools for healthcare data anonymization},
	url = {https://doi.org/10.1080/03081079.2023.2173749},
	doi = {10.1080/03081079.2023.2173749},
	abstract = {Healthcare is a rapidly evolving field. Such development creates opportunities to provide better quality, evidence-based treatment, however, increasing privacy violations. Anonymization can be applied to share data safely. This paper is a literature review of methods and tools for anonymization that includes 1930 papers, 32 of which were selected for the final evaluation. This article is an updated and extended version of the research conducted by our team in 2020 (Vovk 2021a). We found that despite the variety of methods, there is no single method that fits all cases. Certain methods are more widely used and create a background for advanced and secure methods. Our research shows that anonymization methods are used in tools for simplification and automation. Among challenges related to the use of all anonymization methods, is the proper balance between risk and data utility and highlight that the anonymization process is not limited to only technical measures but interdisciplinary.},
	number = {3},
	urldate = {2023-06-16},
	journal = {International Journal of General Systems},
	author = {Vovk, Olga and Piho, Gunnar and Ross, Peeter},
	month = apr,
	year = {2023},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/03081079.2023.2173749},
	keywords = {Anonymization, healthcare data, methods, structured data, tools},
	pages = {326--342},
}

@article{majeed_anonymization_2021,
	title = {Anonymization {Techniques} for {Privacy} {Preserving} {Data} {Publishing}: {A} {Comprehensive} {Survey}},
	volume = {9},
	issn = {2169-3536},
	shorttitle = {Anonymization {Techniques} for {Privacy} {Preserving} {Data} {Publishing}},
	doi = {10.1109/ACCESS.2020.3045700},
	abstract = {Anonymization is a practical solution for preserving user’s privacy in data publishing. Data owners such as hospitals, banks, social network (SN) service providers, and insurance companies anonymize their user’s data before publishing it to protect the privacy of users whereas anonymous data remains useful for legitimate information consumers. Many anonymization models, algorithms, frameworks, and prototypes have been proposed/developed for privacy preserving data publishing (PPDP). These models/algorithms anonymize users’ data which is mainly in the form of tables or graphs depending upon the data owners. It is of paramount importance to provide good perspectives of the whole information privacy area involving both tabular and SN data, and recent anonymization researches. In this paper, we presents a comprehensive survey about SN (i.e., graphs) and relational (i.e., tabular) data anonymization techniques used in the PPDP. We systematically categorize the existing anonymization techniques into relational and structural anonymization, and present an up to date thorough review on existing anonymization techniques and metrics used for their evaluation. Our aim is to provide deeper insights about the PPDP problem involving both graphs and tabular data, possible attacks that can be launched on the sanitized published data, different actors involved in the anonymization scenario, and major differences in amount of private information contained in graphs and relational data, respectively. We present various representative anonymization methods that have been proposed to solve privacy problems in application-specific scenarios of the SNs. Furthermore, we highlight the user’s re-identification methods used by malevolent adversaries to re-identify people uniquely from the privacy preserved published data. Additionally, we discuss the challenges of anonymizing both graphs and tabular data, and elaborate promising research directions. To the best of our knowledge, this is the first work to systematically cover recent PPDP techniques involving both SN and relational data, and it provides a solid foundation for future studies in the PPDP field.},
	journal = {IEEE Access},
	author = {Majeed, Abdul and Lee, Sungchang},
	year = {2021},
	note = {Conference Name: IEEE Access},
	keywords = {Data models, Data privacy, Privacy, Privacy preserving data publishing, Publishing, Social networking (online), Tools, adversary, anonymization, graphs data, information privacy, privacy, relational and structural anonymization, relational data, social networks, utility},
	pages = {8512--8545},
}

@inproceedings{bissoto_gan-based_2021,
	title = {{GAN}-{Based} {Data} {Augmentation} and {Anonymization} for {Skin}-{Lesion} {Analysis}: {A} {Critical} {Review}},
	shorttitle = {{GAN}-{Based} {Data} {Augmentation} and {Anonymization} for {Skin}-{Lesion} {Analysis}},
	url = {https://openaccess.thecvf.com/content/CVPR2021W/ISIC/html/Bissoto_GAN-Based_Data_Augmentation_and_Anonymization_for_Skin-Lesion_Analysis_A_Critical_CVPRW_2021_paper.html},
	language = {en},
	urldate = {2023-06-16},
	author = {Bissoto, Alceu and Valle, Eduardo and Avila, Sandra},
	year = {2021},
	pages = {1847--1856},
}

@article{wang_review_2021,
	title = {A review on medical imaging synthesis using deep learning and its clinical applications},
	volume = {22},
	copyright = {© 2020 The Authors. Journal of Applied Clinical Medical Physics published by Wiley Periodicals, Inc. on behalf of American Association of Physicists in Medicine.},
	issn = {1526-9914},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/acm2.13121},
	doi = {10.1002/acm2.13121},
	abstract = {This paper reviewed the deep learning-based studies for medical imaging synthesis and its clinical application. Specifically, we summarized the recent developments of deep learning-based methods in inter- and intra-modality image synthesis by listing and highlighting the proposed methods, study designs, and reported performances with related clinical applications on representative studies. The challenges among the reviewed studies were then summarized with discussion.},
	language = {en},
	number = {1},
	urldate = {2023-06-16},
	journal = {Journal of Applied Clinical Medical Physics},
	author = {Wang, Tonghe and Lei, Yang and Fu, Yabo and Wynne, Jacob F. and Curran, Walter J. and Liu, Tian and Yang, Xiaofeng},
	year = {2021},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/acm2.13121},
	keywords = {CT, MRI, PET, deep learning, image synthesis, radiation therapy},
	pages = {11--36},
}

@article{zhang_shifting_2022,
	title = {Shifting machine learning for healthcare from development to deployment and from models to data},
	volume = {6},
	copyright = {2022 Springer Nature Limited},
	issn = {2157-846X},
	url = {https://www.nature.com/articles/s41551-022-00898-y},
	doi = {10.1038/s41551-022-00898-y},
	abstract = {In the past decade, the application of machine learning (ML) to healthcare has helped drive the automation of physician tasks as well as enhancements in clinical capabilities and access to care. This progress has emphasized that, from model development to model deployment, data play central roles. In this Review, we provide a data-centric view of the innovations and challenges that are defining ML for healthcare. We discuss deep generative models and federated learning as strategies to augment datasets for improved model performance, as well as the use of the more recent transformer models for handling larger datasets and enhancing the modelling of clinical text. We also discuss data-focused problems in the deployment of ML, emphasizing the need to efficiently deliver data to ML models for timely clinical predictions and to account for natural data shifts that can deteriorate model performance.},
	language = {en},
	number = {12},
	urldate = {2023-06-16},
	journal = {Nature Biomedical Engineering},
	author = {Zhang, Angela and Xing, Lei and Zou, James and Wu, Joseph C.},
	month = dec,
	year = {2022},
	note = {Number: 12
Publisher: Nature Publishing Group},
	keywords = {Biomedical engineering, Computational science, Machine learning, Medical imaging, Translational research},
	pages = {1330--1345},
}

@article{yi_generative_2019,
	title = {Generative adversarial network in medical imaging: {A} review},
	volume = {58},
	issn = {1361-8415},
	shorttitle = {Generative adversarial network in medical imaging},
	url = {https://www.sciencedirect.com/science/article/pii/S1361841518308430},
	doi = {10.1016/j.media.2019.101552},
	abstract = {Generative adversarial networks have gained a lot of attention in the computer vision community due to their capability of data generation without explicitly modelling the probability density function. The adversarial loss brought by the discriminator provides a clever way of incorporating unlabeled samples into training and imposing higher order consistency. This has proven to be useful in many cases, such as domain adaptation, data augmentation, and image-to-image translation. These properties have attracted researchers in the medical imaging community, and we have seen rapid adoption in many traditional and novel applications, such as image reconstruction, segmentation, detection, classification, and cross-modality synthesis. Based on our observations, this trend will continue and we therefore conducted a review of recent advances in medical imaging using the adversarial training scheme with the hope of benefiting researchers interested in this technique.},
	language = {en},
	urldate = {2023-06-16},
	journal = {Medical Image Analysis},
	author = {Yi, Xin and Walia, Ekta and Babyn, Paul},
	month = dec,
	year = {2019},
	keywords = {Deep learning, Generative adversarial network, Generative model, Medical imaging, Review},
	pages = {101552},
}

@inproceedings{shin_medical_2018,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Medical {Image} {Synthesis} for {Data} {Augmentation} and {Anonymization} {Using} {Generative} {Adversarial} {Networks}},
	isbn = {978-3-030-00536-8},
	doi = {10.1007/978-3-030-00536-8_1},
	abstract = {Data diversity is critical to success when training deep learning models. Medical imaging data sets are often imbalanced as pathologic findings are generally rare, which introduces significant challenges when training deep learning models. In this work, we propose a method to generate synthetic abnormal MRI images with brain tumors by training a generative adversarial network using two publicly available data sets of brain MRI. We demonstrate two unique benefits that the synthetic images provide. First, we illustrate improved performance on tumor segmentation by leveraging the synthetic images as a form of data augmentation. Second, we demonstrate the value of generative models as an anonymization tool, achieving comparable tumor segmentation results when trained on the synthetic data versus when trained on real subject data. Together, these results offer a potential solution to two of the largest challenges facing machine learning in medical imaging, namely the small incidence of pathological findings, and the restrictions around sharing of patient data.},
	language = {en},
	booktitle = {Simulation and {Synthesis} in {Medical} {Imaging}},
	publisher = {Springer International Publishing},
	author = {Shin, Hoo-Chang and Tenenholtz, Neil A. and Rogers, Jameson K. and Schwarz, Christopher G. and Senjem, Matthew L. and Gunter, Jeffrey L. and Andriole, Katherine P. and Michalski, Mark},
	editor = {Gooya, Ali and Goksel, Orcun and Oguz, Ipek and Burgos, Ninon},
	year = {2018},
	keywords = {Brain tumor, Deep learning, GAN, Generative models, Image synthesis, MRI, Magnetic resonance imaging, Segmentation},
	pages = {1--11},
}

@inproceedings{jeon_gradient_2021,
	title = {Gradient {Inversion} with {Generative} {Image} {Prior}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper/2021/hash/fa84632d742f2729dc32ce8cb5d49733-Abstract.html},
	abstract = {Federated Learning (FL) is a distributed learning framework, in which the local data never leaves clients’ devices to preserve privacy, and the server trains models on the data via accessing only the gradients of those local data. Without further privacy mechanisms such as differential privacy, this leaves the system vulnerable against an attacker who inverts those gradients to reveal clients’ sensitive data. However, a gradient is often insufficient to reconstruct the user data without any prior knowledge. By exploiting a generative model pretrained on the data distribution, we demonstrate that data privacy can be easily breached. Further, when such prior knowledge is unavailable, we investigate the possibility of learning the prior from a sequence of gradients seen in the process of FL training. We experimentally show that the prior in a form of generative model is learnable from iterative interactions in FL. Our findings demonstrate that additional mechanisms are necessary to prevent privacy leakage in FL.},
	urldate = {2023-06-16},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Jeon, Jinwoo and Kim, jaechang and Lee, Kangwook and Oh, Sewoong and Ok, Jungseul},
	year = {2021},
	pages = {29898--29908},
}

@article{liu_when_2021,
	title = {When {Machine} {Learning} {Meets} {Privacy}: {A} {Survey} and {Outlook}},
	volume = {54},
	issn = {0360-0300},
	shorttitle = {When {Machine} {Learning} {Meets} {Privacy}},
	url = {https://dl.acm.org/doi/10.1145/3436755},
	doi = {10.1145/3436755},
	abstract = {The newly emerged machine learning (e.g., deep learning) methods have become a strong driving force to revolutionize a wide range of industries, such as smart healthcare, financial technology, and surveillance systems. Meanwhile, privacy has emerged as a big concern in this machine learning-based artificial intelligence era. It is important to note that the problem of privacy preservation in the context of machine learning is quite different from that in traditional data privacy protection, as machine learning can act as both friend and foe. Currently, the work on the preservation of privacy and machine learning are still in an infancy stage, as most existing solutions only focus on privacy problems during the machine learning process. Therefore, a comprehensive study on the privacy preservation problems and machine learning is required. This article surveys the state of the art in privacy issues and solutions for machine learning. The survey covers three categories of interactions between privacy and machine learning: (i) private machine learning, (ii) machine learning-aided privacy protection, and (iii) machine learning-based privacy attack and corresponding protection schemes. The current research progress in each category is reviewed and the key challenges are identified. Finally, based on our in-depth analysis of the area of privacy and machine learning, we point out future research directions in this field.},
	number = {2},
	urldate = {2023-06-16},
	journal = {ACM Computing Surveys},
	author = {Liu, Bo and Ding, Ming and Shaham, Sina and Rahayu, Wenny and Farokhi, Farhad and Lin, Zihuai},
	month = mar,
	year = {2021},
	keywords = {Machine learning, deep learning, differential privacy, privacy},
	pages = {31:1--31:36},
}

@inproceedings{zhang_nico_2023,
	title = {{NICO}++: {Towards} {Better} {Benchmarking} for {Domain} {Generalization}},
	abstract = {Despite the remarkable performance that modern deep neural networks have achieved on independent and identically distributed (I.I.D.) data, they can crash under distribution shifts. Most current evaluation methods for domain generalization (DG) adopt the leave-one-out strategy as a compromise on the limited number of domains. We propose a large-scale benchmark with extensive labeled domains named NICO++ along with more rational evaluation methods for comprehensively evaluating DG algorithms. To evaluate DG datasets, we propose two metrics to quantify covariate shift and concept shift, respectively. Two novel generalization bounds from the perspective of data construction are proposed to prove that limited concept shift and significant covariate shift favor the evaluation capability for generalization. Through extensive experiments, NICO++ shows its superior evaluation capability compared with current DG datasets and its contribution in alleviating unfairness caused by the leak of oracle knowledge in model selection. The data and code for the benchmark based on NICO++ are available at https: //github.com/xxgege/NICO-plus.},
	language = {en},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Zhang, Xingxuan and He, Yue and Xu, Renzhe and Yu, Han and Shen, Zheyan and Cui, Peng},
	year = {2023},
}

@article{fridovich-keil_models_2022,
	title = {Models {Out} of {Line}: {A} {Fourier} {Lens} on {Distribution} {Shift} {Robustness}},
	volume = {35},
	shorttitle = {Models {Out} of {Line}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/48736dba3b8d933fabbfdb4f22a7be71-Abstract-Conference.html},
	language = {en},
	urldate = {2023-06-15},
	journal = {Advances in Neural Information Processing Systems},
	author = {Fridovich-Keil, Sara and Bartoldson, Brian and Diffenderfer, James and Kailkhura, Bhavya and Bremer, Timo},
	month = dec,
	year = {2022},
	pages = {11175--11188},
}

@inproceedings{hendrycks_natural_2021,
	title = {Natural {Adversarial} {Examples}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Hendrycks_Natural_Adversarial_Examples_CVPR_2021_paper.html},
	language = {en},
	urldate = {2021-06-16},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Hendrycks, Dan and Zhao, Kevin and Basart, Steven and Steinhardt, Jacob and Song, Dawn},
	year = {2021},
	pages = {15262--15271},
}

@inproceedings{barbu_objectnet_2019,
	title = {{ObjectNet}: {A} large-scale bias-controlled dataset for pushing the limits of object recognition models},
	volume = {32},
	shorttitle = {{ObjectNet}},
	url = {https://proceedings.neurips.cc/paper/2019/hash/97af07a14cacba681feacf3012730892-Abstract.html},
	abstract = {We collect a large real-world test set, ObjectNet, for object recognition with controls where object backgrounds, rotations, and imaging viewpoints are random. Most scientific experiments have controls, confounds which are removed from the data, to ensure that subjects cannot perform a task by exploiting trivial correlations in the data. Historically, large machine learning and computer vision datasets have lacked such controls. This has resulted in models that must be fine-tuned for new datasets and perform better on datasets than in real-world applications. When tested on ObjectNet, object detectors show a 40-45\% drop in performance, with respect to their performance on other benchmarks, due to the controls for biases. Controls make ObjectNet robust to fine-tuning showing only small performance increases. We develop a highly automated platform that enables gathering datasets with controls by crowdsourcing image capturing and annotation. ObjectNet is the same size as the ImageNet test set (50,000 images), and by design does not come paired with a training set in order to encourage generalization. The dataset is both easier than ImageNet (objects are largely centered and unoccluded) and harder (due to the controls). Although we focus on object recognition here, data with controls can be gathered at scale using automated tools throughout machine learning to generate datasets that exercise models in new ways thus providing valuable feedback to researchers. This work opens up new avenues for research in generalizable, robust, and more human-like computer vision and in creating datasets where results are predictive of real-world performance.},
	urldate = {2023-06-13},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Barbu, Andrei and Mayo, David and Alverio, Julian and Luo, William and Wang, Christopher and Gutfreund, Dan and Tenenbaum, Josh and Katz, Boris},
	year = {2019},
}

@inproceedings{shankar_image_2021,
	title = {Do {Image} {Classifiers} {Generalize} {Across} {Time}?},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Shankar_Do_Image_Classifiers_Generalize_Across_Time_ICCV_2021_paper.html},
	language = {en},
	urldate = {2023-06-13},
	author = {Shankar, Vaishaal and Dave, Achal and Roelofs, Rebecca and Ramanan, Deva and Recht, Benjamin and Schmidt, Ludwig},
	year = {2021},
	pages = {9661--9669},
}

@misc{shen_towards_2021,
	title = {Towards {Out}-{Of}-{Distribution} {Generalization}: {A} {Survey}},
	shorttitle = {Towards {Out}-{Of}-{Distribution} {Generalization}},
	url = {http://arxiv.org/abs/2108.13624},
	doi = {10.48550/arXiv.2108.13624},
	abstract = {Classic machine learning methods are built on the \$i.i.d.\$ assumption that training and testing data are independent and identically distributed. However, in real scenarios, the \$i.i.d.\$ assumption can hardly be satisfied, rendering the sharp drop of classic machine learning algorithms' performances under distributional shifts, which indicates the significance of investigating the Out-of-Distribution generalization problem. Out-of-Distribution (OOD) generalization problem addresses the challenging setting where the testing distribution is unknown and different from the training. This paper serves as the first effort to systematically and comprehensively discuss the OOD generalization problem, from the definition, methodology, evaluation to the implications and future directions. Firstly, we provide the formal definition of the OOD generalization problem. Secondly, existing methods are categorized into three parts based on their positions in the whole learning pipeline, namely unsupervised representation learning, supervised model learning and optimization, and typical methods for each category are discussed in detail. We then demonstrate the theoretical connections of different categories, and introduce the commonly used datasets and evaluation metrics. Finally, we summarize the whole literature and raise some future directions for OOD generalization problem. The summary of OOD generalization methods reviewed in this survey can be found at http://out-of-distribution-generalization.com.},
	urldate = {2023-06-13},
	publisher = {arXiv},
	author = {Shen, Zheyan and Liu, Jiashuo and He, Yue and Zhang, Xingxuan and Xu, Renzhe and Yu, Han and Cui, Peng},
	month = aug,
	year = {2021},
	note = {arXiv:2108.13624 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@inproceedings{wang_generalizing_2021,
	title = {Generalizing to {Unseen} {Domains}: {A} {Survey} on {Domain} {Generalization}},
	volume = {5},
	shorttitle = {Generalizing to {Unseen} {Domains}},
	url = {https://www.ijcai.org/proceedings/2021/628},
	doi = {10.24963/ijcai.2021/628},
	abstract = {Electronic proceedings of IJCAI 2021},
	language = {en},
	urldate = {2023-06-13},
	booktitle = {Twenty-{Ninth} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	author = {Wang, Jindong and Lan, Cuiling and Liu, Chang and Ouyang, Yidong and Qin, Tao},
	month = aug,
	year = {2021},
	note = {ISSN: 1045-0823},
	pages = {4627--4635},
}

@inproceedings{geirhos_partial_2021,
	title = {Partial success in closing the gap between human and machine vision},
	url = {https://openreview.net/forum?id=QkljT4mrfs},
	abstract = {A few years ago, the first CNN surpassed human performance on ImageNet. However, it soon became clear that machines lack robustness on more challenging test cases, a major obstacle towards deploying machines "in the wild" and towards obtaining better computational models of human visual perception. Here we ask: Are we making progress in closing the gap between human and machine vision? To answer this question, we tested human observers on a broad range of out-of-distribution (OOD) datasets, recording 85,120 psychophysical trials across 90 participants. We then investigated a range of promising machine learning developments that crucially deviate from standard supervised CNNs along three axes: objective function (self-supervised, adversarially trained, CLIP language-image training), architecture (e.g. vision transformers), and dataset size (ranging from 1M to 1B). Our findings are threefold. (1.) The longstanding distortion robustness gap between humans and CNNs is closing, with the best models now exceeding human feedforward performance on most of the investigated OOD datasets. (2.) There is still a substantial image-level consistency gap, meaning that humans make different errors than models. In contrast, most models systematically agree in their categorisation errors, even substantially different ones like contrastive self-supervised vs. standard supervised models. (3.) In many cases, human-to-model consistency improves when training dataset size is increased by one to three orders of magnitude. Our results give reason for cautious optimism: While there is still much room for improvement, the behavioural difference between human and machine vision is narrowing. In order to measure future progress, 17 OOD datasets with image-level human behavioural data and evaluation code are provided as a toolbox and benchmark at: https://github.com/bethgelab/model-vs-human/},
	language = {en},
	urldate = {2023-06-13},
	author = {Geirhos, Robert and Narayanappa, Kantharaju and Mitzkus, Benjamin and Thieringer, Tizian and Bethge, Matthias and Wichmann, Felix A. and Brendel, Wieland},
	month = nov,
	year = {2021},
}

@article{zhou_domain_2023,
	title = {Domain {Generalization}: {A} {Survey}},
	volume = {45},
	issn = {1939-3539},
	shorttitle = {Domain {Generalization}},
	doi = {10.1109/TPAMI.2022.3195549},
	abstract = {Generalization to out-of-distribution (OOD) data is a capability natural to humans yet challenging for machines to reproduce. This is because most learning algorithms strongly rely on the i.i.d. assumption on source/target data, which is often violated in practice due to domain shift. Domain generalization (DG) aims to achieve OOD generalization by using only source data for model learning. Over the last ten years, research in DG has made great progress, leading to a broad spectrum of methodologies, e.g., those based on domain alignment, meta-learning, data augmentation, or ensemble learning, to name a few; DG has also been studied in various application areas including computer vision, speech recognition, natural language processing, medical imaging, and reinforcement learning. In this paper, for the first time a comprehensive literature review in DG is provided to summarize the developments over the past decade. Specifically, we first cover the background by formally defining DG and relating it to other relevant fields like domain adaptation and transfer learning. Then, we conduct a thorough review into existing methods and theories. Finally, we conclude this survey with insights and discussions on future research directions.},
	number = {4},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Zhou, Kaiyang and Liu, Ziwei and Qiao, Yu and Xiang, Tao and Loy, Chen Change},
	month = apr,
	year = {2023},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Adaptation models, Biomedical imaging, Data models, Face recognition, Handwriting recognition, Out-of-distribution generalization, Soft sensors, Speech recognition, domain shift, machine learning, model robustness},
	pages = {4396--4415},
}

@article{zhou_learning_2022,
	title = {Learning to {Prompt} for {Vision}-{Language} {Models}},
	volume = {130},
	issn = {1573-1405},
	url = {https://doi.org/10.1007/s11263-022-01653-1},
	doi = {10.1007/s11263-022-01653-1},
	abstract = {Large pre-trained vision-language models like CLIP have shown great potential in learning representations that are transferable across a wide range of downstream tasks. Different from the traditional representation learning that is based mostly on discretized labels, vision-language pre-training aligns images and texts in a common feature space, which allows zero-shot transfer to a downstream task via prompting, i.e., classification weights are synthesized from natural language describing classes of interest. In this work, we show that a major challenge for deploying such models in practice is prompt engineering, which requires domain expertise and is extremely time-consuming—one needs to spend a significant amount of time on words tuning since a slight change in wording could have a huge impact on performance. Inspired by recent advances in prompt learning research in natural language processing (NLP), we propose Context Optimization (CoOp), a simple approach specifically for adapting CLIP-like vision-language models for downstream image recognition. Concretely, CoOp models a prompt’s context words with learnable vectors while the entire pre-trained parameters are kept fixed. To handle different image recognition tasks, we provide two implementations of CoOp: unified context and class-specific context. Through extensive experiments on 11 datasets, we demonstrate that CoOp requires as few as one or two shots to beat hand-crafted prompts with a decent margin and is able to gain significant improvements over prompt engineering with more shots, e.g., with 16 shots the average gain is around 15\% (with the highest reaching over 45\%). Despite being a learning-based approach, CoOp achieves superb domain generalization performance compared with the zero-shot model using hand-crafted prompts.},
	language = {en},
	number = {9},
	urldate = {2023-06-12},
	journal = {International Journal of Computer Vision},
	author = {Zhou, Kaiyang and Yang, Jingkang and Loy, Chen Change and Liu, Ziwei},
	month = sep,
	year = {2022},
	pages = {2337--2348},
}

@article{geirhos_shortcut_2020,
	title = {Shortcut learning in deep neural networks},
	volume = {2},
	copyright = {2020 Springer Nature Limited},
	issn = {2522-5839},
	url = {https://www.nature.com/articles/s42256-020-00257-z},
	doi = {10.1038/s42256-020-00257-z},
	abstract = {Deep learning has triggered the current rise of artificial intelligence and is the workhorse of today’s machine intelligence. Numerous success stories have rapidly spread all over science, industry and society, but its limitations have only recently come into focus. In this Perspective we seek to distil how many of deep learning’s failures can be seen as different symptoms of the same underlying problem: shortcut learning. Shortcuts are decision rules that perform well on standard benchmarks but fail to transfer to more challenging testing conditions, such as real-world scenarios. Related issues are known in comparative psychology, education and linguistics, suggesting that shortcut learning may be a common characteristic of learning systems, biological and artificial alike. Based on these observations, we develop a set of recommendations for model interpretation and benchmarking, highlighting recent advances in machine learning to improve robustness and transferability from the lab to real-world applications.},
	language = {en},
	number = {11},
	urldate = {2023-06-12},
	journal = {Nature Machine Intelligence},
	author = {Geirhos, Robert and Jacobsen, Jörn-Henrik and Michaelis, Claudio and Zemel, Richard and Brendel, Wieland and Bethge, Matthias and Wichmann, Felix A.},
	month = nov,
	year = {2020},
	note = {Number: 11
Publisher: Nature Publishing Group},
	keywords = {Computational science, Human behaviour, Information technology, Network models},
	pages = {665--673},
}

@inproceedings{hsiung_towards_2023,
	title = {Towards {Compositional} {Adversarial} {Robustness}: {Generalizing} {Adversarial} {Training} to {Composite} {Semantic} {Perturbations}},
	abstract = {Model robustness against adversarial examples of single perturbation type such as the ℓp-norm has been widely studied, yet its generalization to more realistic scenarios involving multiple semantic perturbations and their composition remains largely unexplored. In this paper, we first propose a novel method for generating composite adversarial examples. Our method can find the optimal attack composition by utilizing component-wise projected gradient descent and automatic attack-order scheduling. We then propose generalized adversarial training (GAT) to extend model robustness from ℓp-ball to composite semantic perturbations, such as the combination of Hue, Saturation, Brightness, Contrast, and Rotation. Results obtained using ImageNet and CIFAR10 datasets indicate that GAT can be robust not only to all the tested types of a single attack, but also to any combination of such attacks. GAT also outperforms baseline ℓ∞-norm bounded adversarial training approaches by a significant margin.},
	language = {en},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Hsiung, Lei and Tsai, Yun-Yun and Chen, Pin-Yu and Ho, Tsung-Yi},
	year = {2023},
}

@misc{kingma_adam_2017,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	shorttitle = {Adam},
	url = {http://arxiv.org/abs/1412.6980},
	doi = {10.48550/arXiv.1412.6980},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
	urldate = {2023-06-12},
	publisher = {arXiv},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	month = jan,
	year = {2017},
	note = {arXiv:1412.6980 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{chen_benchmarking_2023,
	title = {Benchmarking {Robustness} of {Adaptation} {Methods} on {Pre}-trained {Vision}-{Language} {Models}},
	url = {http://arxiv.org/abs/2306.02080},
	doi = {10.48550/arXiv.2306.02080},
	abstract = {Various adaptation methods, such as LoRA, prompts, and adapters, have been proposed to enhance the performance of pre-trained vision-language models in specific domains. The robustness of these adaptation methods against distribution shifts have not been studied. In this study, we assess the robustness of 11 widely-used adaptation methods across 4 vision-language datasets under multimodal corruptions. Concretely, we introduce 7 benchmark datasets, including 96 visual and 87 textual corruptions, to investigate the robustness of different adaptation methods, the impact of available adaptation examples, and the influence of trainable parameter size during adaptation. Our analysis reveals that: 1) Adaptation methods are more sensitive to text corruptions than visual corruptions. 2) Full fine-tuning does not consistently provide the highest robustness; instead, adapters can achieve better robustness with comparable clean performance. 3) Contrary to expectations, our findings indicate that increasing the number of adaptation data and parameters does not guarantee enhanced robustness; instead it results in even lower robustness. We hope this study could benefit future research in the development of robust multimodal adaptation methods. The benchmark, code, and dataset used in this study can be accessed at {\textbackslash}url\{https://adarobustness.github.io\}.},
	urldate = {2023-06-11},
	publisher = {arXiv},
	author = {Chen, Shuo and Gu, Jindong and Han, Zhen and Ma, Yunpu and Torr, Philip and Tresp, Volker},
	month = jun,
	year = {2023},
	note = {arXiv:2306.02080 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@inproceedings{liu_twins_2023,
	title = {{TWINS}: {A} {Fine}-{Tuning} {Framework} for {Improved} {Transferability} of {Adversarial} {Robustness} and {Generalization}},
	shorttitle = {{TWINS}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Liu_TWINS_A_Fine-Tuning_Framework_for_Improved_Transferability_of_Adversarial_Robustness_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-06-09},
	author = {Liu, Ziquan and Xu, Yi and Ji, Xiangyang and Chan, Antoni B.},
	year = {2023},
	pages = {16436--16446},
}

@misc{jeddi_simple_2020,
	title = {A {Simple} {Fine}-tuning {Is} {All} {You} {Need}: {Towards} {Robust} {Deep} {Learning} {Via} {Adversarial} {Fine}-tuning},
	shorttitle = {A {Simple} {Fine}-tuning {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/2012.13628},
	doi = {10.48550/arXiv.2012.13628},
	abstract = {Adversarial Training (AT) with Projected Gradient Descent (PGD) is an effective approach for improving the robustness of the deep neural networks. However, PGD AT has been shown to suffer from two main limitations: i) high computational cost, and ii) extreme overfitting during training that leads to reduction in model generalization. While the effect of factors such as model capacity and scale of training data on adversarial robustness have been extensively studied, little attention has been paid to the effect of a very important parameter in every network optimization on adversarial robustness: the learning rate. In particular, we hypothesize that effective learning rate scheduling during adversarial training can significantly reduce the overfitting issue, to a degree where one does not even need to adversarially train a model from scratch but can instead simply adversarially fine-tune a pre-trained model. Motivated by this hypothesis, we propose a simple yet very effective adversarial fine-tuning approach based on a \${\textbackslash}textit\{slow start, fast decay\}\$ learning rate scheduling strategy which not only significantly decreases computational cost required, but also greatly improves the accuracy and robustness of a deep neural network. Experimental results show that the proposed adversarial fine-tuning approach outperforms the state-of-the-art methods on CIFAR-10, CIFAR-100 and ImageNet datasets in both test accuracy and the robustness, while reducing the computational cost by 8-10\${\textbackslash}times\$. Furthermore, a very important benefit of the proposed adversarial fine-tuning approach is that it enables the ability to improve the robustness of any pre-trained deep neural network without needing to train the model from scratch, which to the best of the authors' knowledge has not been previously demonstrated in research literature.},
	urldate = {2023-06-09},
	publisher = {arXiv},
	author = {Jeddi, Ahmadreza and Shafiee, Mohammad Javad and Wong, Alexander},
	month = dec,
	year = {2020},
	note = {arXiv:2012.13628 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@inproceedings{nohyun_data_2023,
	title = {Data {Valuation} {Without} {Training} of a {Model}},
	url = {https://openreview.net/forum?id=XIzO8zr-WbM},
	abstract = {Many recent works on understanding deep learning try to quantify how much individual data instances influence the optimization and generalization of a model. Such attempts reveal characteristics and importance of individual instances, which may provide useful information in diagnosing and improving deep learning. However, most of the existing works on data valuation require actual training of a model, which often demands high-computational cost. In this paper, we provide a training-free data valuation score, called complexity-gap score, which is a data-centric score to quantify the influence of individual instances in generalization of two-layer overparameterized neural networks. The proposed score can quantify irregularity of the instances and measure how much each data instance contributes in the total movement of the network parameters during training. We theoretically analyze and empirically demonstrate the effectiveness of the complexity-gap score in finding `irregular or mislabeled' data instances, and also provide applications of the score in analyzing datasets and diagnosing training dynamics. Our code is publicly available at https://github.com/JJchy/CG\_score.},
	language = {en},
	urldate = {2023-06-09},
	author = {Nohyun, Ki and Choi, Hoyong and Chung, Hye Won},
	month = feb,
	year = {2023},
}

@article{mishkin_systematic_2017,
	title = {Systematic evaluation of convolution neural network advances on the {Imagenet}},
	volume = {161},
	issn = {1077-3142},
	url = {https://www.sciencedirect.com/science/article/pii/S1077314217300814},
	doi = {10.1016/j.cviu.2017.05.007},
	abstract = {The paper systematically studies the impact of a range of recent advances in convolution neural network (CNN) architectures and learning methods on the object categorization (ILSVRC) problem. The evaluation tests the influence of the following choices of the architecture: non-linearity (ReLU, ELU, maxout, compatability with batch normalization), pooling variants (stochastic, max, average, mixed), network width, classifier design (convolutional, fully-connected, SPP), image pre-processing, and of learning parameters: learning rate, batch size, cleanliness of the data, etc. The performance gains of the proposed modifications are first tested individually and then in combination. The sum of individual gains is greater than the observed improvement when all modifications are introduced, but the “deficit” is small suggesting independence of their benefits. We show that the use of 128 × 128 pixel images is sufficient to make qualitative conclusions about optimal network structure that hold for the full size Caffe and VGG nets. The results are obtained an order of magnitude faster than with the standard 224 pixel images.},
	language = {en},
	urldate = {2023-06-09},
	journal = {Computer Vision and Image Understanding},
	author = {Mishkin, Dmytro and Sergievskiy, Nikolay and Matas, Jiri},
	month = aug,
	year = {2017},
	keywords = {Benchmark, CNN, ImageNet, Non-linearity, Pooling},
	pages = {11--19},
}

@misc{darlow_cinic-10_2018,
	title = {{CINIC}-10 is not {ImageNet} or {CIFAR}-10},
	url = {http://arxiv.org/abs/1810.03505},
	doi = {10.48550/arXiv.1810.03505},
	abstract = {In this brief technical report we introduce the CINIC-10 dataset as a plug-in extended alternative for CIFAR-10. It was compiled by combining CIFAR-10 with images selected and downsampled from the ImageNet database. We present the approach to compiling the dataset, illustrate the example images for different classes, give pixel distributions for each part of the repository, and give some standard benchmarks for well known models. Details for download, usage, and compilation can be found in the associated github repository.},
	urldate = {2023-06-09},
	publisher = {arXiv},
	author = {Darlow, Luke N. and Crowley, Elliot J. and Antoniou, Antreas and Storkey, Amos J.},
	month = oct,
	year = {2018},
	note = {arXiv:1810.03505 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{chrabaszcz_downsampled_2017,
	title = {A {Downsampled} {Variant} of {ImageNet} as an {Alternative} to the {CIFAR} datasets},
	url = {http://arxiv.org/abs/1707.08819},
	doi = {10.48550/arXiv.1707.08819},
	abstract = {The original ImageNet dataset is a popular large-scale benchmark for training Deep Neural Networks. Since the cost of performing experiments (e.g, algorithm design, architecture search, and hyperparameter tuning) on the original dataset might be prohibitive, we propose to consider a downsampled version of ImageNet. In contrast to the CIFAR datasets and earlier downsampled versions of ImageNet, our proposed ImageNet32\${\textbackslash}times\$32 (and its variants ImageNet64\${\textbackslash}times\$64 and ImageNet16\${\textbackslash}times\$16) contains exactly the same number of classes and images as ImageNet, with the only difference that the images are downsampled to 32\${\textbackslash}times\$32 pixels per image (64\${\textbackslash}times\$64 and 16\${\textbackslash}times\$16 pixels for the variants, respectively). Experiments on these downsampled variants are dramatically faster than on the original ImageNet and the characteristics of the downsampled datasets with respect to optimal hyperparameters appear to remain similar. The proposed datasets and scripts to reproduce our results are available at http://image-net.org/download-images and https://github.com/PatrykChrabaszcz/Imagenet32\_Scripts},
	urldate = {2023-06-09},
	publisher = {arXiv},
	author = {Chrabaszcz, Patryk and Loshchilov, Ilya and Hutter, Frank},
	month = aug,
	year = {2017},
	note = {arXiv:1707.08819 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@inproceedings{lee_simple_2018,
	title = {A {Simple} {Unified} {Framework} for {Detecting} {Out}-of-{Distribution} {Samples} and {Adversarial} {Attacks}},
	volume = {31},
	url = {https://proceedings.neurips.cc/paper/2018/hash/abdeb6f575ac5c6676b747bca8d09cc2-Abstract.html},
	abstract = {Detecting test samples drawn sufficiently far away from the training distribution statistically or adversarially is a fundamental requirement for deploying a good classifier in many real-world machine learning applications. However, deep neural networks with the softmax classifier are known to produce highly overconfident posterior distributions even for such abnormal samples. In this paper, we propose a simple yet effective method for detecting any abnormal samples, which is applicable to any pre-trained softmax neural classifier. We obtain the class conditional Gaussian distributions with respect to (low- and upper-level) features of the deep models under Gaussian discriminant analysis, which result in a confidence score based on the Mahalanobis distance. While most prior methods have been evaluated for detecting either out-of-distribution or adversarial samples, but not both, the proposed method achieves the state-of-the-art performances for both cases in our experiments. Moreover, we found that our proposed method is more robust in harsh cases, e.g., when the training dataset has noisy labels or small number of samples. Finally, we show that the proposed method enjoys broader usage by applying it to class-incremental learning: whenever out-of-distribution samples are detected, our classification rule can incorporate new classes well without further training deep models.},
	urldate = {2023-06-09},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Lee, Kimin and Lee, Kibok and Lee, Honglak and Shin, Jinwoo},
	year = {2018},
}

@article{gao_synthetic_2023,
	title = {Synthetic data accelerates the development of generalizable learning-based algorithms for {X}-ray image analysis},
	volume = {5},
	copyright = {2023 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {2522-5839},
	url = {https://www.nature.com/articles/s42256-023-00629-1},
	doi = {10.1038/s42256-023-00629-1},
	abstract = {Artificial intelligence (AI) now enables automated interpretation of medical images. However, AI’s potential use for interventional image analysis remains largely untapped. This is because the post hoc analysis of data collected during live procedures has fundamental and practical limitations, including ethical considerations, expense, scalability, data integrity and a lack of ground truth. Here we demonstrate that creating realistic simulated images from human models is a viable alternative and complement to large-scale in situ data collection. We show that training AI image analysis models on realistically synthesized data, combined with contemporary domain generalization techniques, results in machine learning models that on real data perform comparably to models trained on a precisely matched real data training set. We find that our model transfer paradigm for X-ray image analysis, which we refer to as SyntheX, can even outperform real-data-trained models due to the effectiveness of training on a larger dataset. SyntheX provides an opportunity to markedly accelerate the conception, design and evaluation of X-ray-based intelligent systems. In addition, SyntheX provides the opportunity to test novel instrumentation, design complementary surgical approaches, and envision novel techniques that improve outcomes, save time or mitigate human error, free from the ethical and practical considerations of live human data collection.},
	language = {en},
	number = {3},
	urldate = {2023-06-08},
	journal = {Nature Machine Intelligence},
	author = {Gao, Cong and Killeen, Benjamin D. and Hu, Yicheng and Grupp, Robert B. and Taylor, Russell H. and Armand, Mehran and Unberath, Mathias},
	month = mar,
	year = {2023},
	note = {Number: 3
Publisher: Nature Publishing Group},
	keywords = {Biomedical engineering, Predictive markers},
	pages = {294--308},
}

@misc{shi_effective_2023,
	title = {Effective {Robustness} against {Natural} {Distribution} {Shifts} for {Models} with {Different} {Training} {Data}},
	url = {http://arxiv.org/abs/2302.01381},
	doi = {10.48550/arXiv.2302.01381},
	abstract = {``Effective robustness'' measures the extra out-of-distribution (OOD) robustness beyond what can be predicted from the in-distribution (ID) performance. Existing effective robustness evaluations typically use a single test set such as ImageNet to evaluate ID accuracy. This becomes problematic when evaluating models trained on different data distributions, e.g., comparing models trained on ImageNet vs. zero-shot language-image pre-trained models trained on LAION. In this paper, we propose a new effective robustness evaluation metric to compare the effective robustness of models trained on different data distributions. To do this we control for the accuracy on multiple ID test sets that cover the training distributions for all the evaluated models. Our new evaluation metric provides a better estimate of the effectiveness robustness and explains the surprising effective robustness gains of zero-shot CLIP-like models exhibited when considering only one ID dataset, while the gains diminish under our evaluation.},
	urldate = {2023-06-08},
	publisher = {arXiv},
	author = {Shi, Zhouxing and Carlini, Nicholas and Balashankar, Ananth and Schmidt, Ludwig and Hsieh, Cho-Jui and Beutel, Alex and Qin, Yao},
	month = feb,
	year = {2023},
	note = {arXiv:2302.01381 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@inproceedings{wang_improving_2022,
	title = {Improving {Out}-of-{Distribution} {Generalization} by {Adversarial} {Training} with {Structured} {Priors}},
	url = {https://openreview.net/forum?id=Ku1afTnmozi},
	abstract = {Deep models often fail to generalize well in test domains when the data distribution differs from that in the training domain. Among numerous approaches to address this Out-of-Distribution (OOD) generalization problem, there has been a growing surge of interest in exploiting Adversarial Training (AT) to improve OOD performance. Recent works have revealed that the robust model obtained by conducting sample-wise AT also retains transferability to biased test domains. In this paper, we empirically show that sample-wise AT has limited improvement on OOD performance. Specifically, we find that AT can only maintain performance at smaller scales of perturbation while Universal AT (UAT) is more robust to larger-scale perturbations. This provides us with clues that adversarial perturbations with universal (low dimensional) structures can enhance the robustness against large data distribution shifts that are common in OOD scenarios. Inspired by this, we propose two AT variants with low-rank structures to train OOD-robust models. Extensive experiments on DomainBed benchmark show that our proposed approaches outperform Empirical Risk Minimization (ERM) and sample-wise AT. Our code is available at https://github.com/NOVAglow646/NIPS22-MAT-and-LDAT-for-OOD.},
	language = {en},
	urldate = {2023-06-05},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Wang, Qixun and Wang, Yifei and Zhu, Hong and Wang, Yisen},
	month = oct,
	year = {2022},
}

@inproceedings{taori_measuring_2020,
	title = {Measuring {Robustness} to {Natural} {Distribution} {Shifts} in {Image} {Classification}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/d8330f857a17c53d217014ee776bfd50-Abstract.html},
	abstract = {We study how robust current ImageNet models are to distribution shifts arising from natural variations in datasets. Most research on robustness focuses on synthetic image perturbations (noise, simulated weather artifacts, adversarial examples, etc.), which leaves open how robustness on synthetic distribution shift relates to distribution shift arising in real data. Informed by an evaluation of 204 ImageNet models in 213 different test conditions, we find that there is often little to no transfer of robustness from current synthetic to natural distribution shift. Moreover, most current techniques provide no robustness to the natural distribution shifts in our testbed. The main exception is training on larger and more diverse datasets, which in multiple cases increases robustness, but is still far from closing the performance gaps. Our results indicate that distribution shifts arising in real data are currently an open research problem.},
	urldate = {2023-06-08},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Taori, Rohan and Dave, Achal and Shankar, Vaishaal and Carlini, Nicholas and Recht, Benjamin and Schmidt, Ludwig},
	year = {2020},
	pages = {18583--18599},
}

@inproceedings{debenedetti_light_2023,
	title = {A {Light} {Recipe} to {Train} {Robust} {Vision} {Transformers}},
	url = {https://openreview.net/forum?id=IztT98ky0cKs},
	abstract = {In this paper, we ask whether Vision Transformers (ViTs) can serve as an underlying architecture for improving the adversarial robustness of machine learning models against evasion attacks. While earlier works have focused on improving Convolutional Neural Networks, we show that also ViTs are highly suitable for adversarial training to achieve competitive performance. We achieve this objective using a custom adversarial training recipe, discovered using rigorous ablation studies on a subset of the ImageNet dataset. The canonical training recipe for ViTs recommends strong data augmentation, in part to compensate for the lack of vision inductive bias of attention modules, when compared to convolutions. We show that this recipe achieves suboptimal performance when used for adversarial training. In contrast, we find that omitting all heavy data augmentation, and adding some additional bag-of-tricks (\${\textbackslash}varepsilon\$-warmup and larger weight decay), significantly boosts the performance of robust ViTs. We show that our recipe generalizes to different classes of ViT architectures and large-scale models on full ImageNet-1k. Additionally, investigating the reasons for the robustness of our models, we show that it is easier to generate strong attacks during training when using our recipe and that this leads to better robustness at test time. Finally, we further study one consequence of adversarial training by proposing a way to quantify the semantic nature of adversarial perturbations and highlight its correlation with the robustness of the model. Overall, we recommend that the community should avoid translating the canonical training recipes in ViTs to robust training and rethink common training choices in the context of adversarial training.},
	language = {en},
	urldate = {2023-03-14},
	booktitle = {First {IEEE} {Conference} on {Secure} and {Trustworthy} {Machine} {Learning}},
	author = {Debenedetti, Edoardo and Sehwag, Vikash and Mittal, Prateek},
	month = feb,
	year = {2023},
}

@inproceedings{jacobsen_excessive_2018,
	title = {Excessive {Invariance} {Causes} {Adversarial} {Vulnerability}},
	url = {https://openreview.net/forum?id=BkfbpsAcF7},
	abstract = {Despite their impressive performance, deep neural networks exhibit striking failures on out-of-distribution inputs. One core idea of adversarial example research is to reveal neural network errors under such distribution shifts. We decompose these errors into two complementary sources: sensitivity and invariance. We show deep networks are not only too sensitive to task-irrelevant changes of their input, as is well-known from epsilon-adversarial examples, but are also too invariant to a wide range of task-relevant changes, thus making vast regions in input space vulnerable to adversarial attacks. We show such excessive invariance occurs across various tasks and architecture types. On MNIST and ImageNet one can manipulate the class-specific content of almost any image without changing the hidden activations. We identify an insufficiency of the standard cross-entropy loss as a reason for these failures. Further, we extend this objective based on an information-theoretic analysis so it encourages the model to consider all task-dependent features in its decision. This provides the first approach tailored explicitly to overcome excessive invariance and resulting vulnerabilities.},
	language = {en},
	urldate = {2023-06-07},
	author = {Jacobsen, Joern-Henrik and Behrmann, Jens and Zemel, Richard and Bethge, Matthias},
	month = dec,
	year = {2018},
}

@inproceedings{tramer_fundamental_2020,
	title = {Fundamental {Tradeoffs} between {Invariance} and {Sensitivity} to {Adversarial} {Perturbations}},
	url = {https://proceedings.mlr.press/v119/tramer20a.html},
	abstract = {Adversarial examples are malicious inputs crafted to induce misclassification. Commonly studied {\textbackslash}emph\{sensitivity-based\} adversarial examples introduce semantically-small changes to an input that result in a different model prediction. This paper studies a complementary failure mode, {\textbackslash}emph\{invariance-based\} adversarial examples, that introduce minimal semantic changes that modify an input’s true label yet preserve the model’s prediction. We demonstrate fundamental tradeoffs between these two types of adversarial examples. We show that defenses against sensitivity-based attacks actively harm a model’s accuracy on invariance-based attacks, and that new approaches are needed to resist both attack types. In particular, we break state-of-the-art adversarially-trained and {\textbackslash}emph\{certifiably-robust\} models by generating small perturbations that the models are (provably) robust to, yet that change an input’s class according to human labelers. Finally, we formally show that the existence of excessively invariant classifiers arises from the presence of {\textbackslash}emph\{overly-robust\} predictive features in standard datasets.},
	language = {en},
	urldate = {2023-06-07},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Tramer, Florian and Behrmann, Jens and Carlini, Nicholas and Papernot, Nicolas and Jacobsen, Joern-Henrik},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {9561--9571},
}

@misc{papernot_transferability_2016,
	title = {Transferability in {Machine} {Learning}: from {Phenomena} to {Black}-{Box} {Attacks} using {Adversarial} {Samples}},
	shorttitle = {Transferability in {Machine} {Learning}},
	url = {http://arxiv.org/abs/1605.07277},
	doi = {10.48550/arXiv.1605.07277},
	abstract = {Many machine learning models are vulnerable to adversarial examples: inputs that are specially crafted to cause a machine learning model to produce an incorrect output. Adversarial examples that affect one model often affect another model, even if the two models have different architectures or were trained on different training sets, so long as both models were trained to perform the same task. An attacker may therefore train their own substitute model, craft adversarial examples against the substitute, and transfer them to a victim model, with very little information about the victim. Recent work has further developed a technique that uses the victim model as an oracle to label a synthetic training set for the substitute, so the attacker need not even collect a training set to mount the attack. We extend these recent techniques using reservoir sampling to greatly enhance the efficiency of the training procedure for the substitute model. We introduce new transferability attacks between previously unexplored (substitute, victim) pairs of machine learning model classes, most notably SVMs and decision trees. We demonstrate our attacks on two commercial machine learning classification systems from Amazon (96.19\% misclassification rate) and Google (88.94\%) using only 800 queries of the victim model, thereby showing that existing machine learning approaches are in general vulnerable to systematic black-box attacks regardless of their structure.},
	urldate = {2023-06-07},
	publisher = {arXiv},
	author = {Papernot, Nicolas and McDaniel, Patrick and Goodfellow, Ian},
	month = may,
	year = {2016},
	note = {arXiv:1605.07277 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@article{gourdeau_hardness_2021,
	title = {On the {Hardness} of {Robust} {Classification}},
	volume = {22},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v22/20-285.html},
	abstract = {It is becoming increasingly important to understand the vulnerability of machine learning models to adversarial attacks. In this paper we study the feasibility of adversarially robust learning from the perspective of computational learning theory, considering both sample and computational complexity. In particular, our definition of robust learnability requires polynomial sample complexity. We start with two negative results. We show that no non-trivial concept class can be robustly learned in the distribution-free setting against an adversary who can perturb just a single input bit. We show, moreover, that the class of monotone conjunctions cannot be robustly learned under the uniform distribution against an adversary who can perturb 
ω(logn)
𝜔
(
log
⁡
𝑛
)
 input bits. However, we also show that if the adversary is restricted to perturbing 
O(logn)
𝑂
(
log
⁡
𝑛
)
 bits, then one can robustly learn the class of 
1
1
-decision lists (which subsumes monotone conjunctions) with respect to the class of log-Lipschitz distributions. We then extend this result to show learnability of 2-decision lists and monotone 
k
𝑘
-decision lists in the same distributional and adversarial setting. Finally, we provide a simple proof of the computational hardness of robust learning on the boolean hypercube. Unlike previous results of this nature, our result does not rely on a more restricted model of learning, such as the statistical query model, nor on any hardness assumption other than the existence of an (average-case) hard learning problem in the PAC framework; this allows us to have a clean proof of the reduction, and the assumption is no stronger than assumptions that are used to build cryptographic primitives.},
	number = {273},
	urldate = {2023-06-07},
	journal = {Journal of Machine Learning Research},
	author = {Gourdeau, Pascale and Kanade, Varun and Kwiatkowska, Marta and Worrell, James},
	year = {2021},
	pages = {1--29},
}

@inproceedings{pooladzandi_adaptive_2022,
	title = {Adaptive {Second} {Order} {Coresets} for {Data}-efficient {Machine} {Learning}},
	url = {https://proceedings.mlr.press/v162/pooladzandi22a.html},
	abstract = {Training machine learning models on massive datasets incurs substantial computational costs. To alleviate such costs, there has been a sustained effort to develop data-efficient training methods that can carefully select subsets of the training examples that generalize on par with the full training data. However, existing methods are limited in providing theoretical guarantees for the quality of the models trained on the extracted subsets, and may perform poorly in practice. We propose AdaCore, a method that leverages the geometry of the data to extract subsets of the training examples for efficient machine learning. The key idea behind our method is to dynamically approximate the curvature of the loss function via an exponentially-averaged estimate of the Hessian to select weighted subsets (coresets) that provide a close approximation of the full gradient preconditioned with the Hessian. We prove rigorous guarantees for the convergence of various first and second-order methods applied to the subsets chosen by AdaCore. Our extensive experiments show that AdaCore extracts coresets with higher quality compared to baselines and speeds up training of convex and non-convex machine learning models, such as logistic regression and neural networks, by over 2.9x over the full data and 4.5x over random subsets.},
	language = {en},
	urldate = {2023-06-06},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Pooladzandi, Omead and Davini, David and Mirzasoleiman, Baharan},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {17848--17869},
}

@inproceedings{mindermann_prioritized_2022,
	title = {Prioritized {Training} on {Points} that are {Learnable}, {Worth} {Learning}, and not yet {Learnt}},
	url = {https://proceedings.mlr.press/v162/mindermann22a.html},
	abstract = {Training on web-scale data can take months. But much computation and time is wasted on redundant and noisy points that are already learnt or not learnable. To accelerate training, we introduce Reducible Holdout Loss Selection (RHO-LOSS), a simple but principled technique which selects approximately those points for training that most reduce the model’s generalization loss. As a result, RHO-LOSS mitigates the weaknesses of existing data selection methods: techniques from the optimization literature typically select "hard" (e.g. high loss) points, but such points are often noisy (not learnable) or less task-relevant. Conversely, curriculum learning prioritizes "easy" points, but such points need not be trained on once learned. In contrast, RHO-LOSS selects points that are learnable, worth learning, and not yet learnt. RHO-LOSS trains in far fewer steps than prior art, improves accuracy, and speeds up training on a wide range of datasets, hyperparameters, and architectures (MLPs, CNNs, and BERT). On the large web-scraped image dataset Clothing-1M, RHO-LOSS trains in 18x fewer steps and reaches 2\% higher final accuracy than uniform data shuffling.},
	language = {en},
	urldate = {2023-06-06},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Mindermann, Sören and Brauner, Jan M. and Razzak, Muhammed T. and Sharma, Mrinank and Kirsch, Andreas and Xu, Winnie and Höltgen, Benedikt and Gomez, Aidan N. and Morisot, Adrien and Farquhar, Sebastian and Gal, Yarin},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {15630--15649},
}

@misc{cui_dc-bench_2022,
	title = {{DC}-{BENCH}: {Dataset} {Condensation} {Benchmark}},
	shorttitle = {{DC}-{BENCH}},
	url = {http://arxiv.org/abs/2207.09639},
	doi = {10.48550/arXiv.2207.09639},
	abstract = {Dataset Condensation is a newly emerging technique aiming at learning a tiny dataset that captures the rich information encoded in the original dataset. As the size of datasets contemporary machine learning models rely on becomes increasingly large, condensation methods become a prominent direction for accelerating network training and reducing data storage. Despite numerous methods have been proposed in this rapidly growing field, evaluating and comparing different condensation methods is non-trivial and still remains an open issue. The quality of condensed dataset are often shadowed by many critical contributing factors to the end performance, such as data augmentation and model architectures. The lack of a systematic way to evaluate and compare condensation methods not only hinders our understanding of existing techniques, but also discourages practical usage of the synthesized datasets. This work provides the first large-scale standardized benchmark on Dataset Condensation. It consists of a suite of evaluations to comprehensively reflect the generability and effectiveness of condensation methods through the lens of their generated dataset. Leveraging this benchmark, we conduct a large-scale study of current condensation methods, and report many insightful findings that open up new possibilities for future development. The benchmark library, including evaluators, baseline methods, and generated datasets, is open-sourced to facilitate future research and application.},
	urldate = {2023-06-06},
	publisher = {arXiv},
	author = {Cui, Justin and Wang, Ruochen and Si, Si and Hsieh, Cho-Jui},
	month = oct,
	year = {2022},
	note = {arXiv:2207.09639 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@inproceedings{tiwari_gcr_2022,
	title = {{GCR}: {Gradient} {Coreset} {Based} {Replay} {Buffer} {Selection} for {Continual} {Learning}},
	shorttitle = {{GCR}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Tiwari_GCR_Gradient_Coreset_Based_Replay_Buffer_Selection_for_Continual_Learning_CVPR_2022_paper.html},
	language = {en},
	urldate = {2023-06-06},
	author = {Tiwari, Rishabh and Killamsetty, Krishnateja and Iyer, Rishabh and Shenoy, Pradeep},
	year = {2022},
	pages = {99--108},
}

@article{bartoldson_compute-efficient_2023,
	title = {Compute-{Efficient} {Deep} {Learning}: {Algorithmic} {Trends} and {Opportunities}},
	volume = {24},
	issn = {1533-7928},
	shorttitle = {Compute-{Efficient} {Deep} {Learning}},
	url = {http://jmlr.org/papers/v24/22-1208.html},
	abstract = {Although deep learning has made great progress in recent years, the exploding economic and environmental costs of training neural networks are becoming unsustainable. To address this problem, there has been a great deal of research on *algorithmically-efficient deep learning*, which seeks to reduce training costs not at the hardware or implementation level, but through changes in the semantics of the training program. In this paper, we present a structured and comprehensive overview of the research in this field. First, we formalize the *algorithmic speedup* problem, then we use fundamental building blocks of algorithmically efficient training to develop a taxonomy. Our taxonomy highlights commonalities of seemingly disparate methods and reveals current research gaps. Next, we present evaluation best practices to enable comprehensive, fair, and reliable comparisons of speedup techniques. To further aid research and applications, we discuss common bottlenecks in the training pipeline (illustrated via experiments) and offer taxonomic mitigation strategies for them. Finally, we highlight some unsolved research challenges and present promising future directions.},
	number = {122},
	urldate = {2023-06-06},
	journal = {Journal of Machine Learning Research},
	author = {Bartoldson, Brian R. and Kailkhura, Bhavya and Blalock, Davis},
	year = {2023},
	pages = {1--77},
}

@inproceedings{garg_samples_2023,
	title = {Samples {With} {Low} {Loss} {Curvature} {Improve} {Data} {Efficiency}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Garg_Samples_With_Low_Loss_Curvature_Improve_Data_Efficiency_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-06-06},
	author = {Garg, Isha and Roy, Kaushik},
	year = {2023},
	pages = {20290--20300},
}

@article{killamsetty_glister_2021,
	title = {{GLISTER}: {Generalization} based {Data} {Subset} {Selection} for {Efficient} and {Robust} {Learning}},
	volume = {35},
	copyright = {Copyright (c) 2021 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	shorttitle = {{GLISTER}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/16988},
	doi = {10.1609/aaai.v35i9.16988},
	abstract = {Large scale machine learning and deep models are extremely data-hungry. Unfortunately, obtaining large amounts of labeled data is expensive, and training state-of-the-art models (with hyperparameter tuning) requires significant computing resources and time. Secondly, real-world data is noisy and imbalanced. As a result, several recent papers try to make the training process more efficient and robust. However, most existing work either focuses on robustness or efficiency, but not both. In this work, we introduce GLISTER, a GeneraLIzation based data Subset selecTion for Efficient and Robust learning framework. We formulate GLISTER as a mixed discrete-continuous bi-level optimization problem to select a subset of the training data, which maximizes the log-likelihood on a held-out validation set. We then analyze GLISTER for simple classifiers such as gaussian and multinomial naive-bayes, k-nearest neighbor classifier, and linear regression and show connections to submodularity. Next, we propose an iterative online algorithm GLISTER-ONLINE, which performs data selection iteratively along with the parameter updates, and can be applied to any loss-based learning algorithm. We then show that for a rich class of loss functions including cross-entropy, hinge-loss, squared-loss, and logistic-loss, the inner discrete data selection is an instance of (weakly) submodular optimization, and we analyze conditions for which GLISTER-ONLINE reduces the validation loss and converges. Finally, we propose GLISTER-ACTIVE, an extension to batch active learning, and we empirically demonstrate the performance of GLISTER on a wide range of tasks including, (a) data selection to reduce training time, (b) robust learning under label noise and imbalance settings, and (c) batch-active learning with a number of deep and shallow models. We show that our framework improves upon the state of the art both in efficiency and accuracy (in cases (a) and (c)) and is more efficient compared to other state-of-the-art robust learning algorithms in case (b). The code for GLISTER is at: https://github.com/dssresearch/GLISTER.},
	language = {en},
	number = {9},
	urldate = {2023-06-06},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Killamsetty, Krishnateja and Sivasubramanian, Durga and Ramakrishnan, Ganesh and Iyer, Rishabh},
	month = may,
	year = {2021},
	note = {Number: 9},
	keywords = {Optimization},
	pages = {8110--8118},
}

@inproceedings{zhang_efficient_2021,
	title = {Efficient {Lottery} {Ticket} {Finding}: {Less} {Data} is {More}},
	shorttitle = {Efficient {Lottery} {Ticket} {Finding}},
	url = {https://proceedings.mlr.press/v139/zhang21c.html},
	abstract = {The lottery ticket hypothesis (LTH) reveals the existence of winning tickets (sparse but critical subnetworks) for dense networks, that can be trained in isolation from random initialization to match the latter’s accuracies. However, finding winning tickets requires burdensome computations in the train-prune-retrain process, especially on large-scale datasets (e.g., ImageNet), restricting their practical benefits. This paper explores a new perspective on finding lottery tickets more efficiently, by doing so only with a specially selected subset of data, called Pruning-Aware Critical set (PrAC set), rather than using the full training set. The concept of PrAC set was inspired by the recent observation, that deep networks have samples that are either hard to memorize during training, or easy to forget during pruning. A PrAC set is thus hypothesized to capture those most challenging and informative examples for the dense model. We observe that a high-quality winning ticket can be found with training and pruning the dense network on the very compact PrAC set, which can substantially save training iterations for the ticket finding process. Extensive experiments validate our proposal across diverse datasets and network architectures. Specifically, on CIFAR-10, CIFAR-100, and Tiny ImageNet, we locate effective PrAC sets at 35.32\% 78.19\% of their training set sizes. On top of them, we can obtain the same competitive winning tickets for the corresponding dense networks, yet saving up to 82.85\% 92.77\%, 63.54\% 74.92\%, and 76.14\% 86.56\% training iterations, respectively. Crucially, we show that a PrAC set found is reusable across different network architectures, which can amortize the extra cost of finding PrAC sets, yielding a practical regime for efficient lottery ticket finding.},
	language = {en},
	urldate = {2023-06-06},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Zhang, Zhenyu and Chen, Xuxi and Chen, Tianlong and Wang, Zhangyang},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {12380--12390},
}

@inproceedings{lee_dataset_2022,
	title = {Dataset {Condensation} with {Contrastive} {Signals}},
	url = {https://proceedings.mlr.press/v162/lee22b.html},
	abstract = {Recent studies have demonstrated that gradient matching-based dataset synthesis, or dataset condensation (DC), methods can achieve state-of-theart performance when applied to data-efficient learning tasks. However, in this study, we prove that the existing DC methods can perform worse than the random selection method when taskirrelevant information forms a significant part of the training dataset. We attribute this to the lack of participation of the contrastive signals between the classes resulting from the class-wise gradient matching strategy. To address this problem, we propose Dataset Condensation with Contrastive signals (DCC) by modifying the loss function to enable the DC methods to effectively capture the differences between classes. In addition, we analyze the new loss function in terms of training dynamics by tracking the kernel velocity. Furthermore, we introduce a bi-level warm-up strategy to stabilize the optimization. Our experimental results indicate that while the existing methods are ineffective for fine-grained image classification tasks, the proposed method can successfully generate informative synthetic datasets for the same tasks. Moreover, we demonstrate that the proposed method outperforms the baselines even on benchmark datasets such as SVHN, CIFAR-10, and CIFAR-100. Finally, we demonstrate the high applicability of the proposed method by applying it to continual learning tasks.},
	language = {en},
	urldate = {2023-06-06},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Lee, Saehyung and Chun, Sanghyuk and Jung, Sangwon and Yun, Sangdoo and Yoon, Sungroh},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {12352--12364},
}

@inproceedings{mirzasoleiman_coresets_2020,
	title = {Coresets for {Data}-efficient {Training} of {Machine} {Learning} {Models}},
	url = {https://proceedings.mlr.press/v119/mirzasoleiman20a.html},
	abstract = {Incremental gradient (IG) methods, such as stochastic gradient descent and its variants are commonly used for large scale optimization in machine learning. Despite the sustained effort to make IG methods more data-efficient, it remains an open question how to select a training data subset that can theoretically and practically perform on par with the full dataset. Here we develop CRAIG, a method to select a weighted subset (or coreset) of training data that closely estimates the full gradient by maximizing a submodular function. We prove that applying IG to this subset is guaranteed to converge to the (near)optimal solution with the same convergence rate as that of IG for convex optimization. As a result, CRAIG achieves a speedup that is inversely proportional to the size of the subset. To our knowledge, this is the first rigorous method for data-efficient training of general machine learning models. Our extensive set of experiments show that CRAIG, while achieving practically the same solution, speeds up various IG methods by up to 6x for logistic regression and 3x for training deep neural networks.},
	language = {en},
	urldate = {2023-06-06},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Mirzasoleiman, Baharan and Bilmes, Jeff and Leskovec, Jure},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {6950--6960},
}

@article{muhammad_survey_2022,
	title = {A {Survey} on {Efficient} {Methods} for {Adversarial} {Robustness}},
	volume = {10},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2022.3216291},
	abstract = {Deep learning has revolutionized computer vision with phenomenal success and widespread applications. Despite impressive results in complex problems, neural networks are susceptible to adversarial attacks: small and imperceptible changes in input space that lead these models to incorrect outputs. Adversarial attacks have raised serious concerns, and robustness to these attacks has become a vital issue. Adversarial training, a min-max optimization approach, has shown promise against these attacks. The computational cost of adversarial training, however, makes it prohibitively difficult to scale as well as to be useful in practice. Recently, several works have explored different approaches to make adversarial training computationally more affordable. This paper presents a comprehensive survey on efficient adversarial robustness methods with an aim to present a holistic outlook to make future exploration more systematic and exhaustive. We start by mathematically defining fundamental ideas in adversarially robust learning. We then divide these approaches into two categories based on underlying mechanisms: methods that modify initial adversarial training and techniques that leverage transfer learning to improve efficiency. Finally, based on this overview, we analyze and present an outlook of future directions.},
	journal = {IEEE Access},
	author = {Muhammad, Awais and Bae, Sung-Ho},
	year = {2022},
	note = {Conference Name: IEEE Access},
	keywords = {Adversarial machine learning, Deep learning, Iterative methods, Mathematical models, Neural networks, Optimization, Perturbation methods, Robustness, Training data, adversarial attacks, adversarial learning, adversarial robustness, deep learning, efficient training},
	pages = {118815--118830},
}

@inproceedings{li_efficient_2022,
	title = {Efficient {Self}-supervised {Vision} {Transformers} for {Representation} {Learning}},
	url = {https://openreview.net/forum?id=fVu3o-YUGQK},
	abstract = {This paper investigates two techniques for developing efficient self-supervised vision transformers (EsViT) for visual representation learning. First, we show through a comprehensive empirical study that multi-stage architectures with sparse self-attentions can significantly reduce modeling complexity but with a cost of losing the ability to capture fine-grained correspondences between image regions. Second, we propose a new pre-training task, non-contrastive region-matching, which allows the model to capture fine-grained region dependencies and as a result significantly improves the quality of the learned vision representations. Our results show that combining the two techniques, EsViT achieves 81.3\% top-1 on the ImageNet linear probe evaluation, outperforming prior arts with around an order magnitude of higher throughput. When transferring to downstream linear classification tasks, EsViT outperforms its supervised counterpart on 17 out of 18 datasets. The code and pre-trained models are released at: https://github.com/microsoft/esvit},
	language = {en},
	urldate = {2023-02-23},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Li, Chunyuan and Yang, Jianwei and Zhang, Pengchuan and Gao, Mei and Xiao, Bin and Dai, Xiyang and Yuan, Lu and Gao, Jianfeng},
	month = jan,
	year = {2022},
}

@inproceedings{liu_swin_2021,
	title = {Swin {Transformer}: {Hierarchical} {Vision} {Transformer} {Using} {Shifted} {Windows}},
	shorttitle = {Swin {Transformer}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Liu_Swin_Transformer_Hierarchical_Vision_Transformer_Using_Shifted_Windows_ICCV_2021_paper.html},
	language = {en},
	urldate = {2023-02-23},
	booktitle = {Proceedings of the {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision}},
	author = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
	year = {2021},
	pages = {10012--10022},
}

@inproceedings{touvron_going_2021,
	title = {Going {Deeper} {With} {Image} {Transformers}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Touvron_Going_Deeper_With_Image_Transformers_ICCV_2021_paper.html},
	language = {en},
	urldate = {2023-02-27},
	booktitle = {Proceedings of the {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision}},
	author = {Touvron, Hugo and Cord, Matthieu and Sablayrolles, Alexandre and Synnaeve, Gabriel and Jégou, Hervé},
	year = {2021},
	pages = {32--42},
}

@inproceedings{he_masked_2022,
	title = {Masked {Autoencoders} {Are} {Scalable} {Vision} {Learners}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/He_Masked_Autoencoders_Are_Scalable_Vision_Learners_CVPR_2022_paper.html},
	language = {en},
	urldate = {2023-02-28},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Dollár, Piotr and Girshick, Ross},
	year = {2022},
	pages = {16000--16009},
}

@inproceedings{liu_swin_2022,
	title = {Swin {Transformer} {V2}: {Scaling} {Up} {Capacity} and {Resolution}},
	shorttitle = {Swin {Transformer} {V2}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Liu_Swin_Transformer_V2_Scaling_Up_Capacity_and_Resolution_CVPR_2022_paper.html},
	language = {en},
	urldate = {2023-02-23},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Liu, Ze and Hu, Han and Lin, Yutong and Yao, Zhuliang and Xie, Zhenda and Wei, Yixuan and Ning, Jia and Cao, Yue and Zhang, Zheng and Dong, Li and Wei, Furu and Guo, Baining},
	year = {2022},
	pages = {12009--12019},
}

@inproceedings{zhai_scaling_2022,
	title = {Scaling {Vision} {Transformers}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Zhai_Scaling_Vision_Transformers_CVPR_2022_paper.html},
	language = {en},
	urldate = {2023-02-14},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Zhai, Xiaohua and Kolesnikov, Alexander and Houlsby, Neil and Beyer, Lucas},
	year = {2022},
	pages = {12104--12113},
}

@inproceedings{wang_internimage_2023,
	title = {{InternImage}: {Exploring} {Large}-{Scale} {Vision} {Foundation} {Models} {With} {Deformable} {Convolutions}},
	shorttitle = {{InternImage}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Wang_InternImage_Exploring_Large-Scale_Vision_Foundation_Models_With_Deformable_Convolutions_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-06-06},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Wang, Wenhai and Dai, Jifeng and Chen, Zhe and Huang, Zhenhang and Li, Zhiqi and Zhu, Xizhou and Hu, Xiaowei and Lu, Tong and Lu, Lewei and Li, Hongsheng and Wang, Xiaogang and Qiao, Yu},
	year = {2023},
	pages = {14408--14419},
}

@inproceedings{radosavovic_designing_2020,
	title = {Designing {Network} {Design} {Spaces}},
	url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Radosavovic_Designing_Network_Design_Spaces_CVPR_2020_paper.html},
	urldate = {2023-02-27},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Radosavovic, Ilija and Kosaraju, Raj Prateek and Girshick, Ross and He, Kaiming and Dollar, Piotr},
	year = {2020},
	pages = {10428--10436},
}

@inproceedings{xie_aggregated_2017,
	title = {Aggregated {Residual} {Transformations} for {Deep} {Neural} {Networks}},
	url = {https://openaccess.thecvf.com/content_cvpr_2017/html/Xie_Aggregated_Residual_Transformations_CVPR_2017_paper.html},
	urldate = {2023-02-26},
	author = {Xie, Saining and Girshick, Ross and Dollar, Piotr and Tu, Zhuowen and He, Kaiming},
	year = {2017},
	pages = {1492--1500},
}

@inproceedings{hu_squeeze-and-excitation_2018,
	title = {Squeeze-and-{Excitation} {Networks}},
	url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Hu_Squeeze-and-Excitation_Networks_CVPR_2018_paper.html},
	urldate = {2023-03-01},
	booktitle = {Proceedings of the {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Hu, Jie and Shen, Li and Sun, Gang},
	year = {2018},
	pages = {7132--7141},
}

@inproceedings{balduzzi_shattered_2017,
	title = {The {Shattered} {Gradients} {Problem}: {If} resnets are the answer, then what is the question?},
	shorttitle = {The {Shattered} {Gradients} {Problem}},
	url = {https://proceedings.mlr.press/v70/balduzzi17b.html},
	abstract = {A long-standing obstacle to progress in deep learning is the problem of vanishing and exploding gradients. Although, the problem has largely been overcome via carefully constructed initializations and batch normalization, architectures incorporating skip-connections such as highway and resnets perform much better than standard feedforward architectures despite well-chosen initialization and batch normalization. In this paper, we identify the shattered gradients problem. Specifically, we show that the correlation between gradients in standard feedforward networks decays exponentially with depth resulting in gradients that resemble white noise whereas, in contrast, the gradients in architectures with skip-connections are far more resistant to shattering, decaying sublinearly. Detailed empirical evidence is presented in support of the analysis, on both fully-connected networks and convnets. Finally, we present a new “looks linear” (LL) initialization that prevents shattering, with preliminary experiments showing the new initialization allows to train very deep networks without the addition of skip-connections.},
	language = {en},
	urldate = {2023-06-06},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Balduzzi, David and Frean, Marcus and Leary, Lennox and Lewis, J. P. and Ma, Kurt Wan-Duo and McWilliams, Brian},
	month = jul,
	year = {2017},
	note = {ISSN: 2640-3498},
	pages = {342--350},
}

@misc{nikolenko_synthetic_2019,
	title = {Synthetic {Data} for {Deep} {Learning}},
	url = {http://arxiv.org/abs/1909.11512},
	abstract = {Synthetic data is an increasingly popular tool for training deep learning models, especially in computer vision but also in other areas. In this work, we attempt to provide a comprehensive survey of the various directions in the development and application of synthetic data. First, we discuss synthetic datasets for basic computer vision problems, both low-level (e.g., optical flow estimation) and high-level (e.g., semantic segmentation), synthetic environments and datasets for outdoor and urban scenes (autonomous driving), indoor scenes (indoor navigation), aerial navigation, simulation environments for robotics, applications of synthetic data outside computer vision (in neural programming, bioinformatics, NLP, and more); we also survey the work on improving synthetic data development and alternative ways to produce it such as GANs. Second, we discuss in detail the synthetic-to-real domain adaptation problem that inevitably arises in applications of synthetic data, including synthetic-to-real refinement with GAN-based models and domain adaptation at the feature/model level without explicit data transformations. Third, we turn to privacy-related applications of synthetic data and review the work on generating synthetic datasets with differential privacy guarantees. We conclude by highlighting the most promising directions for further work in synthetic data studies.},
	urldate = {2023-06-06},
	publisher = {arXiv},
	author = {Nikolenko, Sergey I.},
	month = sep,
	year = {2019},
	note = {arXiv:1909.11512 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@inproceedings{salman_adversarially_2020,
	title = {Do {Adversarially} {Robust} {ImageNet} {Models} {Transfer} {Better}?},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/24357dd085d2c4b1a88a7e0692e60294-Abstract.html},
	abstract = {Transfer learning is a widely-used paradigm in deep learning, where models pre-trained on standard datasets can be efficiently adapted to downstream tasks. Typically, better pre-trained models yield better transfer results, suggesting that initial accuracy is a key aspect of transfer learning performance. In this work, we identify another such aspect: we find that adversarially robust models, while less accurate, often perform better than their standard-trained counterparts when used for transfer learning. Specifically, we focus on adversarially robust ImageNet classifiers, and show that they yield improved accuracy on a standard suite of downstream classification tasks. Further analysis uncovers more differences between robust and standard models in the context of transfer learning. Our results are consistent with (and in fact, add to) recent hypotheses stating that robustness leads to improved feature representations. Code and models is available in the supplementary material.},
	urldate = {2023-06-06},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Salman, Hadi and Ilyas, Andrew and Engstrom, Logan and Kapoor, Ashish and Madry, Aleksander},
	year = {2020},
	pages = {3533--3545},
}

@article{chitta_training_2022,
	title = {Training {Data} {Subset} {Search} {With} {Ensemble} {Active} {Learning}},
	volume = {23},
	issn = {1558-0016},
	doi = {10.1109/TITS.2021.3133268},
	abstract = {Deep Neural Networks (DNNs) often rely on vast datasets for training. Given the large size of such datasets, it is conceivable that they contain specific samples that either do not contribute or negatively impact the DNN’s optimization. Modifying the training distribution to exclude such samples could provide an effective solution to improve performance and reduce training time. This paper proposes to scale up ensemble Active Learning (AL) methods to perform acquisition at a large scale (10k to 500k samples at a time). We do this with ensembles of hundreds of models, obtained at a minimal computational cost by reusing intermediate training checkpoints. This allows us to automatically and efficiently perform a training data subset search for large labeled datasets. We observe that our approach obtains favorable subsets of training data, which can be used to train more accurate DNNs than training with the entire dataset. We perform an extensive experimental study of this phenomenon on three image classification benchmarks (CIFAR-10, CIFAR-100, and ImageNet), as well as an internal object detection benchmark for prototyping perception models for autonomous driving. Unlike existing studies, our experiments on object detection are at the scale required for production-ready autonomous driving systems. We provide insights on the impact of different initialization schemes, acquisition functions, and ensemble configurations at this scale. Our results provide strong empirical evidence that optimizing the training data distribution can significantly benefit large-scale vision tasks.},
	number = {9},
	journal = {IEEE Transactions on Intelligent Transportation Systems},
	author = {Chitta, Kashyap and Álvarez, José M. and Haussmann, Elmar and Farabet, Clément},
	month = sep,
	year = {2022},
	note = {Conference Name: IEEE Transactions on Intelligent Transportation Systems},
	keywords = {Active learning, AutoML, Autonomous vehicles, Computational modeling, Data models, Object detection, Training, Training data, Uncertainty, autonomous driving, ensemble, image classification, object detection, uncertainty},
	pages = {14741--14752},
}

@inproceedings{paul_deep_2021,
	title = {Deep {Learning} on a {Data} {Diet}: {Finding} {Important} {Examples} {Early} in {Training}},
	volume = {34},
	shorttitle = {Deep {Learning} on a {Data} {Diet}},
	url = {https://proceedings.neurips.cc/paper/2021/hash/ac56f8fe9eea3e4a365f29f0f1957c55-Abstract.html},
	abstract = {Recent success in deep learning has partially been driven by training increasingly overparametrized networks on ever larger datasets. It is therefore natural to ask: how much of the data is superfluous, which examples are important for generalization, and how do we find them? In this work, we make the striking observation that, in standard vision datasets, simple scores averaged over several weight initializations can be used to identify important examples very early in training. We propose two such scores—the Gradient Normed (GraNd) and the Error L2-Norm (EL2N) scores—and demonstrate their efficacy on a range of architectures and datasets by pruning significant fractions of training data without sacrificing test accuracy. In fact, using EL2N scores calculated a few epochs into training, we can prune half of the CIFAR10 training set while slightly improving test accuracy. Furthermore, for a given dataset, EL2N scores from one architecture or hyperparameter configuration generalize to other configurations. Compared to recent work that prunes data by discarding examples that are rarely forgotten over the course of training, our scores use only local information early in training. We also use our scores to detect noisy examples and study training dynamics through the lens of important examples—we investigate how the data distribution shapes the loss surface and identify subspaces of the model’s data representation that are relatively stable over training.},
	urldate = {2023-06-06},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Paul, Mansheej and Ganguli, Surya and Dziugaite, Gintare Karolina},
	year = {2021},
	pages = {20596--20607},
}

@inproceedings{toneva_empirical_2018,
	title = {An {Empirical} {Study} of {Example} {Forgetting} during {Deep} {Neural} {Network} {Learning}},
	url = {https://openreview.net/forum?id=BJlxm30cKm},
	abstract = {Inspired by the phenomenon of catastrophic forgetting, we investigate the learning dynamics of neural networks as they train on single classification tasks. Our goal is to understand whether a related phenomenon occurs when data does not undergo a clear distributional shift. We define a ``forgetting event'' to have occurred when an individual training example transitions from being classified correctly to incorrectly over the course of learning. Across several benchmark data sets, we find that: (i) certain examples are forgotten with high frequency, and some not at all; (ii) a data set's (un)forgettable examples generalize across neural architectures; and (iii) based on forgetting dynamics, a significant fraction of examples can be omitted from the training data set while still maintaining state-of-the-art generalization performance.},
	language = {en},
	urldate = {2023-06-06},
	author = {Toneva*, Mariya and Sordoni*, Alessandro and Combes*, Remi Tachet des and Trischler, Adam and Bengio, Yoshua and Gordon, Geoffrey J.},
	month = dec,
	year = {2018},
}

@misc{tang_robustart_2022,
	title = {{RobustART}: {Benchmarking} {Robustness} on {Architecture} {Design} and {Training} {Techniques}},
	shorttitle = {{RobustART}},
	url = {http://arxiv.org/abs/2109.05211},
	doi = {10.48550/arXiv.2109.05211},
	abstract = {Deep neural networks (DNNs) are vulnerable to adversarial noises, which motivates the benchmark of model robustness. Existing benchmarks mainly focus on evaluating defenses, but there are no comprehensive studies of how architecture design and training techniques affect robustness. Comprehensively benchmarking their relationships is beneficial for better understanding and developing robust DNNs. Thus, we propose RobustART, the first comprehensive Robustness investigation benchmark on ImageNet regarding ARchitecture design (49 human-designed off-the-shelf architectures and 1200+ networks from neural architecture search) and Training techniques (10+ techniques, e.g., data augmentation) towards diverse noises (adversarial, natural, and system noises). Extensive experiments substantiated several insights for the first time, e.g., (1) adversarial training is effective for the robustness against all noises types for Transformers and MLP-Mixers; (2) given comparable model sizes and aligned training settings, CNNs {\textgreater} Transformers {\textgreater} MLP-Mixers on robustness against natural and system noises; Transformers {\textgreater} MLP-Mixers {\textgreater} CNNs on adversarial robustness; (3) for some light-weight architectures, increasing model sizes or using extra data cannot improve robustness. Our benchmark presents: (1) an open-source platform for comprehensive robustness evaluation; (2) a variety of pre-trained models to facilitate robustness evaluation; and (3) a new view to better understand the mechanism towards designing robust DNNs. We will continuously develop to this ecosystem for the community.},
	urldate = {2023-06-06},
	publisher = {arXiv},
	author = {Tang, Shiyu and Gong, Ruihao and Wang, Yan and Liu, Aishan and Wang, Jiakai and Chen, Xinyun and Yu, Fengwei and Liu, Xianglong and Song, Dawn and Yuille, Alan and Torr, Philip H. S. and Tao, Dacheng},
	month = jan,
	year = {2022},
	note = {arXiv:2109.05211 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{koh_wilds_2021,
	title = {{WILDS}: {A} {Benchmark} of in-the-{Wild} {Distribution} {Shifts}},
	shorttitle = {{WILDS}},
	url = {https://proceedings.mlr.press/v139/koh21a.html},
	abstract = {Distribution shifts—where the training distribution differs from the test distribution—can substantially degrade the accuracy of machine learning (ML) systems deployed in the wild. Despite their ubiquity in the real-world deployments, these distribution shifts are under-represented in the datasets widely used in the ML community today. To address this gap, we present WILDS, a curated benchmark of 10 datasets reflecting a diverse range of distribution shifts that naturally arise in real-world applications, such as shifts across hospitals for tumor identification; across camera traps for wildlife monitoring; and across time and location in satellite imaging and poverty mapping. On each dataset, we show that standard training yields substantially lower out-of-distribution than in-distribution performance. This gap remains even with models trained by existing methods for tackling distribution shifts, underscoring the need for new methods for training models that are more robust to the types of distribution shifts that arise in practice. To facilitate method development, we provide an open-source package that automates dataset loading, contains default model architectures and hyperparameters, and standardizes evaluations. The full paper, code, and leaderboards are available at https://wilds.stanford.edu.},
	language = {en},
	urldate = {2023-06-05},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Koh, Pang Wei and Sagawa, Shiori and Marklund, Henrik and Xie, Sang Michael and Zhang, Marvin and Balsubramani, Akshay and Hu, Weihua and Yasunaga, Michihiro and Phillips, Richard Lanas and Gao, Irena and Lee, Tony and David, Etienne and Stavness, Ian and Guo, Wei and Earnshaw, Berton and Haque, Imran and Beery, Sara M. and Leskovec, Jure and Kundaje, Anshul and Pierson, Emma and Levine, Sergey and Finn, Chelsea and Liang, Percy},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {5637--5664},
}

@inproceedings{miller_accuracy_2021,
	title = {Accuracy on the {Line}: on the {Strong} {Correlation} {Between} {Out}-of-{Distribution} and {In}-{Distribution} {Generalization}},
	shorttitle = {Accuracy on the {Line}},
	url = {https://proceedings.mlr.press/v139/miller21b.html},
	abstract = {For machine learning systems to be reliable, we must understand their performance in unseen, out- of-distribution environments. In this paper, we empirically show that out-of-distribution performance is strongly correlated with in-distribution performance for a wide range of models and distribution shifts. Specifically, we demonstrate strong correlations between in-distribution and out-of- distribution performance on variants of CIFAR- 10 \& ImageNet, a synthetic pose estimation task derived from YCB objects, FMoW-WILDS satellite imagery classification, and wildlife classification in iWildCam-WILDS. The correlation holds across model architectures, hyperparameters, training set size, and training duration, and is more precise than what is expected from existing domain adaptation theory. To complete the picture, we also investigate cases where the correlation is weaker, for instance some synthetic distribution shifts from CIFAR-10-C and the tissue classification dataset Camelyon17-WILDS. Finally, we provide a candidate theory based on a Gaussian data model that shows how changes in the data covariance arising from distribution shift can affect the observed correlations.},
	language = {en},
	urldate = {2023-06-05},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Miller, John P. and Taori, Rohan and Raghunathan, Aditi and Sagawa, Shiori and Koh, Pang Wei and Shankar, Vaishaal and Liang, Percy and Carmon, Yair and Schmidt, Ludwig},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {7721--7735},
}

@inproceedings{lu_harder_2020,
	title = {Harder or {Different}?{A} {Closer} {Look} at {Distribution} {Shift} in {Dataset} {Reproduction}},
	abstract = {We introduce CIFAR-10.2, a new variant of the CIFAR-10 image classiﬁcation dataset. CIFAR10.2 is derived from the same source as CIFAR-10 and assembled via a similar process. In contrast to the CIFAR-10.1 reproduction, our new dataset is six times larger and contains both a training split (size 10,000) and a test split (size 2,000). This allows us to probe the distribution shift from CIFAR-10 by comparing the performance of classiﬁers trained on the original and new datasets. We decompose the accuracy drop due to shift into “harder” and “different” components based on the performance gain from training on the shifted data. Our experiments show that the “different” component is nearly constant across the 33 models in our testbed. We complement our experiments with a theoretical model that predicts similar behavior.},
	language = {en},
	booktitle = {{ICML} 2020 {Workshop} on {Uncertainty} and {Ro}- bustness in {Deep} {Learning}},
	author = {Lu, Shangyun and Nott, Bradley and Olson, Aaron and Todeschini, Alberto and Vahabi, Hossein and Carmon, Yair and Schmidt, Ludwig},
	year = {2020},
}

@article{west_towards_2023,
	title = {Towards quantum enhanced adversarial robustness in machine learning},
	copyright = {2023 Crown},
	issn = {2522-5839},
	url = {https://www.nature.com/articles/s42256-023-00661-1},
	doi = {10.1038/s42256-023-00661-1},
	abstract = {Machine learning algorithms are powerful tools for data-driven tasks such as image classification and feature detection. However, their vulnerability to adversarial examples—input samples manipulated to fool the algorithm—remains a serious challenge. The integration of machine learning with quantum computing has the potential to yield tools offering not only better accuracy and computational efficiency, but also superior robustness against adversarial attacks. Indeed, recent work has employed quantum-mechanical phenomena to defend against adversarial attacks, spurring the rapid development of the field of quantum adversarial machine learning (QAML) and potentially yielding a new source of quantum advantage. Despite promising early results, there remain challenges in building robust real-world QAML tools. In this Perspective, we discuss recent progress in QAML and identify key challenges. We also suggest future research directions that could determine the route to practicality for QAML approaches as quantum computing hardware scales up and noise levels are reduced.},
	language = {en},
	urldate = {2023-06-05},
	journal = {Nature Machine Intelligence},
	author = {West, Maxwell T. and Tsang, Shu-Lok and Low, Jia S. and Hill, Charles D. and Leckie, Christopher and Hollenberg, Lloyd C. L. and Erfani, Sarah M. and Usman, Muhammad},
	month = may,
	year = {2023},
	note = {Publisher: Nature Publishing Group},
	keywords = {Computer science, Quantum information, Quantum simulation},
	pages = {1--9},
}

@inproceedings{ho_denoising_2020,
	title = {Denoising {Diffusion} {Probabilistic} {Models}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html},
	abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN.},
	urldate = {2023-06-05},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
	year = {2020},
	pages = {6840--6851},
}

@inproceedings{dhariwal_diffusion_2021,
	title = {Diffusion {Models} {Beat} {GANs} on {Image} {Synthesis}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper/2021/hash/49ad23d1ec9fa4bd8d77d02681df5cfa-Abstract.html},
	urldate = {2023-06-05},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Dhariwal, Prafulla and Nichol, Alexander},
	year = {2021},
	pages = {8780--8794},
}

@misc{yang_diffusion_2023,
	title = {Diffusion {Models}: {A} {Comprehensive} {Survey} of {Methods} and {Applications}},
	shorttitle = {Diffusion {Models}},
	url = {http://arxiv.org/abs/2209.00796},
	doi = {10.48550/arXiv.2209.00796},
	abstract = {Diffusion models have emerged as a powerful new family of deep generative models with record-breaking performance in many applications, including image synthesis, video generation, and molecule design. In this survey, we provide an overview of the rapidly expanding body of work on diffusion models, categorizing the research into three key areas: efficient sampling, improved likelihood estimation, and handling data with special structures. We also discuss the potential for combining diffusion models with other generative models for enhanced results. We further review the wide-ranging applications of diffusion models in fields spanning from computer vision, natural language generation, temporal data modeling, to interdisciplinary applications in other scientific disciplines. This survey aims to provide a contextualized, in-depth look at the state of diffusion models, identifying the key areas of focus and pointing to potential areas for further exploration. Github: https://github.com/YangLing0818/Diffusion-Models-Papers-Survey-Taxonomy.},
	urldate = {2023-06-05},
	publisher = {arXiv},
	author = {Yang, Ling and Zhang, Zhilong and Song, Yang and Hong, Shenda and Xu, Runsheng and Zhao, Yue and Zhang, Wentao and Cui, Bin and Yang, Ming-Hsuan},
	month = mar,
	year = {2023},
	note = {arXiv:2209.00796 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{croitoru_diffusion_2023,
	title = {Diffusion {Models} in {Vision}: {A} {Survey}},
	issn = {1939-3539},
	shorttitle = {Diffusion {Models} in {Vision}},
	doi = {10.1109/TPAMI.2023.3261988},
	abstract = {Denoising diffusion models represent a recent emerging topic in computer vision, demonstrating remarkable results in the area of generative modeling. A diffusion model is a deep generative model that is based on two stages, a forward diffusion stage and a reverse diffusion stage. In the forward diffusion stage, the input data is gradually perturbed over several steps by adding Gaussian noise. In the reverse stage, a model is tasked at recovering the original input data by learning to gradually reverse the diffusion process, step by step. Diffusion models are widely appreciated for the quality and diversity of the generated samples, despite their known computational burdens, i.e. low speeds due to the high number of steps involved during sampling. In this survey, we provide a comprehensive review of articles on denoising diffusion models applied in vision, comprising both theoretical and practical contributions in the field. First, we identify and present three generic diffusion modeling frameworks, which are based on denoising diffusion probabilistic models, noise conditioned score networks, and stochastic differential equations. We further discuss the relations between diffusion models and other deep generative models, including variational auto-encoders, generative adversarial networks, energy-based models, autoregressive models and normalizing flows. Then, we introduce a multi-perspective categorization of diffusion models applied in computer vision. Finally, we illustrate the current limitations of diffusion models and envision some interesting directions for future research.},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Croitoru, Florinel-Alin and Hondru, Vlad and Ionescu, Radu Tudor and Shah, Mubarak},
	year = {2023},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Computational modeling, Computer vision, Data models, Deep generative modeling, Mathematical models, Noise reduction, Task analysis, Training, denoising diffusion models, diffusion models, image generation, noise conditioned score networks, score-based models},
	pages = {1--20},
}

@misc{mitrovic_chatgpt_2023,
	title = {{ChatGPT} or {Human}? {Detect} and {Explain}. {Explaining} {Decisions} of {Machine} {Learning} {Model} for {Detecting} {Short} {ChatGPT}-generated {Text}},
	shorttitle = {{ChatGPT} or {Human}?},
	url = {http://arxiv.org/abs/2301.13852},
	abstract = {ChatGPT has the ability to generate grammatically flawless and seemingly-human replies to different types of questions from various domains. The number of its users and of its applications is growing at an unprecedented rate. Unfortunately, use and abuse come hand in hand. In this paper, we study whether a machine learning model can be effectively trained to accurately distinguish between original human and seemingly human (that is, ChatGPT-generated) text, especially when this text is short. Furthermore, we employ an explainable artificial intelligence framework to gain insight into the reasoning behind the model trained to differentiate between ChatGPT-generated and human-generated text. The goal is to analyze model's decisions and determine if any specific patterns or characteristics can be identified. Our study focuses on short online reviews, conducting two experiments comparing human-generated and ChatGPT-generated text. The first experiment involves ChatGPT text generated from custom queries, while the second experiment involves text generated by rephrasing original human-generated reviews. We fine-tune a Transformer-based model and use it to make predictions, which are then explained using SHAP. We compare our model with a perplexity score-based approach and find that disambiguation between human and ChatGPT-generated reviews is more challenging for the ML model when using rephrased text. However, our proposed approach still achieves an accuracy of 79\%. Using explainability, we observe that ChatGPT's writing is polite, without specific details, using fancy and atypical vocabulary, impersonal, and typically it does not express feelings.},
	urldate = {2023-06-04},
	publisher = {arXiv},
	author = {Mitrović, Sandra and Andreoletti, Davide and Ayoub, Omran},
	month = jan,
	year = {2023},
	note = {arXiv:2301.13852 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{hao_mixgen_2023,
	title = {{MixGen}: {A} {New} {Multi}-{Modal} {Data} {Augmentation}},
	shorttitle = {{MixGen}},
	url = {https://openaccess.thecvf.com/content/WACV2023W/Pretrain/html/Hao_MixGen_A_New_Multi-Modal_Data_Augmentation_WACVW_2023_paper.html},
	language = {en},
	urldate = {2023-06-04},
	author = {Hao, Xiaoshuai and Zhu, Yi and Appalaraju, Srikar and Zhang, Aston and Zhang, Wanqian and Li, Bo and Li, Mu},
	year = {2023},
	pages = {379--389},
}

@article{xu_multimodal_2023,
	title = {Multimodal {Learning} {With} {Transformers}: {A} {Survey}},
	issn = {1939-3539},
	shorttitle = {Multimodal {Learning} {With} {Transformers}},
	doi = {10.1109/TPAMI.2023.3275156},
	abstract = {Transformer is a promising neural network learner, and has achieved great success in various machine learning tasks. Thanks to the recent prevalence of multimodal applications and Big Data, Transformer-based multimodal learning has become a hot topic in AI research. This paper presents a comprehensive survey of Transformer techniques oriented at multimodal data. The main contents of this survey include: (1) a background of multimodal learning, Transformer ecosystem, and the multimodal Big Data era, (2) a systematic review of Vanilla Transformer, Vision Transformer, and multimodal Transformers, from a geometrically topological perspective, (3) a review of multimodal Transformer applications, via two important paradigms, i.e., for multimodal pretraining and for specific multimodal tasks, (4) a summary of the common challenges and designs shared by the multimodal Transformer models and applications, and (5) a discussion of open problems and potential research directions for the community.},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Xu, Peng and Zhu, Xiatian and Clifton, David A.},
	year = {2023},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Data models, Deep learning, Mathematical models, Surveys, Task analysis, Taxonomy, Transformers, Visualization, introductory, machine learning, multimodal learning, taxonomy, transformer},
	pages = {1--20},
}

@inproceedings{petryk_guiding_2022,
	title = {On {Guiding} {Visual} {Attention} {With} {Language} {Specification}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Petryk_On_Guiding_Visual_Attention_With_Language_Specification_CVPR_2022_paper.html},
	language = {en},
	urldate = {2023-06-02},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Petryk, Suzanne and Dunlap, Lisa and Nasseri, Keyan and Gonzalez, Joseph and Darrell, Trevor and Rohrbach, Anna},
	year = {2022},
	pages = {18092--18102},
}

@inproceedings{yang_unified_2022,
	title = {Unified {Contrastive} {Learning} in {Image}-{Text}-{Label} {Space}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Yang_Unified_Contrastive_Learning_in_Image-Text-Label_Space_CVPR_2022_paper.html},
	language = {en},
	urldate = {2023-06-04},
	author = {Yang, Jianwei and Li, Chunyuan and Zhang, Pengchuan and Xiao, Bin and Liu, Ce and Yuan, Lu and Gao, Jianfeng},
	year = {2022},
	pages = {19163--19173},
}

@misc{mitchell_detectgpt_2023,
	title = {{DetectGPT}: {Zero}-{Shot} {Machine}-{Generated} {Text} {Detection} using {Probability} {Curvature}},
	shorttitle = {{DetectGPT}},
	url = {http://arxiv.org/abs/2301.11305},
	abstract = {The fluency and factual knowledge of large language models (LLMs) heightens the need for corresponding systems to detect whether a piece of text is machine-written. For example, students may use LLMs to complete written assignments, leaving instructors unable to accurately assess student learning. In this paper, we first demonstrate that text sampled from an LLM tends to occupy negative curvature regions of the model's log probability function. Leveraging this observation, we then define a new curvature-based criterion for judging if a passage is generated from a given LLM. This approach, which we call DetectGPT, does not require training a separate classifier, collecting a dataset of real or generated passages, or explicitly watermarking generated text. It uses only log probabilities computed by the model of interest and random perturbations of the passage from another generic pre-trained language model (e.g, T5). We find DetectGPT is more discriminative than existing zero-shot methods for model sample detection, notably improving detection of fake news articles generated by 20B parameter GPT-NeoX from 0.81 AUROC for the strongest zero-shot baseline to 0.95 AUROC for DetectGPT. See https://ericmitchell.ai/detectgpt for code, data, and other project information.},
	urldate = {2023-06-04},
	publisher = {arXiv},
	author = {Mitchell, Eric and Lee, Yoonho and Khazatsky, Alexander and Manning, Christopher D. and Finn, Chelsea},
	month = jan,
	year = {2023},
	note = {arXiv:2301.11305 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@article{alhamoud_generalizability_2023,
	title = {Generalizability of {Adversarial} {Robustness} {Under} {Distribution} {Shifts}},
	issn = {2835-8856},
	url = {https://openreview.net/forum?id=XNFo3dQiCJ&referrer=%5BTMLR%5D(%2Fgroup%3Fid%3DTMLR)},
	abstract = {Recent progress in empirical and certified robustness promises to deliver reliable and deployable Deep Neural Networks (DNNs). Despite that success, most existing evaluations of DNN robustness have been done on images sampled from the same distribution on which the model was trained on. However, in the real world, DNNs may be deployed in dynamic environments that exhibit significant distribution shifts. In this work, we take a first step towards thoroughly investigating the interplay between empirical and certified adversarial robustness on one hand and domain generalization on another. To do so, we train robust models on multiple domains and evaluate their accuracy and robustness on an unseen domain. We observe that: (1) both empirical and certified robustness generalize to unseen domains, and (2) the level of generalizability does not correlate well with input visual similarity, measured by the FID between source and target domains. We also extend our study to cover a real-world medical application, in which adversarial augmentation significantly boosts the generalization of robustness with minimal effect on clean data accuracy.},
	language = {en},
	urldate = {2023-06-03},
	journal = {Transactions on Machine Learning Research},
	author = {Alhamoud, Kumail and Hammoud, Hasan Abed Al Kader and Alfarra, Motasem and Ghanem, Bernard},
	month = may,
	year = {2023},
}

@article{ibrahim_robustness_2023,
	title = {The {Robustness} {Limits} of {SoTA} {Vision} {Models} to {Natural} {Variation}},
	issn = {2835-8856},
	url = {https://openreview.net/forum?id=QhHLwn3D0Y&referrer=%5BTMLR%5D(%2Fgroup%3Fid%3DTMLR)},
	abstract = {Recent state-of-the-art vision models have introduced new architectures, learning paradigms, and larger pretraining data, leading to impressive performance on tasks such as classification. While previous generations of vision models were shown to lack robustness to factors such as pose, the extent to which this next generation of models are more robust remains unclear. To study this question, we develop a dataset of more than 7 million images with controlled changes in pose, position background, lighting color, and size. We study not only how robust recent state-of- the-art models are, but also the extent to which models can generalize to variation in each of these factors. We consider a catalog of recent vision models, including vision transformers (ViT), self-supervised models such as masked autoencoders (MAE), and models trained on larger datasets such as CLIP. We find that even today’s best models are not robust to common changes in pose, size, and background. When some samples varied during training, we found models required a significant portion of instances seen varying to generalize—though eventually robustness did improve. When variability is only witnessed for some classes however, we found that models did not generalize to other classes unless the classes were very similar to those seen varying during training. We hope our work will shed further light on the blind spots of SoTA models and spur the development of more robust vision models.},
	language = {en},
	urldate = {2023-06-03},
	journal = {Transactions on Machine Learning Research},
	author = {Ibrahim, Mark and Garrido, Quentin and Morcos, Ari S. and Bouchacourt, Diane},
	month = jun,
	year = {2023},
}

@misc{kirchenbauer_watermark_2023,
	title = {A {Watermark} for {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2301.10226},
	doi = {10.48550/arXiv.2301.10226},
	abstract = {Potential harms of large language models can be mitigated by watermarking model output, i.e., embedding signals into generated text that are invisible to humans but algorithmically detectable from a short span of tokens. We propose a watermarking framework for proprietary language models. The watermark can be embedded with negligible impact on text quality, and can be detected using an efficient open-source algorithm without access to the language model API or parameters. The watermark works by selecting a randomized set of "green" tokens before a word is generated, and then softly promoting use of green tokens during sampling. We propose a statistical test for detecting the watermark with interpretable p-values, and derive an information-theoretic framework for analyzing the sensitivity of the watermark. We test the watermark using a multi-billion parameter model from the Open Pretrained Transformer (OPT) family, and discuss robustness and security.},
	urldate = {2023-06-03},
	publisher = {arXiv},
	author = {Kirchenbauer, John and Geiping, Jonas and Wen, Yuxin and Katz, Jonathan and Miers, Ian and Goldstein, Tom},
	month = jan,
	year = {2023},
	note = {arXiv:2301.10226 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@inproceedings{gehrmann_gltr_2019,
	address = {Florence, Italy},
	title = {{GLTR}: {Statistical} {Detection} and {Visualization} of {Generated} {Text}},
	shorttitle = {{GLTR}},
	url = {https://aclanthology.org/P19-3019},
	doi = {10.18653/v1/P19-3019},
	abstract = {The rapid improvement of language models has raised the specter of abuse of text generation systems. This progress motivates the development of simple methods for detecting generated text that can be used by non-experts. In this work, we introduce GLTR, a tool to support humans in detecting whether a text was generated by a model. GLTR applies a suite of baseline statistical methods that can detect generation artifacts across multiple sampling schemes. In a human-subjects study, we show that the annotation scheme provided by GLTR improves the human detection-rate of fake text from 54\% to 72\% without any prior training. GLTR is open-source and publicly deployed, and has already been widely used to detect generated outputs.},
	urldate = {2023-06-03},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}: {System} {Demonstrations}},
	publisher = {Association for Computational Linguistics},
	author = {Gehrmann, Sebastian and Strobelt, Hendrik and Rush, Alexander},
	month = jul,
	year = {2019},
	pages = {111--116},
}

@misc{he_mgtbench_2023,
	title = {{MGTBench}: {Benchmarking} {Machine}-{Generated} {Text} {Detection}},
	shorttitle = {{MGTBench}},
	url = {http://arxiv.org/abs/2303.14822},
	abstract = {Nowadays large language models (LLMs) have shown revolutionary power in a variety of natural language processing (NLP) tasks such as text classification, sentiment analysis, language translation, and question-answering. In this way, detecting machine-generated texts (MGTs) is becoming increasingly important as LLMs become more advanced and prevalent. These models can generate human-like language that can be difficult to distinguish from text written by a human, which raises concerns about authenticity, accountability, and potential bias. However, existing detection methods against MGTs are evaluated under different model architectures, datasets, and experimental settings, resulting in a lack of a comprehensive evaluation framework across different methodologies In this paper, we fill this gap by proposing the first benchmark framework for MGT detection, named MGTBench. Extensive evaluations on public datasets with curated answers generated by ChatGPT (the most representative and powerful LLMs thus far) show that most of the current detection methods perform less satisfactorily against MGTs. An exceptional case is ChatGPT Detector, which is trained with ChatGPT-generated texts and shows great performance in detecting MGTs. Nonetheless, we note that only a small fraction of adversarial-crafted perturbations on MGTs can evade the ChatGPT Detector, thus highlighting the need for more robust MGT detection methods. We envision that MGTBench will serve as a benchmark tool to accelerate future investigations involving the evaluation of state-of-the-art MGT detection methods on their respective datasets and the development of more advanced MGT detection methods. Our source code and datasets are available at https://github.com/xinleihe/MGTBench.},
	urldate = {2023-06-03},
	publisher = {arXiv},
	author = {He, Xinlei and Shen, Xinyue and Chen, Zeyuan and Backes, Michael and Zhang, Yang},
	month = mar,
	year = {2023},
	note = {arXiv:2303.14822 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@misc{chakraborty_possibilities_2023,
	title = {On the {Possibilities} of {AI}-{Generated} {Text} {Detection}},
	url = {http://arxiv.org/abs/2304.04736},
	abstract = {Our work focuses on the challenge of detecting outputs generated by Large Language Models (LLMs) from those generated by humans. The ability to distinguish between the two is of utmost importance in numerous applications. However, the possibility and impossibility of such discernment have been subjects of debate within the community. Therefore, a central question is whether we can detect AI-generated text and, if so, when. In this work, we provide evidence that it should almost always be possible to detect the AI-generated text unless the distributions of human and machine generated texts are exactly the same over the entire support. This observation follows from the standard results in information theory and relies on the fact that if the machine text is becoming more like a human, we need more samples to detect it. We derive a precise sample complexity bound of AI-generated text detection, which tells how many samples are needed to detect. This gives rise to additional challenges of designing more complicated detectors that take in n samples to detect than just one, which is the scope of future research on this topic. Our empirical evaluations support our claim about the existence of better detectors demonstrating that AI-Generated text detection should be achievable in the majority of scenarios. Our results emphasize the importance of continued research in this area},
	urldate = {2023-06-03},
	publisher = {arXiv},
	author = {Chakraborty, Souradip and Bedi, Amrit Singh and Zhu, Sicheng and An, Bang and Manocha, Dinesh and Huang, Furong},
	month = apr,
	year = {2023},
	note = {arXiv:2304.04736 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{krishna_paraphrasing_2023,
	title = {Paraphrasing evades detectors of {AI}-generated text, but retrieval is an effective defense},
	url = {http://arxiv.org/abs/2303.13408},
	abstract = {To detect the deployment of large language models for malicious use cases (e.g., fake content creation or academic plagiarism), several approaches have recently been proposed for identifying AI-generated text via watermarks or statistical irregularities. How robust are these detection algorithms to paraphrases of AI-generated text? To stress test these detectors, we first train an 11B parameter paraphrase generation model (DIPPER) that can paraphrase paragraphs, optionally leveraging surrounding text (e.g., user-written prompts) as context. DIPPER also uses scalar knobs to control the amount of lexical diversity and reordering in the paraphrases. Paraphrasing text generated by three large language models (including GPT3.5-davinci-003) with DIPPER successfully evades several detectors, including watermarking, GPTZero, DetectGPT, and OpenAI's text classifier. For example, DIPPER drops the detection accuracy of DetectGPT from 70.3\% to 4.6\% (at a constant false positive rate of 1\%), without appreciably modifying the input semantics. To increase the robustness of AI-generated text detection to paraphrase attacks, we introduce a simple defense that relies on retrieving semantically-similar generations and must be maintained by a language model API provider. Given a candidate text, our algorithm searches a database of sequences previously generated by the API, looking for sequences that match the candidate text within a certain threshold. We empirically verify our defense using a database of 15M generations from a fine-tuned T5-XXL model and find that it can detect 80\% to 97\% of paraphrased generations across different settings, while only classifying 1\% of human-written sequences as AI-generated. We will open source our code, model and data for future research.},
	urldate = {2023-06-03},
	publisher = {arXiv},
	author = {Krishna, Kalpesh and Song, Yixiao and Karpinska, Marzena and Wieting, John and Iyyer, Mohit},
	month = mar,
	year = {2023},
	note = {arXiv:2303.13408 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@misc{sadasivan_can_2023,
	title = {Can {AI}-{Generated} {Text} be {Reliably} {Detected}?},
	url = {http://arxiv.org/abs/2303.11156},
	abstract = {The rapid progress of Large Language Models (LLMs) has made them capable of performing astonishingly well on various tasks including document completion and question answering. The unregulated use of these models, however, can potentially lead to malicious consequences such as plagiarism, generating fake news, spamming, etc. Therefore, reliable detection of AI-generated text can be critical to ensure the responsible use of LLMs. Recent works attempt to tackle this problem either using certain model signatures present in the generated text outputs or by applying watermarking techniques that imprint specific patterns onto them. In this paper, both empirically and theoretically, we show that these detectors are not reliable in practical scenarios. Empirically, we show that paraphrasing attacks, where a light paraphraser is applied on top of the generative text model, can break a whole range of detectors, including the ones using the watermarking schemes as well as neural network-based detectors and zero-shot classifiers. We then provide a theoretical impossibility result indicating that for a sufficiently good language model, even the best-possible detector can only perform marginally better than a random classifier. Finally, we show that even LLMs protected by watermarking schemes can be vulnerable against spoofing attacks where adversarial humans can infer hidden watermarking signatures and add them to their generated text to be detected as text generated by the LLMs, potentially causing reputational damages to their developers. We believe these results can open an honest conversation in the community regarding the ethical and reliable use of AI-generated text.},
	urldate = {2023-06-03},
	publisher = {arXiv},
	author = {Sadasivan, Vinu Sankar and Kumar, Aounon and Balasubramanian, Sriram and Wang, Wenxiao and Feizi, Soheil},
	month = mar,
	year = {2023},
	note = {arXiv:2303.11156 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{wang_m4_2023,
	title = {M4: {Multi}-generator, {Multi}-domain, and {Multi}-lingual {Black}-{Box} {Machine}-{Generated} {Text} {Detection}},
	shorttitle = {M4},
	url = {http://arxiv.org/abs/2305.14902},
	doi = {10.48550/arXiv.2305.14902},
	abstract = {Large language models (LLMs) have demonstrated remarkable capability to generate fluent responses to a wide variety of user queries, but this has also resulted in concerns regarding the potential misuse of such texts in journalism, educational, and academic context. In this work, we aim to develop automatic systems to identify machine-generated text and to detect potential misuse. We first introduce a large-scale benchmark M4, which is multi-generator, multi-domain, and multi-lingual corpus for machine-generated text detection. Using the dataset, we experiment with a number of methods and we show that it is challenging for detectors to generalize well on unseen examples if they are either from different domains or are generated by different large language models. In such cases, detectors tend to misclassify machine-generated text as human-written. These results show that the problem is far from solved and there is a lot of room for improvement. We believe that our dataset M4, which covers different generators, domains and languages, will enable future research towards more robust approaches for this pressing societal problem. The M4 dataset is available at https://github.com/mbzuai-nlp/M4.},
	urldate = {2023-06-03},
	publisher = {arXiv},
	author = {Wang, Yuxia and Mansurov, Jonibek and Ivanov, Petar and Su, Jinyan and Shelmanov, Artem and Tsvigun, Akim and Whitehouse, Chenxi and Afzal, Osama Mohammed and Mahmoud, Tarek and Aji, Alham Fikri and Nakov, Preslav},
	month = may,
	year = {2023},
	note = {arXiv:2305.14902 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{mireshghallah_smaller_2023,
	title = {Smaller {Language} {Models} are {Better} {Black}-box {Machine}-{Generated} {Text} {Detectors}},
	url = {http://arxiv.org/abs/2305.09859},
	doi = {10.48550/arXiv.2305.09859},
	abstract = {With the advent of fluent generative language models that can produce convincing utterances very similar to those written by humans, distinguishing whether a piece of text is machine-generated or human-written becomes more challenging and more important, as such models could be used to spread misinformation, fake news, fake reviews and to mimic certain authors and figures. To this end, there have been a slew of methods proposed to detect machine-generated text. Most of these methods need access to the logits of the target model or need the ability to sample from the target. One such black-box detection method relies on the observation that generated text is locally optimal under the likelihood function of the generator, while human-written text is not. We find that overall, smaller and partially-trained models are better universal text detectors: they can more precisely detect text generated from both small and larger models. Interestingly, we find that whether the detector and generator were trained on the same data is not critically important to the detection success. For instance the OPT-125M model has an AUC of 0.81 in detecting ChatGPT generations, whereas a larger model from the GPT family, GPTJ-6B, has AUC of 0.45.},
	urldate = {2023-06-03},
	publisher = {arXiv},
	author = {Mireshghallah, Fatemehsadat and Mattern, Justus and Gao, Sicun and Shokri, Reza and Berg-Kirkpatrick, Taylor},
	month = may,
	year = {2023},
	note = {arXiv:2305.09859 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{beleites_sample_2013,
	title = {Sample size planning for classification models},
	volume = {760},
	issn = {0003-2670},
	url = {https://www.sciencedirect.com/science/article/pii/S0003267012016479},
	doi = {10.1016/j.aca.2012.11.007},
	abstract = {In biospectroscopy, suitably annotated and statistically independent samples (e.g. patients, batches, etc.) for classifier training and testing are scarce and costly. Learning curves show the model performance as function of the training sample size and can help to determine the sample size needed to train good classifiers. However, building a good model is actually not enough: the performance must also be proven. We discuss learning curves for typical small sample size situations with 5–25 independent samples per class. Although the classification models achieve acceptable performance, the learning curve can be completely masked by the random testing uncertainty due to the equally limited test sample size. In consequence, we determine test sample sizes necessary to achieve reasonable precision in the validation and find that 75–100 samples will usually be needed to test a good but not perfect classifier. Such a data set will then allow refined sample size planning on the basis of the achieved performance. We also demonstrate how to calculate necessary sample sizes in order to show the superiority of one classifier over another: this often requires hundreds of statistically independent test samples or is even theoretically impossible. We demonstrate our findings with a data set of ca. 2550 Raman spectra of single cells (five classes: erythrocytes, leukocytes and three tumour cell lines BT-20, MCF-7 and OCI-AML3) as well as by an extensive simulation that allows precise determination of the actual performance of the models in question.},
	language = {en},
	urldate = {2023-06-03},
	journal = {Analytica Chimica Acta},
	author = {Beleites, Claudia and Neugebauer, Ute and Bocklitz, Thomas and Krafft, Christoph and Popp, Jürgen},
	month = jan,
	year = {2013},
	keywords = {Classification, Design of experiments, Learning curve, Multivariate, Small sample size, Training, Validation},
	pages = {25--33},
}

@misc{kaplan_scaling_2020,
	title = {Scaling {Laws} for {Neural} {Language} {Models}},
	url = {http://arxiv.org/abs/2001.08361},
	doi = {10.48550/arXiv.2001.08361},
	abstract = {We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.},
	urldate = {2023-06-03},
	publisher = {arXiv},
	author = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B. and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
	month = jan,
	year = {2020},
	note = {arXiv:2001.08361 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{rosenfeld_scaling_2021,
	title = {Scaling {Laws} for {Deep} {Learning}},
	url = {http://arxiv.org/abs/2108.07686},
	doi = {10.48550/arXiv.2108.07686},
	abstract = {Running faster will only get you so far -- it is generally advisable to first understand where the roads lead, then get a car ... The renaissance of machine learning (ML) and deep learning (DL) over the last decade is accompanied by an unscalable computational cost, limiting its advancement and weighing on the field in practice. In this thesis we take a systematic approach to address the algorithmic and methodological limitations at the root of these costs. We first demonstrate that DL training and pruning are predictable and governed by scaling laws -- for state of the art models and tasks, spanning image classification and language modeling, as well as for state of the art model compression via iterative pruning. Predictability, via the establishment of these scaling laws, provides the path for principled design and trade-off reasoning, currently largely lacking in the field. We then continue to analyze the sources of the scaling laws, offering an approximation-theoretic view and showing through the exploration of a noiseless realizable case that DL is in fact dominated by error sources very far from the lower error limit. We conclude by building on the gained theoretical understanding of the scaling laws' origins. We present a conjectural path to eliminate one of the current dominant error sources -- through a data bandwidth limiting hypothesis and the introduction of Nyquist learners -- which can, in principle, reach the generalization error lower limit (e.g. 0 in the noiseless case), at finite dataset size.},
	urldate = {2023-06-03},
	publisher = {arXiv},
	author = {Rosenfeld, Jonathan S.},
	month = aug,
	year = {2021},
	note = {arXiv:2108.07686 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@inproceedings{johnson_predicting_2018,
	address = {Melbourne, Australia},
	title = {Predicting accuracy on large datasets from smaller pilot data},
	url = {https://aclanthology.org/P18-2072},
	doi = {10.18653/v1/P18-2072},
	abstract = {Because obtaining training data is often the most difficult part of an NLP or ML project, we develop methods for predicting how much data is required to achieve a desired test accuracy by extrapolating results from models trained on a small pilot training dataset. We model how accuracy varies as a function of training size on subsets of the pilot data, and use that model to predict how much training data would be required to achieve the desired accuracy. We introduce a new performance extrapolation task to evaluate how well different extrapolations predict accuracy on larger training sets. We show that details of hyperparameter optimisation and the extrapolation models can have dramatic effects in a document classification task. We believe this is an important first step in developing methods for estimating the resources required to meet specific engineering performance targets.},
	urldate = {2023-06-03},
	booktitle = {Proceedings of the 56th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 2: {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Johnson, Mark and Anderson, Peter and Dras, Mark and Steedman, Mark},
	month = jul,
	year = {2018},
	pages = {450--455},
}

@article{figueroa_predicting_2012,
	title = {Predicting sample size required for classification performance},
	volume = {12},
	issn = {1472-6947},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3307431/},
	doi = {10.1186/1472-6947-12-8},
	abstract = {Background
Supervised learning methods need annotated data in order to generate efficient models. Annotated data, however, is a relatively scarce resource and can be expensive to obtain. For both passive and active learning methods, there is a need to estimate the size of the annotated sample required to reach a performance target.

Methods
We designed and implemented a method that fits an inverse power law model to points of a given learning curve created using a small annotated training set. Fitting is carried out using nonlinear weighted least squares optimization. The fitted model is then used to predict the classifier's performance and confidence interval for larger sample sizes. For evaluation, the nonlinear weighted curve fitting method was applied to a set of learning curves generated using clinical text and waveform classification tasks with active and passive sampling methods, and predictions were validated using standard goodness of fit measures. As control we used an un-weighted fitting method.

Results
A total of 568 models were fitted and the model predictions were compared with the observed performances. Depending on the data set and sampling method, it took between 80 to 560 annotated samples to achieve mean average and root mean squared error below 0.01. Results also show that our weighted fitting method outperformed the baseline un-weighted method (p {\textless} 0.05).

Conclusions
This paper describes a simple and effective sample size prediction algorithm that conducts weighted fitting of learning curves. The algorithm outperformed an un-weighted algorithm described in previous literature. It can help researchers determine annotation sample size for supervised machine learning.},
	urldate = {2023-06-03},
	journal = {BMC Medical Informatics and Decision Making},
	author = {Figueroa, Rosa L and Zeng-Treitler, Qing and Kandula, Sasikiran and Ngo, Long H},
	month = feb,
	year = {2012},
	pmid = {22336388},
	pmcid = {PMC3307431},
	pages = {8},
}

@misc{cho_how_2016,
	title = {How much data is needed to train a medical image deep learning system to achieve necessary high accuracy?},
	url = {http://arxiv.org/abs/1511.06348},
	doi = {10.48550/arXiv.1511.06348},
	abstract = {The use of Convolutional Neural Networks (CNN) in natural image classification systems has produced very impressive results. Combined with the inherent nature of medical images that make them ideal for deep-learning, further application of such systems to medical image classification holds much promise. However, the usefulness and potential impact of such a system can be completely negated if it does not reach a target accuracy. In this paper, we present a study on determining the optimum size of the training data set necessary to achieve high classification accuracy with low variance in medical image classification systems. The CNN was applied to classify axial Computed Tomography (CT) images into six anatomical classes. We trained the CNN using six different sizes of training data set (5, 10, 20, 50, 100, and 200) and then tested the resulting system with a total of 6000 CT images. All images were acquired from the Massachusetts General Hospital (MGH) Picture Archiving and Communication System (PACS). Using this data, we employ the learning curve approach to predict classification accuracy at a given training sample size. Our research will present a general methodology for determining the training data set size necessary to achieve a certain target classification accuracy that can be easily applied to other problems within such systems.},
	urldate = {2023-06-03},
	publisher = {arXiv},
	author = {Cho, Junghwan and Lee, Kyewook and Shin, Ellie and Choy, Garry and Do, Synho},
	month = jan,
	year = {2016},
	note = {arXiv:1511.06348 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@article{sharma_scaling_2022,
	title = {Scaling {Laws} from the {Data} {Manifold} {Dimension}},
	volume = {23},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v23/20-1111.html},
	abstract = {When data is plentiful, the test loss achieved by well-trained neural networks scales as a power-law 
L∝
N
−α
𝐿
∝
𝑁
−
𝛼
 in the number of network parameters 
N
𝑁
. This empirical scaling law holds for a wide variety of data modalities, and may persist over many orders of magnitude. The scaling law can be explained if neural models are effectively just performing regression on a data manifold of intrinsic dimension 
d
𝑑
. This simple theory predicts that the scaling exponents 
α≈4/d
𝛼
≈
4
/
𝑑
 for cross-entropy and mean-squared error losses. We confirm the theory by independently measuring the intrinsic dimension and the scaling exponents in a teacher/student framework, where we can study a variety of 
d
𝑑
 and 
α
𝛼
 by dialing the properties of random teacher networks. We also test the theory with CNN image classifiers on several datasets and with GPT-type language models.},
	number = {9},
	urldate = {2023-06-03},
	journal = {Journal of Machine Learning Research},
	author = {Sharma, Utkarsh and Kaplan, Jared},
	year = {2022},
	pages = {1--34},
}

@inproceedings{rosenfeld_predictability_2021,
	title = {On the {Predictability} of {Pruning} {Across} {Scales}},
	url = {https://proceedings.mlr.press/v139/rosenfeld21a.html},
	abstract = {We show that the error of iteratively magnitude-pruned networks empirically follows a scaling law with interpretable coefficients that depend on the architecture and task. We functionally approximate the error of the pruned networks, showing it is predictable in terms of an invariant tying width, depth, and pruning level, such that networks of vastly different pruned densities are interchangeable. We demonstrate the accuracy of this approximation over orders of magnitude in depth, width, dataset size, and density. We show that the functional form holds (generalizes) for large scale data (e.g., ImageNet) and architectures (e.g., ResNets). As neural networks become ever larger and costlier to train, our findings suggest a framework for reasoning conceptually and analytically about a standard method for unstructured pruning.},
	language = {en},
	urldate = {2023-06-03},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Rosenfeld, Jonathan S. and Frankle, Jonathan and Carbin, Michael and Shavit, Nir},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {9075--9083},
}

@misc{hestness_deep_2017,
	title = {Deep {Learning} {Scaling} is {Predictable}, {Empirically}},
	url = {http://arxiv.org/abs/1712.00409},
	abstract = {Deep learning (DL) creates impactful advances following a virtuous recipe: model architecture search, creating large training data sets, and scaling computation. It is widely believed that growing training sets and models should improve accuracy and result in better products. As DL application domains grow, we would like a deeper understanding of the relationships between training set size, computational scale, and model accuracy improvements to advance the state-of-the-art. This paper presents a large scale empirical characterization of generalization error and model size growth as training sets grow. We introduce a methodology for this measurement and test four machine learning domains: machine translation, language modeling, image processing, and speech recognition. Our empirical results show power-law generalization error scaling across a breadth of factors, resulting in power-law exponents---the "steepness" of the learning curve---yet to be explained by theoretical work. Further, model improvements only shift the error but do not appear to affect the power-law exponent. We also show that model size scales sublinearly with data size. These scaling relationships have significant implications on deep learning research, practice, and systems. They can assist model debugging, setting accuracy targets, and decisions about data set growth. They can also guide computing system design and underscore the importance of continued computational scaling.},
	urldate = {2023-06-03},
	publisher = {arXiv},
	author = {Hestness, Joel and Narang, Sharan and Ardalani, Newsha and Diamos, Gregory and Jun, Heewoo and Kianinejad, Hassan and Patwary, Md Mostofa Ali and Yang, Yang and Zhou, Yanqi},
	month = dec,
	year = {2017},
	note = {arXiv:1712.00409 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{bahri_explaining_2021,
	title = {Explaining {Neural} {Scaling} {Laws}},
	url = {http://arxiv.org/abs/2102.06701},
	abstract = {The test loss of well-trained neural networks often follows precise power-law scaling relations with either the size of the training dataset or the number of parameters in the network. We propose a theory that explains and connects these scaling laws. We identify variance-limited and resolution-limited scaling behavior for both dataset and model size, for a total of four scaling regimes. The variance-limited scaling follows simply from the existence of a well-behaved infinite data or infinite width limit, while the resolution-limited regime can be explained by positing that models are effectively resolving a smooth data manifold. In the large width limit, this can be equivalently obtained from the spectrum of certain kernels, and we present evidence that large width and large dataset resolution-limited scaling exponents are related by a duality. We exhibit all four scaling regimes in the controlled setting of large random feature and pretrained models and test the predictions empirically on a range of standard architectures and datasets. We also observe several empirical relationships between datasets and scaling exponents: super-classing image tasks does not change exponents, while changing input distribution (via changing datasets or adding noise) has a strong effect. We further explore the effect of architecture aspect ratio on scaling exponents.},
	urldate = {2023-06-03},
	publisher = {arXiv},
	author = {Bahri, Yasaman and Dyer, Ethan and Kaplan, Jared and Lee, Jaehoon and Sharma, Utkarsh},
	month = feb,
	year = {2021},
	note = {arXiv:2102.06701 [cond-mat, stat]},
	keywords = {Computer Science - Machine Learning, Condensed Matter - Disordered Systems and Neural Networks, Statistics - Machine Learning},
}

@inproceedings{sorscher_beyond_2022,
	title = {Beyond neural scaling laws: beating power law scaling via data pruning},
	volume = {35},
	shorttitle = {Beyond neural scaling laws},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/7b75da9b61eda40fa35453ee5d077df6-Abstract-Conference.html},
	language = {en},
	urldate = {2023-06-03},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Sorscher, Ben and Geirhos, Robert and Shekhar, Shashank and Ganguli, Surya and Morcos, Ari},
	month = dec,
	year = {2022},
	pages = {19523--19536},
}

@inproceedings{alabdulmohsin_revisiting_2022,
	title = {Revisiting {Neural} {Scaling} {Laws} in {Language} and {Vision}},
	volume = {35},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/8c22e5e918198702765ecff4b20d0a90-Abstract-Conference.html},
	language = {en},
	urldate = {2023-06-02},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Alabdulmohsin, Ibrahim M. and Neyshabur, Behnam and Zhai, Xiaohua},
	month = dec,
	year = {2022},
	pages = {22300--22312},
}

@inproceedings{caballero_broken_2022,
	title = {Broken {Neural} {Scaling} {Laws}},
	url = {https://openreview.net/forum?id=BfGrlFuNyhJ},
	abstract = {We present a smoothly broken power law functional form that accurately models and extrapolates the scaling behaviors of deep neural networks (i.e. how the evaluation metric of interest varies as the amount of compute used for training, number of model parameters, training dataset size, or upstream performance varies) for each task within a large and diverse set of upstream and downstream tasks, in zero-shot, prompted, and fine-tuned settings. This set includes large-scale vision and unsupervised language tasks, diffusion generative modeling of images, arithmetic, and reinforcement learning. When compared to other functional forms for neural scaling behavior, this functional form yields extrapolations of scaling behavior that are considerably more accurate (root mean squared log error of its extrapolations are 0.86 times that of previous state-of-the-art on average) on this set. Moreover, this functional form accurately models and extrapolates scaling behavior that other functional forms are incapable of expressing such as the non-monotonic transitions present in the scaling behavior of phenomena such as double descent and the delayed, sharp inflection points present in the scaling behavior of tasks such as arithmetic. Code is available at https://github.com/ethancaballero/broken\_neural\_scaling\_laws},
	language = {en},
	urldate = {2023-06-02},
	author = {Caballero, Ethan and Gupta, Kshitij and Rish, Irina and Krueger, David},
	month = dec,
	year = {2022},
}

@misc{dunlap_diversify_2023,
	title = {Diversify {Your} {Vision} {Datasets} with {Automatic} {Diffusion}-{Based} {Augmentation}},
	url = {http://arxiv.org/abs/2305.16289},
	doi = {10.48550/arXiv.2305.16289},
	abstract = {Many fine-grained classification tasks, like rare animal identification, have limited training data and consequently classifiers trained on these datasets often fail to generalize to variations in the domain like changes in weather or location. As such, we explore how natural language descriptions of the domains seen in training data can be used with large vision models trained on diverse pretraining datasets to generate useful variations of the training data. We introduce ALIA (Automated Language-guided Image Augmentation), a method which utilizes large vision and language models to automatically generate natural language descriptions of a dataset's domains and augment the training data via language-guided image editing. To maintain data integrity, a model trained on the original dataset filters out minimal image edits and those which corrupt class-relevant information. The resulting dataset is visually consistent with the original training data and offers significantly enhanced diversity. On fine-grained and cluttered datasets for classification and detection, ALIA surpasses traditional data augmentation and text-to-image generated data by up to 15{\textbackslash}\%, often even outperforming equivalent additions of real data. Code is avilable at https://github.com/lisadunlap/ALIA.},
	urldate = {2023-06-02},
	publisher = {arXiv},
	author = {Dunlap, Lisa and Umino, Alyssa and Zhang, Han and Yang, Jiezhi and Gonzalez, Joseph E. and Darrell, Trevor},
	month = may,
	year = {2023},
	note = {arXiv:2305.16289 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{he_is_2023,
	title = {Is synthetic data from generative models ready for image recognition?},
	url = {https://openreview.net/forum?id=nUmCcZ5RKF},
	abstract = {Recent text-to-image generation models have shown promising results in generating high-fidelity photo-realistic images. Though the results are astonishing to human eyes, how applicable these generated images are for recognition tasks remains under-explored. In this work, we extensively study whether and how synthetic images generated from state-of-the-art text-to-image generation models can be used for image recognition tasks, and focus on two perspectives: synthetic data for improving classification models in the data-scare settings (i.e. zero-shot and few-shot), and synthetic data for large-scale model pre-training for transfer learning. We showcase the powerfulness and shortcomings of synthetic data from existing generative models, and propose strategies for better applying synthetic data for recognition tasks. Code: https://github.com/CVMI-Lab/SyntheticData.},
	language = {en},
	urldate = {2023-05-28},
	booktitle = {The {Eleventh} {International} {Conference} on {Learning} {Representations}},
	author = {He, Ruifei and Sun, Shuyang and Yu, Xin and Xue, Chuhui and Zhang, Wenqing and Torr, Philip and Bai, Song and Qi, Xiaojuan},
	month = feb,
	year = {2023},
}

@inproceedings{schuhmann_laion-5b_2022,
	title = {{LAION}-{5B}: {An} open large-scale dataset for training next generation image-text models},
	shorttitle = {{LAION}-{5B}},
	url = {https://openreview.net/forum?id=M3Y74vmsMcY},
	abstract = {Groundbreaking language-vision architectures like CLIP and DALL-E proved the utility of training on large amounts of noisy image-text data, without relying on expensive accurate labels used in standard vision unimodal supervised learning. The resulting models showed capabilities of strong text-guided image generation and transfer to downstream tasks, while performing remarkably at zero-shot classification with noteworthy out-of-distribution robustness. Since then, large-scale language-vision models like ALIGN, BASIC, GLIDE, Flamingo and Imagen made further improvements. Studying the training and capabilities of such models requires datasets containing billions of image-text pairs. Until now, no datasets of this size have been made openly available for the broader research community. To address this problem and democratize research on large-scale multi-modal models, we present LAION-5B - a dataset consisting of 5.85 billion CLIP-filtered image-text pairs, of which 2.32B contain English language. We show successful replication and fine-tuning of foundational models like CLIP, GLIDE and Stable Diffusion using the dataset, and discuss further experiments enabled with an openly available dataset of this scale. Additionally we provide several nearest neighbor indices, an improved web-interface for dataset exploration and subset generation, and detection scores for watermark, NSFW, and toxic content detection.},
	language = {en},
	urldate = {2023-03-06},
	booktitle = {Thirty-sixth {Conference} on {Neural} {Information} {Processing} {Systems} {Datasets} and {Benchmarks} {Track}},
	author = {Schuhmann, Christoph and Beaumont, Romain and Vencu, Richard and Gordon, Cade W. and Wightman, Ross and Cherti, Mehdi and Coombes, Theo and Katta, Aarush and Mullis, Clayton and Wortsman, Mitchell and Schramowski, Patrick and Kundurthy, Srivatsa R. and Crowson, Katherine and Schmidt, Ludwig and Kaczmarczyk, Robert and Jitsev, Jenia},
	month = oct,
	year = {2022},
}

@inproceedings{mirzasoleiman_coresets_2020,
	title = {Coresets for {Robust} {Training} of {Deep} {Neural} {Networks} against {Noisy} {Labels}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/8493eeaccb772c0878f99d60a0bd2bb3-Abstract.html},
	abstract = {Modern neural networks have the capacity to overfit noisy labels frequently found in real-world datasets. Although great progress has been made, existing techniques are very limited in providing theoretical guarantees for the performance of the neural networks trained with noisy labels. To tackle this challenge, we propose a novel approach with strong theoretical guarantees for robust training of neural networks trained with noisy labels. The key idea behind our method is to select subsets of clean data points that provide an approximately low-rank Jacobian matrix. We then prove that gradient descent applied to the subsets cannot overfit the noisy labels, without regularization or early stopping. Our extensive experiments corroborate our theory and demonstrate that deep networks trained on our subsets achieve a significantly superior performance, e.g., 7\% increase in accuracy on mini Webvision with 50\% noisy labels, compared to state-of-the art.},
	urldate = {2023-05-29},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Mirzasoleiman, Baharan and Cao, Kaidi and Leskovec, Jure},
	year = {2020},
	pages = {11465--11477},
}

@inproceedings{killamsetty_retrieve_2021,
	title = {{RETRIEVE}: {Coreset} {Selection} for {Efficient} and {Robust} {Semi}-{Supervised} {Learning}},
	volume = {34},
	shorttitle = {{RETRIEVE}},
	url = {https://proceedings.neurips.cc/paper/2021/hash/793bc52a941b3951dfdb85fb04f9fd06-Abstract.html},
	urldate = {2023-05-29},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Killamsetty, Krishnateja and Zhao, Xujiang and Chen, Feng and Iyer, Rishabh},
	year = {2021},
	pages = {14488--14501},
}

@article{borji_pros_2022,
	title = {Pros and cons of {GAN} evaluation measures: {New} developments},
	volume = {215},
	issn = {1077-3142},
	shorttitle = {Pros and cons of {GAN} evaluation measures},
	url = {https://www.sciencedirect.com/science/article/pii/S1077314221001685},
	doi = {10.1016/j.cviu.2021.103329},
	abstract = {This work is an update of my previous paper on the same topic published a few years ago (Borji, 2019). With the dramatic progress in generative modeling, a suite of new quantitative and qualitative techniques to evaluate models has emerged. Although some measures such as Inception Score, Fréchet Inception Distance, Precision–Recall, and Perceptual Path Length are relatively more popular, GAN evaluation is not a settled issue and there is still room for improvement. Here, I describe new dimensions that are becoming important in assessing models (e.g. bias and fairness) and discuss the connection between GAN evaluation and deepfakes. These are important areas of concern in the machine learning community today and progress in GAN evaluation can help mitigate them.},
	language = {en},
	urldate = {2023-05-29},
	journal = {Computer Vision and Image Understanding},
	author = {Borji, Ali},
	month = jan,
	year = {2022},
	keywords = {Deepfakes, GAN evaluation, Generative modeling},
	pages = {103329},
}

@article{borji_pros_2019,
	title = {Pros and cons of {GAN} evaluation measures},
	volume = {179},
	issn = {1077-3142},
	url = {https://www.sciencedirect.com/science/article/pii/S1077314218304272},
	doi = {10.1016/j.cviu.2018.10.009},
	abstract = {Generative models, in particular generative adversarial networks (GANs), have gained significant attention in recent years. A number of GAN variants have been proposed and have been utilized in many applications. Despite large strides in terms of theoretical progress, evaluating and comparing GANs remains a daunting task. While several measures have been introduced, as of yet, there is no consensus as to which measure best captures strengths and limitations of models and should be used for fair model comparison. As in other areas of computer vision and machine learning, it is critical to settle on one or few good measures to steer the progress in this field. In this paper, I review and critically discuss more than 24 quantitative and 5 qualitative measures for evaluating generative models with a particular emphasis on GAN-derived models. I also provide a set of 7 desiderata followed by an evaluation of whether a given measure or a family of measures is compatible with them.},
	language = {en},
	urldate = {2023-05-29},
	journal = {Computer Vision and Image Understanding},
	author = {Borji, Ali},
	month = feb,
	year = {2019},
	keywords = {Deep learning, Evaluation, Generative adversarial nets, Generative models, Neural networks},
	pages = {41--65},
}

@inproceedings{besnier_this_2020,
	title = {This {Dataset} {Does} {Not} {Exist}: {Training} {Models} from {Generated} {Images}},
	shorttitle = {This {Dataset} {Does} {Not} {Exist}},
	doi = {10.1109/ICASSP40776.2020.9053146},
	abstract = {Current generative networks are increasingly proficient in generating high-resolution realistic images. These generative networks, especially the conditional ones, can potentially become a great tool for providing new image datasets. This naturally brings the question: Can we train a classifier only on the generated data? This potential availability of nearly unlimited amounts of training data challenges standard practices for training machine learning models, which have been crafted across the years for limited and fixed size datasets. In this work we investigate this question and its related challenges. We identify ways to improve significantly the performance over naive training on randomly generated images with regular heuristics. We propose three standalone techniques that can be applied at different stages of the pipeline, i.e., data generation, training on generated data, and deploying on real data. We evaluate our proposed approaches on a subset of the ImageNet dataset and show encouraging results compared to classifiers trained on real images.},
	booktitle = {{ICASSP} 2020 - 2020 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Besnier, Victor and Jain, Himalaya and Bursuc, Andrei and Cord, Matthieu and Pérez, Patrick},
	month = may,
	year = {2020},
	note = {ISSN: 2379-190X},
	keywords = {Data models, Gallium nitride, Image classification, Standards, Task analysis, Tools, Training, Training data, generative networks},
	pages = {1--5},
}

@misc{antoniou_data_2018,
	title = {Data {Augmentation} {Generative} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1711.04340},
	doi = {10.48550/arXiv.1711.04340},
	abstract = {Effective training of neural networks requires much data. In the low-data regime, parameters are underdetermined, and learnt networks generalise poorly. Data Augmentation alleviates this by using existing data more effectively. However standard data augmentation produces only limited plausible alternative data. Given there is potential to generate a much broader set of augmentations, we design and train a generative model to do data augmentation. The model, based on image conditional Generative Adversarial Networks, takes data from a source domain and learns to take any data item and generalise it to generate other within-class data items. As this generative process does not depend on the classes themselves, it can be applied to novel unseen classes of data. We show that a Data Augmentation Generative Adversarial Network (DAGAN) augments standard vanilla classifiers well. We also show a DAGAN can enhance few-shot learning systems such as Matching Networks. We demonstrate these approaches on Omniglot, on EMNIST having learnt the DAGAN on Omniglot, and VGG-Face data. In our experiments we can see over 13\% increase in accuracy in the low-data regime experiments in Omniglot (from 69\% to 82\%), EMNIST (73.9\% to 76\%) and VGG-Face (4.5\% to 12\%); in Matching Networks for Omniglot we observe an increase of 0.5\% (from 96.9\% to 97.4\%) and an increase of 1.8\% in EMNIST (from 59.5\% to 61.3\%).},
	urldate = {2023-05-28},
	publisher = {arXiv},
	author = {Antoniou, Antreas and Storkey, Amos and Edwards, Harrison},
	month = mar,
	year = {2018},
	note = {arXiv:1711.04340 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@inproceedings{xing_unlabeled_2022,
	title = {Unlabeled {Data} {Help}: {Minimax} {Analysis} and {Adversarial} {Robustness}},
	shorttitle = {Unlabeled {Data} {Help}},
	url = {https://proceedings.mlr.press/v151/xing22a.html},
	abstract = {The recent proposed self-supervised learning (SSL) approaches successfully demonstrate the great potential of supplementing learning algorithms with additional unlabeled data. However, it is still unclear whether the existing SSL algorithms can fully utilize the information of both labelled and unlabeled data. This paper gives an affirmative answer for the reconstruction-based SSL algorithm (Lee et al., 2020) under several statistical models. While existing literature only focuses on establishing the upper bound of the convergence rate, we provide a rigorous minimax analysis, and successfully justify the rate-optimality of the reconstruction-based SSL algorithm under different data generation models. Furthermore, we incorporate the reconstruction-based SSL into the exist- ing adversarial training algorithms and show that learning from unlabeled data helps improve the robustness.},
	language = {en},
	urldate = {2023-05-28},
	booktitle = {Proceedings of {The} 25th {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Xing, Yue and Song, Qifan and Cheng, Guang},
	month = may,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {136--168},
}

@inproceedings{xing_adversarially_2021,
	title = {Adversarially {Robust} {Estimate} and {Risk} {Analysis} in {Linear} {Regression}},
	url = {https://proceedings.mlr.press/v130/xing21c.html},
	abstract = {Adversarial robust learning aims to design algorithms that are robust to small adversarial perturbations on input variables. Beyond the existing studies on the predictive performance to adversarial samples, our goal is to understand statistical properties of adversarial robust estimates and analyze adversarial risk in the setup of linear regression models. By discovering the statistical minimax rate of convergence of adversarial robust estimators, we emphasize the importance of incorporating model information, e.g., sparsity, in adversarial robust learning. Further, we reveal an explicit connection of adversarial and standard estimates, and propose a straightforward two-stage adversarial training framework, which facilitates to utilize model structure information to improve adversarial robustness. In theory, the consistency of the adversarial robust estimator is proven and its Bahadur representation is also developed for the statistical inference purpose. The proposed estimator converges in a sharp rate under either low-dimensional or sparse scenario. Moreover, our theory confirms two phenomena in adversarial robust learning: adversarial robustness hurts generalization, and unlabeled data help improve the generalization. In the end, we conduct numerical simulations to verify our theory.},
	language = {en},
	urldate = {2023-05-28},
	booktitle = {Proceedings of {The} 24th {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Xing, Yue and Zhang, Ruizhi and Cheng, Guang},
	month = mar,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {514--522},
}

@inproceedings{sehwag_improving_2021,
	title = {On improving adversarial robustness using proxy distributions},
	abstract = {We focus on the use of proxy distributions, approximations of the underlying distribution of the training dataset, in improving robust generalization of adversarial training. Adversarially trained networks, when trained on a limited number of samples available in the training set, suffer from a large generalization gap in the robust accuracy. Using proxy distribution, from which we can sample an unlimited number of data points, can enable us to 1) investigate the effect of the number of training samples 2) reduce the robust generalization gap in adversarial training. Earlier Min et al. (2020) argued that more training data can both help or hurt generalization based on the strength of the adversary. Here, using state-ofthe-art attacks in adversarial training and training with up to 2M images, we ﬁnd that more data continues to help generalization in deep neural networks. Next, we ask when incorporating additional samples from the proxy distribution will help? Here we prove that the difference of the robustness of a classiﬁer on proxy and training dataset distribution is upper bounded by the conditional Wasserstein distance between them. It conﬁrms the intuition that samples from a proxy distribution closely approximating training dataset distribution should be able to boost performance. Motivated by this, we leverage samples from state-of-the-art generative models, which can closely approximate training distribution, to improve robustness. In particular, we improve robust accuracy up to 6.5\% and 5.0\% in l∞ and l2 threat model, respectively, on the CIFAR-10 dataset.},
	language = {en},
	booktitle = {{ICLR} 2021 {Workshop} on {Security} and {Safety} in {Machine} {Learning} {Systems}},
	author = {Sehwag, Vikash and Mahloujifar, Saeed and Handina, Tinashe and Dai, Sihui and Xiang, Chong and Chiang, Mung and Mittal, Prateek},
	year = {2021},
}

@inproceedings{sehwag_robust_2022,
	title = {Robust {Learning} {Meets} {Generative} {Models}: {Can} {Proxy} {Distributions} {Improve} {Adversarial} {Robustness}?},
	shorttitle = {Robust {Learning} {Meets} {Generative} {Models}},
	url = {https://openreview.net/forum?id=WVX0NNVBBkV},
	abstract = {While additional training data improves the robustness of deep neural networks against adversarial examples, it presents the challenge of curating a large number of specific real-world samples. We circumvent this challenge by using additional data from proxy distributions learned by advanced generative models. We first seek to formally understand the transfer of robustness from classifiers trained on proxy distributions to the real data distribution. We prove that the difference between the robustness of a classifier on the two distributions is upper bounded by the conditional Wasserstein distance between them. Next we use proxy distributions to significantly improve the performance of adversarial training on five different datasets. For example, we improve robust accuracy by up to \$7.5\$\% and \$6.7\$\% in \${\textbackslash}ell\_\{{\textbackslash}infty\}\$ and \${\textbackslash}ell\_2\$ threat model over baselines that are not using proxy distributions on the CIFAR-10 dataset. We also improve certified robust accuracy by \$7.6\$\% on the CIFAR-10 dataset. We further demonstrate that different generative models brings a disparate improvement in the performance in robust training. We propose a robust discrimination approach to characterize the impact and further provide a deeper understanding of why diffusion-based generative models are a better choice for proxy distribution than generative adversarial networks.},
	language = {en},
	urldate = {2022-10-16},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Sehwag, Vikash and Mahloujifar, Saeed and Handina, Tinashe and Dai, Sihui and Xiang, Chong and Chiang, Mung and Mittal, Prateek},
	month = mar,
	year = {2022},
}

@inproceedings{xing_why_2022,
	title = {Why {Do} {Artificially} {Generated} {Data} {Help} {Adversarial} {Robustness}},
	volume = {35},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/065e259a1d2d955e63b99aac6a3a3081-Abstract-Conference.html},
	language = {en},
	urldate = {2023-05-27},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Xing, Yue and Song, Qifan and Cheng, Guang},
	month = dec,
	year = {2022},
	pages = {954--966},
}

@inproceedings{zhao_minimizing_2023,
	title = {Minimizing {Maximum} {Model} {Discrepancy} for {Transferable} {Black}-{Box} {Targeted} {Attacks}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Zhao_Minimizing_Maximum_Model_Discrepancy_for_Transferable_Black-Box_Targeted_Attacks_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-05-27},
	author = {Zhao, Anqi and Chu, Tong and Liu, Yahao and Li, Wen and Li, Jingjing and Duan, Lixin},
	year = {2023},
	pages = {8153--8162},
}

@article{xing_phase_2022,
	title = {Phase {Transition} from {Clean} {Training} to {Adversarial} {Training}},
	volume = {35},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/3cbf627fa24fb6cb576e04e689b9428b-Abstract-Conference.html},
	language = {en},
	urldate = {2023-05-27},
	journal = {Advances in Neural Information Processing Systems},
	author = {Xing, Yue and Song, Qifan and Cheng, Guang},
	month = dec,
	year = {2022},
	pages = {9330--9343},
}

@inproceedings{yu_efficient_2023,
	title = {Efficient {Loss} {Function} by {Minimizing} the {Detrimental} {Effect} of {Floating}-{Point} {Errors} on {Gradient}-{Based} {Attacks}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Yu_Efficient_Loss_Function_by_Minimizing_the_Detrimental_Effect_of_Floating-Point_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-05-27},
	author = {Yu, Yunrui and Xu, Cheng-Zhong},
	year = {2023},
	pages = {4056--4066},
}

@inproceedings{jin_randomized_2023,
	title = {Randomized {Adversarial} {Training} via {Taylor} {Expansion}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Jin_Randomized_Adversarial_Training_via_Taylor_Expansion_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-05-27},
	author = {Jin, Gaojie and Yi, Xinping and Wu, Dengyu and Mu, Ronghui and Huang, Xiaowei},
	year = {2023},
	pages = {16447--16457},
}

@misc{bansal_leaving_2023,
	title = {Leaving {Reality} to {Imagination}: {Robust} {Classification} via {Generated} {Datasets}},
	shorttitle = {Leaving {Reality} to {Imagination}},
	url = {http://arxiv.org/abs/2302.02503},
	doi = {10.48550/arXiv.2302.02503},
	abstract = {Recent research on robustness has revealed significant performance gaps between neural image classifiers trained on datasets that are similar to the test set, and those that are from a naturally shifted distribution, such as sketches, paintings, and animations of the object categories observed during training. Prior work focuses on reducing this gap by designing engineered augmentations of training data or through unsupervised pretraining of a single large model on massive in-the-wild training datasets scraped from the Internet. However, the notion of a dataset is also undergoing a paradigm shift in recent years. With drastic improvements in the quality, ease-of-use, and access to modern generative models, generated data is pervading the web. In this light, we study the question: How do these generated datasets influence the natural robustness of image classifiers? We find that Imagenet classifiers trained on real data augmented with generated data achieve higher accuracy and effective robustness than standard training and popular augmentation strategies in the presence of natural distribution shifts. We analyze various factors influencing these results, including the choice of conditioning strategies and the amount of generated data. Additionally, we find that the standard ImageNet classifiers suffer a performance degradation of upto 20{\textbackslash}\% on the generated data, indicating their fragility at accurately classifying the objects under novel variations. Lastly, we demonstrate that the image classifiers, which have been trained on real data augmented with generated data from the base generative model, exhibit greater resilience to natural distribution shifts compared to the classifiers trained on real data augmented with generated data from the finetuned generative model on the real data. The code, models, and datasets are available at https://github.com/Hritikbansal/generative-robustness.},
	urldate = {2023-05-27},
	publisher = {arXiv},
	author = {Bansal, Hritik and Grover, Aditya},
	month = may,
	year = {2023},
	note = {arXiv:2302.02503 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Multimedia},
}

@article{de_lange_continual_2022,
	title = {A {Continual} {Learning} {Survey}: {Defying} {Forgetting} in {Classification} {Tasks}},
	volume = {44},
	issn = {1939-3539},
	shorttitle = {A {Continual} {Learning} {Survey}},
	doi = {10.1109/TPAMI.2021.3057446},
	abstract = {Artificial neural networks thrive in solving the classification problem for a particular rigid task, acquiring knowledge through generalized learning behaviour from a distinct training phase. The resulting network resembles a static entity of knowledge, with endeavours to extend this knowledge without targeting the original task resulting in a catastrophic forgetting. Continual learning shifts this paradigm towards networks that can continually accumulate knowledge over different tasks without the need to retrain from scratch. We focus on task incremental classification, where tasks arrive sequentially and are delineated by clear boundaries. Our main contributions concern: (1) a taxonomy and extensive overview of the state-of-the-art; (2) a novel framework to continually determine the stability-plasticity trade-off of the continual learner; (3) a comprehensive experimental comparison of 11 state-of-the-art continual learning methods; and (4) baselines. We empirically scrutinize method strengths and weaknesses on three benchmarks, considering Tiny Imagenet and large-scale unbalanced iNaturalist and a sequence of recognition datasets. We study the influence of model capacity, weight decay and dropout regularization, and the order in which the tasks are presented, and qualitatively compare methods in terms of required memory, computation time, and storage.},
	number = {7},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {De Lange, Matthias and Aljundi, Rahaf and Masana, Marc and Parisot, Sarah and Jia, Xu and Leonardis, Aleš and Slabaugh, Gregory and Tuytelaars, Tinne},
	month = jul,
	year = {2022},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Continual learning, Interference, Knowledge engineering, Learning systems, Neural networks, Task analysis, Training, Training data, catastrophic forgetting, classification, lifelong learning, neural networks, task incremental learning},
	pages = {3366--3385},
}

@article{masana_class-incremental_2023,
	title = {Class-{Incremental} {Learning}: {Survey} and {Performance} {Evaluation} on {Image} {Classification}},
	volume = {45},
	issn = {1939-3539},
	shorttitle = {Class-{Incremental} {Learning}},
	doi = {10.1109/TPAMI.2022.3213473},
	abstract = {For future learning systems, incremental learning is desirable because it allows for: efficient resource usage by eliminating the need to retrain from scratch at the arrival of new data; reduced memory usage by preventing or limiting the amount of data required to be stored – also important when privacy limitations are imposed; and learning that more closely resembles human learning. The main challenge for incremental learning is catastrophic forgetting, which refers to the precipitous drop in performance on previously learned tasks after learning a new one. Incremental learning of deep neural networks has seen explosive growth in recent years. Initial work focused on task-incremental learning, where a task-ID is provided at inference time. Recently, we have seen a shift towards class-incremental learning where the learner must discriminate at inference time between all classes seen in previous tasks without recourse to a task-ID. In this paper, we provide a complete survey of existing class-incremental learning methods for image classification, and in particular, we perform an extensive experimental evaluation on thirteen class-incremental methods. We consider several new experimental scenarios, including a comparison of class-incremental methods on multiple large-scale image classification datasets, an investigation into small and large domain shifts, and a comparison of various network architectures.},
	number = {5},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Masana, Marc and Liu, Xialei and Twardowski, Bartłomiej and Menta, Mikel and Bagdanov, Andrew D. and van de Weijer, Joost},
	month = may,
	year = {2023},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Class-incremental learning, Image classification, Learning systems, Network architecture, Privacy, Task analysis, Training, Training data, catastrophic forgetting, continual learning, incremental learning, lifelong learning},
	pages = {5513--5533},
}

@article{van_de_ven_three_2022,
	title = {Three types of incremental learning},
	volume = {4},
	copyright = {2022 The Author(s)},
	issn = {2522-5839},
	url = {https://www.nature.com/articles/s42256-022-00568-3},
	doi = {10.1038/s42256-022-00568-3},
	abstract = {Incrementally learning new information from a non-stationary stream of data, referred to as ‘continual learning’, is a key feature of natural intelligence, but a challenging problem for deep neural networks. In recent years, numerous deep learning methods for continual learning have been proposed, but comparing their performances is difficult due to the lack of a common framework. To help address this, we describe three fundamental types, or ‘scenarios’, of continual learning: task-incremental, domain-incremental and class-incremental learning. Each of these scenarios has its own set of challenges. To illustrate this, we provide a comprehensive empirical comparison of currently used continual learning strategies, by performing the Split MNIST and Split CIFAR-100 protocols according to each scenario. We demonstrate substantial differences between the three scenarios in terms of difficulty and in terms of the effectiveness of different strategies. The proposed categorization aims to structure the continual learning field, by forming a key foundation for clearly defining benchmark problems.},
	language = {en},
	number = {12},
	urldate = {2023-05-27},
	journal = {Nature Machine Intelligence},
	author = {van de Ven, Gido M. and Tuytelaars, Tinne and Tolias, Andreas S.},
	month = dec,
	year = {2022},
	note = {Number: 12
Publisher: Nature Publishing Group},
	keywords = {Computer science, Learning algorithms, Software},
	pages = {1185--1197},
}

@misc{azizi_synthetic_2023,
	title = {Synthetic {Data} from {Diffusion} {Models} {Improves} {ImageNet} {Classification}},
	url = {http://arxiv.org/abs/2304.08466},
	doi = {10.48550/arXiv.2304.08466},
	abstract = {Deep generative models are becoming increasingly powerful, now generating diverse high fidelity photo-realistic samples given text prompts. Have they reached the point where models of natural images can be used for generative data augmentation, helping to improve challenging discriminative tasks? We show that large-scale text-to image diffusion models can be fine-tuned to produce class conditional models with SOTA FID (1.76 at 256x256 resolution) and Inception Score (239 at 256x256). The model also yields a new SOTA in Classification Accuracy Scores (64.96 for 256x256 generative samples, improving to 69.24 for 1024x1024 samples). Augmenting the ImageNet training set with samples from the resulting models yields significant improvements in ImageNet classification accuracy over strong ResNet and Vision Transformer baselines.},
	urldate = {2023-05-27},
	publisher = {arXiv},
	author = {Azizi, Shekoofeh and Kornblith, Simon and Saharia, Chitwan and Norouzi, Mohammad and Fleet, David J.},
	month = apr,
	year = {2023},
	note = {arXiv:2304.08466 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{dumoulin_guide_2018,
	title = {A guide to convolution arithmetic for deep learning},
	url = {http://arxiv.org/abs/1603.07285},
	doi = {10.48550/arXiv.1603.07285},
	abstract = {We introduce a guide to help deep learning practitioners understand and manipulate convolutional neural network architectures. The guide clarifies the relationship between various properties (input shape, kernel shape, zero padding, strides and output shape) of convolutional, pooling and transposed convolutional layers, as well as the relationship between convolutional and transposed convolutional layers. Relationships are derived for various cases, and are illustrated in order to make them intuitive.},
	urldate = {2023-05-27},
	publisher = {arXiv},
	author = {Dumoulin, Vincent and Visin, Francesco},
	month = jan,
	year = {2018},
	note = {arXiv:1603.07285 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@inproceedings{steinhardt_certified_2017,
	title = {Certified {Defenses} for {Data} {Poisoning} {Attacks}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper/2017/hash/9d7311ba459f9e45ed746755a32dcd11-Abstract.html},
	abstract = {Machine learning systems trained on user-provided data are susceptible to data poisoning attacks, whereby malicious users inject false training data with the aim of corrupting the learned model. While recent work has proposed a number of attacks and defenses, little is understood about the worst-case loss of a defense in the face of a determined attacker. We address this by constructing approximate upper bounds on the loss across a broad family  of attacks, for defenders that first perform outlier removal followed by empirical risk minimization. Our approximation relies on two assumptions: (1) that the dataset is large enough for  statistical concentration between train and test error to hold, and (2) that outliers  within the clean (non-poisoned) data do not have a strong effect on the model. Our bound comes paired with a candidate attack that often nearly matches the upper bound, giving us a powerful tool for quickly assessing defenses on a given dataset. Empirically, we find that even under a simple defense, the MNIST-1-7 and Dogfish datasets are resilient to attack, while in contrast the IMDB sentiment dataset can be driven from 12\% to 23\% test error by adding only 3\% poisoned data.},
	urldate = {2023-05-27},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Steinhardt, Jacob and Koh, Pang Wei W and Liang, Percy S},
	year = {2017},
}

@misc{ktena_generative_2023,
	title = {Generative models improve fairness of medical classifiers under distribution shifts},
	url = {http://arxiv.org/abs/2304.09218},
	doi = {10.48550/arXiv.2304.09218},
	abstract = {A ubiquitous challenge in machine learning is the problem of domain generalisation. This can exacerbate bias against groups or labels that are underrepresented in the datasets used for model development. Model bias can lead to unintended harms, especially in safety-critical applications like healthcare. Furthermore, the challenge is compounded by the difficulty of obtaining labelled data due to high cost or lack of readily available domain expertise. In our work, we show that learning realistic augmentations automatically from data is possible in a label-efficient manner using generative models. In particular, we leverage the higher abundance of unlabelled data to capture the underlying data distribution of different conditions and subgroups for an imaging modality. By conditioning generative models on appropriate labels, we can steer the distribution of synthetic examples according to specific requirements. We demonstrate that these learned augmentations can surpass heuristic ones by making models more robust and statistically fair in- and out-of-distribution. To evaluate the generality of our approach, we study 3 distinct medical imaging contexts of varying difficulty: (i) histopathology images from a publicly available generalisation benchmark, (ii) chest X-rays from publicly available clinical datasets, and (iii) dermatology images characterised by complex shifts and imaging conditions. Complementing real training samples with synthetic ones improves the robustness of models in all three medical tasks and increases fairness by improving the accuracy of diagnosis within underrepresented groups. This approach leads to stark improvements OOD across modalities: 7.7\% prediction accuracy improvement in histopathology, 5.2\% in chest radiology with 44.6\% lower fairness gap and a striking 63.5\% improvement in high-risk sensitivity for dermatology with a 7.5x reduction in fairness gap.},
	urldate = {2023-05-27},
	publisher = {arXiv},
	author = {Ktena, Ira and Wiles, Olivia and Albuquerque, Isabela and Rebuffi, Sylvestre-Alvise and Tanno, Ryutaro and Roy, Abhijit Guha and Azizi, Shekoofeh and Belgrave, Danielle and Kohli, Pushmeet and Karthikesalingam, Alan and Cemgil, Taylan and Gowal, Sven},
	month = apr,
	year = {2023},
	note = {arXiv:2304.09218 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{sadasivan_can_2023,
	title = {Can {AI}-{Generated} {Text} be {Reliably} {Detected}?},
	url = {http://arxiv.org/abs/2303.11156},
	doi = {10.48550/arXiv.2303.11156},
	abstract = {The rapid progress of Large Language Models (LLMs) has made them capable of performing astonishingly well on various tasks including document completion and question answering. The unregulated use of these models, however, can potentially lead to malicious consequences such as plagiarism, generating fake news, spamming, etc. Therefore, reliable detection of AI-generated text can be critical to ensure the responsible use of LLMs. Recent works attempt to tackle this problem either using certain model signatures present in the generated text outputs or by applying watermarking techniques that imprint specific patterns onto them. In this paper, both empirically and theoretically, we show that these detectors are not reliable in practical scenarios. Empirically, we show that paraphrasing attacks, where a light paraphraser is applied on top of the generative text model, can break a whole range of detectors, including the ones using the watermarking schemes as well as neural network-based detectors and zero-shot classifiers. We then provide a theoretical impossibility result indicating that for a sufficiently good language model, even the best-possible detector can only perform marginally better than a random classifier. Finally, we show that even LLMs protected by watermarking schemes can be vulnerable against spoofing attacks where adversarial humans can infer hidden watermarking signatures and add them to their generated text to be detected as text generated by the LLMs, potentially causing reputational damages to their developers. We believe these results can open an honest conversation in the community regarding the ethical and reliable use of AI-generated text.},
	urldate = {2023-05-27},
	publisher = {arXiv},
	author = {Sadasivan, Vinu Sankar and Kumar, Aounon and Balasubramanian, Sriram and Wang, Wenxiao and Feizi, Soheil},
	month = mar,
	year = {2023},
	note = {arXiv:2303.11156 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{liu_comprehensive_2023,
	title = {A {Comprehensive} {Study} on {Robustness} of {Image} {Classification} {Models}: {Benchmarking} and {Rethinking}},
	shorttitle = {A {Comprehensive} {Study} on {Robustness} of {Image} {Classification} {Models}},
	url = {http://arxiv.org/abs/2302.14301},
	doi = {10.48550/arXiv.2302.14301},
	abstract = {The robustness of deep neural networks is usually lacking under adversarial examples, common corruptions, and distribution shifts, which becomes an important research problem in the development of deep learning. Although new deep learning methods and robustness improvement techniques have been constantly proposed, the robustness evaluations of existing methods are often inadequate due to their rapid development, diverse noise patterns, and simple evaluation metrics. Without thorough robustness evaluations, it is hard to understand the advances in the field and identify the effective methods. In this paper, we establish a comprehensive robustness benchmark called {\textbackslash}textbf\{ARES-Bench\} on the image classification task. In our benchmark, we evaluate the robustness of 55 typical deep learning models on ImageNet with diverse architectures (e.g., CNNs, Transformers) and learning algorithms (e.g., normal supervised training, pre-training, adversarial training) under numerous adversarial attacks and out-of-distribution (OOD) datasets. Using robustness curves as the major evaluation criteria, we conduct large-scale experiments and draw several important findings, including: 1) there is an inherent trade-off between adversarial and natural robustness for the same model architecture; 2) adversarial training effectively improves adversarial robustness, especially when performed on Transformer architectures; 3) pre-training significantly improves natural robustness based on more training data or self-supervised learning. Based on ARES-Bench, we further analyze the training tricks in large-scale adversarial training on ImageNet. By designing the training settings accordingly, we achieve the new state-of-the-art adversarial robustness. We have made the benchmarking results and code platform publicly available.},
	urldate = {2023-05-26},
	publisher = {arXiv},
	author = {Liu, Chang and Dong, Yinpeng and Xiang, Wenzhao and Yang, Xiao and Su, Hang and Zhu, Jun and Chen, Yuefeng and He, Yuan and Xue, Hui and Zheng, Shibao},
	month = feb,
	year = {2023},
	note = {arXiv:2302.14301 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{guo_deepcore_2022,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{DeepCore}: {A} {Comprehensive} {Library} for {Coreset} {Selection} in {Deep} {Learning}},
	isbn = {978-3-031-12423-5},
	shorttitle = {{DeepCore}},
	doi = {10.1007/978-3-031-12423-5_14},
	abstract = {Coreset selection, which aims to select a subset of the most informative training samples, is a long-standing learning problem that can benefit many downstream tasks such as data-efficient learning, continual learning, neural architecture search, active learning, etc. However, many existing coreset selection methods are not designed for deep learning, which may have high complexity and poor generalization performance. In addition, the recently proposed methods are evaluated on models, datasets, and settings of different complexities. To advance the research of coreset selection in deep learning, we contribute a comprehensive code library (The code is available in https://github.com/PatrickZH/DeepCore.), namely DeepCore, and provide an empirical study on popular coreset selection methods on CIFAR10 and ImageNet datasets. Extensive experiments on CIFAR10 and ImageNet datasets verify that, although various methods have advantages in certain experiment settings, random selection is still a strong baseline.},
	language = {en},
	booktitle = {Database and {Expert} {Systems} {Applications}},
	publisher = {Springer International Publishing},
	author = {Guo, Chengcheng and Zhao, Bo and Bai, Yanbing},
	editor = {Strauss, Christine and Cuzzocrea, Alfredo and Kotsis, Gabriele and Tjoa, A. Min and Khalil, Ismail},
	year = {2022},
	keywords = {Coreset selection, Data-efficient learning, Deep learning},
	pages = {181--195},
}

@inproceedings{citovsky_leveraging_2023,
	title = {Leveraging {Importance} {Weights} in {Subset} {Selection}},
	url = {https://openreview.net/forum?id=9Nj_gNdvqYf},
	abstract = {We present a subset selection algorithm designed to work with arbitrary model families in a practical batch setting. In such a setting, an algorithm can sample examples one at a time but, in order to limit overhead costs, is only able to update its state (i.e. further train model weights) once a large enough batch of examples is selected. Our algorithm, IWeS, selects examples by importance sampling where the sampling probability assigned to each example is based on the entropy of models trained on previously selected batches. IWeS admits significant performance improvement compared to other subset selection algorithms for seven publicly available datasets. Additionally, it is competitive in an active learning setting, where the label information is not available at selection time. We also provide an initial theoretical analysis to support our importance weighting approach, proving generalization and sampling rate bounds.},
	language = {en},
	urldate = {2023-05-26},
	author = {Citovsky, Gui and DeSalvo, Giulia and Kumar, Sanjiv and Ramalingam, Srikumar and Rostamizadeh, Afshin and Wang, Yunjuan},
	month = feb,
	year = {2023},
}

@inproceedings{hsiung_carben_2022,
	address = {Vienna, Austria},
	title = {{CARBEN}: {Composite} {Adversarial} {Robustness} {Benchmark}},
	isbn = {978-1-956792-00-3},
	shorttitle = {{CARBEN}},
	url = {https://www.ijcai.org/proceedings/2022/851},
	doi = {10.24963/ijcai.2022/851},
	abstract = {Prior literature on adversarial attack methods has mainly focused on attacking with and defending against a single threat model, e.g., perturbations bounded in Lp ball. However, multiple threat models can be combined into composite perturbations. One such approach, composite adversarial attack (CAA), not only expands the perturbable space of the image, but also may be overlooked by current modes of robustness evaluation. This paper demonstrates how CAA’s attack order affects the resulting image, and provides real-time inferences of different models, which will facilitate users’ configuration of the parameters of the attack level and their rapid evaluation of model prediction. A leaderboard to benchmark adversarial robustness against CAA is also introduced.},
	language = {en},
	urldate = {2023-05-26},
	booktitle = {Proceedings of the {Thirty}-{First} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Hsiung, Lei and Tsai, Yun-Yun and Chen, Pin-Yu and Ho, Tsung-Yi},
	month = jul,
	year = {2022},
	pages = {5908--5911},
}

@inproceedings{madaan_learning_2021,
	title = {Learning to {Generate} {Noise} for {Multi}-{Attack} {Robustness}},
	url = {https://proceedings.mlr.press/v139/madaan21a.html},
	abstract = {Adversarial learning has emerged as one of the successful techniques to circumvent the susceptibility of existing methods against adversarial perturbations. However, the majority of existing defense methods are tailored to defend against a single category of adversarial perturbation (e.g. \${\textbackslash}ell\_{\textbackslash}infty\$-attack). In safety-critical applications, this makes these methods extraneous as the attacker can adopt diverse adversaries to deceive the system. Moreover, training on multiple perturbations simultaneously significantly increases the computational overhead during training. To address these challenges, we propose a novel meta-learning framework that explicitly learns to generate noise to improve the model’s robustness against multiple types of attacks. Its key component is {\textbackslash}emph\{Meta Noise Generator (MNG)\} that outputs optimal noise to stochastically perturb a given sample, such that it helps lower the error on diverse adversarial perturbations. By utilizing samples generated by MNG, we train a model by enforcing the label consistency across multiple perturbations. We validate the robustness of models trained by our scheme on various datasets and against a wide variety of perturbations, demonstrating that it significantly outperforms the baselines across multiple perturbations with a marginal computational cost.},
	language = {en},
	urldate = {2023-05-26},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Madaan, Divyam and Shin, Jinwoo and Hwang, Sung Ju},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {7279--7289},
}

@misc{dai_multirobustbench_2023,
	title = {{MultiRobustBench}: {Benchmarking} {Robustness} {Against} {Multiple} {Attacks}},
	shorttitle = {{MultiRobustBench}},
	url = {http://arxiv.org/abs/2302.10980},
	doi = {10.48550/arXiv.2302.10980},
	abstract = {The bulk of existing research in defending against adversarial examples focuses on defending against a single (typically bounded Lp-norm) attack, but for a practical setting, machine learning (ML) models should be robust to a wide variety of attacks. In this paper, we present the first unified framework for considering multiple attacks against ML models. Our framework is able to model different levels of learner's knowledge about the test-time adversary, allowing us to model robustness against unforeseen attacks and robustness against unions of attacks. Using our framework, we present the first leaderboard, MultiRobustBench, for benchmarking multiattack evaluation which captures performance across attack types and attack strengths. We evaluate the performance of 16 defended models for robustness against a set of 9 different attack types, including Lp-based threat models, spatial transformations, and color changes, at 20 different attack strengths (180 attacks total). Additionally, we analyze the state of current defenses against multiple attacks. Our analysis shows that while existing defenses have made progress in terms of average robustness across the set of attacks used, robustness against the worst-case attack is still a big open problem as all existing models perform worse than random guessing.},
	urldate = {2023-05-26},
	publisher = {arXiv},
	author = {Dai, Sihui and Mahloujifar, Saeed and Xiang, Chong and Sehwag, Vikash and Chen, Pin-Yu and Mittal, Prateek},
	month = may,
	year = {2023},
	note = {arXiv:2302.10980 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@inproceedings{kireev_effectiveness_2022,
	title = {On the effectiveness of adversarial training against common corruptions},
	url = {https://proceedings.mlr.press/v180/kireev22a.html},
	abstract = {The literature on robustness towards common corruptions shows no consensus on whether adversarial training can improve the performance in this setting. First, we show that, when used with an appropriately selected perturbation radius, Lp adversarial training can serve as a strong baseline against common corruptions improving both accuracy and calibration. Then we explain why adversarial training performs better than data augmentation with simple Gaussian noise which has been observed to be a meaningful baseline on common corruptions. Related to this, we identify the sigma-overfitting phenomenon when Gaussian augmentation overfits to a particular standard deviation used for training which has a significant detrimental effect on common corruption accuracy. We discuss how to alleviate this problem and then how to further enhance Lp adversarial training by introducing an efficient relaxation of adversarial training with learned perceptual image patch similarity as the distance metric. Through experiments on CIFAR-10 and ImageNet-100, we show that our approach does not only improve the Lp adversarial training baseline but also has cumulative gains with data augmentation methods such as AugMix, DeepAugment, ANT, and SIN, leading to state-of-the-art performance on common corruptions.},
	language = {en},
	urldate = {2023-05-26},
	booktitle = {Proceedings of the {Thirty}-{Eighth} {Conference} on {Uncertainty} in {Artificial} {Intelligence}},
	publisher = {PMLR},
	author = {Kireev, Klim and Andriushchenko, Maksym and Flammarion, Nicolas},
	month = aug,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {1012--1021},
}

@inproceedings{teti_lcanets_2022,
	title = {{LCANets}: {Lateral} {Competition} {Improves} {Robustness} {Against} {Corruption} and {Attack}},
	shorttitle = {{LCANets}},
	url = {https://proceedings.mlr.press/v162/teti22a.html},
	abstract = {Although Convolutional Neural Networks (CNNs) achieve high accuracy on image recognition tasks, they lack robustness against realistic corruptions and fail catastrophically when deliberately attacked. Previous CNNs with representations similar to primary visual cortex (V1) were more robust to adversarial attacks on images than current adversarial defense techniques, but they required training on large-scale neural recordings or handcrafting neuroscientific models. Motivated by evidence that neural activity in V1 is sparse, we develop a class of hybrid CNNs, called LCANets, which feature a frontend that performs sparse coding via local lateral competition. We demonstrate that LCANets achieve competitive clean accuracy to standard CNNs on action and image recognition tasks and significantly greater accuracy under various image corruptions. We also perform the first adversarial attacks with full knowledge of a sparse coding CNN layer by attacking LCANets with white-box and black-box attacks, and we show that, contrary to previous hypotheses, sparse coding layers are not very robust to white-box attacks. Finally, we propose a way to use sparse coding layers as a plug-and-play robust frontend by showing that they significantly increase the robustness of adversarially-trained CNNs over corruptions and attacks.},
	language = {en},
	urldate = {2023-05-26},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Teti, Michael and Kenyon, Garrett and Migliori, Ben and Moore, Juston},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {21232--21252},
}

@inproceedings{sitawarin_part-based_2023,
	title = {Part-{Based} {Models} {Improve} {Adversarial} {Robustness}},
	url = {https://openreview.net/forum?id=bAMTaeqluh4},
	abstract = {We show that combining human prior knowledge with end-to-end learning can improve the robustness of deep neural networks by introducing a part-based model for object classification. We believe that the richer form of annotation helps guide neural networks to learn more robust features without requiring more samples or larger models. Our model combines a part segmentation model with a tiny classifier and is trained end-to-end to simultaneously segment objects into parts and then classify the segmented object. Empirically, our part-based models achieve both higher accuracy and higher adversarial robustness than a ResNet-50 baseline on all three datasets. For instance, the clean accuracy of our part models is up to 15 percentage points higher than the baseline's, given the same level of robustness. Our experiments indicate that these models also reduce texture bias and yield better robustness against common corruptions and spurious correlations. The code is publicly available at https://github.com/chawins/adv-part-model.},
	language = {en},
	urldate = {2023-05-26},
	author = {Sitawarin, Chawin and Pongmala, Kornrapat and Chen, Yizheng and Carlini, Nicholas and Wagner, David},
	month = feb,
	year = {2023},
}

@article{li_recognizing_2023,
	title = {Recognizing {Object} by {Components} with {Human} {Prior} {Knowledge} {Enhances} {Adversarial} {Robustness} of {Deep} {Neural} {Networks}},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2023.3237935},
	abstract = {Adversarial attacks can easily fool object recognition systems based on deep neural networks (DNNs). Although many defense methods have been proposed in recent years, most of them can still be adaptively evaded. One reason for the weak adversarial robustness may be that DNNs are only supervised by category labels and do not have part-based inductive bias like the recognition process of humans. Inspired by a well-known theory in cognitive psychology – recognition-by-components, we propose a novel object recognition model ROCK (Recognizing Object by Components with human prior Knowledge). It first segments parts of objects from images, then scores part segmentation results with predefined human prior knowledge, and finally outputs prediction based on the scores. The first stage of ROCK corresponds to the process of decomposing objects into parts in human vision. The second stage corresponds to the decision process of the human brain. ROCK shows better robustness than classical recognition models across various attack settings. These results encourage researchers to rethink the rationality of currently widely-used DNN-based object recognition models and explore the potential of part-based models, once important but recently ignored, for improving robustness.},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Li, Xiao and Wang, Ziqi and Zhang, Bo and Sun, Fuchun and Hu, Xiaolin},
	year = {2023},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Adaptation models, Adversarial Robustness, Birds, Cognitive Psychology Inspired, Dogs, Image segmentation, Object recognition, Part-based Model, Robust Object Recognition, Robustness, Rocks},
	pages = {1--13},
}

@inproceedings{huang_revisiting_2023,
	title = {Revisiting {Residual} {Networks} for {Adversarial} {Robustness}},
	abstract = {Efforts to improve the adversarial robustness of convolutional neural networks have primarily focused on developing more effective adversarial training methods. In contrast, little attention was devoted to analyzing the role of architectural elements (e.g., topology, depth, and width) on adversarial robustness. This paper seeks to bridge this gap and present a holistic study on the impact of architectural design on adversarial robustness. We focus on residual networks and consider architecture design at the block level as well as at the network scaling level. In both cases, we first derive insights through systematic experiments. Then we design a robust residual block, dubbed RobustResBlock, and a compound scaling rule, dubbed RobustScaling, to distribute depth and width at the desired FLOP count. Finally, we combine RobustResBlock and RobustScaling and present a portfolio of adversarially robust residual networks, RobustResNets, spanning a broad spectrum of model capacities. Experimental validation across multiple datasets and adversarial attacks demonstrate that RobustResNets consistently outperform both the standard WRNs and other existing robust architectures, achieving state-ofthe-art AutoAttack robust accuracy 63.7\% with 500K external data while being 2× more compact in terms of parameters. Code is available at this URL.},
	language = {en},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Huang, Shihua and Lu, Zhichao and Deb, Kalyanmoy and Boddeti, Vishnu Naresh},
	year = {2023},
}

@inproceedings{li_subspace_2022,
	title = {Subspace {Adversarial} {Training}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Li_Subspace_Adversarial_Training_CVPR_2022_paper.html},
	language = {en},
	urldate = {2022-07-06},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Li, Tao and Wu, Yingwen and Chen, Sizhe and Fang, Kun and Huang, Xiaolin},
	year = {2022},
	pages = {13409--13418},
}

@inproceedings{zhang_revisiting_2022,
	title = {Revisiting and {Advancing} {Fast} {Adversarial} {Training} {Through} {The} {Lens} of {Bi}-{Level} {Optimization}},
	url = {https://proceedings.mlr.press/v162/zhang22ak.html},
	abstract = {Adversarial training (AT) is a widely recognized defense mechanism to gain the robustness of deep neural networks against adversarial attacks. It is built on min-max optimization (MMO), where the minimizer (i.e., defender) seeks a robust model to minimize the worst-case training loss in the presence of adversarial examples crafted by the maximizer (i.e., attacker). However, the conventional MMO method makes AT hard to scale. Thus, Fast-AT and other recent algorithms attempt to simplify MMO by replacing its maximization step with the single gradient sign-based attack generation step. Although easy to implement, FAST-AT lacks theoretical guarantees, and its empirical performance is unsatisfactory due to the issue of robust catastrophic overfitting when training with strong adversaries. In this paper, we advance Fast-AT from the fresh perspective of bi-level optimization (BLO). We first show that the commonly-used Fast-AT is equivalent to using a stochastic gradient algorithm to solve a linearized BLO problem involving a sign operation. However, the discrete nature of the sign operation makes it difficult to understand the algorithm performance. Inspired by BLO, we design and analyze a new set of robust training algorithms termed Fast Bi-level AT (Fast-BAT), which effectively defends sign-based projected gradient descent (PGD) attacks without using any gradient sign method or explicit robust regularization. In practice, we show that our method yields substantial robustness improvements over multiple baselines across multiple models and datasets.},
	language = {en},
	urldate = {2023-05-26},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Zhang, Yihua and Zhang, Guanhua and Khanduri, Prashant and Hong, Mingyi and Chang, Shiyu and Liu, Sijia},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {26693--26712},
}

@article{huang_revisiting_nodate,
	title = {Revisiting {Residual} {Networks} for {Adversarial} {Robustness}},
	abstract = {Efforts to improve the adversarial robustness of convolutional neural networks have primarily focused on developing more effective adversarial training methods. In contrast, little attention was devoted to analyzing the role of architectural elements (e.g., topology, depth, and width) on adversarial robustness. This paper seeks to bridge this gap and present a holistic study on the impact of architectural design on adversarial robustness. We focus on residual networks and consider architecture design at the block level as well as at the network scaling level. In both cases, we first derive insights through systematic experiments. Then we design a robust residual block, dubbed RobustResBlock, and a compound scaling rule, dubbed RobustScaling, to distribute depth and width at the desired FLOP count. Finally, we combine RobustResBlock and RobustScaling and present a portfolio of adversarially robust residual networks, RobustResNets, spanning a broad spectrum of model capacities. Experimental validation across multiple datasets and adversarial attacks demonstrate that RobustResNets consistently outperform both the standard WRNs and other existing robust architectures, achieving state-ofthe-art AutoAttack robust accuracy 63.7\% with 500K external data while being 2× more compact in terms of parameters. Code is available at this URL.},
	language = {en},
	author = {Huang, Shihua and Lu, Zhichao and Deb, Kalyanmoy and Boddeti, Vishnu Naresh},
}

@inproceedings{lecuyer_certified_2019,
	title = {Certified {Robustness} to {Adversarial} {Examples} with {Differential} {Privacy}},
	isbn = {978-1-5386-6660-9},
	url = {https://www.computer.org/csdl/proceedings-article/sp/2019/666000a656/1dlwlw6wEw0},
	doi = {10.1109/SP.2019.00044},
	abstract = {Adversarial examples that fool machine learning models, particularly deep neural networks, have been a topic of intense research interest, with attacks and defenses being developed in a tight back-and-forth. Most past defenses are best effort and have been shown to be vulnerable to sophisticated attacks. Recently a set of certified defenses have been introduced, which provide guarantees of robustness to norm-bounded attacks. However these defenses either do not scale to large datasets or are limited in the types of models they can support. This paper presents the first certified defense that both scales to large networks and datasets (such as Google's Inception network for ImageNet) and applies broadly to arbitrary model types. Our defense, called PixelDP, is based on a novel connection between robustness against adversarial examples and differential privacy, a cryptographically-inspired privacy formalism, that provides a rigorous, generic, and flexible foundation for defense.},
	language = {English},
	urldate = {2023-05-24},
	publisher = {IEEE Computer Society},
	author = {Lecuyer, Mathias and Atlidakis, Vaggelis and Geambasu, Roxana and Hsu, Daniel and Jana, Suman},
	month = may,
	year = {2019},
	pages = {656--672},
}

@inproceedings{pinto_are_2021,
	title = {Are {Vision} {Transformers} {Always} {More} {Robust} {Than} {Convolutional} {Neural} {Networks}?},
	url = {https://openreview.net/forum?id=CSXa8LJMttt},
	abstract = {Since Transformer architectures have been popularised in Computer Vision, several papers started analysing their properties in terms of calibration, out-of-distribution detection and data-shift robustness. Most of these papers conclude that Transformers, due to some intrinsic properties (presumably the lack of restrictive inductive biases and the computationally intensive self-attention mechanism), outperform Convolutional Neural Networks (CNNs). In this paper we question this conclusion: in some relevant cases, CNNs, with a pre-training and fine-tuning procedure similar to the one used for transformers, exhibit competitive robustness. To fully understand this behaviour, our evidence suggests that researchers should focus on the interaction between pre-training, fine-tuning and the considered architectures rather than on intrinsic properties of Transformers. For this reason, we present some preliminary analyses that shed some light on the impact of pre-training and fine-tuning on out-of-distribution detection and data-shift.},
	language = {en},
	urldate = {2023-05-24},
	author = {Pinto, Francesco and Torr, Philip and Dokania, Puneet K.},
	month = dec,
	year = {2021},
}

@misc{wei_beyond_2023,
	title = {Beyond {Empirical} {Risk} {Minimization}: {Local} {Structure} {Preserving} {Regularization} for {Improving} {Adversarial} {Robustness}},
	shorttitle = {Beyond {Empirical} {Risk} {Minimization}},
	url = {http://arxiv.org/abs/2303.16861},
	doi = {10.48550/arXiv.2303.16861},
	abstract = {It is broadly known that deep neural networks are susceptible to being fooled by adversarial examples with perturbations imperceptible by humans. Various defenses have been proposed to improve adversarial robustness, among which adversarial training methods are most effective. However, most of these methods treat the training samples independently and demand a tremendous amount of samples to train a robust network, while ignoring the latent structural information among these samples. In this work, we propose a novel Local Structure Preserving (LSP) regularization, which aims to preserve the local structure of the input space in the learned embedding space. In this manner, the attacking effect of adversarial samples lying in the vicinity of clean samples can be alleviated. We show strong empirical evidence that with or without adversarial training, our method consistently improves the performance of adversarial robustness on several image classification datasets compared to the baselines and some state-of-the-art approaches, thus providing promising direction for future research.},
	urldate = {2023-05-24},
	publisher = {arXiv},
	author = {Wei, Wei and Zhou, Jiahuan and Wu, Ying},
	month = mar,
	year = {2023},
	note = {arXiv:2303.16861 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@misc{sehwag_better_2019,
	title = {Better the {Devil} you {Know}: {An} {Analysis} of {Evasion} {Attacks} using {Out}-of-{Distribution} {Adversarial} {Examples}},
	shorttitle = {Better the {Devil} you {Know}},
	url = {http://arxiv.org/abs/1905.01726},
	doi = {10.48550/arXiv.1905.01726},
	abstract = {A large body of recent work has investigated the phenomenon of evasion attacks using adversarial examples for deep learning systems, where the addition of norm-bounded perturbations to the test inputs leads to incorrect output classification. Previous work has investigated this phenomenon in closed-world systems where training and test inputs follow a pre-specified distribution. However, real-world implementations of deep learning applications, such as autonomous driving and content classification are likely to operate in the open-world environment. In this paper, we demonstrate the success of open-world evasion attacks, where adversarial examples are generated from out-of-distribution inputs (OOD adversarial examples). In our study, we use 11 state-of-the-art neural network models trained on 3 image datasets of varying complexity. We first demonstrate that state-of-the-art detectors for out-of-distribution data are not robust against OOD adversarial examples. We then consider 5 known defenses for adversarial examples, including state-of-the-art robust training methods, and show that against these defenses, OOD adversarial examples can achieve up to 4\${\textbackslash}times\$ higher target success rates compared to adversarial examples generated from in-distribution data. We also take a quantitative look at how open-world evasion attacks may affect real-world systems. Finally, we present the first steps towards a robust open-world machine learning system.},
	urldate = {2023-05-23},
	publisher = {arXiv},
	author = {Sehwag, Vikash and Bhagoji, Arjun Nitin and Song, Liwei and Sitawarin, Chawin and Cullina, Daniel and Chiang, Mung and Mittal, Prateek},
	month = may,
	year = {2019},
	note = {arXiv:1905.01726 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{hornik_approximation_1991,
	title = {Approximation capabilities of multilayer feedforward networks},
	volume = {4},
	issn = {0893-6080},
	url = {https://www.sciencedirect.com/science/article/pii/089360809190009T},
	doi = {10.1016/0893-6080(91)90009-T},
	abstract = {We show that standard multilayer feedforward networks with as few as a single hidden layer and arbitrary bounded and nonconstant activation function are universal approximators with respect to Lp(μ) performance criteria, for arbitrary finite input environment measures μ, provided only that sufficiently many hidden units are available. If the activation function is continuous, bounded and nonconstant, then continuous mappings can be learned uniformly over compact input sets. We also give very general conditions ensuring that networks with sufficiently smooth activation functions are capable of arbitrarily accurate approximation to a function and its derivatives.},
	language = {en},
	number = {2},
	urldate = {2023-05-21},
	journal = {Neural Networks},
	author = {Hornik, Kurt},
	month = jan,
	year = {1991},
	keywords = {() approximation, Activation function, Input environment measure, Multilayer feedforward networks, Smooth approximation, Sobolev spaces, Uniform approximation, Universal approximation capabilities},
	pages = {251--257},
}

@inproceedings{saharia_photorealistic_2022,
	title = {Photorealistic {Text}-to-{Image} {Diffusion} {Models} with {Deep} {Language} {Understanding}},
	url = {https://openreview.net/forum?id=08Yk-n5l2Al},
	abstract = {We present Imagen, a text-to-image diffusion model with an unprecedented degree of photorealism and a deep level of language understanding. Imagen builds on the power of large transformer language models in understanding text and hinges on the strength of diffusion models in high-fidelity image generation. Our key discovery is that generic large language models (e.g., T5), pretrained on text-only corpora, are surprisingly effective at encoding text for image synthesis: increasing the size of the language model in Imagen boosts both sample fidelity and image-text alignment much more than increasing the size of the image diffusion model. Imagen achieves a new state-of-the-art FID score of 7.27 on the COCO dataset, without ever training on COCO, and human raters find Imagen samples to be on par with the COCO data itself in image-text alignment. To assess text-to-image models in greater depth, we introduce DrawBench, a comprehensive and challenging benchmark for text-to-image models. With DrawBench, we compare Imagen with recent methods including VQ-GAN+CLIP, Latent Diffusion Models, and DALL-E 2, and find that human raters prefer Imagen over other models in side-by-side comparisons, both in terms of sample quality and image-text alignment.},
	language = {en},
	urldate = {2023-03-07},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Saharia, Chitwan and Chan, William and Saxena, Saurabh and Li, Lala and Whang, Jay and Denton, Emily and Ghasemipour, Seyed Kamyar Seyed and Gontijo-Lopes, Raphael and Ayan, Burcu Karagol and Salimans, Tim and Ho, Jonathan and Fleet, David J. and Norouzi, Mohammad},
	month = oct,
	year = {2022},
}

@inproceedings{karras_elucidating_2022,
	title = {Elucidating the {Design} {Space} of {Diffusion}-{Based} {Generative} {Models}},
	url = {https://openreview.net/forum?id=k7FuTOWMOc7},
	abstract = {We argue that the theory and practice of diffusion-based generative models are currently unnecessarily convoluted and seek to remedy the situation by presenting a design space that clearly separates the concrete design choices. This lets us identify several changes to both the sampling and training processes, as well as preconditioning of the score networks. Together, our improvements yield new state-of-the-art FID of 1.79 for CIFAR-10 in a class-conditional setting and 1.97 in an unconditional setting, with much faster sampling (35 network evaluations per image) than prior designs. To further demonstrate their modular nature, we show that our design changes dramatically improve both the efficiency and quality obtainable with pre-trained score networks from previous work, including improving the FID of a previously trained ImageNet-64 model from 2.07 to near-SOTA 1.55, and after re-training with our proposed improvements to a new SOTA of 1.36.},
	language = {en},
	urldate = {2023-05-19},
	author = {Karras, Tero and Aittala, Miika and Aila, Timo and Laine, Samuli},
	month = oct,
	year = {2022},
}

@misc{sun_certified_2021,
	title = {Certified {Adversarial} {Defenses} {Meet} {Out}-of-{Distribution} {Corruptions}: {Benchmarking} {Robustness} and {Simple} {Baselines}},
	shorttitle = {Certified {Adversarial} {Defenses} {Meet} {Out}-of-{Distribution} {Corruptions}},
	url = {http://arxiv.org/abs/2112.00659},
	doi = {10.48550/arXiv.2112.00659},
	abstract = {Certified robustness guarantee gauges a model's robustness to test-time attacks and can assess the model's readiness for deployment in the real world. In this work, we critically examine how the adversarial robustness guarantees from randomized smoothing-based certification methods change when state-of-the-art certifiably robust models encounter out-of-distribution (OOD) data. Our analysis demonstrates a previously unknown vulnerability of these models to low-frequency OOD data such as weather-related corruptions, rendering these models unfit for deployment in the wild. To alleviate this issue, we propose a novel data augmentation scheme, FourierMix, that produces augmentations to improve the spectral coverage of the training data. Furthermore, we propose a new regularizer that encourages consistent predictions on noise perturbations of the augmented data to improve the quality of the smoothed models. We find that FourierMix augmentations help eliminate the spectral bias of certifiably robust models enabling them to achieve significantly better robustness guarantees on a range of OOD benchmarks. Our evaluation also uncovers the inability of current OOD benchmarks at highlighting the spectral biases of the models. To this end, we propose a comprehensive benchmarking suite that contains corruptions from different regions in the spectral domain. Evaluation of models trained with popular augmentation methods on the proposed suite highlights their spectral biases and establishes the superiority of FourierMix trained models at achieving better-certified robustness guarantees under OOD shifts over the entire frequency spectrum.},
	urldate = {2023-05-18},
	publisher = {arXiv},
	author = {Sun, Jiachen and Mehra, Akshay and Kailkhura, Bhavya and Chen, Pin-Yu and Hendrycks, Dan and Hamm, Jihun and Mao, Z. Morley},
	month = dec,
	year = {2021},
	note = {arXiv:2112.00659 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@inproceedings{zhang_branch_2022,
	title = {A {Branch} and {Bound} {Framework} for {Stronger} {Adversarial} {Attacks} of {ReLU} {Networks}},
	url = {https://proceedings.mlr.press/v162/zhang22ae.html},
	abstract = {Strong adversarial attacks are important for evaluating the true robustness of deep neural networks. Most existing attacks search in the input space, e.g., using gradient descent, and may miss adversarial examples due to non-convexity. In this work, we systematically search adversarial examples in the activation space of ReLU networks to tackle hard instances where none of the existing adversarial attacks succeed. Unfortunately, searching the activation space typically relies on generic mixed integer programming (MIP) solvers and is limited to small networks and easy problem instances. To improve scalability and practicability, we use branch and bound (BaB) with specialized GPU-based bound propagation methods, and propose a top-down beam-search approach to quickly identify the subspace that may contain adversarial examples. Moreover, we build an adversarial candidates pool using cheap attacks to further assist the search in activation space via diving techniques and a bottom-up large neighborhood search. Our adversarial attack framework, BaB-Attack, opens up a new opportunity for designing novel adversarial attacks not limited to searching the input space, and enables us to borrow techniques from integer programming theory and neural network verification. In experiments, we can successfully generate adversarial examples when existing attacks on input space fail. Compared to off-the-shelf MIP solver based attacks that requires significant computations, we outperform in both success rates and efficiency.},
	language = {en},
	urldate = {2023-05-18},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Zhang, Huan and Wang, Shiqi and Xu, Kaidi and Wang, Yihan and Jana, Suman and Hsieh, Cho-Jui and Kolter, Zico},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {26591--26604},
}

@article{fukushima_neocognitron_1988,
	title = {Neocognitron: {A} hierarchical neural network capable of visual pattern recognition},
	volume = {1},
	issn = {0893-6080},
	shorttitle = {Neocognitron},
	url = {https://www.sciencedirect.com/science/article/pii/0893608088900147},
	doi = {10.1016/0893-6080(88)90014-7},
	abstract = {A neural network model for visual pattern recognition, called the “neocognitron,” was previously proposed by the author. In this paper, we discuss the mechanism of the model in detail. In order to demonstrate the ability of the neocognitron, we also discuss a pattern-recognition system which works with the mechanism of the neocognitron. The system has been implemented on a minicomputer and has been trained to recognize handwritten numerals. The neocognitron is a hierarchical network consisting of many layers of cells, and has variable connections between the cells in adjoining layers. It can acquire the ability to recognize patterns by learning, and can be trained to recognize any set of patterns. After finishing the process of learning, pattern recognition is performed on the basis of similarity in shape between patterns, and is not affected by deformation, nor by changes in size, nor by shifts in the position of the input patterns. In the hierarchical network of the neocognitron, local features of the input pattern are extracted by the cells of a lower stage, and they are gradually integrated into more global features. Finally, each cell of the highest stage integrates all the information of the input pattern, and responds only to one specific pattern. Thus, the response of the cells of the highest stage shows the final result of the pattern-recognition of the network. During this process of extracting and integrating features, errors in the relative position of local features are gradually tolerated. The operation of tolerating positional error a little at a time at each stage, rather than all in one step, plays an important role in endowing the network with an ability to recognize even distorted patterns.},
	language = {en},
	number = {2},
	urldate = {2023-05-18},
	journal = {Neural Networks},
	author = {Fukushima, Kunihiko},
	month = jan,
	year = {1988},
	pages = {119--130},
}

@inproceedings{raghu_expressive_2017,
	title = {On the {Expressive} {Power} of {Deep} {Neural} {Networks}},
	url = {https://proceedings.mlr.press/v70/raghu17a.html},
	abstract = {We propose a new approach to the problem of neural network expressivity, which seeks to characterize how structural properties of a neural network family affect the functions it is able to compute. Our approach is based on an interrelated set of measures of expressivity, unified by the novel notion of trajectory length, which measures how the output of a network changes as the input sweeps along a one-dimensional path. Our findings show that: (1) The complexity of the computed function grows exponentially with depth (2) All weights are not equal: trained networks are more sensitive to their lower (initial) layer weights (3) Trajectory regularization is a simpler alternative to batch normalization, with the same performance.},
	language = {en},
	urldate = {2023-05-18},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Raghu, Maithra and Poole, Ben and Kleinberg, Jon and Ganguli, Surya and Sohl-Dickstein, Jascha},
	month = jul,
	year = {2017},
	note = {ISSN: 2640-3498},
	pages = {2847--2854},
}

@inproceedings{rolnick_power_2018,
	title = {The power of deeper networks for expressing natural functions},
	url = {https://openreview.net/forum?id=SyProzZAW},
	abstract = {It is well-known that neural networks are universal approximators, but that deeper networks tend in practice to be more powerful than shallower ones. We shed light on this by proving that the total number of neurons m required to approximate natural classes of multivariate polynomials of n variables grows only linearly with n for deep neural networks, but grows exponentially when merely a single hidden layer is allowed. We also provide evidence that when the number of hidden layers is increased from 1 to k, the neuron requirement grows exponentially not with n but with n{\textasciicircum}\{1/k\}, suggesting that the minimum number of layers required for practical expressibility grows only logarithmically with n.},
	language = {en},
	urldate = {2023-05-18},
	author = {Rolnick, David and Tegmark, Max},
	month = feb,
	year = {2018},
}

@inproceedings{ferrari_complete_2022,
	title = {Complete {Verification} via {Multi}-{Neuron} {Relaxation} {Guided} {Branch}-and-{Bound}},
	url = {https://openreview.net/forum?id=l_amHf1oaK},
	abstract = {State-of-the-art neural network verifiers are fundamentally based on one of two paradigms: either encoding the whole verification problem via tight multi-neuron convex relaxations or applying a Branch-and-Bound (BaB) procedure leveraging imprecise but fast bounding methods on a large number of easier subproblems. The former can capture complex multi-neuron dependencies but sacrifices completeness due to the inherent limitations of convex relaxations. The latter enables complete verification but becomes increasingly ineffective on larger and more challenging networks. In this work, we present a novel complete verifier which combines the strengths of both paradigms: it leverages multi-neuron relaxations to drastically reduce the number of subproblems generated during the BaB process and an efficient GPU-based dual optimizer to solve the remaining ones. An extensive evaluation demonstrates that our verifier achieves a new state-of-the-art on both established benchmarks as well as networks with significantly higher accuracy than previously considered. The latter result (up to 28\% certification gains) indicates meaningful progress towards creating verifiers that can handle practically relevant networks.},
	language = {en},
	urldate = {2023-05-18},
	author = {Ferrari, Claudio and Mueller, Mark Niklas and Jovanović, Nikola and Vechev, Martin},
	month = jan,
	year = {2022},
}

@inproceedings{ding_trojan_2019,
	address = {Cham},
	series = {Lecture {Notes} of the {Institute} for {Computer} {Sciences}, {Social} {Informatics} and {Telecommunications} {Engineering}},
	title = {Trojan {Attack} on {Deep} {Generative} {Models} in {Autonomous} {Driving}},
	isbn = {978-3-030-37228-6},
	doi = {10.1007/978-3-030-37228-6_15},
	abstract = {Deep generative models (DGMs) have empowered unprecedented innovations in many application domains. However, their security has not been thoroughly assessed when deploying such models in practice, especially in those mission-critical tasks like autonomous driving. In this work, we draw attention to a new attack surface of DGMs, which is the data used in the training phase. We demonstrate that the training data poisoning, the injection of specially-crafted data, are able to teach Trojan behaviors to a DGM without influencing the original training goal. Such Trojan attack will be activated after model deployment only if certain rare triggers are present in an input. For example, a rain-removal DGM after poisoning can, while removing raindrops in input images, change a traffic light from red to green if this traffic light has a specific appearance (i.e. a trigger). Clearly severe consequences can occur if such poisoned model is deployed on vehicle. Our study shows that launching our Trojan attack is feasible on different DGM categories designed for the autonomous driving scenario, and existing defense methods cannot effectively defeat it. We also introduce a concealing technique to make our data poisoning more inconspicuous during the training. In the end, we propose some potential defense strategies inspiring future explorations.},
	language = {en},
	booktitle = {Security and {Privacy} in {Communication} {Networks}},
	publisher = {Springer International Publishing},
	author = {Ding, Shaohua and Tian, Yulong and Xu, Fengyuan and Li, Qun and Zhong, Sheng},
	editor = {Chen, Songqing and Choo, Kim-Kwang Raymond and Fu, Xinwen and Lou, Wenjing and Mohaisen, Aziz},
	year = {2019},
	keywords = {Autonomous driving, Data poisoning, Deep generative models, Trojan attacks},
	pages = {299--318},
}

@inproceedings{nie_diffusion_2022,
	title = {Diffusion {Models} for {Adversarial} {Purification}},
	url = {https://proceedings.mlr.press/v162/nie22a.html},
	abstract = {Adversarial purification refers to a class of defense methods that remove adversarial perturbations using a generative model. These methods do not make assumptions on the form of attack and the classification model, and thus can defend pre-existing classifiers against unseen threats. However, their performance currently falls behind adversarial training methods. In this work, we propose DiffPure that uses diffusion models for adversarial purification: Given an adversarial example, we first diffuse it with a small amount of noise following a forward diffusion process, and then recover the clean image through a reverse generative process. To evaluate our method against strong adaptive attacks in an efficient and scalable way, we propose to use the adjoint method to compute full gradients of the reverse generative process. Extensive experiments on three image datasets including CIFAR-10, ImageNet and CelebA-HQ with three classifier architectures including ResNet, WideResNet and ViT demonstrate that our method achieves the state-of-the-art results, outperforming current adversarial training and adversarial purification methods, often by a large margin.},
	language = {en},
	urldate = {2023-05-18},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Nie, Weili and Guo, Brandon and Huang, Yujia and Xiao, Chaowei and Vahdat, Arash and Anandkumar, Animashree},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {16805--16827},
}

@inproceedings{sohl-dickstein_deep_2015,
	title = {Deep {Unsupervised} {Learning} using {Nonequilibrium} {Thermodynamics}},
	url = {https://proceedings.mlr.press/v37/sohl-dickstein15.html},
	abstract = {A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.},
	language = {en},
	urldate = {2023-05-18},
	booktitle = {Proceedings of the 32nd {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Sohl-Dickstein, Jascha and Weiss, Eric and Maheswaranathan, Niru and Ganguli, Surya},
	month = jun,
	year = {2015},
	note = {ISSN: 1938-7228},
	pages = {2256--2265},
}

@inproceedings{ehlers_formal_2017,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Formal {Verification} of {Piece}-{Wise} {Linear} {Feed}-{Forward} {Neural} {Networks}},
	isbn = {978-3-319-68167-2},
	doi = {10.1007/978-3-319-68167-2_19},
	abstract = {We present an approach for the verification of feed-forward neural networks in which all nodes have a piece-wise linear activation function. Such networks are often used in deep learning and have been shown to be hard to verify for modern satisfiability modulo theory (SMT) and integer linear programming (ILP) solvers.},
	language = {en},
	booktitle = {Automated {Technology} for {Verification} and {Analysis}},
	publisher = {Springer International Publishing},
	author = {Ehlers, Rüdiger},
	editor = {D'Souza, Deepak and Narayan Kumar, K.},
	year = {2017},
	pages = {269--286},
}

@inproceedings{tjeng_evaluating_2018,
	title = {Evaluating {Robustness} of {Neural} {Networks} with {Mixed} {Integer} {Programming}},
	url = {https://openreview.net/forum?id=HyGIdiRqtm},
	abstract = {Neural networks trained only to optimize for training accuracy can often be fooled by adversarial examples --- slightly perturbed inputs misclassified with high confidence. Verification of networks enables us to gauge their vulnerability to such adversarial examples. We formulate verification of piecewise-linear neural networks as a mixed integer program. On a representative task of finding minimum adversarial distortions, our verifier is two to three orders of magnitude quicker than the state-of-the-art. We achieve this computational speedup via tight formulations for non-linearities, as well as a novel presolve algorithm that makes full use of all information available. The computational speedup allows us to verify properties on convolutional and residual networks with over 100,000 ReLUs --- several orders of magnitude more than networks previously verified by any complete verifier. In particular, we determine for the first time the exact adversarial accuracy of an MNIST classifier to perturbations with bounded l-∞ norm ε=0.1: for this classifier, we find an adversarial example for 4.38\% of samples, and a certificate of robustness to norm-bounded perturbations for the remainder. Across all robust training procedures and network architectures considered, and for both the MNIST and CIFAR-10 datasets, we are able to certify more samples than the state-of-the-art and find more adversarial examples than a strong first-order attack.},
	language = {en},
	urldate = {2023-05-16},
	author = {Tjeng, Vincent and Xiao, Kai Y. and Tedrake, Russ},
	month = dec,
	year = {2018},
}

@article{casper_robust_2022,
	title = {Robust {Feature}-{Level} {Adversaries} are {Interpretability} {Tools}},
	volume = {35},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/d616a353c711f11c722e3f28d2d9e956-Abstract-Conference.html},
	language = {en},
	urldate = {2023-05-16},
	journal = {Advances in Neural Information Processing Systems},
	author = {Casper, Stephen and Nadeau, Max and Hadfield-Menell, Dylan and Kreiman, Gabriel},
	month = dec,
	year = {2022},
	pages = {33093--33106},
}

@misc{ibrahim_towards_2023,
	title = {Towards {Out}-of-{Distribution} {Adversarial} {Robustness}},
	url = {http://arxiv.org/abs/2210.03150},
	doi = {10.48550/arXiv.2210.03150},
	abstract = {Adversarial robustness continues to be a major challenge for deep learning. A core issue is that robustness to one type of attack often fails to transfer to other attacks. While prior work establishes a theoretical trade-off in robustness against different \$L\_p\$ norms, we show that there is potential for improvement against many commonly used attacks by adopting a domain generalisation approach. Concretely, we treat each type of attack as a domain, and apply the Risk Extrapolation method (REx), which promotes similar levels of robustness against all training attacks. Compared to existing methods, we obtain similar or superior worst-case adversarial robustness on attacks seen during training. Moreover, we achieve superior performance on families or tunings of attacks only encountered at test time. On ensembles of attacks, our approach improves the accuracy from 3.4\% the best existing baseline to 25.9\% on MNIST, and from 16.9\% to 23.5\% on CIFAR10.},
	urldate = {2023-05-16},
	publisher = {arXiv},
	author = {Ibrahim, Adam and Guille-Escuret, Charles and Mitliagkas, Ioannis and Rish, Irina and Krueger, David and Bashivan, Pouya},
	month = feb,
	year = {2023},
	note = {arXiv:2210.03150 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@inproceedings{sehwag_analyzing_2019,
	address = {New York, NY, USA},
	series = {{AISec}'19},
	title = {Analyzing the {Robustness} of {Open}-{World} {Machine} {Learning}},
	isbn = {978-1-4503-6833-9},
	url = {https://dl.acm.org/doi/10.1145/3338501.3357372},
	doi = {10.1145/3338501.3357372},
	abstract = {When deploying machine learning models in real-world applications, an open-world learning framework is needed to deal with both normal in-distribution inputs and undesired out-of-distribution (OOD) inputs. Open-world learning frameworks include OOD detectors that aim to discard input examples which are not from the same distribution as the training data of machine learning classifiers. However, our understanding of current OOD detectors is limited to the setting of benign OOD data, and an open question is whether they are robust in the presence of adversaries. In this paper, we present the first analysis of the robustness of open-world learning frameworks in the presence of adversaries by introducing and designing øodAdvExamples. Our experimental results show that current OOD detectors can be easily evaded by slightly perturbing benign OOD inputs, revealing a severe limitation of current open-world learning frameworks. Furthermore, we find that øodAdvExamples also pose a strong threat to adversarial training based defense methods in spite of their effectiveness against in-distribution adversarial attacks. To counteract these threats and ensure the trustworthy detection of OOD inputs, we outline a preliminary design for a robust open-world machine learning framework.},
	urldate = {2023-05-16},
	booktitle = {Proceedings of the 12th {ACM} {Workshop} on {Artificial} {Intelligence} and {Security}},
	publisher = {Association for Computing Machinery},
	author = {Sehwag, Vikash and Bhagoji, Arjun Nitin and Song, Liwei and Sitawarin, Chawin and Cullina, Daniel and Chiang, Mung and Mittal, Prateek},
	month = nov,
	year = {2019},
	keywords = {adversarial example, deep learning, open world recognition},
	pages = {105--116},
}

@misc{gowal_effectiveness_2019,
	title = {On the {Effectiveness} of {Interval} {Bound} {Propagation} for {Training} {Verifiably} {Robust} {Models}},
	url = {http://arxiv.org/abs/1810.12715},
	doi = {10.48550/arXiv.1810.12715},
	abstract = {Recent work has shown that it is possible to train deep neural networks that are provably robust to norm-bounded adversarial perturbations. Most of these methods are based on minimizing an upper bound on the worst-case loss over all possible adversarial perturbations. While these techniques show promise, they often result in difficult optimization procedures that remain hard to scale to larger networks. Through a comprehensive analysis, we show how a simple bounding technique, interval bound propagation (IBP), can be exploited to train large provably robust neural networks that beat the state-of-the-art in verified accuracy. While the upper bound computed by IBP can be quite weak for general networks, we demonstrate that an appropriate loss and clever hyper-parameter schedule allow the network to adapt such that the IBP bound is tight. This results in a fast and stable learning algorithm that outperforms more sophisticated methods and achieves state-of-the-art results on MNIST, CIFAR-10 and SVHN. It also allows us to train the largest model to be verified beyond vacuous bounds on a downscaled version of ImageNet.},
	urldate = {2023-05-15},
	publisher = {arXiv},
	author = {Gowal, Sven and Dvijotham, Krishnamurthy and Stanforth, Robert and Bunel, Rudy and Qin, Chongli and Uesato, Jonathan and Arandjelovic, Relja and Mann, Timothy and Kohli, Pushmeet},
	month = aug,
	year = {2019},
	note = {arXiv:1810.12715 [cs, stat]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{mueller_certified_2023,
	title = {Certified {Training}: {Small} {Boxes} are {All} {You} {Need}},
	shorttitle = {Certified {Training}},
	url = {https://openreview.net/forum?id=7oFuxtJtUMH},
	abstract = {To obtain, deterministic guarantees of adversarial robustness, specialized training methods are used. We propose, SABR, a novel such certified training method, based on the key insight that propagating interval bounds for a small but carefully selected subset of the adversarial input region is sufficient to approximate the worst-case loss over the whole region while significantly reducing approximation errors. We show in an extensive empirical evaluation that SABR outperforms existing certified defenses in terms of both standard and certifiable accuracies across perturbation magnitudes and datasets, pointing to a new class of certified training methods promising to alleviate the robustness-accuracy trade-off.},
	language = {en},
	urldate = {2023-05-15},
	author = {Mueller, Mark Niklas and Eckert, Franziska and Fischer, Marc and Vechev, Martin},
	month = feb,
	year = {2023},
}

@misc{mao_taps_2023,
	title = {{TAPS}: {Connecting} {Certified} and {Adversarial} {Training}},
	shorttitle = {{TAPS}},
	url = {http://arxiv.org/abs/2305.04574},
	doi = {10.48550/arXiv.2305.04574},
	abstract = {Training certifiably robust neural networks remains a notoriously hard problem. On one side, adversarial training optimizes under-approximations of the worst-case loss, which leads to insufficient regularization for certification, while on the other, sound certified training methods optimize loose over-approximations, leading to over-regularization and poor (standard) accuracy. In this work we propose TAPS, an (unsound) certified training method that combines IBP and PGD training to yield precise, although not necessarily sound, worst-case loss approximations, reducing over-regularization and increasing certified and standard accuracies. Empirically, TAPS achieves a new state-of-the-art in many settings, e.g., reaching a certified accuracy of \$22{\textbackslash}\%\$ on TinyImageNet for \${\textbackslash}ell\_{\textbackslash}infty\$-perturbations with radius \${\textbackslash}epsilon=1/255\$.},
	urldate = {2023-05-15},
	publisher = {arXiv},
	author = {Mao, Yuhao and Müller, Mark Niklas and Fischer, Marc and Vechev, Martin},
	month = may,
	year = {2023},
	note = {arXiv:2305.04574 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@inproceedings{eykholt_robust_2018,
	title = {Robust {Physical}-{World} {Attacks} on {Deep} {Learning} {Visual} {Classification}},
	url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Eykholt_Robust_Physical-World_Attacks_CVPR_2018_paper},
	urldate = {2023-05-14},
	author = {Eykholt, Kevin and Evtimov, Ivan and Fernandes, Earlence and Li, Bo and Rahmati, Amir and Xiao, Chaowei and Prakash, Atul and Kohno, Tadayoshi and Song, Dawn},
	year = {2018},
	pages = {1625--1634},
}

@inproceedings{cheung_adaaug_2022,
	title = {{AdaAug}: {Learning} {Class}- and {Instance}-adaptive {Data} {Augmentation} {Policies}},
	shorttitle = {{AdaAug}},
	url = {https://openreview.net/forum?id=rWXfFogxRJN},
	abstract = {Data augmentation is an effective way to improve the generalization capability of modern deep learning models. However, the underlying augmentation methods mostly rely on handcrafted operations. Moreover, an augmentation policy useful to one dataset may not transfer well to other datasets. Therefore, Automated Data Augmentation (AutoDA) methods, like {\textbackslash}textit\{AutoAugment\} and {\textbackslash}textit\{Population-based Augmentation\}, have been proposed recently to automate the process of searching for optimal augmentation policies. However, the augmentation policies found are not adaptive to the dataset used, hindering the effectiveness of these AutoDA methods. In this paper, we propose a novel AutoDA method called {\textbackslash}texttt\{AdaAug\} to efficiently learn adaptive augmentation policies in a class-dependent and potentially instance-dependent manner. Our experiments show that the adaptive augmentation policies learned by our method transfer well to unseen datasets such as the Oxford Flowers, Oxford-IIT Pets, FGVC Aircraft, and Stanford Cars datasets when compared with other AutoDA baselines. In addition, our method also achieves state-of-the-art performance on the CIFAR-10, CIFAR-100, and SVHN datasets.},
	language = {en},
	urldate = {2023-03-12},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Cheung, Tsz-Him and Yeung, Dit-Yan},
	month = jan,
	year = {2022},
}

@inproceedings{dosovitskiy_image_2021,
	title = {An {Image} is {Worth} 16x16 {Words}: {Transformers} for {Image} {Recognition} at {Scale}},
	shorttitle = {An {Image} is {Worth} 16x16 {Words}},
	url = {https://openreview.net/forum?id=YicbFdNTTy&utm_campaign=f86497ed3a-EMAIL_CAMPAIGN_2019_04_24_03_18_COPY_01&utm_medium=email&utm_source=Deep%20Learning%20Weekly&utm_term=0_384567b42d-f86497ed3a-72965345},
	abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
	language = {en},
	urldate = {2023-02-18},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
	year = {2021},
}

@inproceedings{zhang_mixup_2018,
	title = {mixup: {Beyond} {Empirical} {Risk} {Minimization}},
	shorttitle = {mixup},
	url = {https://openreview.net/forum?id=r1Ddp1-Rb&;noteId=r1Ddp1-Rb),},
	abstract = {Training on convex combinations between random training examples and their labels improves generalization in deep neural networks},
	language = {en},
	urldate = {2021-09-05},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Zhang, Hongyi and Cisse, Moustapha and Dauphin, Yann N. and Lopez-Paz, David},
	month = feb,
	year = {2018},
}

@inproceedings{hendrycks_augmix_2020,
	title = {{AugMix}: {A} {Simple} {Data} {Processing} {Method} to {Improve} {Robustness} and {Uncertainty}},
	shorttitle = {{AugMix}},
	url = {https://openreview.net/forum?id=S1gmrxHFvB},
	abstract = {We obtain state-of-the-art on robustness to data shifts, and we maintain calibration under data shift even though even when accuracy drops},
	language = {en},
	urldate = {2021-12-03},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Hendrycks*, Dan and Mu*, Norman and Cubuk, Ekin Dogus and Zoph, Barret and Gilmer, Justin and Lakshminarayanan, Balaji},
	year = {2020},
}

@inproceedings{wang_augmax_2021,
	title = {{AugMax}: {Adversarial} {Composition} of {Random} {Augmentations} for {Robust} {Training}},
	shorttitle = {{AugMax}},
	url = {https://openreview.net/forum?id=P5MtdcVdFZ4},
	abstract = {Data augmentation is a simple yet effective way to improve the robustness of deep neural networks (DNNs). Diversity and hardness are two complementary dimensions of data augmentation to achieve...},
	language = {en},
	urldate = {2021-12-02},
	booktitle = {Thirty-{Fifth} {Conference} on {Neural} {Information} {Processing} {Systems}},
	author = {Wang, Haotao and Xiao, Chaowei and Kossaifi, Jean and Yu, Zhiding and Anandkumar, Anima and Wang, Zhangyang},
	month = may,
	year = {2021},
}

@inproceedings{mo_when_2022,
	title = {When {Adversarial} {Training} {Meets} {Vision} {Transformers}: {Recipes} from {Training} to {Architecture}},
	shorttitle = {When {Adversarial} {Training} {Meets} {Vision} {Transformers}},
	url = {https://openreview.net/forum?id=ZV9WAe-Q0J},
	abstract = {Vision Transformers (ViTs) have recently achieved competitive performance in broad vision tasks. Unfortunately, on popular threat models, naturally trained ViTs are shown to provide no more adversarial robustness than convolutional neural networks (CNNs). Adversarial training is still required for ViTs to defend against such adversarial attacks. In this paper, we provide the first and comprehensive study on the adversarial training recipe of ViTs via extensive evaluation of various training techniques across benchmark datasets. We find that pre-training and SGD optimizer are necessary for ViTs' adversarial training. Further considering ViT as a new type of model architecture, we investigate its adversarial robustness from the perspective of its unique architectural components. We find, when randomly masking gradients from some attention blocks or masking perturbations on some patches during adversarial training, the adversarial robustness of ViTs can be remarkably improved, which may potentially open up a line of work to explore the architectural information inside the newly designed models like ViTs. Our code is available at https://github.com/mo666666/When-Adversarial-Training-Meets-Vision-Transformers.},
	language = {en},
	urldate = {2023-02-18},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Mo, Yichuan and Wu, Dongxian and Wang, Yifei and Guo, Yiwen and Wang, Yisen},
	month = oct,
	year = {2022},
}

@book{wei_relation_2023,
	title = {On the {Relation} between {Sharpness}-{Aware} {Minimization} and {Adversarial} {Robustness}},
	abstract = {We present a novel understanding of Sharpness-Aware Minimization (SAM) in the context of adversarial robustness. In this paper, we point out that both SAM and adversarial training (AT) can be viewed as specific feature perturbations, which improve adversarial robustness. However, we note that SAM and AT are distinct in terms of perturbation strength, leading to different accuracy and robustness trade-offs. We provide theoretical evidence for these claims in a simplified model with rigorous mathematical proofs. Furthermore, we conduct experiment to demonstrate that only utilizing SAM can achieve superior adversarial robustness compared to standard training, which is an unexpected benefit. As adversarial training can suffer from a decrease in clean accuracy, we show that using SAM alone can improve robustness without sacrificing clean accuracy. Our code will be available upon publication.},
	author = {Wei, Zeming and Zhu, Jingyu and Yihao, Zhang},
	month = may,
	year = {2023},
}

@inproceedings{foret_sharpness-aware_2021,
	title = {Sharpness-aware {Minimization} for {Efficiently} {Improving} {Generalization}},
	url = {https://openreview.net/forum?id=6Tm1mposlrM},
	abstract = {In today's heavily overparameterized models, the value of the training loss provides few guarantees on model generalization ability. Indeed, optimizing only the training loss value, as is commonly done, can easily lead to suboptimal model quality. Motivated by the connection between geometry of the loss landscape and generalization---including a generalization bound that we prove here---we introduce a novel, effective procedure for instead simultaneously minimizing loss value and loss sharpness. In particular, our procedure, Sharpness-Aware Minimization (SAM), seeks parameters that lie in neighborhoods having uniformly low loss; this formulation results in a min-max optimization problem on which gradient descent can be performed efficiently. We present empirical results showing that SAM improves model generalization across a variety of benchmark datasets (e.g., CIFAR-\{10, 100\}, ImageNet, finetuning tasks) and models, yielding novel state-of-the-art performance for several. Additionally, we find that SAM natively provides robustness to label noise on par with that provided by state-of-the-art procedures that specifically target learning with noisy labels.},
	language = {en},
	urldate = {2023-05-10},
	author = {Foret, Pierre and Kleiner, Ariel and Mobahi, Hossein and Neyshabur, Behnam},
	month = jan,
	year = {2021},
}

@misc{ortiz-jimenez_catastrophic_2022,
	title = {Catastrophic overfitting is a bug but also a feature},
	url = {http://arxiv.org/abs/2206.08242},
	doi = {10.48550/arXiv.2206.08242},
	abstract = {Despite clear computational advantages in building robust neural networks, adversarial training (AT) using single-step methods is unstable as it suffers from catastrophic overfitting (CO): Networks gain non-trivial robustness during the first stages of adversarial training, but suddenly reach a breaking point where they quickly lose all robustness in just a few iterations. Although some works have succeeded at preventing CO, the different mechanisms that lead to this remarkable failure mode are still poorly understood. In this work, however, we find that the interplay between the structure of the data and the dynamics of AT plays a fundamental role in CO. Specifically, through active interventions on typical datasets of natural images, we establish a causal link between the structure of the data and the onset of CO in single-step AT methods. This new perspective provides important insights into the mechanisms that lead to CO and paves the way towards a better understanding of the general dynamics of robust model construction. The code to reproduce the experiments of this paper can be found at https://github.com/gortizji/co\_features .},
	urldate = {2023-05-10},
	publisher = {arXiv},
	author = {Ortiz-Jiménez, Guillermo and de Jorge, Pau and Sanyal, Amartya and Bibi, Adel and Dokania, Puneet K. and Frossard, Pascal and Rogéz, Gregory and Torr, Philip H. S.},
	month = jun,
	year = {2022},
	note = {arXiv:2206.08242 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@inproceedings{jiang_learning_2021,
	title = {Learning to {Defend} by {Learning} to {Attack}},
	url = {https://proceedings.mlr.press/v130/jiang21a.html},
	abstract = {Adversarial training provides a principled approach for training robust neural networks. From an optimization perspective, adversarial training is essentially solving a bilevel optimization problem. The leader problem is trying to learn a robust classifier, while the follower maximization is trying to generate adversarial samples. Unfortunately, such a bilevel problem is difficult to solve due to its highly complicated structure. This work proposes a new adversarial training method based on a generic learning-to-learn (L2L) framework. Specifically, instead of applying existing hand-designed algorithms for the inner problem, we learn an optimizer, which is parametrized as a convolutional neural network. At the same time, a robust classifier is learned to defense the adversarial attack generated by the learned optimizer. Experiments over CIFAR-10 and CIFAR-100 datasets demonstrate that L2L outperforms existing adversarial training methods in both classification accuracy and computational efficiency. Moreover, our L2L framework can be extended to generative adversarial imitation learning and stabilize the training.},
	language = {en},
	urldate = {2023-05-05},
	booktitle = {Proceedings of {The} 24th {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Jiang, Haoming and Chen, Zhehui and Shi, Yuyang and Dai, Bo and Zhao, Tuo},
	month = mar,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {577--585},
}

@inproceedings{jang_adversarial_2019,
	title = {Adversarial {Defense} via {Learning} to {Generate} {Diverse} {Attacks}},
	url = {https://openaccess.thecvf.com/content_ICCV_2019/html/Jang_Adversarial_Defense_via_Learning_to_Generate_Diverse_Attacks_ICCV_2019_paper.html},
	urldate = {2023-05-05},
	author = {Jang, Yunseok and Zhao, Tianchen and Hong, Seunghoon and Lee, Honglak},
	year = {2019},
	pages = {2740--2749},
}

@inproceedings{grathwohl_backpropagation_2018,
	title = {Backpropagation through the {Void}: {Optimizing} control variates for black-box gradient estimation},
	shorttitle = {Backpropagation through the {Void}},
	url = {https://openreview.net/forum?id=SyzKd1bCW},
	abstract = {Gradient-based optimization is the foundation of deep learning and reinforcement learning. Even when the mechanism being optimized is unknown or not differentiable, optimization using high-variance or biased gradient estimates is still often the best strategy. We introduce a general framework for learning low-variance, unbiased gradient estimators for black-box functions of random variables, based on gradients of a learned function. These estimators can be jointly trained with model parameters or policies, and are applicable in both discrete and continuous settings. We give unbiased, adaptive analogs of state-of-the-art reinforcement learning methods such as advantage actor-critic. We also demonstrate this framework for training discrete latent-variable models.},
	language = {en},
	urldate = {2023-04-23},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Grathwohl, Will and Choi, Dami and Wu, Yuhuai and Roeder, Geoff and Duvenaud, David},
	year = {2018},
}

@misc{zhu_robust_2023,
	title = {Robust {Neural} {Architecture} {Search}},
	url = {http://arxiv.org/abs/2304.02845},
	doi = {10.48550/arXiv.2304.02845},
	abstract = {Neural Architectures Search (NAS) becomes more and more popular over these years. However, NAS-generated models tends to suffer greater vulnerability to various malicious attacks. Lots of robust NAS methods leverage adversarial training to enhance the robustness of NAS-generated models, however, they neglected the nature accuracy of NAS-generated models. In our paper, we propose a novel NAS method, Robust Neural Architecture Search (RNAS). To design a regularization term to balance accuracy and robustness, RNAS generates architectures with both high accuracy and good robustness. To reduce search cost, we further propose to use noise examples instead adversarial examples as input to search architectures. Extensive experiments show that RNAS achieves state-of-the-art (SOTA) performance on both image classification and adversarial attacks, which illustrates the proposed RNAS achieves a good tradeoff between robustness and accuracy.},
	urldate = {2023-04-09},
	publisher = {arXiv},
	author = {Zhu, Xunyu and Li, Jian and Liu, Yong and Wang, Weiping},
	month = apr,
	year = {2023},
	note = {arXiv:2304.02845 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@misc{zhai_adversarially_2019,
	title = {Adversarially {Robust} {Generalization} {Just} {Requires} {More} {Unlabeled} {Data}},
	url = {http://arxiv.org/abs/1906.00555},
	doi = {10.48550/arXiv.1906.00555},
	abstract = {Neural network robustness has recently been highlighted by the existence of adversarial examples. Many previous works show that the learned networks do not perform well on perturbed test data, and significantly more labeled data is required to achieve adversarially robust generalization. In this paper, we theoretically and empirically show that with just more unlabeled data, we can learn a model with better adversarially robust generalization. The key insight of our results is based on a risk decomposition theorem, in which the expected robust risk is separated into two parts: the stability part which measures the prediction stability in the presence of perturbations, and the accuracy part which evaluates the standard classification accuracy. As the stability part does not depend on any label information, we can optimize this part using unlabeled data. We further prove that for a specific Gaussian mixture problem, adversarially robust generalization can be almost as easy as the standard generalization in supervised learning if a sufficiently large amount of unlabeled data is provided. Inspired by the theoretical findings, we further show that a practical adversarial training algorithm that leverages unlabeled data can improve adversarial robust generalization on MNIST and Cifar-10.},
	urldate = {2023-04-03},
	publisher = {arXiv},
	author = {Zhai, Runtian and Cai, Tianle and He, Di and Dan, Chen and He, Kun and Hopcroft, John and Wang, Liwei},
	month = sep,
	year = {2019},
	note = {arXiv:1906.00555 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{nikolenko_synthetic_2019,
	title = {Synthetic {Data} for {Deep} {Learning}},
	url = {http://arxiv.org/abs/1909.11512},
	doi = {10.48550/arXiv.1909.11512},
	abstract = {Synthetic data is an increasingly popular tool for training deep learning models, especially in computer vision but also in other areas. In this work, we attempt to provide a comprehensive survey of the various directions in the development and application of synthetic data. First, we discuss synthetic datasets for basic computer vision problems, both low-level (e.g., optical flow estimation) and high-level (e.g., semantic segmentation), synthetic environments and datasets for outdoor and urban scenes (autonomous driving), indoor scenes (indoor navigation), aerial navigation, simulation environments for robotics, applications of synthetic data outside computer vision (in neural programming, bioinformatics, NLP, and more); we also survey the work on improving synthetic data development and alternative ways to produce it such as GANs. Second, we discuss in detail the synthetic-to-real domain adaptation problem that inevitably arises in applications of synthetic data, including synthetic-to-real refinement with GAN-based models and domain adaptation at the feature/model level without explicit data transformations. Third, we turn to privacy-related applications of synthetic data and review the work on generating synthetic datasets with differential privacy guarantees. We conclude by highlighting the most promising directions for further work in synthetic data studies.},
	urldate = {2023-04-03},
	publisher = {arXiv},
	author = {Nikolenko, Sergey I.},
	month = sep,
	year = {2019},
	note = {arXiv:1909.11512 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@misc{jordon_synthetic_2022,
	title = {Synthetic {Data} -- what, why and how?},
	url = {http://arxiv.org/abs/2205.03257},
	doi = {10.48550/arXiv.2205.03257},
	abstract = {This explainer document aims to provide an overview of the current state of the rapidly expanding work on synthetic data technologies, with a particular focus on privacy. The article is intended for a non-technical audience, though some formal definitions have been given to provide clarity to specialists. This article is intended to enable the reader to quickly become familiar with the notion of synthetic data, as well as understand some of the subtle intricacies that come with it. We do believe that synthetic data is a very useful tool, and our hope is that this report highlights that, while drawing attention to nuances that can easily be overlooked in its deployment.},
	urldate = {2023-04-03},
	publisher = {arXiv},
	author = {Jordon, James and Szpruch, Lukasz and Houssiau, Florimond and Bottarelli, Mirko and Cherubin, Giovanni and Maple, Carsten and Cohen, Samuel N. and Weller, Adrian},
	month = may,
	year = {2022},
	note = {arXiv:2205.03257 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{howard_improvements_2013,
	title = {Some {Improvements} on {Deep} {Convolutional} {Neural} {Network} {Based} {Image} {Classification}},
	url = {http://arxiv.org/abs/1312.5402},
	doi = {10.48550/arXiv.1312.5402},
	abstract = {We investigate multiple techniques to improve upon the current state of the art deep convolutional neural network based image classification pipeline. The techiques include adding more image transformations to training data, adding more transformations to generate additional predictions at test time and using complementary models applied to higher resolution images. This paper summarizes our entry in the Imagenet Large Scale Visual Recognition Challenge 2013. Our system achieved a top 5 classification error rate of 13.55\% using no external data which is over a 20\% relative improvement on the previous year's winner.},
	urldate = {2023-04-03},
	publisher = {arXiv},
	author = {Howard, Andrew G.},
	month = dec,
	year = {2013},
	note = {arXiv:1312.5402 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{hendrycks_1using_2019,
	title = {{1Using} {Self}-{Supervised} {Learning} {Can} {Improve} {Model} {Robustness} and {Uncertainty}},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper/2019/hash/a2b15837edac15df90721968986f7f8e-Abstract.html},
	abstract = {Self-supervision provides effective representations for downstream tasks without requiring labels. However, existing approaches lag behind fully supervised training and are often not thought beneficial beyond obviating or reducing the need for annotations. We find that self-supervision can benefit robustness in a variety of ways, including robustness to adversarial examples, label corruption, and common input corruptions. Additionally, self-supervision greatly benefits out-of-distribution detection on difficult, near-distribution outliers, so much so that it exceeds the performance of fully supervised methods. These results demonstrate the promise of self-supervision for improving robustness and uncertainty estimation and establish these tasks as new axes of evaluation for future self-supervised learning research.},
	urldate = {2022-07-21},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Hendrycks, Dan and Mazeika, Mantas and Kadavath, Saurav and Song, Dawn},
	year = {2019},
}

@misc{gavrikov_extended_2023,
	title = {An {Extended} {Study} of {Human}-like {Behavior} under {Adversarial} {Training}},
	url = {http://arxiv.org/abs/2303.12669},
	doi = {10.48550/arXiv.2303.12669},
	abstract = {Neural networks have a number of shortcomings. Amongst the severest ones is the sensitivity to distribution shifts which allows models to be easily fooled into wrong predictions by small perturbations to inputs that are often imperceivable to humans and do not have to carry semantic meaning. Adversarial training poses a partial solution to address this issue by training models on worst-case perturbations. Yet, recent work has also pointed out that the reasoning in neural networks is different from humans. Humans identify objects by shape, while neural nets mainly employ texture cues. Exemplarily, a model trained on photographs will likely fail to generalize to datasets containing sketches. Interestingly, it was also shown that adversarial training seems to favorably increase the shift toward shape bias. In this work, we revisit this observation and provide an extensive analysis of this effect on various architectures, the common \${\textbackslash}ell\_2\$- and \${\textbackslash}ell\_{\textbackslash}infty\$-training, and Transformer-based models. Further, we provide a possible explanation for this phenomenon from a frequency perspective.},
	urldate = {2023-03-29},
	publisher = {arXiv},
	author = {Gavrikov, Paul and Keuper, Janis and Keuper, Margret},
	month = mar,
	year = {2023},
	note = {arXiv:2303.12669 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{li_improved_2023,
	title = {Improved {Adversarial} {Training} {Through} {Adaptive} {Instance}-wise {Loss} {Smoothing}},
	copyright = {All rights reserved},
	url = {http://arxiv.org/abs/2303.14077},
	doi = {10.48550/arXiv.2303.14077},
	abstract = {Deep neural networks can be easily fooled into making incorrect predictions through corruption of the input by adversarial perturbations: human-imperceptible artificial noise. So far adversarial training has been the most successful defense against such adversarial attacks. This work focuses on improving adversarial training to boost adversarial robustness. We first analyze, from an instance-wise perspective, how adversarial vulnerability evolves during adversarial training. We find that during training an overall reduction of adversarial loss is achieved by sacrificing a considerable proportion of training samples to be more vulnerable to adversarial attack, which results in an uneven distribution of adversarial vulnerability among data. Such "uneven vulnerability", is prevalent across several popular robust training methods and, more importantly, relates to overfitting in adversarial training. Motivated by this observation, we propose a new adversarial training method: Instance-adaptive Smoothness Enhanced Adversarial Training (ISEAT). It jointly smooths both input and weight loss landscapes in an adaptive, instance-specific, way to enhance robustness more for those samples with higher adversarial vulnerability. Extensive experiments demonstrate the superiority of our method over existing defense methods. Noticeably, our method, when combined with the latest data augmentation and semi-supervised learning techniques, achieves state-of-the-art robustness against \${\textbackslash}ell\_\{{\textbackslash}infty\}\$-norm constrained attacks on CIFAR10 of 59.32\% for Wide ResNet34-10 without extra data, and 61.55\% for Wide ResNet28-10 with extra data. Code is available at https://github.com/TreeLLi/Instance-adaptive-Smoothness-Enhanced-AT.},
	urldate = {2023-03-28},
	publisher = {arXiv},
	author = {Li, Lin and Spratling, Michael},
	month = mar,
	year = {2023},
	note = {arXiv:2303.14077 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@inproceedings{li_data_2023,
	title = {Data augmentation alone can improve adversarial training},
	copyright = {All rights reserved},
	url = {https://openreview.net/forum?id=y4uc4NtTWaq},
	abstract = {Adversarial training suffers from the issue of robust overfitting, which seriously impairs its generalization performance. Data augmentation, which is effective at preventing overfitting in standard training, has been observed by many previous works to be ineffective in mitigating overfitting in adversarial training. This work proves that, contrary to previous findings, data augmentation alone can significantly boost accuracy and robustness in adversarial training. We find that the hardness and the diversity of data augmentation are important factors in combating robust overfitting. In general, diversity can improve both accuracy and robustness, while hardness can boost robustness at the cost of accuracy within a certain limit and degrade them both over that limit. To mitigate robust overfitting, we first propose a new crop transformation Cropshift with improved diversity compared to the conventional one (Padcrop). We then propose a new data augmentation scheme, based on Cropshift, with much improved diversity and well-balanced hardness. Empirically, our augmentation method achieves the state-of-the-art accuracy and robustness for data augmentations in adversarial training. Furthermore, it matches, or even exceeds when combined with weight averaging, the performance of the best contemporary regularization methods for alleviating robust overfitting.},
	language = {en},
	urldate = {2023-03-27},
	booktitle = {The {Eleventh} {International} {Conference} on {Learning} {Representations}},
	author = {Li, Lin and Spratling, Michael W.},
	month = feb,
	year = {2023},
}

@inproceedings{dong_adversarial_2020,
	title = {Adversarial {Distributional} {Training} for {Robust} {Deep} {Learning}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper_files/paper/2020/hash/5de8a36008b04a6167761fa19b61aa6c-Abstract.html},
	abstract = {Adversarial training (AT) is among the most effective techniques to improve model robustness by augmenting training data with adversarial examples. However, most existing AT methods adopt a specific attack to craft adversarial examples, leading to the unreliable robustness against other unseen attacks. Besides, a single attack algorithm could be insufficient to explore the space of perturbations. In this paper, we introduce adversarial distributional training (ADT), a novel framework for learning robust models. ADT is formulated as a minimax optimization problem, where the inner maximization aims to learn an adversarial distribution to characterize the potential adversarial examples around a natural one under an entropic regularizer, and the outer minimization aims to train robust models by minimizing the expected loss over the worst-case adversarial distributions. Through a theoretical analysis, we develop a general algorithm for solving ADT, and present three approaches for parameterizing the adversarial distributions, ranging from the typical Gaussian distributions to the flexible implicit ones. Empirical results on several benchmarks validate the effectiveness of ADT compared with the state-of-the-art AT methods.},
	urldate = {2023-03-26},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Dong, Yinpeng and Deng, Zhijie and Pang, Tianyu and Zhu, Jun and Su, Hang},
	year = {2020},
	pages = {8270--8283},
}

@inproceedings{addepalli_efficient_2022,
	title = {Efficient and {Effective} {Augmentation} {Strategy} for {Adversarial} {Training}},
	url = {https://openreview.net/forum?id=ODkBI1d3phW},
	abstract = {Adversarial training of Deep Neural Networks is known to be significantly more data-hungry when compared to standard training. Furthermore, complex data augmentations such as AutoAugment, which have led to substantial gains in standard training of image classifiers, have not been successful with Adversarial Training. We first explain this contrasting behavior by viewing augmentation during training as a problem of domain generalization, and further propose Diverse Augmentation-based Joint Adversarial Training (DAJAT) to use data augmentations effectively in adversarial training. We aim to handle the conflicting goals of enhancing the diversity of the training dataset and training with data that is close to the test distribution by using a combination of simple and complex augmentations with separate batch normalization layers during training. We further utilize the popular Jensen-Shannon divergence loss to encourage the {\textbackslash}emph\{joint\} learning of the {\textbackslash}emph\{diverse augmentations\}, thereby allowing simple augmentations to guide the learning of complex ones. Lastly, to improve the computational efficiency of the proposed method, we propose and utilize a two-step defense, Ascending Constraint Adversarial Training (ACAT), that uses an increasing epsilon schedule and weight-space smoothing to prevent gradient masking. The proposed method DAJAT achieves substantially better robustness-accuracy trade-off when compared to existing methods on the RobustBench Leaderboard on ResNet-18 and WideResNet-34-10. The code for implementing DAJAT is available here: https://github.com/val-iisc/DAJAT},
	language = {en},
	urldate = {2023-03-25},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Addepalli, Sravanti and Jain, Samyak and Radhakrishnan, Venkatesh Babu},
	month = oct,
	year = {2022},
}

@article{mohamed_monte_2020,
	title = {Monte {Carlo} gradient estimation in machine learning},
	volume = {21},
	issn = {1532-4435},
	abstract = {This paper is a broad and accessible survey of the methods we have at our disposal for Monte Carlo gradient estimation in machine learning and across the statistical sciences: the problem of computing the gradient of an expectation of a function with respect to parameters defining the distribution that is integrated; the problem of sensitivity analysis. In machine learning research, this gradient problem lies at the core of many learning problems, in supervised, unsupervised and reinforcement learning. We will generally seek to rewrite such gradients in a form that allows for Monte Carlo estimation, allowing them to be easily and efficiently used and analysed. We explore three strategies--the pathwise, score function, and measure-valued gradient estimators-- exploring their historical development, derivation, and underlying assumptions. We describe their use in other fields, show how they are related and can be combined, and expand on their possible generalisations. Wherever Monte Carlo gradient estimators have been derived and deployed in the past, important advances have followed. A deeper and more widely-held understanding of this problem will lead to further advances, and it is these advances that we wish to support.},
	number = {1},
	journal = {The Journal of Machine Learning Research},
	author = {Mohamed, Shakir and Rosca, Mihaela and Figurnov, Michael and Mnih, Andriy},
	month = jan,
	year = {2020},
	keywords = {Monte Carlo, gradient estimation, measure-valued estimator, pathwise estimator, score-function estimator, sensitivity analysis, variance reduction},
	pages = {132:5183--132:5244},
}

@misc{bengio_estimating_2013,
	title = {Estimating or {Propagating} {Gradients} {Through} {Stochastic} {Neurons} for {Conditional} {Computation}},
	url = {http://arxiv.org/abs/1308.3432},
	doi = {10.48550/arXiv.1308.3432},
	abstract = {Stochastic neurons and hard non-linearities can be useful for a number of reasons in deep learning models, but in many cases they pose a challenging problem: how to estimate the gradient of a loss function with respect to the input of such stochastic or non-smooth neurons? I.e., can we "back-propagate" through these stochastic neurons? We examine this question, existing approaches, and compare four families of solutions, applicable in different settings. One of them is the minimum variance unbiased gradient estimator for stochatic binary neurons (a special case of the REINFORCE algorithm). A second approach, introduced here, decomposes the operation of a binary stochastic neuron into a stochastic binary part and a smooth differentiable part, which approximates the expected effect of the pure stochatic binary neuron to first order. A third approach involves the injection of additive or multiplicative noise in a computational graph that is otherwise differentiable. A fourth approach heuristically copies the gradient with respect to the stochastic output directly as an estimator of the gradient with respect to the sigmoid argument (we call this the straight-through estimator). To explore a context where these estimators are useful, we consider a small-scale version of \{{\textbackslash}em conditional computation\}, where sparse stochastic units form a distributed representation of gaters that can turn off in combinatorially many ways large chunks of the computation performed in the rest of the neural network. In this case, it is important that the gating units produce an actual 0 most of the time. The resulting sparsity can be potentially be exploited to greatly reduce the computational cost of large deep networks for which conditional computation would be useful.},
	urldate = {2023-03-20},
	publisher = {arXiv},
	author = {Bengio, Yoshua and Léonard, Nicholas and Courville, Aaron},
	month = aug,
	year = {2013},
	note = {arXiv:1308.3432 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@inproceedings{wang_adversarial_2021,
	title = {Adversarial {GLUE}: {A} {Multi}-{Task} {Benchmark} for {Robustness} {Evaluation} of {Language} {Models}},
	volume = {1},
	shorttitle = {Adversarial {GLUE}},
	url = {https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/335f5352088d7d9bf74191e006d8e24c-Abstract-round2.html},
	language = {en},
	urldate = {2023-03-20},
	booktitle = {Proceedings of the {Neural} {Information} {Processing} {Systems} {Track} on {Datasets} and {Benchmarks}},
	author = {Wang, Boxin and Xu, Chejian and Wang, Shuohang and Gan, Zhe and Cheng, Yu and Gao, Jianfeng and Awadallah, Ahmed and Li, Bo},
	month = dec,
	year = {2021},
}

@inproceedings{singh_flava_2022,
	title = {{FLAVA}: {A} {Foundational} {Language} and {Vision} {Alignment} {Model}},
	shorttitle = {{FLAVA}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Singh_FLAVA_A_Foundational_Language_and_Vision_Alignment_Model_CVPR_2022_paper.html},
	language = {en},
	urldate = {2023-02-19},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Singh, Amanpreet and Hu, Ronghang and Goswami, Vedanuj and Couairon, Guillaume and Galuba, Wojciech and Rohrbach, Marcus and Kiela, Douwe},
	year = {2022},
	pages = {15638--15650},
}

@inproceedings{bai_are_2021,
	title = {Are {Transformers} more robust than {CNNs}?},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper/2021/hash/e19347e1c3ca0c0b97de5fb3b690855a-Abstract.html},
	abstract = {Transformer emerges as a powerful tool for visual recognition. In addition to demonstrating competitive performance on a broad range of visual benchmarks,  recent works also argue that Transformers are much more robust than Convolutions Neural Networks (CNNs). Nonetheless, surprisingly, we find these conclusions are drawn from unfair experimental settings, where Transformers and CNNs are compared at different scales and are applied with distinct training frameworks. In this paper, we aim to provide the first fair \& in-depth comparisons between Transformers and CNNs, focusing on robustness evaluations. With our unified training setup, we first challenge the previous belief that Transformers outshine CNNs when measuring adversarial robustness. More surprisingly, we find CNNs can easily be as robust as Transformers on defending against adversarial attacks, if they properly adopt Transformers' training recipes. While regarding generalization on out-of-distribution samples, we show pre-training on (external) large-scale datasets is not a fundamental request for enabling Transformers to achieve better performance than CNNs. Moreover, our ablations suggest such stronger generalization is largely benefited by the Transformer's self-attention-like architectures per se, rather than by other training setups. We hope this work can help the community better understand and benchmark the robustness of Transformers and CNNs. The code and models are publicly available at: https://github.com/ytongbai/ViTs-vs-CNNs.},
	urldate = {2023-03-18},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Bai, Yutong and Mei, Jieru and Yuille, Alan L and Xie, Cihang},
	year = {2021},
	pages = {26831--26843},
}

@misc{wang_better_2023,
	title = {Better {Diffusion} {Models} {Further} {Improve} {Adversarial} {Training}},
	url = {http://arxiv.org/abs/2302.04638},
	doi = {10.48550/arXiv.2302.04638},
	abstract = {It has been recognized that the data generated by the denoising diffusion probabilistic model (DDPM) improves adversarial training. After two years of rapid development in diffusion models, a question naturally arises: can better diffusion models further improve adversarial training? This paper gives an affirmative answer by employing the most recent diffusion model which has higher efficiency (\${\textbackslash}sim 20\$ sampling steps) and image quality (lower FID score) compared with DDPM. Our adversarially trained models achieve state-of-the-art performance on RobustBench using only generated data (no external datasets). Under the \${\textbackslash}ell\_{\textbackslash}infty\$-norm threat model with \${\textbackslash}epsilon=8/255\$, our models achieve \$70.69{\textbackslash}\%\$ and \$42.67{\textbackslash}\%\$ robust accuracy on CIFAR-10 and CIFAR-100, respectively, i.e. improving upon previous state-of-the-art models by \$+4.58{\textbackslash}\%\$ and \$+8.03{\textbackslash}\%\$. Under the \${\textbackslash}ell\_2\$-norm threat model with \${\textbackslash}epsilon=128/255\$, our models achieve \$84.86{\textbackslash}\%\$ on CIFAR-10 (\$+4.44{\textbackslash}\%\$). These results also beat previous works that use external data. Our code is available at https://github.com/wzekai99/DM-Improves-AT.},
	urldate = {2023-03-15},
	publisher = {arXiv},
	author = {Wang, Zekai and Pang, Tianyu and Du, Chao and Lin, Min and Liu, Weiwei and Yan, Shuicheng},
	month = feb,
	year = {2023},
	note = {arXiv:2302.04638 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@misc{singh_revisiting_2023,
	title = {Revisiting {Adversarial} {Training} for {ImageNet}: {Architectures}, {Training} and {Generalization} across {Threat} {Models}},
	shorttitle = {Revisiting {Adversarial} {Training} for {ImageNet}},
	url = {http://arxiv.org/abs/2303.01870},
	doi = {10.48550/arXiv.2303.01870},
	abstract = {While adversarial training has been extensively studied for ResNet architectures and low resolution datasets like CIFAR, much less is known for ImageNet. Given the recent debate about whether transformers are more robust than convnets, we revisit adversarial training on ImageNet comparing ViTs and ConvNeXts. Extensive experiments show that minor changes in architecture, most notably replacing PatchStem with ConvStem, and training scheme have a significant impact on the achieved robustness. These changes not only increase robustness in the seen \${\textbackslash}ell\_{\textbackslash}infty\$-threat model, but even more so improve generalization to unseen \${\textbackslash}ell\_1/{\textbackslash}ell\_2\$-robustness. Our modified ConvNeXt, ConvNeXt + ConvStem, yields the most robust models across different ranges of model parameters and FLOPs.},
	urldate = {2023-03-14},
	publisher = {arXiv},
	author = {Singh, Naman D. and Croce, Francesco and Hein, Matthias},
	month = mar,
	year = {2023},
	note = {arXiv:2303.01870 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@inproceedings{yu_adversarial_2022,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Adversarial {Contrastive} {Learning} via {Asymmetric} {InfoNCE}},
	isbn = {978-3-031-20065-6},
	doi = {10.1007/978-3-031-20065-6_4},
	abstract = {Contrastive learning (CL) has recently been applied to adversarial learning tasks. Such practice considers adversarial samples as additional positive views of an instance, and by maximizing their agreements with each other, yields better adversarial robustness. However, this mechanism can be potentially flawed, since adversarial perturbations may cause instance-level identity confusion, which can impede CL performance by pulling together different instances with separate identities. To address this issue, we propose to treat adversarial samples unequally when contrasted, with an asymmetric InfoNCE objective (A-InfoNCE) that allows discriminating considerations of adversarial samples. Specifically, adversaries are viewed as inferior positives that induce weaker learning signals, or as hard negatives exhibiting higher contrast to other negative samples. In the asymmetric fashion, the adverse impacts of conflicting objectives between CL and adversarial learning can be effectively mitigated. Experiments show that our approach consistently outperforms existing Adversarial CL methods across different finetuning schemes without additional computational cost. The proposed A-InfoNCE is also a generic form that can be readily extended to other CL methods. Code is available at https://github.com/yqy2001/A-InfoNCE.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2022},
	publisher = {Springer Nature Switzerland},
	author = {Yu, Qiying and Lou, Jieming and Zhan, Xianyuan and Li, Qizhang and Zuo, Wangmeng and Liu, Yang and Liu, Jingjing},
	editor = {Avidan, Shai and Brostow, Gabriel and Cissé, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
	year = {2022},
	keywords = {Adversarial contrastive learning, Robustness, Self-supervised learning},
	pages = {53--69},
}

@article{li_adversarial_2022,
	title = {Adversarial supervised contrastive learning},
	issn = {1573-0565},
	url = {https://doi.org/10.1007/s10994-022-06269-7},
	doi = {10.1007/s10994-022-06269-7},
	abstract = {Contrastive learning is prevalently used in pre-training deep models, followed with fine-tuning in downstream tasks for better performance or faster training. However, pre-trained models from contrastive learning are barely robust against adversarial examples in downstream tasks since the representations learned by self-supervision may lack the robustness and also the class-wise discrimination. To tackle the above problems, we adapt the contrastive learning scheme to adversarial examples for robustness enhancement, and also extend the self-supervised contrastive approach to the supervised setting for the ability to discriminate on classes. Equipped with our new designs, we proposed adversarial supervised contrastive learning (ASCL), a novel framework for robust pre-training. Despite its simplicity, extensive experiments show that ASCL achieves significant margins in adversarial robustness over the prior arts, proceeding towards either the lightweight standard fine-tuning or adversarial fine-tuning. Moreover, ASCL also shows benefits for robustness to diverse natural corruptions, suggesting the wide applicability to all sorts of practical scenarios. Notably, ASCL demonstrate impressive results in robust transfer learning.},
	language = {en},
	urldate = {2023-03-12},
	journal = {Machine Learning},
	author = {Li, Zhuorong and Yu, Daiwei and Wu, Minghui and Jin, Canghong and Yu, Hongchuan},
	month = nov,
	year = {2022},
	keywords = {Adversarial attack, Adversarial robustness, Consistency regularization, Contrastive learning, Self-supervised learning},
}

@inproceedings{zhang_decoupled_2022,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Decoupled {Adversarial} {Contrastive} {Learning} for {Self}-supervised {Adversarial} {Robustness}},
	isbn = {978-3-031-20056-4},
	doi = {10.1007/978-3-031-20056-4_42},
	abstract = {Adversarial training (AT) for robust representation learning and self-supervised learning (SSL) for unsupervised representation learning are two active research fields. Integrating AT into SSL, multiple prior works have accomplished a highly significant yet challenging task: learning robust representation without labels. A widely used framework is adversarial contrastive learning which couples AT and SSL, and thus constitutes a very complex optimization problem. Inspired by the divide-and-conquer philosophy, we conjecture that it might be simplified as well as improved by solving two sub-problems: non-robust SSL and pseudo-supervised AT. This motivation shifts the focus of the task from seeking an optimal integrating strategy for a coupled problem to finding sub-solutions for sub-problems. With this said, this work discards prior practices of directly introducing AT to SSL frameworks and proposed a two-stage framework termed Decoupled Adversarial Contrastive Learning (DeACL). Extensive experimental results demonstrate that our DeACL achieves SOTA self-supervised adversarial robustness while significantly reducing the training time, which validates its effectiveness and efficiency. Moreover, our DeACL constitutes a more explainable solution, and its success also bridges the gap with semi-supervised AT for exploiting unlabeled samples for robust representation learning. The code is publicly accessible at https://github.com/pantheon5100/DeACL.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2022},
	publisher = {Springer Nature Switzerland},
	author = {Zhang, Chaoning and Zhang, Kang and Zhang, Chenshuang and Niu, Axi and Feng, Jiu and Yoo, Chang D. and Kweon, In So},
	editor = {Avidan, Shai and Brostow, Gabriel and Cissé, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
	year = {2022},
	keywords = {Adversarial contrastive learning, Adversarial robustness, Adversarial training, Self-supervised learning},
	pages = {725--742},
}

@article{zhou_metaaugment_2021,
	title = {{MetaAugment}: {Sample}-{Aware} {Data} {Augmentation} {Policy} {Learning}},
	volume = {35},
	issn = {2374-3468, 2159-5399},
	shorttitle = {{MetaAugment}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/17324},
	doi = {10.1609/aaai.v35i12.17324},
	abstract = {Automated data augmentation has shown superior performance in image recognition. Existing works search for datasetlevel augmentation policies without considering individual sample variations, which are likely to be sub-optimal. On the other hand, learning different policies for different samples naively could greatly increase the computing cost. In this paper, we learn a sample-aware data augmentation policy efﬁciently by formulating it as a sample reweighting problem. Speciﬁcally, an augmentation policy network takes a transformation and the corresponding augmented image as inputs, and outputs a weight to adjust the augmented image loss computed by a task network. At training stage, the task network minimizes the weighted losses of augmented training images, while the policy network minimizes the loss of the task network on a validation set via meta-learning. We theoretically prove the convergence of the training procedure and further derive the exact convergence rate. Superior performance is achieved on widely-used benchmarks including CIFAR-10/100, Omniglot, and ImageNet.},
	language = {en},
	number = {12},
	urldate = {2023-03-12},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Zhou, Fengwei and Li, Jiawei and Xie, Chuanlong and Chen, Fei and Hong, Lanqing and Sun, Rui and Li, Zhenguo},
	month = may,
	year = {2021},
	pages = {11097--11105},
}

@misc{ghofrani_role_2023,
	title = {On the {Role} of {Contrastive} {Representation} {Learning} in {Adversarial} {Robustness}: {An} {Empirical} {Study}},
	shorttitle = {On the {Role} of {Contrastive} {Representation} {Learning} in {Adversarial} {Robustness}},
	url = {http://arxiv.org/abs/2302.02502},
	doi = {10.48550/arXiv.2302.02502},
	abstract = {Self-supervised contrastive learning has solved one of the significant obstacles in deep learning by alleviating the annotation cost. This advantage comes with the price of false negative-pair selection without any label information. Supervised contrastive learning has emerged as an extension of contrastive learning to eliminate this issue. However, aside from accuracy, there is a lack of understanding about the impacts of adversarial training on the representations learned by these learning schemes. In this work, we utilize supervised learning as a baseline to comprehensively study the robustness of contrastive and supervised contrastive learning under different adversarial training scenarios. Then, we begin by looking at how adversarial training affects the learned representations in hidden layers, discovering more redundant representations between layers of the model. Our results on CIFAR-10 and CIFAR-100 image classification benchmarks demonstrate that this redundancy is highly reduced by adversarial fine-tuning applied to the contrastive learning scheme, leading to more robust representations. However, adversarial fine-tuning is not very effective for supervised contrastive learning and supervised learning schemes. Our code is released at https://github.com/softsys4ai/CL-Robustness.},
	urldate = {2023-03-12},
	publisher = {arXiv},
	author = {Ghofrani, Fatemeh and Yaghouti, Mehdi and Jamshidi, Pooyan},
	month = feb,
	year = {2023},
	note = {arXiv:2302.02502 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{driess_palm-e_2023,
	title = {{PaLM}-{E}: {An} {Embodied} {Multimodal} {Language} {Model}},
	shorttitle = {{PaLM}-{E}},
	url = {http://arxiv.org/abs/2303.03378},
	doi = {10.48550/arXiv.2303.03378},
	abstract = {Large language models excel at a wide range of complex tasks. However, enabling general inference in the real world, e.g., for robotics problems, raises the challenge of grounding. We propose embodied language models to directly incorporate real-world continuous sensor modalities into language models and thereby establish the link between words and percepts. Input to our embodied language model are multi-modal sentences that interleave visual, continuous state estimation, and textual input encodings. We train these encodings end-to-end, in conjunction with a pre-trained large language model, for multiple embodied tasks including sequential robotic manipulation planning, visual question answering, and captioning. Our evaluations show that PaLM-E, a single large embodied multimodal model, can address a variety of embodied reasoning tasks, from a variety of observation modalities, on multiple embodiments, and further, exhibits positive transfer: the model benefits from diverse joint training across internet-scale language, vision, and visual-language domains. Our largest model, PaLM-E-562B with 562B parameters, in addition to being trained on robotics tasks, is a visual-language generalist with state-of-the-art performance on OK-VQA, and retains generalist language capabilities with increasing scale.},
	urldate = {2023-03-09},
	publisher = {arXiv},
	author = {Driess, Danny and Xia, Fei and Sajjadi, Mehdi S. M. and Lynch, Corey and Chowdhery, Aakanksha and Ichter, Brian and Wahid, Ayzaan and Tompson, Jonathan and Vuong, Quan and Yu, Tianhe and Huang, Wenlong and Chebotar, Yevgen and Sermanet, Pierre and Duckworth, Daniel and Levine, Sergey and Vanhoucke, Vincent and Hausman, Karol and Toussaint, Marc and Greff, Klaus and Zeng, Andy and Mordatch, Igor and Florence, Pete},
	month = mar,
	year = {2023},
	note = {arXiv:2303.03378 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{wu_visual_2023,
	title = {Visual {ChatGPT}: {Talking}, {Drawing} and {Editing} with {Visual} {Foundation} {Models}},
	shorttitle = {Visual {ChatGPT}},
	url = {http://arxiv.org/abs/2303.04671},
	doi = {10.48550/arXiv.2303.04671},
	abstract = {ChatGPT is attracting a cross-field interest as it provides a language interface with remarkable conversational competency and reasoning capabilities across many domains. However, since ChatGPT is trained with languages, it is currently not capable of processing or generating images from the visual world. At the same time, Visual Foundation Models, such as Visual Transformers or Stable Diffusion, although showing great visual understanding and generation capabilities, they are only experts on specific tasks with one-round fixed inputs and outputs. To this end, We build a system called {\textbackslash}textbf\{Visual ChatGPT\}, incorporating different Visual Foundation Models, to enable the user to interact with ChatGPT by 1) sending and receiving not only languages but also images 2) providing complex visual questions or visual editing instructions that require the collaboration of multiple AI models with multi-steps. 3) providing feedback and asking for corrected results. We design a series of prompts to inject the visual model information into ChatGPT, considering models of multiple inputs/outputs and models that require visual feedback. Experiments show that Visual ChatGPT opens the door to investigating the visual roles of ChatGPT with the help of Visual Foundation Models. Our system is publicly available at {\textbackslash}url\{https://github.com/microsoft/visual-chatgpt\}.},
	urldate = {2023-03-09},
	publisher = {arXiv},
	author = {Wu, Chenfei and Yin, Shengming and Qi, Weizhen and Wang, Xiaodong and Tang, Zecheng and Duan, Nan},
	month = mar,
	year = {2023},
	note = {arXiv:2303.04671 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{gowal_self-supervised_2021,
	title = {Self-supervised {Adversarial} {Robustness} for the {Low}-label, {High}-data {Regime}},
	url = {https://openreview.net/forum?id=bgQek2O63w},
	abstract = {Recent work discovered that training models to be invariant to adversarial perturbations requires substantially larger datasets than those required for standard classification. Perhaps more surprisingly, these larger datasets can be "mostly" unlabeled. Pseudo-labeling, a technique simultaneously pioneered by four separate and simultaneous works in 2019, has been proposed as a competitive alternative to labeled data for training adversarially robust models. However, when the amount of labeled data decreases, the performance of pseudo-labeling catastrophically drops, thus questioning the theoretical insights put forward by Uesato et al. (2019), which suggest that the sample complexity for learning an adversarially robust model from unlabeled data should match the fully supervised case. We introduce Bootstrap Your Own Robust Latents (BYORL), a self-supervised learning technique based on BYOL for training adversarially robust models. Our method enables us to train robust representations without any labels (reconciling practice with theory). Most notably, this robust representation can be leveraged by a linear classifier to train adversarially robust models, even when the linear classifier is not trained adversarially. We evaluate BYORL and pseudo-labeling on CIFAR-10 and ImageNet and demonstrate that BYORL achieves significantly higher robustness (i.e., models resulting from BYORL are up to two times more accurate). Experiments on CIFAR-10 against \${\textbackslash}ell\_2\$ and \${\textbackslash}ell\_{\textbackslash}infty\$ norm-bounded perturbations demonstrate that BYORL achieves near state-of-the-art robustness with as little as 500 labeled examples. We also note that against \${\textbackslash}ell\_2\$ norm-bounded perturbations of size \${\textbackslash}epsilon = 128/255\$, BYORL surpasses the known state-of-the-art with an accuracy under attack of 77.61\% (against 72.91\% for the prior art).},
	language = {en},
	urldate = {2023-03-07},
	author = {Gowal, Sven and Huang, Po-Sen and Oord, Aaron van den and Mann, Timothy and Kohli, Pushmeet},
	month = mar,
	year = {2021},
}

@inproceedings{kim_adversarial_2020,
	title = {Adversarial {Self}-{Supervised} {Contrastive} {Learning}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/1f1baa5b8edac74eb4eaa329f14a0361-Abstract.html},
	abstract = {Existing adversarial learning approaches mostly use class labels to generate adversarial samples that lead to incorrect predictions, which are then used to augment the training of the model for improved robustness. While some recent works propose semi-supervised adversarial learning methods that utilize unlabeled data, they still require class labels. However, do we really need class labels at all, for adversarially robust training of deep neural networks? In this paper, we propose a novel adversarial attack for unlabeled data, which makes the model confuse the instance-level identities of the perturbed data samples. Further, we present a self-supervised contrastive learning framework to adversarially train a robust neural network without labeled data, which aims to maximize the similarity between a random augmentation of a data sample and its instance-wise adversarial perturbation. We validate our method, Robust Contrastive Learning (RoCL), on multiple benchmark datasets, on which it obtains comparable robust accuracy over state-of-the-art supervised adversarial learning methods, and significantly improved robustness against the {\textbackslash}emph\{black box\} and unseen types of attacks. Moreover, with further joint fine-tuning with supervised adversarial loss, RoCL obtains even higher robust accuracy over using self-supervised learning alone. Notably, RoCL also demonstrate impressive results in robust transfer learning.},
	urldate = {2023-03-07},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Kim, Minseon and Tack, Jihoon and Hwang, Sung Ju},
	year = {2020},
	pages = {2983--2994},
}

@inproceedings{rombach_high-resolution_2022,
	title = {High-{Resolution} {Image} {Synthesis} {With} {Latent} {Diffusion} {Models}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.html},
	language = {en},
	urldate = {2023-03-07},
	author = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Björn},
	year = {2022},
	pages = {10684--10695},
}

@article{yu_scaling_2022,
	title = {Scaling {Autoregressive} {Models} for {Content}-{Rich} {Text}-to-{Image} {Generation}},
	issn = {2835-8856},
	url = {https://openreview.net/forum?id=AFDcYJKhND},
	abstract = {We present the Pathways Autoregressive Text-to-Image (Parti) model, which generates high-fidelity photorealistic images and supports content-rich synthesis involving complex compositions and world knowledge. Parti treats text-to-image generation as a sequence-to-sequence modeling problem, akin to machine translation, with sequences of image tokens as the target outputs rather than text tokens in another language. This strategy can naturally tap into the rich body of prior work on large language models, which have seen continued advances in capabilities and performance through scaling data and model sizes. Our approach is simple: First, Parti uses a Transformer-based image tokenizer, ViT-VQGAN, to encode images as sequences of discrete tokens. Second, we achieve consistent quality improvements by scaling the encoder-decoder Transformer model up to 20B parameters, with a new state-of-the-art zero-shot FID score of 7.23 and finetuned FID score of 3.22 on MS-COCO. Our detailed analysis on Localized Narratives as well as PartiPrompts (P2), a new holistic benchmark of over 1600 English prompts, demonstrate the effectiveness of Parti across a wide variety of categories and difficulty aspects. We also explore and highlight limitations of our models in order to define and exemplify key areas of focus for further improvements.},
	language = {en},
	urldate = {2023-03-06},
	journal = {Transactions on Machine Learning Research},
	author = {Yu, Jiahui and Xu, Yuanzhong and Koh, Jing Yu and Luong, Thang and Baid, Gunjan and Wang, Zirui and Vasudevan, Vijay and Ku, Alexander and Yang, Yinfei and Ayan, Burcu Karagol and Hutchinson, Ben and Han, Wei and Parekh, Zarana and Li, Xin and Zhang, Han and Baldridge, Jason and Wu, Yonghui},
	month = nov,
	year = {2022},
}

@misc{ramesh_hierarchical_2022,
	title = {Hierarchical {Text}-{Conditional} {Image} {Generation} with {CLIP} {Latents}},
	url = {http://arxiv.org/abs/2204.06125},
	doi = {10.48550/arXiv.2204.06125},
	abstract = {Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.},
	urldate = {2023-03-06},
	publisher = {arXiv},
	author = {Ramesh, Aditya and Dhariwal, Prafulla and Nichol, Alex and Chu, Casey and Chen, Mark},
	month = apr,
	year = {2022},
	note = {arXiv:2204.06125 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{nichol_glide_2022,
	title = {{GLIDE}: {Towards} {Photorealistic} {Image} {Generation} and {Editing} with {Text}-{Guided} {Diffusion} {Models}},
	shorttitle = {{GLIDE}},
	url = {https://proceedings.mlr.press/v162/nichol22a.html},
	abstract = {Diffusion models have recently been shown to generate high-quality synthetic images, especially when paired with a guidance technique to trade off diversity for fidelity. We explore diffusion models for the problem of text-conditional image synthesis and compare two different guidance strategies: CLIP guidance and classifier-free guidance. We find that the latter is preferred by human evaluators for both photorealism and caption similarity, and often produces photorealistic samples. Samples from a 3.5 billion parameter text-conditional diffusion model using classifier-free guidance are favored by human evaluators to those from DALL-E, even when the latter uses expensive CLIP reranking. Additionally, we find that our models can be fine-tuned to perform image inpainting, enabling powerful text-driven image editing. We train a smaller model on a filtered dataset and release the code and weights at https://github.com/openai/glide-text2im.},
	language = {en},
	urldate = {2023-03-06},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Nichol, Alexander Quinn and Dhariwal, Prafulla and Ramesh, Aditya and Shyam, Pranav and Mishkin, Pamela and Mcgrew, Bob and Sutskever, Ilya and Chen, Mark},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {16784--16804},
}

@misc{pham_combined_2022,
	title = {Combined {Scaling} for {Open}-{Vocabulary} {Image} {Classification}},
	url = {http://arxiv.org/abs/2111.10050},
	doi = {10.48550/arXiv.2111.10050},
	abstract = {We present a combined scaling method - named BASIC - that achieves 85.7\% top-1 accuracy on the ImageNet ILSVRC-2012 validation set without learning from any labeled ImageNet example. This accuracy surpasses best published similar models - CLIP and ALIGN - by 9.3\%. Our BASIC model also shows significant improvements in robustness benchmarks. For instance, on 5 test sets with natural distribution shifts such as ImageNet-\{A,R,V2,Sketch\} and ObjectNet, our model achieves 84.3\% top-1 average accuracy, only a small drop from its original ImageNet accuracy. To achieve these results, we scale up the contrastive learning framework of CLIP and ALIGN in three dimensions: data size, model size, and batch size. Our dataset has 6.6B noisy image-text pairs, which is 4x larger than ALIGN, and 16x larger than CLIP. Our largest model has 3B weights, which is 3.75x larger in parameters and 8x larger in FLOPs than ALIGN and CLIP. Finally, our batch size is 65536 which is 2x more than CLIP and 4x more than ALIGN. We encountered two main challenges with the scaling rules of BASIC. First, the main challenge with implementing the combined scaling rules of BASIC is the limited memory of accelerators, such as GPUs and TPUs. To overcome the memory limit, we propose two simple methods which make use of gradient checkpointing and model parallelism. Second, while increasing the dataset size and the model size has been the defacto method to improve the performance of deep learning models like BASIC, the effect of a large contrastive batch size on such contrastive-trained image-text models is not well-understood. To shed light on the benefits of large contrastive batch sizes, we develop a theoretical framework which shows that larger contrastive batch sizes lead to smaller generalization gaps for image-text models such as BASIC.},
	urldate = {2023-03-06},
	publisher = {arXiv},
	author = {Pham, Hieu and Dai, Zihang and Ghiasi, Golnaz and Kawaguchi, Kenji and Liu, Hanxiao and Yu, Adams Wei and Yu, Jiahui and Chen, Yi-Ting and Luong, Minh-Thang and Wu, Yonghui and Tan, Mingxing and Le, Quoc V.},
	month = apr,
	year = {2022},
	note = {arXiv:2111.10050 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{yu_coca_2022,
	title = {{CoCa}: {Contrastive} {Captioners} are {Image}-{Text} {Foundation} {Models}},
	issn = {2835-8856},
	shorttitle = {{CoCa}},
	url = {https://openreview.net/forum?id=Ee277P3AYC},
	abstract = {Exploring large-scale pretrained foundation models is of significant interest in computer vision because these models can be quickly transferred to many downstream tasks. This paper presents Contrastive Captioner (CoCa), a minimalist design to pretrain an image-text encoder-decoder foundation model jointly with contrastive loss and captioning loss, thereby subsuming model capabilities from contrastive approaches like CLIP and generative methods like SimVLM. In contrast to standard encoder-decoder transformers where all decoder layers attend to encoder outputs, CoCa omits cross-attention in the first half of decoder layers to encode unimodal text representations, and cascades the remaining decoder layers which cross-attend to the image encoder for multimodal image-text representations. We apply a contrastive loss between unimodal image and text embeddings, in addition to a captioning loss on the multimodal decoder outputs which predicts text tokens autoregressively. By sharing the same computational graph, the two training objectives are computed efficiently with minimal overhead. CoCa is pretrained end-to-end and from scratch on both web-scale alt-text data and annotated images by treating all labels simply as text, seamlessly unifying natural language supervision for representation learning. Empirically, CoCa achieves state-of-the-art performance with zero-shot transfer or minimal task-specific adaptation on a broad range of downstream tasks, spanning visual recognition (ImageNet, Kinetics-400/600/700, Moments-in-Time), crossmodal retrieval (MSCOCO, Flickr30K, MSR-VTT), multimodal understanding (VQA, SNLI-VE, NLVR2), and image captioning (MSCOCO, NoCaps). Notably on ImageNet classification, CoCa obtains 86.3\% zero-shot top-1 accuracy, 90.6\% with a frozen encoder and learned classification head, and 91.0\% with a finetuned encoder.},
	language = {en},
	urldate = {2023-03-06},
	journal = {Transactions on Machine Learning Research},
	author = {Yu, Jiahui and Wang, Zirui and Vasudevan, Vijay and Yeung, Legg and Seyedhosseini, Mojtaba and Wu, Yonghui},
	month = aug,
	year = {2022},
}

@inproceedings{desai_redcaps_2022,
	title = {{RedCaps}: {Web}-curated image-text data created by the people, for the people},
	shorttitle = {{RedCaps}},
	url = {https://openreview.net/forum?id=VjJxBi1p9zh},
	abstract = {Large datasets of paired images and text have become increasingly popular for learning generic representations for vision and vision-and-language tasks. Such datasets have been built by querying search engines or collecting HTML alt-text – since web data is noisy, they require complex filtering pipelines to maintain quality. We explore alternate data sources to collect high quality data with minimal filtering. We introduce RedCaps – a large-scale dataset of 12M image-text pairs collected from Reddit. Images and captions from Reddit depict and describe a wide variety of objects and scenes. We collect data from a manually curated set of subreddits, which give coarse image labels and allow us to steer the dataset composition without labeling individual instances. We show that captioning models trained on RedCaps produce rich and varied captions preferred by humans, and learn visual representations that transfer to many downstream tasks.},
	language = {en},
	urldate = {2023-03-06},
	author = {Desai, Karan and Kaul, Gaurav and Aysola, Zubin Trivadi and Johnson, Justin},
	month = jan,
	year = {2022},
}

@inproceedings{srinivasan_wit_2021,
	address = {New York, NY, USA},
	series = {{SIGIR} '21},
	title = {{WIT}: {Wikipedia}-based {Image} {Text} {Dataset} for {Multimodal} {Multilingual} {Machine} {Learning}},
	isbn = {978-1-4503-8037-9},
	shorttitle = {{WIT}},
	url = {https://doi.org/10.1145/3404835.3463257},
	doi = {10.1145/3404835.3463257},
	abstract = {The milestone improvements brought about by deep representation learning and pre-training techniques have led to large performance gains across downstream NLP, IR and Vision tasks. Multimodal modeling techniques aim to leverage large high-quality visio-linguistic datasets for learning complementary information across image and text modalities. In this paper, we introduce the Wikipedia-based Image Text (WIT) Dataset to better facilitate multimodal, multilingual learning. WIT is composed of a curated set of 37.5 million entity rich image-text examples with 11.5 million unique images across 108 Wikipedia languages. Its size enables WIT to be used as a pretraining dataset for multimodal models, as we show when applied to downstream tasks such as image-text retrieval. WIT has four main and unique advantages. First, WIT is the largest multimodal dataset by the number of image-text examples by 3x (at the time of writing). Second, WIT is massively multilingual (first of its kind) with coverage over 100+ languages (each of which has at least 12K examples) and provides cross-lingual texts for many images. Third, WIT represents a more diverse set of concepts and real world entities relative to what previous datasets cover. Lastly, WIT provides a very challenging real-world test set, as we empirically illustrate using an image-text retrieval task as an example. WIT Dataset is available for download and use via a Creative Commons license here: https://github.com/google-research-datasets/wit.},
	urldate = {2023-03-05},
	booktitle = {Proceedings of the 44th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {Association for Computing Machinery},
	author = {Srinivasan, Krishna and Raman, Karthik and Chen, Jiecao and Bendersky, Michael and Najork, Marc},
	month = jul,
	year = {2021},
	keywords = {dataset, image-text retrieval, machine learning, multilingual, multimodal, neural networks, wikipedia},
	pages = {2443--2449},
}

@inproceedings{pont-tuset_connecting_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Connecting {Vision} and {Language} with {Localized} {Narratives}},
	isbn = {978-3-030-58558-7},
	doi = {10.1007/978-3-030-58558-7_38},
	abstract = {We propose Localized Narratives, a new form of multimodal image annotations connecting vision and language. We ask annotators to describe an image with their voice while simultaneously hovering their mouse over the region they are describing. Since the voice and the mouse pointer are synchronized, we can localize every single word in the description. This dense visual grounding takes the form of a mouse trace segment per word and is unique to our data. We annotated 849k images with Localized Narratives: the whole COCO, Flickr30k, and ADE20K datasets, and 671k images of Open Images, all of which we make publicly available. We provide an extensive analysis of these annotations showing they are diverse, accurate, and efficient to produce. We also demonstrate their utility on the application of controlled image captioning.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Pont-Tuset, Jordi and Uijlings, Jasper and Changpinyo, Soravit and Soricut, Radu and Ferrari, Vittorio},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	pages = {647--664},
}

@article{young_image_2014,
	title = {From image descriptions to visual denotations: {New} similarity metrics for semantic inference over event descriptions},
	volume = {2},
	issn = {2307-387X},
	shorttitle = {From image descriptions to visual denotations},
	url = {https://doi.org/10.1162/tacl_a_00166},
	doi = {10.1162/tacl_a_00166},
	abstract = {We propose to use the visual denotations of
linguistic expressions (i.e. the set of images they describe) to define novel denotational similarity metrics, which we show
to be at least as beneficial as distributional similarities for two tasks that
require semantic inference. To compute these denotational similarities, we
construct a denotation graph, i.e. a subsumption
hierarchy over constituents and their denotations, based on a large corpus of
30K images and 150K descriptive captions.},
	urldate = {2023-03-06},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Young, Peter and Lai, Alice and Hodosh, Micah and Hockenmaier, Julia},
	month = feb,
	year = {2014},
	pages = {67--78},
}

@article{goyal_making_2019,
	title = {Making the {V} in {VQA} {Matter}: {Elevating} the {Role} of {Image} {Understanding} in {Visual} {Question} {Answering}},
	volume = {127},
	issn = {1573-1405},
	shorttitle = {Making the {V} in {VQA} {Matter}},
	url = {https://doi.org/10.1007/s11263-018-1116-0},
	doi = {10.1007/s11263-018-1116-0},
	abstract = {The problem of visual question answering (VQA) is of significant importance both as a challenging research question and for the rich set of applications it enables. In this context, however, inherent structure in our world and bias in our language tend to be a simpler signal for learning than visual modalities, resulting in VQA models that ignore visual information, leading to an inflated sense of their capability. We propose to counter these language priors for the task of VQA and make vision (the V in VQA) matter! Specifically, we balance the popular VQA dataset (Antol et al., in: ICCV, 2015) by collecting complementary images such that every question in our balanced dataset is associated with not just a single image, but rather a pair of similar images that result in two different answers to the question. Our dataset is by construction more balanced than the original VQA dataset and has approximately twice the number of image-question pairs. Our complete balanced dataset is publicly available at http://visualqa.org/as part of the 2nd iteration of the VQA Dataset and Challenge (VQA v2.0). We further benchmark a number of state-of-art VQA models on our balanced dataset. All models perform significantly worse on our balanced dataset, suggesting that these models have indeed learned to exploit language priors. This finding provides the first concrete empirical evidence for what seems to be a qualitative sense among practitioners. We also present interesting insights from analysis of the participant entries in VQA Challenge 2017, organized by us on the proposed VQA v2.0 dataset. The results of the challenge were announced in the 2nd VQA Challenge Workshop at the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2017. Finally, our data collection protocol for identifying complementary images enables us to develop a novel interpretable model, which in addition to providing an answer to the given (image, question) pair, also provides a counter-example based explanation. Specifically, it identifies an image that is similar to the original image, but it believes has a different answer to the same question. This can help in building trust for machines among their users.},
	language = {en},
	number = {4},
	urldate = {2023-03-06},
	journal = {International Journal of Computer Vision},
	author = {Goyal, Yash and Khot, Tejas and Agrawal, Aishwarya and Summers-Stay, Douglas and Batra, Dhruv and Parikh, Devi},
	month = apr,
	year = {2019},
	pages = {398--414},
}

@inproceedings{ordonez_im2text_2011,
	title = {{Im2Text}: {Describing} {Images} {Using} 1 {Million} {Captioned} {Photographs}},
	volume = {24},
	shorttitle = {{Im2Text}},
	url = {https://papers.nips.cc/paper/2011/hash/5dd9db5e033da9c6fb5ba83c7a7ebea9-Abstract.html},
	abstract = {We develop and demonstrate automatic image description methods using a large captioned photo collection.  One contribution is our technique for the automatic collection of this new dataset -- performing a huge number of Flickr queries and then filtering the noisy results down to 1 million images with associated visually relevant captions.  Such a collection allows us to approach the extremely challenging problem of description generation using relatively simple non-parametric methods and produces surprisingly effective results. We also develop methods incorporating many state of the art, but fairly noisy, estimates of image content to produce even more pleasing results. Finally we introduce a new objective performance measure for image captioning.},
	urldate = {2023-03-06},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Ordonez, Vicente and Kulkarni, Girish and Berg, Tamara},
	year = {2011},
}

@inproceedings{zhu_visual7w_2016,
	title = {{Visual7W}: {Grounded} {Question} {Answering} in {Images}},
	shorttitle = {{Visual7W}},
	url = {https://openaccess.thecvf.com/content_cvpr_2016/html/Zhu_Visual7W_Grounded_Question_CVPR_2016_paper.html},
	urldate = {2023-03-06},
	author = {Zhu, Yuke and Groth, Oliver and Bernstein, Michael and Fei-Fei, Li},
	year = {2016},
	pages = {4995--5004},
}

@inproceedings{hudson_gqa_2019,
	title = {{GQA}: {A} {New} {Dataset} for {Real}-{World} {Visual} {Reasoning} and {Compositional} {Question} {Answering}},
	shorttitle = {{GQA}},
	url = {https://openaccess.thecvf.com/content_CVPR_2019/html/Hudson_GQA_A_New_Dataset_for_Real-World_Visual_Reasoning_and_Compositional_CVPR_2019_paper.html},
	urldate = {2023-03-06},
	author = {Hudson, Drew A. and Manning, Christopher D.},
	year = {2019},
	pages = {6700--6709},
}

@misc{chen_microsoft_2015,
	title = {Microsoft {COCO} {Captions}: {Data} {Collection} and {Evaluation} {Server}},
	shorttitle = {Microsoft {COCO} {Captions}},
	url = {http://arxiv.org/abs/1504.00325},
	doi = {10.48550/arXiv.1504.00325},
	abstract = {In this paper we describe the Microsoft COCO Caption dataset and evaluation server. When completed, the dataset will contain over one and a half million captions describing over 330,000 images. For the training and validation images, five independent human generated captions will be provided. To ensure consistency in evaluation of automatic caption generation algorithms, an evaluation server is used. The evaluation server receives candidate captions and scores them using several popular metrics, including BLEU, METEOR, ROUGE and CIDEr. Instructions for using the evaluation server are provided.},
	urldate = {2023-03-06},
	publisher = {arXiv},
	author = {Chen, Xinlei and Fang, Hao and Lin, Tsung-Yi and Vedantam, Ramakrishna and Gupta, Saurabh and Dollar, Piotr and Zitnick, C. Lawrence},
	month = apr,
	year = {2015},
	note = {arXiv:1504.00325 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
}

@article{krishna_visual_2017,
	title = {Visual {Genome}: {Connecting} {Language} and {Vision} {Using} {Crowdsourced} {Dense} {Image} {Annotations}},
	volume = {123},
	issn = {1573-1405},
	shorttitle = {Visual {Genome}},
	url = {https://doi.org/10.1007/s11263-016-0981-7},
	doi = {10.1007/s11263-016-0981-7},
	abstract = {Despite progress in perceptual tasks such as image classification, computers still perform poorly on cognitive tasks such as image description and question answering. Cognition is core to tasks that involve not just recognizing, but reasoning about our visual world. However, models used to tackle the rich content in images for cognitive tasks are still being trained using the same datasets designed for perceptual tasks. To achieve success at cognitive tasks, models need to understand the interactions and relationships between objects in an image. When asked “What vehicle is the person riding?”, computers will need to identify the objects in an image as well as the relationships riding(man, carriage) and pulling(horse, carriage) to answer correctly that “the person is riding a horse-drawn carriage.” In this paper, we present the Visual Genome dataset to enable the modeling of such relationships. We collect dense annotations of objects, attributes, and relationships within each image to learn these models. Specifically, our dataset contains over 108K images where each image has an average of \$\$35\$\$objects, \$\$26\$\$attributes, and \$\$21\$\$pairwise relationships between objects. We canonicalize the objects, attributes, relationships, and noun phrases in region descriptions and questions answer pairs to WordNet synsets. Together, these annotations represent the densest and largest dataset of image descriptions, objects, attributes, relationships, and question answer pairs.},
	language = {en},
	number = {1},
	urldate = {2023-03-06},
	journal = {International Journal of Computer Vision},
	author = {Krishna, Ranjay and Zhu, Yuke and Groth, Oliver and Johnson, Justin and Hata, Kenji and Kravitz, Joshua and Chen, Stephanie and Kalantidis, Yannis and Li, Li-Jia and Shamma, David A. and Bernstein, Michael S. and Fei-Fei, Li},
	month = may,
	year = {2017},
	pages = {32--73},
}

@inproceedings{sharma_conceptual_2018,
	address = {Melbourne, Australia},
	title = {Conceptual {Captions}: {A} {Cleaned}, {Hypernymed}, {Image} {Alt}-text {Dataset} {For} {Automatic} {Image} {Captioning}},
	shorttitle = {Conceptual {Captions}},
	url = {https://aclanthology.org/P18-1238},
	doi = {10.18653/v1/P18-1238},
	abstract = {We present a new dataset of image caption annotations, Conceptual Captions, which contains an order of magnitude more images than the MS-COCO dataset (Lin et al., 2014) and represents a wider variety of both images and image caption styles. We achieve this by extracting and filtering image caption annotations from billions of webpages. We also present quantitative evaluations of a number of image captioning models and show that a model architecture based on Inception-ResNetv2 (Szegedy et al., 2016) for image-feature extraction and Transformer (Vaswani et al., 2017) for sequence modeling achieves the best performance when trained on the Conceptual Captions dataset.},
	urldate = {2023-03-06},
	booktitle = {Proceedings of the 56th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Sharma, Piyush and Ding, Nan and Goodman, Sebastian and Soricut, Radu},
	month = jul,
	year = {2018},
	pages = {2556--2565},
}

@misc{li_blip-2_2023,
	title = {{BLIP}-2: {Bootstrapping} {Language}-{Image} {Pre}-training with {Frozen} {Image} {Encoders} and {Large} {Language} {Models}},
	shorttitle = {{BLIP}-2},
	url = {http://arxiv.org/abs/2301.12597},
	doi = {10.48550/arXiv.2301.12597},
	abstract = {The cost of vision-and-language pre-training has become increasingly prohibitive due to end-to-end training of large-scale models. This paper proposes BLIP-2, a generic and efficient pre-training strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models. BLIP-2 bridges the modality gap with a lightweight Querying Transformer, which is pre-trained in two stages. The first stage bootstraps vision-language representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen language model. BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods. For example, our model outperforms Flamingo80B by 8.7\% on zero-shot VQAv2 with 54x fewer trainable parameters. We also demonstrate the model's emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions.},
	urldate = {2023-03-03},
	publisher = {arXiv},
	author = {Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
	month = jan,
	year = {2023},
	note = {arXiv:2301.12597 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{huang_language_2023,
	title = {Language {Is} {Not} {All} {You} {Need}: {Aligning} {Perception} with {Language} {Models}},
	shorttitle = {Language {Is} {Not} {All} {You} {Need}},
	url = {http://arxiv.org/abs/2302.14045},
	doi = {10.48550/arXiv.2302.14045},
	abstract = {A big convergence of language, multimodal perception, action, and world modeling is a key step toward artificial general intelligence. In this work, we introduce Kosmos-1, a Multimodal Large Language Model (MLLM) that can perceive general modalities, learn in context (i.e., few-shot), and follow instructions (i.e., zero-shot). Specifically, we train Kosmos-1 from scratch on web-scale multimodal corpora, including arbitrarily interleaved text and images, image-caption pairs, and text data. We evaluate various settings, including zero-shot, few-shot, and multimodal chain-of-thought prompting, on a wide range of tasks without any gradient updates or finetuning. Experimental results show that Kosmos-1 achieves impressive performance on (i) language understanding, generation, and even OCR-free NLP (directly fed with document images), (ii) perception-language tasks, including multimodal dialogue, image captioning, visual question answering, and (iii) vision tasks, such as image recognition with descriptions (specifying classification via text instructions). We also show that MLLMs can benefit from cross-modal transfer, i.e., transfer knowledge from language to multimodal, and from multimodal to language. In addition, we introduce a dataset of Raven IQ test, which diagnoses the nonverbal reasoning capability of MLLMs.},
	urldate = {2023-03-03},
	publisher = {arXiv},
	author = {Huang, Shaohan and Dong, Li and Wang, Wenhui and Hao, Yaru and Singhal, Saksham and Ma, Shuming and Lv, Tengchao and Cui, Lei and Mohammed, Owais Khan and Patra, Barun and Liu, Qiang and Aggarwal, Kriti and Chi, Zewen and Bjorck, Johan and Chaudhary, Vishrav and Som, Subhojit and Song, Xia and Wei, Furu},
	month = mar,
	year = {2023},
	note = {arXiv:2302.14045 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{li_grounded_2022,
	title = {Grounded {Language}-{Image} {Pre}-{Training}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Li_Grounded_Language-Image_Pre-Training_CVPR_2022_paper.html},
	language = {en},
	urldate = {2023-03-03},
	author = {Li, Liunian Harold and Zhang, Pengchuan and Zhang, Haotian and Yang, Jianwei and Li, Chunyuan and Zhong, Yiwu and Wang, Lijuan and Yuan, Lu and Zhang, Lei and Hwang, Jenq-Neng and Chang, Kai-Wei and Gao, Jianfeng},
	year = {2022},
	pages = {10965--10975},
}

@inproceedings{suhr_corpus_2019,
	address = {Florence, Italy},
	title = {A {Corpus} for {Reasoning} about {Natural} {Language} {Grounded} in {Photographs}},
	url = {https://aclanthology.org/P19-1644},
	doi = {10.18653/v1/P19-1644},
	abstract = {We introduce a new dataset for joint reasoning about natural language and images, with a focus on semantic diversity, compositionality, and visual reasoning challenges. The data contains 107,292 examples of English sentences paired with web photographs. The task is to determine whether a natural language caption is true about a pair of photographs. We crowdsource the data using sets of visually rich images and a compare-and-contrast task to elicit linguistically diverse language. Qualitative analysis shows the data requires compositional joint reasoning, including about quantities, comparisons, and relations. Evaluation using state-of-the-art visual reasoning methods shows the data presents a strong challenge.},
	urldate = {2023-03-03},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Suhr, Alane and Zhou, Stephanie and Zhang, Ally and Zhang, Iris and Bai, Huajun and Artzi, Yoav},
	month = jul,
	year = {2019},
	pages = {6418--6428},
}

@inproceedings{zhu_uni-perceiver_2022,
	title = {Uni-{Perceiver}: {Pre}-{Training} {Unified} {Architecture} for {Generic} {Perception} for {Zero}-{Shot} and {Few}-{Shot} {Tasks}},
	shorttitle = {Uni-{Perceiver}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Zhu_Uni-Perceiver_Pre-Training_Unified_Architecture_for_Generic_Perception_for_Zero-Shot_and_CVPR_2022_paper.html},
	language = {en},
	urldate = {2023-03-03},
	author = {Zhu, Xizhou and Zhu, Jinguo and Li, Hao and Wu, Xiaoshi and Li, Hongsheng and Wang, Xiaohua and Dai, Jifeng},
	year = {2022},
	pages = {16804--16815},
}

@misc{su_towards_2022,
	title = {Towards {All}-in-one {Pre}-training via {Maximizing} {Multi}-modal {Mutual} {Information}},
	url = {http://arxiv.org/abs/2211.09807},
	doi = {10.48550/arXiv.2211.09807},
	abstract = {To effectively exploit the potential of large-scale models, various pre-training strategies supported by massive data from different sources are proposed, including supervised pre-training, weakly-supervised pre-training, and self-supervised pre-training. It has been proved that combining multiple pre-training strategies and data from various modalities/sources can greatly boost the training of large-scale models. However, current works adopt a multi-stage pre-training system, where the complex pipeline may increase the uncertainty and instability of the pre-training. It is thus desirable that these strategies can be integrated in a single-stage manner. In this paper, we first propose a general multi-modal mutual information formula as a unified optimization target and demonstrate that all existing approaches are special cases of our framework. Under this unified perspective, we propose an all-in-one single-stage pre-training approach, named Maximizing Multi-modal Mutual Information Pre-training (M3I Pre-training). Our approach achieves better performance than previous pre-training methods on various vision benchmarks, including ImageNet classification, COCO object detection, LVIS long-tailed object detection, and ADE20k semantic segmentation. Notably, we successfully pre-train a billion-level parameter image backbone and achieve state-of-the-art performance on various benchmarks. Code shall be released at https://github.com/OpenGVLab/M3I-Pretraining.},
	urldate = {2023-03-03},
	publisher = {arXiv},
	author = {Su, Weijie and Zhu, Xizhou and Tao, Chenxin and Lu, Lewei and Li, Bin and Huang, Gao and Qiao, Yu and Wang, Xiaogang and Zhou, Jie and Dai, Jifeng},
	month = nov,
	year = {2022},
	note = {arXiv:2211.09807 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{desai_redcaps_2022-1,
	title = {{RedCaps}: {Web}-curated image-text data created by the people, for the people},
	shorttitle = {{RedCaps}},
	url = {https://openreview.net/forum?id=VjJxBi1p9zh},
	abstract = {Large datasets of paired images and text have become increasingly popular for learning generic representations for vision and vision-and-language tasks. Such datasets have been built by querying search engines or collecting HTML alt-text – since web data is noisy, they require complex filtering pipelines to maintain quality. We explore alternate data sources to collect high quality data with minimal filtering. We introduce RedCaps – a large-scale dataset of 12M image-text pairs collected from Reddit. Images and captions from Reddit depict and describe a wide variety of objects and scenes. We collect data from a manually curated set of subreddits, which give coarse image labels and allow us to steer the dataset composition without labeling individual instances. We show that captioning models trained on RedCaps produce rich and varied captions preferred by humans, and learn visual representations that transfer to many downstream tasks.},
	language = {en},
	urldate = {2023-03-02},
	author = {Desai, Karan and Kaul, Gaurav and Aysola, Zubin Trivadi and Johnson, Justin},
	month = jan,
	year = {2022},
}

@inproceedings{srinivasan_wit_2021-1,
	address = {New York, NY, USA},
	series = {{SIGIR} '21},
	title = {{WIT}: {Wikipedia}-based {Image} {Text} {Dataset} for {Multimodal} {Multilingual} {Machine} {Learning}},
	isbn = {978-1-4503-8037-9},
	shorttitle = {{WIT}},
	url = {https://doi.org/10.1145/3404835.3463257},
	doi = {10.1145/3404835.3463257},
	abstract = {The milestone improvements brought about by deep representation learning and pre-training techniques have led to large performance gains across downstream NLP, IR and Vision tasks. Multimodal modeling techniques aim to leverage large high-quality visio-linguistic datasets for learning complementary information across image and text modalities. In this paper, we introduce the Wikipedia-based Image Text (WIT) Dataset to better facilitate multimodal, multilingual learning. WIT is composed of a curated set of 37.5 million entity rich image-text examples with 11.5 million unique images across 108 Wikipedia languages. Its size enables WIT to be used as a pretraining dataset for multimodal models, as we show when applied to downstream tasks such as image-text retrieval. WIT has four main and unique advantages. First, WIT is the largest multimodal dataset by the number of image-text examples by 3x (at the time of writing). Second, WIT is massively multilingual (first of its kind) with coverage over 100+ languages (each of which has at least 12K examples) and provides cross-lingual texts for many images. Third, WIT represents a more diverse set of concepts and real world entities relative to what previous datasets cover. Lastly, WIT provides a very challenging real-world test set, as we empirically illustrate using an image-text retrieval task as an example. WIT Dataset is available for download and use via a Creative Commons license here: https://github.com/google-research-datasets/wit.},
	urldate = {2023-03-02},
	booktitle = {Proceedings of the 44th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {Association for Computing Machinery},
	author = {Srinivasan, Krishna and Raman, Karthik and Chen, Jiecao and Bendersky, Michael and Najork, Marc},
	month = jul,
	year = {2021},
	keywords = {dataset, image-text retrieval, machine learning, multilingual, multimodal, neural networks, wikipedia},
	pages = {2443--2449},
}

@inproceedings{hjelm_learning_2019,
	title = {Learning deep representations by mutual information estimation and maximization},
	url = {https://openreview.net/forum?id=Bklr3j0cKX},
	abstract = {This work investigates unsupervised learning of representations by maximizing mutual information between an input and the output of a deep neural network encoder. Importantly, we show that structure matters: incorporating knowledge about locality in the input into the objective can significantly improve a representation's suitability for downstream tasks. We further control characteristics of the representation by matching to a prior distribution adversarially. Our method, which we call Deep InfoMax (DIM), outperforms a number of popular unsupervised learning methods and compares favorably with fully-supervised learning on several classification tasks in with some standard architectures. DIM opens new avenues for unsupervised learning of representations and is an important step towards flexible formulations of representation learning objectives for specific end-goals.},
	language = {en},
	urldate = {2023-03-01},
	author = {Hjelm, R. Devon and Fedorov, Alex and Lavoie-Marchildon, Samuel and Grewal, Karan and Bachman, Phil and Trischler, Adam and Bengio, Yoshua},
	year = {2019},
}

@inproceedings{wu_unsupervised_2018,
	title = {Unsupervised {Feature} {Learning} via {Non}-{Parametric} {Instance} {Discrimination}},
	url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Wu_Unsupervised_Feature_Learning_CVPR_2018_paper.html},
	urldate = {2023-03-01},
	author = {Wu, Zhirong and Xiong, Yuanjun and Yu, Stella X. and Lin, Dahua},
	year = {2018},
	pages = {3733--3742},
}

@inproceedings{xie_simmim_2022,
	title = {{SimMIM}: {A} {Simple} {Framework} for {Masked} {Image} {Modeling}},
	shorttitle = {{SimMIM}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Xie_SimMIM_A_Simple_Framework_for_Masked_Image_Modeling_CVPR_2022_paper.html},
	language = {en},
	urldate = {2023-02-28},
	author = {Xie, Zhenda and Zhang, Zheng and Cao, Yue and Lin, Yutong and Bao, Jianmin and Yao, Zhuliang and Dai, Qi and Hu, Han},
	year = {2022},
	pages = {9653--9663},
}

@inproceedings{huang_gpipe_2019,
	title = {{GPipe}: {Efficient} {Training} of {Giant} {Neural} {Networks} using {Pipeline} {Parallelism}},
	volume = {32},
	shorttitle = {{GPipe}},
	url = {https://proceedings.neurips.cc/paper/2019/hash/093f65e080a295f8076b1c5722a46aa2-Abstract.html},
	abstract = {Scaling up deep neural network capacity has been known as an effective approach to improving model quality for several different machine learning tasks. In many cases, increasing model capacity beyond the memory limit of a single accelerator has required developing special algorithms or infrastructure. These solutions are often architecture-specific and do not transfer to other machine learning tasks. To address the need for efficient and task-independent model parallelism, we introduce TensorPipe, a pipeline parallelism library that allows scaling any network that can be expressed as a sequence of layers.  By pipelining different sub-sequences of layers on separate accelerators, TensorPipe provides the flexibility of scaling a variety of different networks to gigantic sizes efficiently. Moreover, TensorPipe utilizes a novel batch-splitting pipelining algorithm,  resulting in almost linear speedup when a model is partitioned across multiple accelerators. We demonstrate the  advantages  of  TensorPipe  by  training  large-scale  neural  networks  on  two different tasks with distinct network architectures: (i)Image Classification: We train a 557-million-parameter AmoebaNet model and attain a top-1 accuracy of 84.4\% on ImageNet-2012, (ii)Multilingual Neural Machine Translation: We train a single 6-billion-parameter, 128-layer Transformer model on a corpus spanning over 100 languages and achieve better quality than all bilingual models.},
	urldate = {2023-02-27},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Huang, Yanping and Cheng, Youlong and Bapna, Ankur and Firat, Orhan and Chen, Dehao and Chen, Mia and Lee, HyoukJoong and Ngiam, Jiquan and Le, Quoc V and Wu, Yonghui and Chen, zhifeng},
	year = {2019},
}

@inproceedings{xie_self-training_2020,
	title = {Self-{Training} {With} {Noisy} {Student} {Improves} {ImageNet} {Classification}},
	url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Xie_Self-Training_With_Noisy_Student_Improves_ImageNet_Classification_CVPR_2020_paper.html},
	urldate = {2023-02-27},
	author = {Xie, Qizhe and Luong, Minh-Thang and Hovy, Eduard and Le, Quoc V.},
	year = {2020},
	pages = {10687--10698},
}

@inproceedings{dai_coatnet_2021,
	title = {{CoAtNet}: {Marrying} {Convolution} and {Attention} for {All} {Data} {Sizes}},
	volume = {34},
	shorttitle = {{CoAtNet}},
	url = {https://proceedings.neurips.cc/paper/2021/hash/20568692db622456cc42a2e853ca21f8-Abstract.html},
	abstract = {Transformers have attracted increasing interests in computer vision, but they still fall behind state-of-the-art convolutional networks. In this work, we show that while Transformers tend to have larger model capacity, their generalization can be worse than convolutional networks due to the lack of the right inductive bias. To effectively combine the strengths from both architectures, we present CoAtNets(pronounced "coat" nets), a family of hybrid models built from two key insights: (1) depthwise Convolution and self-Attention can be naturally unified via simple relative attention; (2) vertically stacking convolution layers and attention layers in a principled way is surprisingly effective in improving generalization, capacity and efficiency. Experiments show that our CoAtNets achieve state-of-the-art performance under different resource constraints across various datasets: Without extra data, CoAtNet achieves 86.0\% ImageNet top-1 accuracy; When pre-trained with 13M images from ImageNet-21K, our CoAtNet achieves 88.56\% top-1 accuracy, matching ViT-huge pre-trained with 300M images from JFT-300M while using 23x less data; Notably, when we further scale up CoAtNet with JFT-3B, it achieves 90.88\% top-1 accuracy on ImageNet, establishing a new state-of-the-art result.},
	urldate = {2023-02-27},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Dai, Zihang and Liu, Hanxiao and Le, Quoc V and Tan, Mingxing},
	year = {2021},
	pages = {3965--3977},
}

@misc{zhou_deepvit_2021,
	title = {{DeepViT}: {Towards} {Deeper} {Vision} {Transformer}},
	shorttitle = {{DeepViT}},
	url = {http://arxiv.org/abs/2103.11886},
	doi = {10.48550/arXiv.2103.11886},
	abstract = {Vision transformers (ViTs) have been successfully applied in image classification tasks recently. In this paper, we show that, unlike convolution neural networks (CNNs)that can be improved by stacking more convolutional layers, the performance of ViTs saturate fast when scaled to be deeper. More specifically, we empirically observe that such scaling difficulty is caused by the attention collapse issue: as the transformer goes deeper, the attention maps gradually become similar and even much the same after certain layers. In other words, the feature maps tend to be identical in the top layers of deep ViT models. This fact demonstrates that in deeper layers of ViTs, the self-attention mechanism fails to learn effective concepts for representation learning and hinders the model from getting expected performance gain. Based on above observation, we propose a simple yet effective method, named Re-attention, to re-generate the attention maps to increase their diversity at different layers with negligible computation and memory cost. The pro-posed method makes it feasible to train deeper ViT models with consistent performance improvements via minor modification to existing ViT models. Notably, when training a deep ViT model with 32 transformer blocks, the Top-1 classification accuracy can be improved by 1.6\% on ImageNet. Code is publicly available at https://github.com/zhoudaquan/dvit\_repo.},
	urldate = {2023-02-27},
	publisher = {arXiv},
	author = {Zhou, Daquan and Kang, Bingyi and Jin, Xiaojie and Yang, Linjie and Lian, Xiaochen and Jiang, Zihang and Hou, Qibin and Feng, Jiashi},
	month = apr,
	year = {2021},
	note = {arXiv:2103.11886 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{bello_revisiting_2021,
	title = {Revisiting {ResNets}: {Improved} {Training} and {Scaling} {Strategies}},
	volume = {34},
	shorttitle = {Revisiting {ResNets}},
	url = {https://proceedings.neurips.cc/paper/2021/hash/bef4d169d8bddd17d68303877a3ea945-Abstract.html},
	abstract = {Novel computer vision architectures monopolize the spotlight, but the impact of the model architecture is often conflated with simultaneous changes to training methodology and scaling strategies.Our work revisits the canonical ResNet and studies these three aspects in an effort to disentangle them. Perhaps surprisingly, we find that training and scaling strategies may matter more than architectural changes, and further, that the resulting ResNets match recent state-of-the-art models. We show that the best performing scaling strategy depends on the training regime and offer two new scaling strategies: (1) scale model depth in regimes where overfitting can occur (width scaling is preferable otherwise); (2) increase image resolution more slowly than previously recommended.Using improved training and scaling strategies, we design a family of ResNet architectures, ResNet-RS, which are 1.7x - 2.7x faster than EfficientNets on TPUs, while achieving similar accuracies on ImageNet. In a large-scale semi-supervised learning setup, ResNet-RS achieves 86.2\% top-1 ImageNet accuracy, while being 4.7x faster than EfficientNet-NoisyStudent. The training techniques improve transfer performance on a suite of downstream tasks (rivaling state-of-the-art self-supervised algorithms) and extend to video classification on Kinetics-400. We recommend practitioners use these simple revised ResNets as baselines for future research.},
	urldate = {2023-02-27},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Bello, Irwan and Fedus, William and Du, Xianzhi and Cubuk, Ekin Dogus and Srinivas, Aravind and Lin, Tsung-Yi and Shlens, Jonathon and Zoph, Barret},
	year = {2021},
	pages = {22614--22627},
}

@inproceedings{goyal_scaling_2019,
	title = {Scaling and {Benchmarking} {Self}-{Supervised} {Visual} {Representation} {Learning}},
	url = {https://openaccess.thecvf.com/content_ICCV_2019/html/Goyal_Scaling_and_Benchmarking_Self-Supervised_Visual_Representation_Learning_ICCV_2019_paper.html},
	urldate = {2023-02-27},
	author = {Goyal, Priya and Mahajan, Dhruv and Gupta, Abhinav and Misra, Ishan},
	year = {2019},
	pages = {6391--6400},
}

@inproceedings{doersch_unsupervised_2015,
	title = {Unsupervised {Visual} {Representation} {Learning} by {Context} {Prediction}},
	url = {https://www.cv-foundation.org/openaccess/content_iccv_2015/html/Doersch_Unsupervised_Visual_Representation_ICCV_2015_paper.html},
	urldate = {2023-02-27},
	author = {Doersch, Carl and Gupta, Abhinav and Efros, Alexei A.},
	year = {2015},
	pages = {1422--1430},
}

@inproceedings{caron_unsupervised_2019,
	title = {Unsupervised {Pre}-{Training} of {Image} {Features} on {Non}-{Curated} {Data}},
	url = {https://openaccess.thecvf.com/content_ICCV_2019/html/Caron_Unsupervised_Pre-Training_of_Image_Features_on_Non-Curated_Data_ICCV_2019_paper.html},
	urldate = {2023-02-27},
	author = {Caron, Mathilde and Bojanowski, Piotr and Mairal, Julien and Joulin, Armand},
	year = {2019},
	pages = {2959--2968},
}

@article{thomee_yfcc100m_2016,
	title = {{YFCC100M}: the new data in multimedia research},
	volume = {59},
	issn = {0001-0782},
	shorttitle = {{YFCC100M}},
	url = {https://doi.org/10.1145/2812802},
	doi = {10.1145/2812802},
	abstract = {This publicly available curated dataset of almost 100 million photos and videos is free and legal for all.},
	number = {2},
	urldate = {2023-02-27},
	journal = {Communications of the ACM},
	author = {Thomee, Bart and Shamma, David A. and Friedland, Gerald and Elizalde, Benjamin and Ni, Karl and Poland, Douglas and Borth, Damian and Li, Li-Jia},
	month = jan,
	year = {2016},
	pages = {64--73},
}

@inproceedings{ridnik_imagenet-21k_2021,
	title = {{ImageNet}-{21K} {Pretraining} for the {Masses}},
	url = {https://openreview.net/forum?id=Zkj_VcZ6ol},
	abstract = {ImageNet-1K serves as the primary dataset for pretraining deep learning models for computer vision tasks. ImageNet-21K dataset, which is bigger and more diverse, is used less frequently for pretraining, mainly due to its complexity, low accessibility, and underestimation of its added value. This paper aims to close this gap, and make high-quality efficient pretraining on ImageNet-21K available for everyone. Via a dedicated preprocessing stage, utilization of WordNet hierarchical structure, and a novel training scheme called semantic softmax, we show that various models significantly benefit from ImageNet-21K pretraining on numerous datasets and tasks, including small mobile-oriented models. We also show that we outperform previous ImageNet-21K pretraining schemes for prominent new models like ViT and Mixer. Our proposed pretraining pipeline is efficient, accessible, and leads to SoTA reproducible results, from a publicly available dataset. The training code and pretrained models are available at: https://github.com/Alibaba-MIIL/ImageNet21K},
	language = {en},
	urldate = {2023-02-26},
	author = {Ridnik, Tal and Ben-Baruch, Emanuel and Noy, Asaf and Zelnik-Manor, Lihi},
	month = dec,
	year = {2021},
}

@inproceedings{caron_emerging_2021,
	title = {Emerging {Properties} in {Self}-{Supervised} {Vision} {Transformers}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Caron_Emerging_Properties_in_Self-Supervised_Vision_Transformers_ICCV_2021_paper.html},
	language = {en},
	urldate = {2023-02-26},
	author = {Caron, Mathilde and Touvron, Hugo and Misra, Ishan and Jégou, Hervé and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand},
	year = {2021},
	pages = {9650--9660},
}

@misc{goyal_self-supervised_2021,
	title = {Self-supervised {Pretraining} of {Visual} {Features} in the {Wild}},
	url = {http://arxiv.org/abs/2103.01988},
	doi = {10.48550/arXiv.2103.01988},
	abstract = {Recently, self-supervised learning methods like MoCo, SimCLR, BYOL and SwAV have reduced the gap with supervised methods. These results have been achieved in a control environment, that is the highly curated ImageNet dataset. However, the premise of self-supervised learning is that it can learn from any random image and from any unbounded dataset. In this work, we explore if self-supervision lives to its expectation by training large models on random, uncurated images with no supervision. Our final SElf-supERvised (SEER) model, a RegNetY with 1.3B parameters trained on 1B random images with 512 GPUs achieves 84.2\% top-1 accuracy, surpassing the best self-supervised pretrained model by 1\% and confirming that self-supervised learning works in a real world setting. Interestingly, we also observe that self-supervised models are good few-shot learners achieving 77.9\% top-1 with access to only 10\% of ImageNet. Code: https://github.com/facebookresearch/vissl},
	urldate = {2023-02-26},
	publisher = {arXiv},
	author = {Goyal, Priya and Caron, Mathilde and Lefaudeux, Benjamin and Xu, Min and Wang, Pengchao and Pai, Vivek and Singh, Mannat and Liptchinsky, Vitaliy and Misra, Ishan and Joulin, Armand and Bojanowski, Piotr},
	month = mar,
	year = {2021},
	note = {arXiv:2103.01988 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{beal_billion-scale_2022,
	title = {Billion-{Scale} {Pretraining} {With} {Vision} {Transformers} for {Multi}-{Task} {Visual} {Representations}},
	url = {https://openaccess.thecvf.com/content/WACV2022/html/Beal_Billion-Scale_Pretraining_With_Vision_Transformers_for_Multi-Task_Visual_Representations_WACV_2022_paper.html},
	language = {en},
	urldate = {2023-02-26},
	author = {Beal, Josh and Wu, Hao-Yu and Park, Dong Huk and Zhai, Andrew and Kislyuk, Dmitry},
	year = {2022},
	pages = {564--573},
}

@inproceedings{misra_self-supervised_2020,
	title = {Self-{Supervised} {Learning} of {Pretext}-{Invariant} {Representations}},
	url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Misra_Self-Supervised_Learning_of_Pretext-Invariant_Representations_CVPR_2020_paper.html},
	urldate = {2023-02-26},
	author = {Misra, Ishan and Maaten, Laurens van der},
	year = {2020},
	pages = {6707--6717},
}

@inproceedings{caron_unsupervised_2020,
	title = {Unsupervised {Learning} of {Visual} {Features} by {Contrasting} {Cluster} {Assignments}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/70feb62b69f16e0238f741fab228fec2-Abstract.html},
	abstract = {Unsupervised image representations have significantly reduced the gap with supervised pretraining, notably with the recent achievements of contrastive learning methods. These contrastive methods typically work online and rely on a large number of explicit pairwise feature comparisons, which is computationally challenging. In this paper, we propose an online algorithm, SwAV, that takes advantage of contrastive methods without requiring to compute pairwise comparisons. Specifically, our method simultaneously clusters the data while enforcing consistency between cluster assignments produced for different augmentations (or views) of the same image, instead of comparing features directly as in contrastive learning. Simply put, we use a swapped prediction mechanism where we predict the code of a view from the representation of another view. Our method can be trained with large and small batches and can scale to unlimited amounts of data. Compared to previous contrastive methods, our method is more memory efficient since it does not require a large memory bank or a special momentum network. In addition, we also propose a new data augmentation strategy, multi-crop, that uses a mix of views with different resolutions in place of two full-resolution views, without increasing the memory or compute requirements. We validate our findings by achieving 75.3\% top-1 accuracy on ImageNet with ResNet-50, as well as surpassing supervised pretraining on all the considered transfer tasks.},
	urldate = {2023-02-26},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Caron, Mathilde and Misra, Ishan and Mairal, Julien and Goyal, Priya and Bojanowski, Piotr and Joulin, Armand},
	year = {2020},
	pages = {9912--9924},
}

@inproceedings{gidaris_unsupervised_2018,
	title = {Unsupervised {Representation} {Learning} by {Predicting} {Image} {Rotations}},
	url = {https://openreview.net/forum?id=S1v4N2l0-},
	abstract = {Over the last years, deep convolutional neural networks (ConvNets) have transformed the field of computer vision thanks to their unparalleled capacity to learn high level semantic image features. However, in order to successfully learn those features, they usually require massive amounts of manually labeled data, which is both expensive and impractical to scale. Therefore, unsupervised semantic feature learning, i.e., learning without requiring manual annotation effort, is of crucial importance in order to successfully harvest the vast amount of visual data that are available today. In our work we propose to learn image features by training ConvNets to recognize the 2d rotation that is applied to the image that it gets as input. We demonstrate both qualitatively and quantitatively that this apparently simple task actually provides a very powerful supervisory signal for semantic feature learning. We exhaustively evaluate our method in various unsupervised feature learning benchmarks and we exhibit in all of them state-of-the-art performance. Specifically, our results on those benchmarks demonstrate dramatic improvements w.r.t. prior state-of-the-art approaches in unsupervised representation learning and thus significantly close the gap with supervised feature learning. For instance, in PASCAL VOC 2007 detection task our unsupervised pre-trained AlexNet model achieves the state-of-the-art (among unsupervised methods) mAP of 54.4\%\$that is only 2.4 points lower from the supervised case. We get similarly striking results when we transfer our unsupervised learned features on various other tasks, such as ImageNet classification, PASCAL classification, PASCAL segmentation, and CIFAR-10 classification. The code and models of our paper will be published on: https://github.com/gidariss/FeatureLearningRotNet},
	language = {en},
	urldate = {2023-02-26},
	author = {Gidaris, Spyros and Singh, Praveer and Komodakis, Nikos},
	year = {2018},
}

@inproceedings{caron_deep_2018,
	title = {Deep {Clustering} for {Unsupervised} {Learning} of {Visual} {Features}},
	url = {https://openaccess.thecvf.com/content_ECCV_2018/html/Mathilde_Caron_Deep_Clustering_for_ECCV_2018_paper.html},
	urldate = {2023-02-26},
	author = {Caron, Mathilde and Bojanowski, Piotr and Joulin, Armand and Douze, Matthijs},
	year = {2018},
	pages = {132--149},
}

@inproceedings{singh_revisiting_2022,
	title = {Revisiting {Weakly} {Supervised} {Pre}-{Training} of {Visual} {Perception} {Models}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Singh_Revisiting_Weakly_Supervised_Pre-Training_of_Visual_Perception_Models_CVPR_2022_paper.html},
	language = {en},
	urldate = {2023-02-24},
	author = {Singh, Mannat and Gustafson, Laura and Adcock, Aaron and de Freitas Reis, Vinicius and Gedik, Bugra and Kosaraju, Raj Prateek and Mahajan, Dhruv and Girshick, Ross and Dollár, Piotr and van der Maaten, Laurens},
	year = {2022},
	pages = {804--814},
}

@inproceedings{he_rethinking_2019,
	title = {Rethinking {ImageNet} {Pre}-{Training}},
	url = {https://openaccess.thecvf.com/content_ICCV_2019/html/He_Rethinking_ImageNet_Pre-Training_ICCV_2019_paper.html},
	urldate = {2023-02-24},
	author = {He, Kaiming and Girshick, Ross and Dollar, Piotr},
	year = {2019},
	pages = {4918--4927},
}

@inproceedings{joulin_learning_2016,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Learning {Visual} {Features} from {Large} {Weakly} {Supervised} {Data}},
	isbn = {978-3-319-46478-7},
	doi = {10.1007/978-3-319-46478-7_5},
	abstract = {Convolutional networks trained on large supervised datasets produce visual features which form the basis for the state-of-the-art in many computer-vision problems. Further improvements of these visual features will likely require even larger manually labeled data sets, which severely limits the pace at which progress can be made. In this paper, we explore the potential of leveraging massive, weakly-labeled image collections for learning good visual features. We train convolutional networks on a dataset of 100 million Flickr photos and comments, and show that these networks produce features that perform well in a range of vision problems. We also show that the networks appropriately capture word similarity and learn correspondences between different languages.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2016},
	publisher = {Springer International Publishing},
	author = {Joulin, Armand and van der Maaten, Laurens and Jabri, Allan and Vasilache, Nicolas},
	editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
	year = {2016},
	keywords = {Cosine Similarity, Dictionary Size, French Word, Visual Feature, Word Pair},
	pages = {67--84},
}

@inproceedings{mahajan_exploring_2018,
	title = {Exploring the {Limits} of {Weakly} {Supervised} {Pretraining}},
	url = {https://openaccess.thecvf.com/content_ECCV_2018/html/Dhruv_Mahajan_Exploring_the_Limits_ECCV_2018_paper.html},
	urldate = {2023-02-24},
	author = {Mahajan, Dhruv and Girshick, Ross and Ramanathan, Vignesh and He, Kaiming and Paluri, Manohar and Li, Yixuan and Bharambe, Ashwin and van der Maaten, Laurens},
	year = {2018},
	pages = {181--196},
}

@inproceedings{he_momentum_2020,
	title = {Momentum {Contrast} for {Unsupervised} {Visual} {Representation} {Learning}},
	url = {https://openaccess.thecvf.com/content_CVPR_2020/html/He_Momentum_Contrast_for_Unsupervised_Visual_Representation_Learning_CVPR_2020_paper.html},
	urldate = {2023-02-23},
	author = {He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
	year = {2020},
	pages = {9729--9738},
}

@inproceedings{chen_empirical_2021,
	title = {An {Empirical} {Study} of {Training} {Self}-{Supervised} {Vision} {Transformers}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Chen_An_Empirical_Study_of_Training_Self-Supervised_Vision_Transformers_ICCV_2021_paper.html},
	language = {en},
	urldate = {2023-02-23},
	author = {Chen, Xinlei and Xie, Saining and He, Kaiming},
	year = {2021},
	pages = {9640--9649},
}

@inproceedings{li_mst_2021,
	title = {{MST}: {Masked} {Self}-{Supervised} {Transformer} for {Visual} {Representation}},
	volume = {34},
	shorttitle = {{MST}},
	url = {https://proceedings.neurips.cc/paper/2021/hash/6dbbe6abe5f14af882ff977fc3f35501-Abstract.html},
	abstract = {Transformer has been widely used for self-supervised pre-training in Natural Language Processing (NLP) and achieved great success. However, it has not been fully explored in visual self-supervised learning. Meanwhile, previous methods only consider the high-level feature and learning representation from a global perspective, which may fail to transfer to the downstream dense prediction tasks focusing on local features. In this paper, we present a novel Masked Self-supervised Transformer approach named MST, which can explicitly capture the local context of an image while preserving the global semantic information. Specifically, inspired by the Masked Language Modeling (MLM) in NLP, we propose a masked token strategy based on the multi-head self-attention map, which dynamically masks some tokens of local patches without damaging the crucial structure for self-supervised learning. More importantly, the masked tokens together with the remaining tokens are further recovered by a global image decoder, which preserves the spatial information of the image and is more friendly to the downstream dense prediction tasks. The experiments on multiple datasets demonstrate the effectiveness and generality of the proposed method. For instance, MST achieves Top-1 accuracy of 76.9\% with DeiT-S only using 300-epoch pre-training by linear evaluation, which outperforms supervised methods with the same epoch by 0.4\% and its comparable variant DINO by 1.0\%. For dense prediction tasks, MST also achieves 42.7\% mAP on MS COCO object detection and 74.04\% mIoU on Cityscapes segmentation only with 100-epoch pre-training.},
	urldate = {2023-02-23},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Li, Zhaowen and Chen, Zhiyang and Yang, Fan and Li, Wei and Zhu, Yousong and Zhao, Chaoyang and Deng, Rui and Wu, Liwei and Zhao, Rui and Tang, Ming and Wang, Jinqiao},
	year = {2021},
	pages = {13165--13176},
}

@inproceedings{bao_beit_2022,
	title = {{BEiT}: {BERT} {Pre}-{Training} of {Image} {Transformers}},
	shorttitle = {{BEiT}},
	url = {https://openreview.net/forum?id=p-BhZSz59o4},
	abstract = {We introduce a self-supervised vision representation model BEiT, which stands for Bidirectional Encoder representation from Image Transformers. Following BERT developed in the natural language processing area, we propose a masked image modeling task to pretrain vision Transformers. Specifically, each image has two views in our pre-training, i.e., image patches (such as 16 x 16 pixels), and visual tokens (i.e., discrete tokens). We first ``tokenize'' the original image into visual tokens. Then we randomly mask some image patches and fed them into the backbone Transformer. The pre-training objective is to recover the original visual tokens based on the corrupted image patches. After pre-training BEiT, we directly fine-tune the model parameters on downstream tasks by appending task layers upon the pretrained encoder. Experimental results on image classification and semantic segmentation show that our model achieves competitive results with previous pre-training methods.},
	language = {en},
	urldate = {2023-02-23},
	author = {Bao, Hangbo and Dong, Li and Piao, Songhao and Wei, Furu},
	month = jan,
	year = {2022},
}

@inproceedings{chen_generative_2020,
	title = {Generative {Pretraining} {From} {Pixels}},
	url = {https://proceedings.mlr.press/v119/chen20s.html},
	abstract = {Inspired by progress in unsupervised representation learning for natural language, we examine whether similar models can learn useful representations for images. We train a sequence Transformer to auto-regressively predict pixels, without incorporating knowledge of the 2D input structure. Despite training on low-resolution ImageNet without labels, we find that a GPT-2 scale model learns strong image representations as measured by linear probing, fine-tuning, and low-data classification. On CIFAR-10, we achieve 96.3\% accuracy with a linear probe, outperforming a supervised Wide ResNet, and 99.0\% accuracy with full fine-tuning, matching the top supervised pre-trained models. We are also competitive with self-supervised benchmarks on ImageNet when substituting pixels for a VQVAE encoding, achieving 69.0\% top-1 accuracy on a linear probe of our features.},
	language = {en},
	urldate = {2023-02-23},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Chen, Mark and Radford, Alec and Child, Rewon and Wu, Jeffrey and Jun, Heewoo and Luan, David and Sutskever, Ilya},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {1691--1703},
}

@inproceedings{fan_when_2021,
	title = {When does {Contrastive} {Learning} {Preserve} {Adversarial} {Robustness} from {Pretraining} to {Finetuning}?},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper/2021/hash/b36ed8a07e3cd80ee37138524690eca1-Abstract.html},
	abstract = {Contrastive learning (CL) can learn generalizable feature representations and achieve state-of-the-art performance of downstream tasks by finetuning a linear classifier on top of it.  However, as adversarial robustness becomes vital in image classification,  it remains unclear whether or not CL is able to preserve robustness to downstream tasks. The main challenge is that in the self-supervised pretraining + supervised finetuning paradigm, adversarial robustness is easily forgotten due to a learning task mismatch from pretraining to finetuning. We call such challenge 'cross-task robustness transferability'. To address the above problem, in this paper we revisit and advance CL principles through the lens of robustness enhancement.  We show that (1) the design of contrastive views matters: High-frequency components of images are beneficial to improving model robustness; (2) Augmenting CL with pseudo-supervision stimulus (e.g., resorting to feature clustering) helps preserve robustness without forgetting. Equipped with our new designs, we propose AdvCL, a novel  adversarial contrastive pretraining framework. We show that AdvCL is able to enhance cross-task robustness transferability without loss of model accuracy and finetuning efficiency. With a thorough experimental study,  we demonstrate that AdvCL outperforms the state-of-the-art self-supervised robust learning methods across multiple datasets (CIFAR-10, CIFAR-100, and STL-10) and finetuning schemes  (linear evaluation and full model finetuning).},
	urldate = {2023-02-19},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Fan, Lijie and Liu, Sijia and Chen, Pin-Yu and Zhang, Gaoyuan and Gan, Chuang},
	year = {2021},
	pages = {21480--21492},
}

@inproceedings{jiang_robust_2020,
	title = {Robust {Pre}-{Training} by {Adversarial} {Contrastive} {Learning}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/ba7e36c43aff315c00ec2b8625e3b719-Abstract.html},
	abstract = {Recent work has shown that, when integrated with adversarial training, self-supervised pre-training can lead to state-of-the-art robustness In this work, we improve robustness-aware self-supervised pre-training by learning representations that are consistent under both data augmentations and adversarial perturbations. Our approach leverages a recent contrastive learning framework, which learns representations by maximizing feature consistency under differently augmented views. This fits particularly well with the goal of adversarial robustness, as one cause of adversarial fragility is the lack of feature invariance, i.e., small input perturbations can result in undesirable large changes in features or even predicted labels. We explore various options to formulate the contrastive task, and demonstrate that by injecting adversarial perturbations, contrastive pre-training can lead to models that are both label-efficient and robust. We empirically evaluate the proposed Adversarial Contrastive Learning (ACL) and show it can consistently outperform existing methods. For example on the CIFAR-10 dataset, ACL outperforms the previous state-of-the-art unsupervised robust pre-training approach by 2.99\% on robust accuracy and 2.14\% on standard accuracy. We further demonstrate that ACL pre-training can improve semi-supervised adversarial training, even when only a few labeled examples are available. Our codes and pre-trained models have been released at: https://github.com/VITA-Group/Adversarial-Contrastive-Learning.},
	urldate = {2023-02-19},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Jiang, Ziyu and Chen, Tianlong and Chen, Ting and Wang, Zhangyang},
	year = {2020},
	pages = {16199--16210},
}

@inproceedings{wang_simvlm_2022,
	title = {{SimVLM}: {Simple} {Visual} {Language} {Model} {Pretraining} with {Weak} {Supervision}},
	shorttitle = {{SimVLM}},
	url = {https://openreview.net/forum?id=GUrhfTuf_3},
	abstract = {With recent progress in joint modeling of visual and textual representations, Vision-Language Pretraining (VLP) has achieved impressive performance on many multimodal downstream tasks. However, the requirement for expensive annotations including clean image captions and regional labels limits the scalability of existing approaches, and complicates the pretraining procedure with the introduction of multiple dataset-specific objectives. In this work, we relax these constraints and present a minimalist pretraining framework, named Simple Visual Language Model (SimVLM). Unlike prior work, SimVLM reduces the training complexity by exploiting large-scale weak supervision, and is trained end-to-end with a single prefix language modeling objective. Without utilizing extra data or task-specific customization, the resulting model significantly outperforms previous pretraining methods and achieves new state-of-the-art results on a wide range of discriminative and generative vision-language benchmarks, including VQA (+3.74\% vqa-score), NLVR2 (+1.17\% accuracy), SNLI-VE (+1.37\% accuracy) and image captioning tasks (+10.1\% average CIDEr score). Furthermore, we demonstrate that SimVLM acquires strong generalization and transfer ability, enabling zero-shot behavior including open-ended visual question answering and cross-modality transfer.},
	language = {en},
	urldate = {2023-02-19},
	author = {Wang, Zirui and Yu, Jiahui and Yu, Adams Wei and Dai, Zihang and Tsvetkov, Yulia and Cao, Yuan},
	month = jan,
	year = {2022},
}

@article{zhou_unified_2020,
	title = {Unified {Vision}-{Language} {Pre}-{Training} for {Image} {Captioning} and {VQA}},
	volume = {34},
	issn = {2374-3468, 2159-5399},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/7005},
	doi = {10.1609/aaai.v34i07.7005},
	abstract = {This paper presents a uniﬁed Vision-Language Pre-training (VLP) model. The model is uniﬁed in that (1) it can be ﬁnetuned for either vision-language generation (e.g., image captioning) or understanding (e.g., visual question answering) tasks, and (2) it uses a shared multi-layer transformer network for both encoding and decoding, which differs from many existing methods where the encoder and decoder are implemented using separate models. The uniﬁed VLP model is pre-trained on a large amount of image-text pairs using the unsupervised learning objectives of two tasks: bidirectional and sequence-to-sequence (seq2seq) masked vision-language prediction. The two tasks differ solely in what context the prediction conditions on. This is controlled by utilizing speciﬁc self-attention masks for the shared transformer network. To the best of our knowledge, VLP is the ﬁrst reported model that achieves state-of-the-art results on both vision-language generation and understanding tasks, as disparate as image captioning and visual question answering, across three challenging benchmark datasets: COCO Captions, Flickr30k Captions, and VQA 2.0. The code and the pre-trained models are available at https://github.com/LuoweiZhou/VLP.},
	language = {en},
	number = {07},
	urldate = {2023-02-19},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Zhou, Luowei and Palangi, Hamid and Zhang, Lei and Hu, Houdong and Corso, Jason and Gao, Jianfeng},
	month = apr,
	year = {2020},
	pages = {13041--13049},
}

@inproceedings{bao_vlmo_2022,
	title = {{VLMo}: {Unified} {Vision}-{Language} {Pre}-{Training} with {Mixture}-of-{Modality}-{Experts}},
	shorttitle = {{VLMo}},
	url = {https://openreview.net/forum?id=bydKs84JEyw},
	abstract = {We present a unified Vision-Language pretrained Model (VLMo) that jointly learns a dual encoder and a fusion encoder with a modular Transformer network. Specifically, we introduce Multiway Transformer, where each block contains a pool of modality-specific experts and a shared self-attention layer. Because of the modeling flexibility of Multiway Transformer, pretrained VLMo can be fine-tuned as a fusion encoder for vision-language classification tasks, or used as a dual encoder for efficient image-text retrieval. Moreover, we propose a stagewise pre-training strategy, which effectively leverages large-scale image-only and text-only data besides image-text pairs. Experimental results show that VLMo achieves state-of-the-art results on various vision-language tasks, including VQA, NLVR2 and image-text retrieval.},
	language = {en},
	urldate = {2023-02-19},
	author = {Bao, Hangbo and Wang, Wenhui and Dong, Li and Liu, Qiang and Mohammed, Owais Khan and Aggarwal, Kriti and Som, Subhojit and Piao, Songhao and Wei, Furu},
	month = oct,
	year = {2022},
}

@inproceedings{lu_unified-io_2023,
	title = {{UNIFIED}-{IO}: {A} {Unified} {Model} for {Vision}, {Language}, and {Multi}-modal {Tasks}},
	shorttitle = {{UNIFIED}-{IO}},
	url = {https://openreview.net/forum?id=E01k9048soZ},
	abstract = {We propose Unified-IO, a model that performs a large variety of AI tasks spanning classical computer vision tasks, including pose estimation, object detection, depth estimation and image generation, vision-and-language tasks such as region captioning and referring expression, to natural language processing tasks such as question answering and paraphrasing. Developing a single unified model for such a large variety of tasks poses unique challenges due to the heterogeneous inputs and outputs pertaining to each task, including RGB images, per-pixel maps, binary masks, bounding boxes, and language. We achieve this unification by homogenizing every supported input and output into a sequence of discrete vocabulary tokens. This common representation across all tasks allows us to train a single transformer-based architecture, jointly on over 90 diverse datasets in the vision and language fields. Unified-IO is the first model capable of performing all 7 tasks on the GRIT benchmark and produces strong results across 16 diverse benchmarks like NYUv2-Depth, ImageNet, VQA2.0, OK-VQA, Swig, VizWizGround, BoolQ, and SciTail, with no task-specific fine-tuning. Code and pre-trained models will be made publicly available.},
	language = {en},
	urldate = {2023-02-19},
	author = {Lu, Jiasen and Clark, Christopher and Zellers, Rowan and Mottaghi, Roozbeh and Kembhavi, Aniruddha},
	month = feb,
	year = {2023},
}

@inproceedings{huang_seeing_2021,
	address = {Nashville, TN, USA},
	title = {Seeing {Out} of {tHe} {bOx}: {End}-to-{End} {Pre}-training for {Vision}-{Language} {Representation} {Learning}},
	isbn = {978-1-66544-509-2},
	shorttitle = {Seeing {Out} of {tHe} {bOx}},
	url = {https://ieeexplore.ieee.org/document/9577775/},
	doi = {10.1109/CVPR46437.2021.01278},
	abstract = {We study joint learning of Convolutional Neural Network (CNN) and Transformer for vision-language pre-training (VLPT) which aims to learn cross-modal alignments from millions of image-text pairs. State-of-the-art approaches extract salient image regions and align regions with words step-by-step. As region-based visual features usually represent parts of an image, it is challenging for existing visionlanguage models to fully understand the semantics from paired natural languages. In this paper, we propose SOHO to “Seeing Out of tHe bOx” that takes a whole image as input, and learns vision-language representation in an endto-end manner. SOHO does not require bounding box annotations which enables inference 10 times faster than regionbased approaches. In particular, SOHO learns to extract comprehensive yet compact image features through a visual dictionary (VD) that facilitates cross-modal understanding. VD is designed to represent consistent visual abstractions of similar semantics. It is updated on-the-ﬂy and utilized in our proposed pre-training task Masked Visual Modeling (MVM). We conduct experiments on four well-established vision-language tasks by following standard VLPT settings. In particular, SOHO achieves absolute gains of 2.0\% R@1 score on MSCOCO text retrieval 5k test split, 1.5\% accuracy on NLVR2 test-P split, 6.7\% accuracy on SNLI-VE test split, respectively.},
	language = {en},
	urldate = {2023-02-19},
	booktitle = {2021 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Huang, Zhicheng and Zeng, Zhaoyang and Huang, Yupan and Liu, Bei and Fu, Dongmei and Fu, Jianlong},
	month = jun,
	year = {2021},
	pages = {12971--12980},
}

@inproceedings{xue_probing_2021,
	title = {Probing {Inter}-modality: {Visual} {Parsing} with {Self}-{Attention} for {Vision}-and-{Language} {Pre}-training},
	shorttitle = {Probing {Inter}-modality},
	url = {https://openreview.net/forum?id=e0nZIFEpmYh},
	abstract = {Vision-Language Pre-training (VLP) aims to learn multi-modal representations from image-text pairs and serves for downstream vision-language tasks in a fine-tuning fashion. The dominant VLP models adopt a CNN-Transformer architecture, which embeds images with a CNN, and then aligns images and text with a Transformer. Visual relationship between visual contents plays an important role in image understanding and is the basic for inter-modal alignment learning. However, CNNs have limitations in visual relation learning due to local receptive field's weakness in modeling long-range dependencies. Thus the two objectives of learning visual relation and inter-modal alignment are encapsulated in the same Transformer network. Such design might restrict the inter-modal alignment learning in the Transformer by ignoring the specialized characteristic of each objective. To tackle this, we propose a fully Transformer visual embedding for VLP to better learn visual relation and further promote inter-modal alignment. Specifically, we propose a metric named Inter-Modality Flow (IMF) to measure the interaction between vision and language modalities (i.e., inter-modality). We also design a novel masking optimization mechanism named Masked Feature Regression (MFR) in Transformer to further promote the inter-modality learning. To the best of our knowledge, this is the first study to explore the benefit of Transformer for visual feature learning in VLP. We verify our method on a wide range of vision-language tasks, including Visual Question Answering (VQA), Visual Entailment and Visual Reasoning. Our approach not only outperforms the state-of-the-art VLP performance, but also shows benefits on the IMF metric.},
	language = {en},
	urldate = {2023-02-19},
	author = {Xue, Hongwei and Huang, Yupan and Liu, Bei and Peng, Houwen and Fu, Jianlong and Li, Houqiang and Luo, Jiebo},
	month = dec,
	year = {2021},
}

@inproceedings{li_unimo_2021,
	address = {Online},
	title = {{UNIMO}: {Towards} {Unified}-{Modal} {Understanding} and {Generation} via {Cross}-{Modal} {Contrastive} {Learning}},
	shorttitle = {{UNIMO}},
	url = {https://aclanthology.org/2021.acl-long.202},
	doi = {10.18653/v1/2021.acl-long.202},
	abstract = {Existed pre-training methods either focus on single-modal tasks or multi-modal tasks, and cannot effectively adapt to each other. They can only utilize single-modal data (i.e., text or image) or limited multi-modal data (i.e., image-text pairs). In this work, we propose a UNIfied-MOdal pre-training architecture, namely UNIMO, which can effectively adapt to both single-modal and multi-modal understanding and generation tasks. Large scale of free text corpus and image collections are utilized to improve the capability of visual and textual understanding, and cross-modal contrastive learning (CMCL) is leveraged to align the textual and visual information into a unified semantic space, over a corpus of image-text pairs augmented with related images and texts. With the help of rich non-paired single-modal data, our model is able to learn more generalizable representations, by allowing textual knowledge and visual knowledge to enhance each other in the unified semantic space. The experimental results show that UNIMO greatly improves the performance of several single-modal and multi-modal downstream tasks. Our code and pre-trained models are public at https://github.com/PaddlePaddle/Research/tree/master/NLP/UNIMO.},
	urldate = {2023-02-19},
	booktitle = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Li, Wei and Gao, Can and Niu, Guocheng and Xiao, Xinyan and Liu, Hao and Liu, Jiachen and Wu, Hua and Wang, Haifeng},
	month = aug,
	year = {2021},
	pages = {2592--2607},
}

@inproceedings{cho_unifying_2021,
	title = {Unifying {Vision}-and-{Language} {Tasks} via {Text} {Generation}},
	url = {https://proceedings.mlr.press/v139/cho21a.html},
	abstract = {Existing methods for vision-and-language learning typically require designing task-specific architectures and objectives for each task. For example, a multi-label answer classifier for visual question answering, a region scorer for referring expression comprehension, and a language decoder for image captioning, etc. To alleviate these hassles, in this work, we propose a unified framework that learns different tasks in a single architecture with the same language modeling objective, i.e., multimodal conditional text generation, where our models learn to generate labels in text based on the visual and textual inputs. On 7 popular vision-and-language benchmarks, including visual question answering, referring expression comprehension, visual commonsense reasoning, most of which have been previously modeled as discriminative tasks, our generative approach (with a single unified architecture) reaches comparable performance to recent task-specific state-of-the-art vision-and-language models. Moreover, our generative approach shows better generalization ability on questions that have rare answers. Also, we show that our framework allows multi-task learning in a single architecture with a single set of parameters, achieving similar performance to separately optimized single-task models. Our code is publicly available at: https://github.com/j-min/VL-T5},
	language = {en},
	urldate = {2023-02-19},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Cho, Jaemin and Lei, Jie and Tan, Hao and Bansal, Mohit},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {1931--1942},
}

@misc{huo_wenlan_2021,
	title = {{WenLan}: {Bridging} {Vision} and {Language} by {Large}-{Scale} {Multi}-{Modal} {Pre}-{Training}},
	shorttitle = {{WenLan}},
	url = {http://arxiv.org/abs/2103.06561},
	doi = {10.48550/arXiv.2103.06561},
	abstract = {Multi-modal pre-training models have been intensively explored to bridge vision and language in recent years. However, most of them explicitly model the cross-modal interaction between image-text pairs, by assuming that there exists strong semantic correlation between the text and image modalities. Since this strong assumption is often invalid in real-world scenarios, we choose to implicitly model the cross-modal correlation for large-scale multi-modal pre-training, which is the focus of the Chinese project `WenLan' led by our team. Specifically, with the weak correlation assumption over image-text pairs, we propose a two-tower pre-training model called BriVL within the cross-modal contrastive learning framework. Unlike OpenAI CLIP that adopts a simple contrastive learning method, we devise a more advanced algorithm by adapting the latest method MoCo into the cross-modal scenario. By building a large queue-based dictionary, our BriVL can incorporate more negative samples in limited GPU resources. We further construct a large Chinese multi-source image-text dataset called RUC-CAS-WenLan for pre-training our BriVL model. Extensive experiments demonstrate that the pre-trained BriVL model outperforms both UNITER and OpenAI CLIP on various downstream tasks.},
	urldate = {2023-02-19},
	publisher = {arXiv},
	author = {Huo, Yuqi and Zhang, Manli and Liu, Guangzhen and Lu, Haoyu and Gao, Yizhao and Yang, Guoxing and Wen, Jingyuan and Zhang, Heng and Xu, Baogui and Zheng, Weihao and Xi, Zongzheng and Yang, Yueqian and Hu, Anwen and Zhao, Jinming and Li, Ruichen and Zhao, Yida and Zhang, Liang and Song, Yuqing and Hong, Xin and Cui, Wanqing and Hou, Danyang and Li, Yingyan and Li, Junyi and Liu, Peiyu and Gong, Zheng and Jin, Chuhao and Sun, Yuchong and Chen, Shizhe and Lu, Zhiwu and Dou, Zhicheng and Jin, Qin and Lan, Yanyan and Zhao, Wayne Xin and Song, Ruihua and Wen, Ji-Rong},
	month = jul,
	year = {2021},
	note = {arXiv:2103.06561 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Information Retrieval},
}

@inproceedings{li_blip_2022,
	title = {{BLIP}: {Bootstrapping} {Language}-{Image} {Pre}-training for {Unified} {Vision}-{Language} {Understanding} and {Generation}},
	shorttitle = {{BLIP}},
	url = {https://proceedings.mlr.press/v162/li22n.html},
	abstract = {Vision-Language Pre-training (VLP) has advanced the performance for many vision-language tasks. However, most existing pre-trained models only excel in either understanding-based tasks or generation-based tasks. Furthermore, performance improvement has been largely achieved by scaling up the dataset with noisy image-text pairs collected from the web, which is a suboptimal source of supervision. In this paper, we propose BLIP, a new VLP framework which transfers flexibly to both vision-language understanding and generation tasks. BLIP effectively utilizes the noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. We achieve state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval (+2.7\% in average recall@1), image captioning (+2.8\% in CIDEr), and VQA (+1.6\% in VQA score). BLIP also demonstrates strong generalization ability when directly transferred to video-language tasks in a zero-shot manner. Code and models are available at https://github.com/salesforce/BLIP.},
	language = {en},
	urldate = {2023-02-19},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {12888--12900},
}

@inproceedings{wang_ofa_2022,
	title = {{OFA}: {Unifying} {Architectures}, {Tasks}, and {Modalities} {Through} a {Simple} {Sequence}-to-{Sequence} {Learning} {Framework}},
	shorttitle = {{OFA}},
	url = {https://proceedings.mlr.press/v162/wang22al.html},
	abstract = {In this work, we pursue a unified paradigm for multimodal pretraining to break the shackles of complex task/modality-specific customization. We propose OFA, a Task-Agnostic and Modality-Agnostic framework that supports Task Comprehensiveness. OFA unifies a diverse set of cross-modal and unimodal tasks, including image generation, visual grounding, image captioning, image classification, language modeling, etc., in a simple sequence-to-sequence learning framework. OFA follows the instruction-based learning in both pretraining and finetuning stages, requiring no extra task-specific layers for downstream tasks. In comparison with the recent state-of-the-art vision \& language models that rely on extremely large cross-modal datasets, OFA is pretrained on only 20M publicly available image-text pairs. Despite its simplicity and relatively small-scale training data, OFA achieves new SOTAs in a series of cross-modal tasks while attaining highly competitive performances on uni-modal tasks. Our further analysis indicates that OFA can also effectively transfer to unseen tasks and unseen domains. Our code and models are publicly available at https://github.com/OFA-Sys/OFA.},
	language = {en},
	urldate = {2023-02-19},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Wang, Peng and Yang, An and Men, Rui and Lin, Junyang and Bai, Shuai and Li, Zhikang and Ma, Jianxin and Zhou, Chang and Zhou, Jingren and Yang, Hongxia},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {23318--23340},
}

@inproceedings{kim_vilt_2021,
	title = {{ViLT}: {Vision}-and-{Language} {Transformer} {Without} {Convolution} or {Region} {Supervision}},
	shorttitle = {{ViLT}},
	url = {https://proceedings.mlr.press/v139/kim21k.html},
	abstract = {Vision-and-Language Pre-training (VLP) has improved performance on various joint vision-and-language downstream tasks. Current approaches to VLP heavily rely on image feature extraction processes, most of which involve region supervision (e.g., object detection) and the convolutional architecture (e.g., ResNet). Although disregarded in the literature, we find it problematic in terms of both (1) efficiency/speed, that simply extracting input features requires much more computation than the multimodal interaction steps; and (2) expressive power, as it is upper bounded to the expressive power of the visual embedder and its predefined visual vocabulary. In this paper, we present a minimal VLP model, Vision-and-Language Transformer (ViLT), monolithic in the sense that the processing of visual inputs is drastically simplified to just the same convolution-free manner that we process textual inputs. We show that ViLT is up to tens of times faster than previous VLP models, yet with competitive or better downstream task performance. Our code and pre-trained weights are available at https://github.com/dandelin/vilt.},
	language = {en},
	urldate = {2023-02-19},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Kim, Wonjae and Son, Bokyung and Kim, Ildoo},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {5583--5594},
}

@inproceedings{lu_vilbert_2019,
	title = {{ViLBERT}: {Pretraining} {Task}-{Agnostic} {Visiolinguistic} {Representations} for {Vision}-and-{Language} {Tasks}},
	volume = {32},
	shorttitle = {{ViLBERT}},
	url = {https://proceedings.neurips.cc/paper/2019/hash/c74d97b01eae257e44aa9d5bade97baf-Abstract.html},
	abstract = {We present ViLBERT (short for Vision-and-Language BERT), a model for learning task-agnostic joint representations of image content and natural language. We extend the popular BERT architecture to a multi-modal two-stream model, processing both visual and textual inputs in separate streams that interact through co-attentional transformer layers. We pretrain our model through two proxy tasks on the large, automatically collected Conceptual Captions dataset and then transfer it to multiple established vision-and-language tasks -- visual question answering, visual commonsense reasoning, referring expressions, and caption-based image retrieval -- by making only minor additions to the base architecture. We observe significant improvements across tasks compared to existing task-specific models -- achieving state-of-the-art on all four tasks. Our work represents a shift away from learning groundings between vision and language only as part of task training and towards treating visual grounding as a pretrainable and transferable capability.},
	urldate = {2023-02-19},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Lu, Jiasen and Batra, Dhruv and Parikh, Devi and Lee, Stefan},
	year = {2019},
}

@inproceedings{tan_lxmert_2019,
	address = {Hong Kong, China},
	title = {{LXMERT}: {Learning} {Cross}-{Modality} {Encoder} {Representations} from {Transformers}},
	shorttitle = {{LXMERT}},
	url = {https://aclanthology.org/D19-1514},
	doi = {10.18653/v1/D19-1514},
	abstract = {Vision-and-language reasoning requires an understanding of visual concepts, language semantics, and, most importantly, the alignment and relationships between these two modalities. We thus propose the LXMERT (Learning Cross-Modality Encoder Representations from Transformers) framework to learn these vision-and-language connections. In LXMERT, we build a large-scale Transformer model that consists of three encoders: an object relationship encoder, a language encoder, and a cross-modality encoder. Next, to endow our model with the capability of connecting vision and language semantics, we pre-train the model with large amounts of image-and-sentence pairs, via five diverse representative pre-training tasks: masked language modeling, masked object prediction (feature regression and label classification), cross-modality matching, and image question answering. These tasks help in learning both intra-modality and cross-modality relationships. After fine-tuning from our pre-trained parameters, our model achieves the state-of-the-art results on two visual question answering datasets (i.e., VQA and GQA). We also show the generalizability of our pre-trained cross-modality model by adapting it to a challenging visual-reasoning task, NLVR2, and improve the previous best result by 22\% absolute (54\% to 76\%). Lastly, we demonstrate detailed ablation studies to prove that both our novel model components and pre-training strategies significantly contribute to our strong results. Code and pre-trained models publicly available at: https://github.com/airsplay/lxmert},
	urldate = {2023-02-19},
	booktitle = {Proceedings of the 2019 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} and the 9th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({EMNLP}-{IJCNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Tan, Hao and Bansal, Mohit},
	month = nov,
	year = {2019},
	pages = {5100--5111},
}

@inproceedings{du_survey_2022,
	title = {A {Survey} of {Vision}-{Language} {Pre}-{Trained} {Models}},
	abstract = {As transformer evolves, pre-trained models have advanced at a breakneck pace in recent years. They have dominated the mainstream techniques in natural language processing (NLP) and computer vision (CV). How to adapt pre-training to the field of Vision-and-Language (V-L) learning and improve downstream task performance becomes a focus of multimodal learning. In this paper, we review the recent progress in Vision-Language PreTrained Models (VL-PTMs). As the core content, we first briefly introduce several ways to encode raw images and texts to single-modal embeddings before pre-training. Then, we dive into the mainstream architectures of VL-PTMs in modeling the interaction between text and image representations. We further present widely-used pre-training tasks, and then we introduce some common downstream tasks. We finally conclude this paper and present some promising research directions. Our survey aims to provide researchers with synthesis and pointer to related research.},
	language = {en},
	booktitle = {Proceedings of the {Thirty}-{First} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	author = {Du, Yifan and Liu, Zikang and Li, Junyi and Zhao, Wayne Xin},
	year = {2022},
}

@inproceedings{long_vision-and-language_2022,
	address = {Vienna, Austria},
	title = {Vision-and-{Language} {Pretrained} {Models}: {A} {Survey}},
	isbn = {978-1-956792-00-3},
	shorttitle = {Vision-and-{Language} {Pretrained} {Models}},
	url = {https://www.ijcai.org/proceedings/2022/773},
	doi = {10.24963/ijcai.2022/773},
	abstract = {Pretrained models have produced great success in both Computer Vision (CV) and Natural Language Processing (NLP). This progress leads to learning joint representations of vision and language pretraining by feeding visual and linguistic contents into a multi-layer transformer, Visual-Language Pretrained Models (VLPMs). In this paper, we present an overview of the major advances achieved in VLPMs for producing joint representations of vision and language. As the preliminaries, we briefly describe the general task definition and genetic architecture of VLPMs. We first discuss the language and vision data encoding methods and then present the mainstream VLPM structure as the core content. We further summarise several essential pretraining and fine-tuning strategies. Finally, we highlight three future directions for both CV and NLP researchers to provide insightful guidance.},
	language = {en},
	urldate = {2023-02-19},
	booktitle = {Proceedings of the {Thirty}-{First} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Long, Siqu and Cao, Feiqi and Han, Soyeon Caren and Yang, Haiqin},
	month = jul,
	year = {2022},
	pages = {5530--5537},
}

@article{wang_image_2022,
	title = {Image as a {Foreign} {Language}: {BEiT} {Pretraining} for {All} {Vision} and {Vision}-{Language} {Tasks}},
	shorttitle = {Image as a {Foreign} {Language}},
	url = {https://arxiv.org/abs/2208.10442v2},
	doi = {10.48550/arXiv.2208.10442},
	abstract = {A big convergence of language, vision, and multimodal pretraining is emerging. In this work, we introduce a general-purpose multimodal foundation model BEiT-3, which achieves state-of-the-art transfer performance on both vision and vision-language tasks. Specifically, we advance the big convergence from three aspects: backbone architecture, pretraining task, and model scaling up. We introduce Multiway Transformers for general-purpose modeling, where the modular architecture enables both deep fusion and modality-specific encoding. Based on the shared backbone, we perform masked "language" modeling on images (Imglish), texts (English), and image-text pairs ("parallel sentences") in a unified manner. Experimental results show that BEiT-3 obtains state-of-the-art performance on object detection (COCO), semantic segmentation (ADE20K), image classification (ImageNet), visual reasoning (NLVR2), visual question answering (VQAv2), image captioning (COCO), and cross-modal retrieval (Flickr30K, COCO).},
	language = {en},
	urldate = {2023-02-19},
	author = {Wang, Wenhui and Bao, Hangbo and Dong, Li and Bjorck, Johan and Peng, Zhiliang and Liu, Qiang and Aggarwal, Kriti and Mohammed, Owais Khan and Singhal, Saksham and Som, Subhojit and Wei, Furu},
	month = aug,
	year = {2022},
}

@inproceedings{ni_m3p_2021,
	title = {{M3P}: {Learning} {Universal} {Representations} via {Multitask} {Multilingual} {Multimodal} {Pre}-{Training}},
	shorttitle = {{M3P}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Ni_M3P_Learning_Universal_Representations_via_Multitask_Multilingual_Multimodal_Pre-Training_CVPR_2021_paper.html},
	language = {en},
	urldate = {2023-02-19},
	author = {Ni, Minheng and Huang, Haoyang and Su, Lin and Cui, Edward and Bharti, Taroon and Wang, Lijuan and Zhang, Dongdong and Duan, Nan},
	year = {2021},
	pages = {3977--3986},
}

@article{huang_pixel-bert_2020,
	title = {Pixel-{BERT}: {Aligning} {Image} {Pixels} with {Text} by {Deep} {Multi}-{Modal} {Transformers}},
	shorttitle = {Pixel-{BERT}},
	url = {https://arxiv.org/abs/2004.00849v2},
	doi = {10.48550/arXiv.2004.00849},
	abstract = {We propose Pixel-BERT to align image pixels with text by deep multi-modal transformers that jointly learn visual and language embedding in a unified end-to-end framework. We aim to build a more accurate and thorough connection between image pixels and language semantics directly from image and sentence pairs instead of using region-based image features as the most recent vision and language tasks. Our Pixel-BERT which aligns semantic connection in pixel and text level solves the limitation of task-specific visual representation for vision and language tasks. It also relieves the cost of bounding box annotations and overcomes the unbalance between semantic labels in visual task and language semantic. To provide a better representation for down-stream tasks, we pre-train a universal end-to-end model with image and sentence pairs from Visual Genome dataset and MS-COCO dataset. We propose to use a random pixel sampling mechanism to enhance the robustness of visual representation and to apply the Masked Language Model and Image-Text Matching as pre-training tasks. Extensive experiments on downstream tasks with our pre-trained model show that our approach makes the most state-of-the-arts in downstream tasks, including Visual Question Answering (VQA), image-text retrieval, Natural Language for Visual Reasoning for Real (NLVR). Particularly, we boost the performance of a single model in VQA task by 2.17 points compared with SOTA under fair comparison.},
	language = {en},
	urldate = {2023-02-19},
	author = {Huang, Zhicheng and Zeng, Zhaoyang and Liu, Bei and Fu, Dongmei and Fu, Jianlong},
	month = apr,
	year = {2020},
}

@article{li_unicoder-vl_2020,
	title = {Unicoder-{VL}: {A} {Universal} {Encoder} for {Vision} and {Language} by {Cross}-{Modal} {Pre}-{Training}},
	volume = {34},
	copyright = {Copyright (c) 2020 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	shorttitle = {Unicoder-{VL}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/6795},
	doi = {10.1609/aaai.v34i07.6795},
	abstract = {We propose Unicoder-VL, a universal encoder that aims to learn joint representations of vision and language in a pre-training manner. Borrow ideas from cross-lingual pre-trained models, such as XLM (Lample and Conneau 2019) and Unicoder (Huang et al. 2019), both visual and linguistic contents are fed into a multi-layer Transformer (Vaswani et al. 2017) for the cross-modal pre-training, where three pre-trained tasks are employed, including Masked Language Modeling(MLM), Masked Object Classification(MOC) and Visual-linguistic Matching(VLM). The first two tasks learn context-aware representations for input tokens based on linguistic and visual contents jointly. The last task tries to predict whether an image and a text describe each other. After pretraining on large-scale image-caption pairs, we transfer Unicoder-VL to caption-based image-text retrieval and visual commonsense reasoning, with just one additional output layer. We achieve state-of-the-art or comparable results on both two tasks and show the powerful ability of the cross-modal pre-training.},
	language = {en},
	number = {07},
	urldate = {2023-02-19},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Li, Gen and Duan, Nan and Fang, Yuejian and Gong, Ming and Jiang, Daxin},
	month = apr,
	year = {2020},
	note = {Number: 07},
	pages = {11336--11344},
}

@inproceedings{li_oscar_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Oscar: {Object}-{Semantics} {Aligned} {Pre}-training for {Vision}-{Language} {Tasks}},
	isbn = {978-3-030-58577-8},
	shorttitle = {Oscar},
	doi = {10.1007/978-3-030-58577-8_8},
	abstract = {Large-scale pre-training methods of learning cross-modal representations on image-text pairs are becoming popular for vision-language tasks. While existing methods simply concatenate image region features and text features as input to the model to be pre-trained and use self-attention to learn image-text semantic alignments in a brute force manner, in this paper, we propose a new learning method Oscar (Object-Semantics Aligned Pre-training), which uses object tags detected in images as anchor points to significantly ease the learning of alignments. Our method is motivated by the observation that the salient objects in an image can be accurately detected, and are often mentioned in the paired text. We pre-train an Oscar model on the public corpus of 6.5 million text-image pairs, and fine-tune it on downstream tasks, creating new state-of-the-arts on six well-established vision-language understanding and generation tasks (The code and pre-trained models are released: https://github.com/microsoft/Oscar).},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Li, Xiujun and Yin, Xi and Li, Chunyuan and Zhang, Pengchuan and Hu, Xiaowei and Zhang, Lei and Wang, Lijuan and Hu, Houdong and Dong, Li and Wei, Furu and Choi, Yejin and Gao, Jianfeng},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	keywords = {Object semantics, Pre-training, Vision-and-language},
	pages = {121--137},
}

@article{li_visualbert_2019,
	title = {{VisualBERT}: {A} {Simple} and {Performant} {Baseline} for {Vision} and {Language}},
	shorttitle = {{VisualBERT}},
	url = {https://arxiv.org/abs/1908.03557v1},
	doi = {10.48550/arXiv.1908.03557},
	abstract = {We propose VisualBERT, a simple and flexible framework for modeling a broad range of vision-and-language tasks. VisualBERT consists of a stack of Transformer layers that implicitly align elements of an input text and regions in an associated input image with self-attention. We further propose two visually-grounded language model objectives for pre-training VisualBERT on image caption data. Experiments on four vision-and-language tasks including VQA, VCR, NLVR2, and Flickr30K show that VisualBERT outperforms or rivals with state-of-the-art models while being significantly simpler. Further analysis demonstrates that VisualBERT can ground elements of language to image regions without any explicit supervision and is even sensitive to syntactic relationships, tracking, for example, associations between verbs and image regions corresponding to their arguments.},
	language = {en},
	urldate = {2023-02-19},
	author = {Li, Liunian Harold and Yatskar, Mark and Yin, Da and Hsieh, Cho-Jui and Chang, Kai-Wei},
	month = aug,
	year = {2019},
}

@inproceedings{ge_miles_2022,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{MILES}: {Visual} {BERT} {Pre}-training with {Injected} {Language} {Semantics} for {Video}-{Text} {Retrieval}},
	isbn = {978-3-031-19833-5},
	shorttitle = {{MILES}},
	doi = {10.1007/978-3-031-19833-5_40},
	abstract = {Dominant pre-training work for video-text retrieval mainly adopt the “dual-encoder” architectures to enable efficient retrieval, where two separate encoders are used to contrast global video and text representations, but ignore detailed local semantics. The recent success of image BERT pre-training with masked visual modeling that promotes the learning of local visual context, motivates a possible solution to address the above limitation. In this work, we for the first time investigate masked visual modeling in video-text pre-training with the “dual-encoder” architecture. We perform Masked visual modeling with Injected LanguagE Semantics (MILES) by employing an extra snapshot video encoder as an evolving “tokenizer” to produce reconstruction targets for masked video patch prediction. Given the corrupted video, the video encoder is trained to recover text-aligned features of the masked patches via reasoning with the visible regions along the spatial and temporal dimensions, which enhances the discriminativeness of local visual features and the fine-grained cross-modality alignment. Our method outperforms state-of-the-art methods for text-to-video retrieval on four datasets with both zero-shot and fine-tune evaluation protocols. Our approach also surpasses the baseline models significantly on zero-shot action recognition, which can be cast as video-to-text retrieval.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2022},
	publisher = {Springer Nature Switzerland},
	author = {Ge, Yuying and Ge, Yixiao and Liu, Xihui and Wang, Jinpeng and Wu, Jianping and Shan, Ying and Qie, Xiaohu and Luo, Ping},
	editor = {Avidan, Shai and Brostow, Gabriel and Cissé, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
	year = {2022},
	keywords = {Masked visual modeling, Video-text retrieval},
	pages = {691--708},
}

@inproceedings{su_vl-bert_2021,
	title = {{VL}-{BERT}: {Pre}-training of {Generic} {Visual}-{Linguistic} {Representations}},
	shorttitle = {{VL}-{BERT}},
	url = {https://openreview.net/forum?id=SygXPaEYvH},
	abstract = {We introduce a new pre-trainable generic representation for visual-linguistic tasks, called Visual-Linguistic BERT (VL-BERT for short). VL-BERT adopts the simple yet powerful Transformer model as the backbone, and extends it to take both visual and linguistic embedded features as input. In it, each element of the input is either of a word from the input sentence, or a region-of-interest (RoI) from the input image. It is designed to fit for most of the visual-linguistic downstream tasks. To better exploit the generic representation, we pre-train VL-BERT on the massive-scale Conceptual Captions dataset, together with text-only corpus. Extensive empirical analysis demonstrates that the pre-training procedure can better align the visual-linguistic clues and benefit the downstream tasks, such as visual commonsense reasoning, visual question answering and referring expression comprehension. It is worth noting that VL-BERT achieved the first place of single model on the leaderboard of the VCR benchmark.},
	language = {en},
	urldate = {2023-02-19},
	author = {Su, Weijie and Zhu, Xizhou and Cao, Yue and Li, Bin and Lu, Lewei and Wei, Furu and Dai, Jifeng},
	month = mar,
	year = {2021},
}

@inproceedings{chen_uniter_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{UNITER}: {UNiversal} {Image}-{TExt} {Representation} {Learning}},
	isbn = {978-3-030-58577-8},
	shorttitle = {{UNITER}},
	doi = {10.1007/978-3-030-58577-8_7},
	abstract = {Joint image-text embedding is the bedrock for most Vision-and-Language (V+L) tasks, where multimodality inputs are simultaneously processed for joint visual and textual understanding. In this paper, we introduce UNITER, a UNiversal Image-TExt Representation, learned through large-scale pre-training over four image-text datasets (COCO, Visual Genome, Conceptual Captions, and SBU Captions), which can power heterogeneous downstream V+L tasks with joint multimodal embeddings. We design four pre-training tasks: Masked Language Modeling (MLM), Masked Region Modeling (MRM, with three variants), Image-Text Matching (ITM), and Word-Region Alignment (WRA). Different from previous work that applies joint random masking to both modalities, we use conditional masking on pre-training tasks (i.e., masked language/region modeling is conditioned on full observation of image/text). In addition to ITM for global image-text alignment, we also propose WRA via the use of Optimal Transport (OT) to explicitly encourage fine-grained alignment between words and image regions during pre-training. Comprehensive analysis shows that both conditional masking and OT-based WRA contribute to better pre-training. We also conduct a thorough ablation study to find an optimal combination of pre-training tasks. Extensive experiments show that UNITER achieves new state of the art across six V+L tasks (over nine datasets), including Visual Question Answering, Image-Text Retrieval, Referring Expression Comprehension, Visual Commonsense Reasoning, Visual Entailment, and NLVR\$\${\textasciicircum}2\$\$2(Code is available at https://github.com/ChenRocks/UNITER.).},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Chen, Yen-Chun and Li, Linjie and Yu, Licheng and El Kholy, Ahmed and Ahmed, Faisal and Gan, Zhe and Cheng, Yu and Liu, Jingjing},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	pages = {104--120},
}

@inproceedings{li_supervision_2022,
	title = {Supervision {Exists} {Everywhere}: {A} {Data} {Efficient} {Contrastive} {Language}-{Image} {Pre}-training {Paradigm}},
	shorttitle = {Supervision {Exists} {Everywhere}},
	url = {https://openreview.net/forum?id=zq1iJkNk3uN},
	abstract = {Recently, large-scale Contrastive Language-Image Pre-training (CLIP) has attracted unprecedented attention for its impressive zero-shot recognition ability and excellent transferability to downstream tasks. However, CLIP is quite data-hungry and requires 400M image-text pairs for pre-training, thereby restricting its adoption. This work proposes a novel training paradigm, Data efficient CLIP (DeCLIP), to alleviate this limitation. We demonstrate that by carefully utilizing the widespread supervision among the image-text pairs, our De-CLIP can learn generic visual features more efficiently. Instead of using the single image-text contrastive supervision, we fully exploit data potential through the use of (1) self-supervision within each modality; (2) multi-view supervision across modalities; (3) nearest-neighbor supervision from other similar pairs. Benefiting from intrinsic supervision, our DeCLIP-ResNet50 can achieve 60.4\% zero-shot top1 accuracy on ImageNet, which is 0.8\% above the CLIP-ResNet50 while using 7.1×fewer data. Our DeCLIP-ResNet50 outperforms its counterpart in 8 out of 11 visual datasets when transferred to downstream tasks. Moreover, Scaling up the model and computing also works well in our framework.},
	language = {en},
	urldate = {2023-02-19},
	author = {Li, Yangguang and Liang, Feng and Zhao, Lichen and Cui, Yufeng and Ouyang, Wanli and Shao, Jing and Yu, Fengwei and Yan, Junjie},
	month = jan,
	year = {2022},
}

@inproceedings{yao_filip_2022,
	title = {{FILIP}: {Fine}-grained {Interactive} {Language}-{Image} {Pre}-{Training}},
	shorttitle = {{FILIP}},
	url = {https://openreview.net/forum?id=cpDhcsEDC2},
	abstract = {Unsupervised large-scale vision-language pre-training has shown promising advances on various downstream tasks. Existing methods often model the cross-modal interaction either via the similarity of the global feature of each modality which misses sufficient information, or finer-grained interactions using cross/self-attention upon visual and textual tokens. However, cross/self-attention suffers from inferior efficiency in both training and inference. In this paper, we introduce a large-scale Fine-grained Interactive Language-Image Pre-training (FILIP) to achieve finer-level alignment through a cross-modal late interaction mechanism, which uses a token-wise maximum similarity between visual and textual tokens to guide the contrastive objective. FILIP successfully leverages the finer-grained expressiveness between image patches and textual words by modifying only contrastive loss, while simultaneously gaining the ability to pre-compute image and text representations offline at inference, keeping both large-scale training and inference efficient. Furthermore, we construct a new large-scale image-text pair dataset called FILIP300M for pre-training. Experiments show that FILIP achieves state-of-the-art performance on multiple downstream vision-language tasks including zero-shot image classification and image-text retrieval. The visualization on word-patch alignment further shows that FILIP can learn meaningful fine-grained features with promising localization ability.},
	language = {en},
	urldate = {2023-02-19},
	author = {Yao, Lewei and Huang, Runhui and Hou, Lu and Lu, Guansong and Niu, Minzhe and Xu, Hang and Liang, Xiaodan and Li, Zhenguo and Jiang, Xin and Xu, Chunjing},
	month = jan,
	year = {2022},
}

@inproceedings{li_fine-grained_2022,
	title = {Fine-{Grained} {Semantically} {Aligned} {Vision}-{Language} {Pre}-{Training}},
	url = {https://openreview.net/forum?id=yam42JWePu},
	abstract = {Large-scale vision-language pre-training has shown impressive advances in a wide range of downstream tasks. Existing methods mainly model the cross-modal alignment by the similarity of the global representations of images and text, or advanced cross-modal attention upon image and text features. However, they fail to explicitly learn the fine-grained semantic alignment between visual regions and textual phrases, as only global image-text alignment information is available. In this paper, we introduce LOUPE, a fine-grained semantically aLigned visiOn-langUage PrE-training framework, which learns fine-grained semantic alignment from the novel perspective of game-theoretic interactions. To efficiently estimate the game-theoretic interactions, we further propose an uncertainty-aware neural Shapley interaction learning module. Experiments show that LOUPE achieves state-of-the-art performance on a variety of vision-language tasks. Without any object-level human annotations and fine-tuning, LOUPE achieves competitive performance on object detection and visual grounding. More importantly, LOUPE opens a new promising direction of learning fine-grained semantics from large-scale raw image-text pairs.},
	language = {en},
	urldate = {2023-02-19},
	author = {Li, Juncheng and He, Xin and Wei, Longhui and Qian, Long and Zhu, Linchao and Xie, Lingxi and Zhuang, Yueting and Tian, Qi and Tang, Siliang},
	month = oct,
	year = {2022},
}

@article{picot_adversarial_2023,
	title = {Adversarial {Robustness} {Via} {Fisher}-{Rao} {Regularization}},
	volume = {45},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2022.3174724},
	abstract = {Adversarial robustness has become a topic of growing interest in machine learning since it was observed that neural networks tend to be brittle. We propose an information-geometric formulation of adversarial defense and introduce Fire, a new Fisher-Rao regularization for the categorical cross-entropy loss, which is based on the geodesic distance between the softmax outputs corresponding to natural and perturbed input features. Based on the information-geometric properties of the class of softmax distributions, we derive an explicit characterization of the Fisher-Rao Distance (FRD) for the binary and multiclass cases, and draw some interesting properties as well as connections with standard regularization metrics. Furthermore, we verify on a simple linear and Gaussian model, that all Pareto-optimal points in the accuracy-robustness region can be reached by Fire while other state-of-the-art methods fail. Empirically, we evaluate the performance of various classifiers trained with the proposed loss on standard datasets, showing up to a simultaneous 1\% of improvement in terms of clean and robust performances while reducing the training time by 20\% over the best-performing methods.},
	number = {3},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Picot, Marine and Messina, Francisco and Boudiaf, Malik and Labeau, Fabrice and Ayed, Ismail Ben and Piantanida, Pablo},
	month = mar,
	year = {2023},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Adversarial machine learning, Adversarial regularization, Manifolds, Neural networks, Perturbation methods, Robustness, Standards, Training, adversarial training, computer vision, deep learning, fisher-rao distance, information geometry, neural networks, safety AI},
	pages = {2698--2710},
}

@article{wei_adversarial_2023,
	title = {Adversarial {Sticker}: {A} {Stealthy} {Attack} {Method} in the {Physical} {World}},
	volume = {45},
	issn = {1939-3539},
	shorttitle = {Adversarial {Sticker}},
	doi = {10.1109/TPAMI.2022.3176760},
	abstract = {To assess the vulnerability of deep learning in the physical world, recent works introduce adversarial patches and apply them on different tasks. In this paper, we propose another kind of adversarial patch: the Meaningful Adversarial Sticker, a physically feasible and stealthy attack method by using real stickers existing in our life. Unlike the previous adversarial patches by designing perturbations, our method manipulates the sticker's pasting position and rotation angle on the objects to perform physical attacks. Because the position and rotation angle are less affected by the printing loss and color distortion, adversarial stickers can keep good attacking performance in the physical world. Besides, to make adversarial stickers more practical in real scenes, we conduct attacks in the black-box setting with the limited information rather than the white-box setting with all the details of threat models. To effectively solve for the sticker's parameters, we design the Region based Heuristic Differential Evolution Algorithm, which utilizes the new-found regional aggregation of effective solutions and the adaptive adjustment strategy of the evaluation criteria. Our method is comprehensively verified in the face recognition and then extended to the image retrieval and traffic sign recognition. Extensive experiments show the proposed method is effective and efficient in complex physical conditions and has a good generalization for different tasks.},
	number = {3},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Wei, Xingxing and Guo, Ying and Yu, Jie},
	month = mar,
	year = {2023},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Adaptation models, Deep learning models, Face recognition, Image recognition, Image retrieval, Perturbation methods, TV, Task analysis, adversarial examples, adversarial patch, physical world, robustness},
	pages = {2711--2725},
}

@inproceedings{wu_towards_2022,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Towards {Efficient} {Adversarial} {Training} on {Vision} {Transformers}},
	isbn = {978-3-031-19778-9},
	doi = {10.1007/978-3-031-19778-9_18},
	abstract = {Vision Transformer (ViT), as a powerful alternative to Convolutional Neural Network (CNN), has received much attention. Recent work showed that ViTs are also vulnerable to adversarial examples like CNNs. To build robust ViTs, an intuitive way is to apply adversarial training since it has been shown as one of the most effective ways to accomplish robust CNNs. However, one major limitation of adversarial training is its heavy computational cost. The self-attention mechanism adopted by ViTs is a computationally intense operation whose expense increases quadratically with the number of input patches, making adversarial training on ViTs even more time-consuming. In this work, we first comprehensively study fast adversarial training on a variety of vision transformers and illustrate the relationship between the efficiency and robustness. Then, to expediate adversarial training on ViTs, we propose an efficient Attention Guided Adversarial Training mechanism. Specifically, relying on the specialty of self-attention, we actively remove certain patch embeddings of each layer with an attention-guided dropping strategy during adversarial training. The slimmed self-attention modules accelerate the adversarial training on ViTs significantly. With only \$\$65{\textbackslash}\%\$\$65\%of the fast adversarial training time, we match the state-of-the-art results on the challenging ImageNet benchmark.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2022},
	publisher = {Springer Nature Switzerland},
	author = {Wu, Boxi and Gu, Jindong and Li, Zhifeng and Cai, Deng and He, Xiaofei and Liu, Wei},
	editor = {Avidan, Shai and Brostow, Gabriel and Cissé, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
	year = {2022},
	keywords = {Adversarial training, Robustness, Vision transformer},
	pages = {307--325},
}

@inproceedings{naseer_intriguing_2021,
	title = {Intriguing {Properties} of {Vision} {Transformers}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper/2021/hash/c404a5adbf90e09631678b13b05d9d7a-Abstract.html},
	abstract = {Vision transformers (ViT) have demonstrated impressive performance across numerous machine vision tasks. These models are based on multi-head self-attention mechanisms that can flexibly attend to a sequence of image patches to encode contextual cues. An important question is how such flexibility (in attending image-wide context conditioned on a given patch) can facilitate handling nuisances in natural images e.g., severe occlusions, domain shifts, spatial permutations, adversarial and natural perturbations. We systematically study this question via an extensive set of experiments encompassing three ViT families and provide comparisons with a high-performing convolutional neural network (CNN). We show and analyze the following intriguing properties of ViT: (a)Transformers are highly robust to severe occlusions, perturbations and domain shifts, e.g., retain as high as 60\% top-1 accuracy on ImageNet even after randomly occluding 80\% of the image content. (b)The robustness towards occlusions is not due to texture bias, instead we show that ViTs are significantly less biased towards local textures, compared to CNNs. When properly trained to encode shape-based features, ViTs demonstrate shape recognition capability comparable to that of human visual system, previously unmatched in the literature. (c)Using ViTs to encode shape representation leads to an interesting consequence of accurate semantic segmentation without pixel-level supervision. (d)Off-the-shelf features from a single ViT model can be combined to create a feature ensemble,  leading to high accuracy rates across a range of classification datasets in both traditional and few-shot learning paradigms.  We show effective features of ViTs are due to flexible and dynamic receptive fields possible via self-attention mechanisms. Our code will be publicly released.},
	urldate = {2023-02-18},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Naseer, Muhammad Muzammal and Ranasinghe, Kanchana and Khan, Salman H and Hayat, Munawar and Shahbaz Khan, Fahad and Yang, Ming-Hsuan},
	year = {2021},
	pages = {23296--23308},
}

@inproceedings{han_transformer_2021,
	title = {Transformer in {Transformer}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper/2021/hash/854d9fca60b4bd07f9bb215d59ef5561-Abstract.html},
	urldate = {2023-02-18},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Han, Kai and Xiao, An and Wu, Enhua and Guo, Jianyuan and XU, Chunjing and Wang, Yunhe},
	year = {2021},
	pages = {15908--15919},
}

@inproceedings{chen_pre-trained_2021,
	title = {Pre-{Trained} {Image} {Processing} {Transformer}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Chen_Pre-Trained_Image_Processing_Transformer_CVPR_2021_paper.html},
	language = {en},
	urldate = {2023-02-18},
	author = {Chen, Hanting and Wang, Yunhe and Guo, Tianyu and Xu, Chang and Deng, Yiping and Liu, Zhenhua and Ma, Siwei and Xu, Chunjing and Xu, Chao and Gao, Wen},
	year = {2021},
	pages = {12299--12310},
}

@inproceedings{dascoli_convit_2021,
	title = {{ConViT}: {Improving} {Vision} {Transformers} with {Soft} {Convolutional} {Inductive} {Biases}},
	shorttitle = {{ConViT}},
	url = {https://proceedings.mlr.press/v139/d-ascoli21a.html},
	abstract = {Convolutional architectures have proven extremely successful for vision tasks. Their hard inductive biases enable sample-efficient learning, but come at the cost of a potentially lower performance ceiling. Vision Transformers (ViTs) rely on more flexible self-attention layers, and have recently outperformed CNNs for image classification. However, they require costly pre-training on large external datasets or distillation from pre-trained convolutional networks. In this paper, we ask the following question: is it possible to combine the strengths of these two architectures while avoiding their respective limitations? To this end, we introduce gated positional self-attention (GPSA), a form of positional self-attention which can be equipped with a “soft" convolutional inductive bias. We initialise the GPSA layers to mimic the locality of convolutional layers, then give each attention head the freedom to escape locality by adjusting a gating parameter regulating the attention paid to position versus content information. The resulting convolutional-like ViT architecture, ConViT, outperforms the DeiT on ImageNet, while offering a much improved sample efficiency. We further investigate the role of locality in learning by first quantifying how it is encouraged in vanilla self-attention layers, then analysing how it is escaped in GPSA layers. We conclude by presenting various ablations to better understand the success of the ConViT. Our code and models are released publicly at https://github.com/facebookresearch/convit.},
	language = {en},
	urldate = {2023-02-18},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {D’Ascoli, Stéphane and Touvron, Hugo and Leavitt, Matthew L. and Morcos, Ari S. and Biroli, Giulio and Sagun, Levent},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {2286--2296},
}

@inproceedings{graham_levit_2021,
	title = {{LeViT}: {A} {Vision} {Transformer} in {ConvNet}'s {Clothing} for {Faster} {Inference}},
	shorttitle = {{LeViT}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Graham_LeViT_A_Vision_Transformer_in_ConvNets_Clothing_for_Faster_Inference_ICCV_2021_paper.html},
	language = {en},
	urldate = {2023-02-18},
	author = {Graham, Benjamin and El-Nouby, Alaaeldin and Touvron, Hugo and Stock, Pierre and Joulin, Armand and Jégou, Hervé and Douze, Matthijs},
	year = {2021},
	pages = {12259--12269},
}

@inproceedings{touvron_training_2021,
	title = {Training data-efficient image transformers \& distillation through attention},
	url = {https://proceedings.mlr.press/v139/touvron21a.html},
	abstract = {Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. These high-performing vision transformers are pre-trained with hundreds of millions of images using a large infrastructure, thereby limiting their adoption. In this work, we produce competitive convolution-free transformers trained on ImageNet only using a single computer in less than 3 days. Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1\% (single-crop) on ImageNet with no external data. We also introduce a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention, typically from a convnet teacher. The learned transformers are competitive (85.2\% top-1 acc.) with the state of the art on ImageNet, and similarly when transferred to other tasks. We will share our code and models.},
	language = {en},
	urldate = {2023-02-18},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and Jegou, Herve},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {10347--10357},
}

@inproceedings{dou_empirical_2022,
	title = {An {Empirical} {Study} of {Training} {End}-to-{End} {Vision}-and-{Language} {Transformers}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Dou_An_Empirical_Study_of_Training_End-to-End_Vision-and-Language_Transformers_CVPR_2022_paper.html},
	language = {en},
	urldate = {2023-02-18},
	author = {Dou, Zi-Yi and Xu, Yichong and Gan, Zhe and Wang, Jianfeng and Wang, Shuohang and Wang, Lijuan and Zhu, Chenguang and Zhang, Pengchuan and Yuan, Lu and Peng, Nanyun and Liu, Zicheng and Zeng, Michael},
	year = {2022},
	pages = {18166--18176},
}

@inproceedings{tan_efficientnetv2_2021,
	title = {{EfficientNetV2}: {Smaller} {Models} and {Faster} {Training}},
	shorttitle = {{EfficientNetV2}},
	url = {https://proceedings.mlr.press/v139/tan21a.html},
	abstract = {This paper introduces EfficientNetV2, a new family of convolutional networks that have faster training speed and better parameter efficiency than previous models. To develop these models, we use a combination of training-aware neural architecture search and scaling, to jointly optimize training speed and parameter efficiency. The models were searched from the search space enriched with new ops such as Fused-MBConv. Our experiments show that EfficientNetV2 models train much faster than state-of-the-art models while being up to 6.8x smaller. Our training can be further sped up by progressively increasing the image size during training, but it often causes a drop in accuracy. To compensate for this accuracy drop, we propose an improved method of progressive learning, which adaptively adjusts regularization (e.g. data augmentation) along with image size. With progressive learning, our EfficientNetV2 significantly outperforms previous models on ImageNet and CIFAR/Cars/Flowers datasets. By pretraining on the same ImageNet21k, our EfficientNetV2 achieves 87.3\% top-1 accuracy on ImageNet ILSVRC2012, outperforming the recent ViT by 2.0\% accuracy while training 5x-11x faster using the same computing resources.},
	language = {en},
	urldate = {2023-02-18},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Tan, Mingxing and Le, Quoc},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {10096--10106},
}

@inproceedings{jaegle_perceiver_2021,
	title = {Perceiver: {General} {Perception} with {Iterative} {Attention}},
	shorttitle = {Perceiver},
	url = {https://proceedings.mlr.press/v139/jaegle21a.html},
	abstract = {Biological systems understand the world by simultaneously processing high-dimensional inputs from modalities as diverse as vision, audition, touch, proprioception, etc. The perception models used in deep learning on the other hand are designed for individual modalities, often relying on domain-specific assumptions such as the local grid structures exploited by virtually all existing vision models. These priors introduce helpful inductive biases, but also lock models to individual modalities. In this paper we introduce the Perceiver \{–\} a model that builds upon Transformers and hence makes few architectural assumptions about the relationship between its inputs, but that also scales to hundreds of thousands of inputs, like ConvNets. The model leverages an asymmetric attention mechanism to iteratively distill inputs into a tight latent bottleneck, allowing it to scale to handle very large inputs. We show that this architecture is competitive with or outperforms strong, specialized models on classification tasks across various modalities: images, point clouds, audio, video and video+audio. The Perceiver obtains performance comparable to ResNet-50 and ViT on ImageNet without 2D convolutions by directly attending to 50,000 pixels. It is also competitive in all modalities in AudioSet.},
	language = {en},
	urldate = {2023-02-18},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Jaegle, Andrew and Gimeno, Felix and Brock, Andy and Vinyals, Oriol and Zisserman, Andrew and Carreira, Joao},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {4651--4664},
}

@inproceedings{tolstikhin_mlp-mixer_2021,
	title = {{MLP}-{Mixer}: {An} all-{MLP} {Architecture} for {Vision}},
	volume = {34},
	shorttitle = {{MLP}-{Mixer}},
	url = {https://proceedings.neurips.cc/paper/2021/hash/cba0a4ee5ccd02fda0fe3f9a3e7b89fe-Abstract.html},
	abstract = {Convolutional Neural Networks (CNNs) are the go-to model for computer vision. Recently, attention-based networks, such as the Vision Transformer, have also become popular. In this paper we show that while convolutions and attention are both sufficient for good performance, neither of them are necessary. We present MLP-Mixer, an architecture based exclusively on multi-layer perceptrons (MLPs). MLP-Mixer contains two types of layers: one with MLPs applied independently to image patches (i.e. "mixing" the per-location features), and one with MLPs applied across patches (i.e. "mixing" spatial information). When trained on large datasets, or with modern regularization schemes, MLP-Mixer attains competitive scores on image classification benchmarks, with pre-training and inference cost comparable to state-of-the-art models. We hope that these results spark further research beyond the realms of well established CNNs and Transformers.},
	urldate = {2023-02-18},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Tolstikhin, Ilya O and Houlsby, Neil and Kolesnikov, Alexander and Beyer, Lucas and Zhai, Xiaohua and Unterthiner, Thomas and Yung, Jessica and Steiner, Andreas and Keysers, Daniel and Uszkoreit, Jakob and Lucic, Mario and Dosovitskiy, Alexey},
	year = {2021},
	pages = {24261--24272},
}

@inproceedings{wu_self-supervised_2022,
	title = {Self-supervised {Models} are {Good} {Teaching} {Assistants} for {Vision} {Transformers}},
	url = {https://proceedings.mlr.press/v162/wu22c.html},
	abstract = {Transformers have shown remarkable progress on computer vision tasks in the past year. Compared to their CNN counterparts, transformers usually need the help of distillation to achieve comparable results on middle or small sized datasets. Meanwhile, recent researches discover that when transformers are trained with supervised and self-supervised manner respectively, the captured patterns are quite different both qualitatively and quantitatively. These findings motivate us to introduce an self-supervised teaching assistant (SSTA) besides the commonly used supervised teacher to improve the performance of transformers. Specifically, we propose a head-level knowledge distillation method that selects the most important head of the supervised teacher and self-supervised teaching assistant, and let the student mimic the attention distribution of these two heads, so as to make the student focus on the relationship between tokens deemed by the teacher and the teacher assistant. Extensive experiments verify the effectiveness of SSTA and demonstrate that the proposed SSTA is a good compensation to the supervised teacher. Meanwhile, some analytical experiments towards multiple perspectives (e.g. prediction, shape bias, robustness, and transferability to downstream tasks) with supervised teachers, self-supervised teaching assistants and students are inductive and may inspire future researches.},
	language = {en},
	urldate = {2023-02-18},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Wu, Haiyan and Gao, Yuting and Zhang, Yinqi and Lin, Shaohui and Xie, Yuan and Sun, Xing and Li, Ke},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {24031--24042},
}

@inproceedings{gao_pyramidclip_2022,
	title = {{PyramidCLIP}: {Hierarchical} {Feature} {Alignment} for {Vision}-language {Model} {Pretraining}},
	shorttitle = {{PyramidCLIP}},
	url = {https://openreview.net/forum?id=7YTh6S8HIY},
	abstract = {Large-scale vision-language pre-training has achieved promising results on downstream tasks. Existing methods highly rely on the assumption that the image-text pairs crawled from the Internet are in perfect one-to-one correspondence. However, in real scenarios, this assumption can be difficult to hold: the text description, obtained by crawling the affiliated metadata of the image, often suffers from the semantic mismatch and the mutual compatibility. To address these issues, we introduce PyramidCLIP, which constructs an input pyramid with different semantic levels for each modality, and aligns visual elements and linguistic elements in the form of hierarchy via peer-level semantics alignment and cross-level relation alignment. Furthermore, we soften the loss of negative samples (unpaired samples) so as to weaken the strict constraint during the pre-training stage, thus mitigating the risk of forcing the model to distinguish compatible negative pairs. Experiments on five downstream tasks demonstrate the effectiveness of the proposed PyramidCLIP. In particular, with the same amount of 15 million pre-training image-text pairs, PyramidCLIP exceeds CLIP on ImageNet zero-shot classification top-1 accuracy by 10.6\%/13.2\%/10.0\% with ResNet50/ViT-B32/ViT-B16 based image encoder respectively. When scaling to larger datasets, PyramidCLIP achieves the state-of-the-art results on several downstream tasks. In particular, the results of PyramidCLIP-ResNet50 trained on 143M image-text pairs surpass that of CLIP using 400M data on ImageNet zero-shot classification task, significantly improving the data efficiency of CLIP.},
	language = {en},
	urldate = {2023-02-18},
	author = {Gao, Yuting and Liu, Jinfeng and Xu, Zihan and Zhang, Jun and Li, Ke and Ji, Rongrong and Shen, Chunhua},
	month = oct,
	year = {2022},
}

@inproceedings{liu_convnet_2022,
	title = {A {ConvNet} for the 2020s},
	abstract = {The “Roaring 20s” of visual recognition began with the introduction of Vision Transformers (ViTs), which quickly superseded ConvNets as the state-of-the-art image classiﬁcation model. A vanilla ViT, on the other hand, faces difﬁculties when applied to general computer vision tasks such as object detection and semantic segmentation. It is the hierarchical Transformers (e.g., Swin Transformers) that reintroduced several ConvNet priors, making Transformers practically viable as a generic vision backbone and demonstrating remarkable performance on a wide variety of vision tasks. However, the effectiveness of such hybrid approaches is still largely credited to the intrinsic superiority of Transformers, rather than the inherent inductive biases of convolutions. In this work, we reexamine the design spaces and test the limits of what a pure ConvNet can achieve. We gradually “modernize” a standard ResNet toward the design of a vision Transformer, and discover several key components that contribute to the performance difference along the way. The outcome of this exploration is a family of pure ConvNet models dubbed ConvNeXt. Constructed entirely from standard ConvNet modules, ConvNeXts compete favorably with Transformers in terms of accuracy and scalability, achieving 87.8\% ImageNet top-1 accuracy and outperforming Swin Transformers on COCO detection and ADE20K segmentation, while maintaining the simplicity and efﬁciency of standard ConvNets.},
	language = {en},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Liu, Zhuang and Mao, Hanzi and Wu, Chao-Yuan and Feichtenhofer, Christoph and Darrell, Trevor and Xie, Saining},
	year = {2022},
}

@misc{chen_pali_2022,
	title = {{PaLI}: {A} {Jointly}-{Scaled} {Multilingual} {Language}-{Image} {Model}},
	shorttitle = {{PaLI}},
	url = {http://arxiv.org/abs/2209.06794},
	doi = {10.48550/arXiv.2209.06794},
	abstract = {Effective scaling and a flexible task interface enable large language models to excel at many tasks. PaLI (Pathways Language and Image model) extends this approach to the joint modeling of language and vision. PaLI generates text based on visual and textual inputs, and with this interface performs many vision, language, and multimodal tasks, in many languages. To train PaLI, we make use of large pretrained encoder-decoder language models and Vision Transformers (ViTs). This allows us to capitalize on their existing capabilities and leverage the substantial cost of training them. We find that joint scaling of the vision and language components is important. Since existing Transformers for language are much larger than their vision counterparts, we train the largest ViT to date (ViT-e) to quantify the benefits from even larger-capacity vision models. To train PaLI, we create a large multilingual mix of pretraining tasks, based on a new image-text training set containing 10B images and texts in over 100 languages. PaLI achieves state-of-the-art in multiple vision and language tasks (such as captioning, visual question-answering, scene-text understanding), while retaining a simple, modular, and scalable design.},
	urldate = {2023-02-18},
	publisher = {arXiv},
	author = {Chen, Xi and Wang, Xiao and Changpinyo, Soravit and Piergiovanni, A. J. and Padlewski, Piotr and Salz, Daniel and Goodman, Sebastian and Grycner, Adam and Mustafa, Basil and Beyer, Lucas and Kolesnikov, Alexander and Puigcerver, Joan and Ding, Nan and Rong, Keran and Akbari, Hassan and Mishra, Gaurav and Xue, Linting and Thapliyal, Ashish and Bradbury, James and Kuo, Weicheng and Seyedhosseini, Mojtaba and Jia, Chao and Ayan, Burcu Karagol and Riquelme, Carlos and Steiner, Andreas and Angelova, Anelia and Zhai, Xiaohua and Houlsby, Neil and Soricut, Radu},
	month = sep,
	year = {2022},
	note = {arXiv:2209.06794 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{sun_revisiting_2017,
	address = {Venice},
	title = {Revisiting {Unreasonable} {Effectiveness} of {Data} in {Deep} {Learning} {Era}},
	isbn = {978-1-5386-1032-9},
	url = {http://ieeexplore.ieee.org/document/8237359/},
	doi = {10.1109/ICCV.2017.97},
	abstract = {The success of deep learning in vision can be attributed to: (a) models with high capacity; (b) increased computational power; and (c) availability of large-scale labeled data. Since 2012, there have been signiﬁcant advances in representation capabilities of the models and computational capabilities of GPUs. But the size of the biggest dataset has surprisingly remained constant. What will happen if we increase the dataset size by 10× or 100×? This paper takes a step towards clearing the clouds of mystery surrounding the relationship between ‘enormous data’ and visual deep learning. By exploiting the JFT-300M dataset which has more than 375M noisy labels for 300M images, we investigate how the performance of current vision tasks would change if this data was used for representation learning. Our paper delivers some surprising (and some expected) ﬁndings. First, we ﬁnd that the performance on vision tasks increases logarithmically based on volume of training data size. Second, we show that representation learning (or pretraining) still holds a lot of promise. One can improve performance on many vision tasks by just training a better base model. Finally, as expected, we present new state-of-theart results for different vision tasks including image classiﬁcation, object detection, semantic segmentation and human pose estimation. Our sincere hope is that this inspires vision community to not undervalue the data and develop collective efforts in building larger datasets.},
	language = {en},
	urldate = {2023-02-18},
	booktitle = {2017 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Sun, Chen and Shrivastava, Abhinav and Singh, Saurabh and Gupta, Abhinav},
	month = oct,
	year = {2017},
	pages = {843--852},
}

@inproceedings{kolesnikov_big_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Big {Transfer} ({BiT}): {General} {Visual} {Representation} {Learning}},
	isbn = {978-3-030-58558-7},
	shorttitle = {Big {Transfer} ({BiT})},
	doi = {10.1007/978-3-030-58558-7_29},
	abstract = {Transfer of pre-trained representations improves sample efficiency and simplifies hyperparameter tuning when training deep neural networks for vision. We revisit the paradigm of pre-training on large supervised datasets and fine-tuning the model on a target task. We scale up pre-training, and propose a simple recipe that we call Big Transfer (BiT). By combining a few carefully selected components, and transferring using a simple heuristic, we achieve strong performance on over 20 datasets. BiT performs well across a surprisingly wide range of data regimes—from 1 example per class to 1M total examples. BiT achieves 87.5\% top-1 accuracy on ILSVRC-2012, 99.4\% on CIFAR-10, and 76.3\% on the 19 task Visual Task Adaptation Benchmark (VTAB). On small datasets, BiT attains 76.8\% on ILSVRC-2012 with 10 examples per class, and 97.0\% on CIFAR-10 with 10 examples per class. We conduct detailed analysis of the main components that lead to high transfer performance.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Kolesnikov, Alexander and Beyer, Lucas and Zhai, Xiaohua and Puigcerver, Joan and Yung, Jessica and Gelly, Sylvain and Houlsby, Neil},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	pages = {491--507},
}

@inproceedings{chen_simple_2020,
	title = {A {Simple} {Framework} for {Contrastive} {Learning} of {Visual} {Representations}},
	url = {https://proceedings.mlr.press/v119/chen20j.html},
	abstract = {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5\% top-1 accuracy, which is a 7\% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1\% of the labels, we achieve 85.8\% top-5 accuracy, outperforming AlexNet with 100X fewer labels.},
	language = {en},
	urldate = {2023-02-18},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {1597--1607},
}

@inproceedings{henaff_efficient_2021,
	title = {Efficient {Visual} {Pretraining} {With} {Contrastive} {Detection}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Henaff_Efficient_Visual_Pretraining_With_Contrastive_Detection_ICCV_2021_paper.html},
	language = {en},
	urldate = {2023-02-18},
	author = {Hénaff, Olivier J. and Koppula, Skanda and Alayrac, Jean-Baptiste and van den Oord, Aaron and Vinyals, Oriol and Carreira, João},
	year = {2021},
	pages = {10086--10096},
}

@misc{wu_visual_2020,
	title = {Visual {Transformers}: {Token}-based {Image} {Representation} and {Processing} for {Computer} {Vision}},
	shorttitle = {Visual {Transformers}},
	url = {http://arxiv.org/abs/2006.03677},
	doi = {10.48550/arXiv.2006.03677},
	abstract = {Computer vision has achieved remarkable success by (a) representing images as uniformly-arranged pixel arrays and (b) convolving highly-localized features. However, convolutions treat all image pixels equally regardless of importance; explicitly model all concepts across all images, regardless of content; and struggle to relate spatially-distant concepts. In this work, we challenge this paradigm by (a) representing images as semantic visual tokens and (b) running transformers to densely model token relationships. Critically, our Visual Transformer operates in a semantic token space, judiciously attending to different image parts based on context. This is in sharp contrast to pixel-space transformers that require orders-of-magnitude more compute. Using an advanced training recipe, our VTs significantly outperform their convolutional counterparts, raising ResNet accuracy on ImageNet top-1 by 4.6 to 7 points while using fewer FLOPs and parameters. For semantic segmentation on LIP and COCO-stuff, VT-based feature pyramid networks (FPN) achieve 0.35 points higher mIoU while reducing the FPN module's FLOPs by 6.5x.},
	urldate = {2023-02-18},
	publisher = {arXiv},
	author = {Wu, Bichen and Xu, Chenfeng and Dai, Xiaoliang and Wan, Alvin and Zhang, Peizhao and Yan, Zhicheng and Tomizuka, Masayoshi and Gonzalez, Joseph and Keutzer, Kurt and Vajda, Peter},
	month = nov,
	year = {2020},
	note = {arXiv:2006.03677 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing},
}

@misc{dehghani_scaling_2023,
	title = {Scaling {Vision} {Transformers} to 22 {Billion} {Parameters}},
	url = {http://arxiv.org/abs/2302.05442},
	doi = {10.48550/arXiv.2302.05442},
	abstract = {The scaling of Transformers has driven breakthrough capabilities for language models. At present, the largest large language models (LLMs) contain upwards of 100B parameters. Vision Transformers (ViT) have introduced the same architecture to image and video modelling, but these have not yet been successfully scaled to nearly the same degree; the largest dense ViT contains 4B parameters (Chen et al., 2022). We present a recipe for highly efficient and stable training of a 22B-parameter ViT (ViT-22B) and perform a wide variety of experiments on the resulting model. When evaluated on downstream tasks (often with a lightweight linear model on frozen features), ViT-22B demonstrates increasing performance with scale. We further observe other interesting benefits of scale, including an improved tradeoff between fairness and performance, state-of-the-art alignment to human visual perception in terms of shape/texture bias, and improved robustness. ViT-22B demonstrates the potential for "LLM-like" scaling in vision, and provides key steps towards getting there.},
	urldate = {2023-02-17},
	publisher = {arXiv},
	author = {Dehghani, Mostafa and Djolonga, Josip and Mustafa, Basil and Padlewski, Piotr and Heek, Jonathan and Gilmer, Justin and Steiner, Andreas and Caron, Mathilde and Geirhos, Robert and Alabdulmohsin, Ibrahim and Jenatton, Rodolphe and Beyer, Lucas and Tschannen, Michael and Arnab, Anurag and Wang, Xiao and Riquelme, Carlos and Minderer, Matthias and Puigcerver, Joan and Evci, Utku and Kumar, Manoj and van Steenkiste, Sjoerd and Elsayed, Gamaleldin F. and Mahendran, Aravindh and Yu, Fisher and Oliver, Avital and Huot, Fantine and Bastings, Jasmijn and Collier, Mark Patrick and Gritsenko, Alexey and Birodkar, Vighnesh and Vasconcelos, Cristina and Tay, Yi and Mensink, Thomas and Kolesnikov, Alexander and Pavetić, Filip and Tran, Dustin and Kipf, Thomas and Lučić, Mario and Zhai, Xiaohua and Keysers, Daniel and Harmsen, Jeremiah and Houlsby, Neil},
	month = feb,
	year = {2023},
	note = {arXiv:2302.05442 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@inproceedings{vaswani_attention_2017,
	title = {Attention is {All} you {Need}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
	abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.},
	urldate = {2023-02-16},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
	year = {2017},
}

@inproceedings{jia_scaling_2021,
	title = {Scaling {Up} {Visual} and {Vision}-{Language} {Representation} {Learning} {With} {Noisy} {Text} {Supervision}},
	url = {https://proceedings.mlr.press/v139/jia21b.html},
	abstract = {Pre-trained representations are becoming crucial for many NLP and perception tasks. While representation learning in NLP has transitioned to training on raw text without human annotations, visual and vision-language representations still rely heavily on curated training datasets that are expensive or require expert knowledge. For vision applications, representations are mostly learned using datasets with explicit class labels such as ImageNet or OpenImages. For vision-language, popular datasets like Conceptual Captions, MSCOCO, or CLIP all involve a non-trivial data collection (and cleaning) process. This costly curation process limits the size of datasets and hence hinders the scaling of trained models. In this paper, we leverage a noisy dataset of over one billion image alt-text pairs, obtained without expensive filtering or post-processing steps in the Conceptual Captions dataset. A simple dual-encoder architecture learns to align visual and language representations of the image and text pairs using a contrastive loss. We show that the scale of our corpus can make up for its noise and leads to state-of-the-art representations even with such a simple learning scheme. Our visual representation achieves strong performance when transferred to classification tasks such as ImageNet and VTAB. The aligned visual and language representations enables zero-shot image classification and also set new state-of-the-art results on Flickr30K and MSCOCO image-text retrieval benchmarks, even when compared with more sophisticated cross-attention models. The representations also enable cross-modality search with complex text and text + image queries.},
	language = {en},
	urldate = {2023-02-16},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Jia, Chao and Yang, Yinfei and Xia, Ye and Chen, Yi-Ting and Parekh, Zarana and Pham, Hieu and Le, Quoc and Sung, Yun-Hsuan and Li, Zhen and Duerig, Tom},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {4904--4916},
}

@inproceedings{changpinyo_conceptual_2021,
	title = {Conceptual {12M}: {Pushing} {Web}-{Scale} {Image}-{Text} {Pre}-{Training} {To} {Recognize} {Long}-{Tail} {Visual} {Concepts}},
	shorttitle = {Conceptual {12M}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Changpinyo_Conceptual_12M_Pushing_Web-Scale_Image-Text_Pre-Training_To_Recognize_Long-Tail_Visual_CVPR_2021_paper.html},
	language = {en},
	urldate = {2023-02-16},
	author = {Changpinyo, Soravit and Sharma, Piyush and Ding, Nan and Soricut, Radu},
	year = {2021},
	pages = {3558--3568},
}

@inproceedings{radford_learning_2021,
	title = {Learning {Transferable} {Visual} {Models} {From} {Natural} {Language} {Supervision}},
	url = {https://proceedings.mlr.press/v139/radford21a.html},
	abstract = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on.},
	language = {en},
	urldate = {2023-02-16},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {8748--8763},
}

@inproceedings{ramesh_zero-shot_2021,
	title = {Zero-{Shot} {Text}-to-{Image} {Generation}},
	url = {https://proceedings.mlr.press/v139/ramesh21a.html},
	abstract = {Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion.},
	language = {en},
	urldate = {2023-02-16},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {8821--8831},
}

@inproceedings{nanda_fairness_2021,
	address = {New York, NY, USA},
	series = {{FAccT} '21},
	title = {Fairness {Through} {Robustness}: {Investigating} {Robustness} {Disparity} in {Deep} {Learning}},
	isbn = {978-1-4503-8309-7},
	shorttitle = {Fairness {Through} {Robustness}},
	url = {https://doi.org/10.1145/3442188.3445910},
	doi = {10.1145/3442188.3445910},
	abstract = {Deep neural networks (DNNs) are increasingly used in real-world applications (e.g. facial recognition). This has resulted in concerns about the fairness of decisions made by these models. Various notions and measures of fairness have been proposed to ensure that a decision-making system does not disproportionately harm (or benefit) particular subgroups of the population. In this paper, we argue that traditional notions of fairness that are only based on models' outputs are not sufficient when the model is vulnerable to adversarial attacks. We argue that in some cases, it may be easier for an attacker to target a particular subgroup, resulting in a form of robustness bias. We show that measuring robustness bias is a challenging task for DNNs and propose two methods to measure this form of bias. We then conduct an empirical study on state-of-the-art neural networks on commonly used real-world datasets such as CIFAR-10, CIFAR-100, Adience, and UTKFace and show that in almost all cases there are subgroups (in some cases based on sensitive attributes like race, gender, etc) which are less robust and are thus at a disadvantage. We argue that this kind of bias arises due to both the data distribution and the highly complex nature of the learned decision boundary in the case of DNNs, thus making mitigation of such biases a non-trivial task. Our results show that robustness bias is an important criterion to consider while auditing real-world systems that rely on DNNs for decision making. Code to reproduce all our results can be found here: https://github.com/nvedant07/Fairness-Through-Robustness},
	urldate = {2023-02-15},
	booktitle = {Proceedings of the 2021 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {Association for Computing Machinery},
	author = {Nanda, Vedant and Dooley, Samuel and Singla, Sahil and Feizi, Soheil and Dickerson, John P.},
	month = mar,
	year = {2021},
	pages = {466--477},
}

@inproceedings{bender_dangers_2021,
	address = {New York, NY, USA},
	series = {{FAccT} '21},
	title = {On the {Dangers} of {Stochastic} {Parrots}: {Can} {Language} {Models} {Be} {Too} {Big}? \&\#x1f99c;},
	isbn = {978-1-4503-8309-7},
	shorttitle = {On the {Dangers} of {Stochastic} {Parrots}},
	url = {https://doi.org/10.1145/3442188.3445922},
	doi = {10.1145/3442188.3445922},
	abstract = {The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.},
	urldate = {2023-02-15},
	booktitle = {Proceedings of the 2021 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {Association for Computing Machinery},
	author = {Bender, Emily M. and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
	month = mar,
	year = {2021},
	pages = {610--623},
}

@misc{bommasani_opportunities_2022,
	title = {On the {Opportunities} and {Risks} of {Foundation} {Models}},
	url = {http://arxiv.org/abs/2108.07258},
	doi = {10.48550/arXiv.2108.07258},
	abstract = {AI is undergoing a paradigm shift with the rise of models (e.g., BERT, DALL-E, GPT-3) that are trained on broad data at scale and are adaptable to a wide range of downstream tasks. We call these models foundation models to underscore their critically central yet incomplete character. This report provides a thorough account of the opportunities and risks of foundation models, ranging from their capabilities (e.g., language, vision, robotics, reasoning, human interaction) and technical principles(e.g., model architectures, training procedures, data, systems, security, evaluation, theory) to their applications (e.g., law, healthcare, education) and societal impact (e.g., inequity, misuse, economic and environmental impact, legal and ethical considerations). Though foundation models are based on standard deep learning and transfer learning, their scale results in new emergent capabilities,and their effectiveness across so many tasks incentivizes homogenization. Homogenization provides powerful leverage but demands caution, as the defects of the foundation model are inherited by all the adapted models downstream. Despite the impending widespread deployment of foundation models, we currently lack a clear understanding of how they work, when they fail, and what they are even capable of due to their emergent properties. To tackle these questions, we believe much of the critical research on foundation models will require deep interdisciplinary collaboration commensurate with their fundamentally sociotechnical nature.},
	urldate = {2023-02-16},
	publisher = {arXiv},
	author = {Bommasani, Rishi and Hudson, Drew A. and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S. and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and Brynjolfsson, Erik and Buch, Shyamal and Card, Dallas and Castellon, Rodrigo and Chatterji, Niladri and Chen, Annie and Creel, Kathleen and Davis, Jared Quincy and Demszky, Dora and Donahue, Chris and Doumbouya, Moussa and Durmus, Esin and Ermon, Stefano and Etchemendy, John and Ethayarajh, Kawin and Fei-Fei, Li and Finn, Chelsea and Gale, Trevor and Gillespie, Lauren and Goel, Karan and Goodman, Noah and Grossman, Shelby and Guha, Neel and Hashimoto, Tatsunori and Henderson, Peter and Hewitt, John and Ho, Daniel E. and Hong, Jenny and Hsu, Kyle and Huang, Jing and Icard, Thomas and Jain, Saahil and Jurafsky, Dan and Kalluri, Pratyusha and Karamcheti, Siddharth and Keeling, Geoff and Khani, Fereshte and Khattab, Omar and Koh, Pang Wei and Krass, Mark and Krishna, Ranjay and Kuditipudi, Rohith and Kumar, Ananya and Ladhak, Faisal and Lee, Mina and Lee, Tony and Leskovec, Jure and Levent, Isabelle and Li, Xiang Lisa and Li, Xuechen and Ma, Tengyu and Malik, Ali and Manning, Christopher D. and Mirchandani, Suvir and Mitchell, Eric and Munyikwa, Zanele and Nair, Suraj and Narayan, Avanika and Narayanan, Deepak and Newman, Ben and Nie, Allen and Niebles, Juan Carlos and Nilforoshan, Hamed and Nyarko, Julian and Ogut, Giray and Orr, Laurel and Papadimitriou, Isabel and Park, Joon Sung and Piech, Chris and Portelance, Eva and Potts, Christopher and Raghunathan, Aditi and Reich, Rob and Ren, Hongyu and Rong, Frieda and Roohani, Yusuf and Ruiz, Camilo and Ryan, Jack and Ré, Christopher and Sadigh, Dorsa and Sagawa, Shiori and Santhanam, Keshav and Shih, Andy and Srinivasan, Krishnan and Tamkin, Alex and Taori, Rohan and Thomas, Armin W. and Tramèr, Florian and Wang, Rose E. and Wang, William and Wu, Bohan and Wu, Jiajun and Wu, Yuhuai and Xie, Sang Michael and Yasunaga, Michihiro and You, Jiaxuan and Zaharia, Matei and Zhang, Michael and Zhang, Tianyi and Zhang, Xikun and Zhang, Yuhui and Zheng, Lucia and Zhou, Kaitlyn and Liang, Percy},
	month = jul,
	year = {2022},
	note = {arXiv:2108.07258 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computers and Society, Computer Science - Machine Learning},
}

@article{liu_mitigating_2023,
	title = {Mitigating robust overfitting via self-residual-calibration regularization},
	volume = {317},
	issn = {0004-3702},
	url = {https://www.sciencedirect.com/science/article/pii/S0004370223000231},
	doi = {10.1016/j.artint.2023.103877},
	abstract = {Overfitting in adversarial training has attracted the interest of researchers in the community of artificial intelligence and machine learning in recent years. To address this issue, in this paper we begin by evaluating the defense performances of several calibration methods on various robust models. Our analysis and experiments reveal two intriguing properties: 1) a well-calibrated robust model is decreasing the confidence of robust model; 2) there is a trade-off between the confidences of natural and adversarial images. These new properties offer a straightforward insight into designing a simple but effective regularization, called Self-Residual-Calibration (SRC). The proposed SRC calculates the absolute residual between adversarial and natural logit features corresponding to the ground-truth labels. Furthermore, we utilize the pinball loss to minimize the quantile residual between them, resulting in more robust regularization. Extensive experiments indicate that our SRC can effectively mitigate the overfitting problem while improving the robustness of state-of-the-art models. Importantly, SRC is complementary to various regularization methods. When combined with them, we are capable of achieving the top-rank performance on the AutoAttack benchmark leaderboard.},
	language = {en},
	urldate = {2023-02-15},
	journal = {Artificial Intelligence},
	author = {Liu, Hong and Zhong, Zhun and Sebe, Nicu and Satoh, Shin'ichi},
	month = apr,
	year = {2023},
	keywords = {Adversarial defense, Adversarial training, Regularization, Robust overfitting, Self-residual-calibration},
	pages = {103877},
}

@article{goldblum_dataset_2023,
	title = {Dataset {Security} for {Machine} {Learning}: {Data} {Poisoning}, {Backdoor} {Attacks}, and {Defenses}},
	volume = {45},
	issn = {1939-3539},
	shorttitle = {Dataset {Security} for {Machine} {Learning}},
	doi = {10.1109/TPAMI.2022.3162397},
	abstract = {As machine learning systems grow in scale, so do their training data requirements, forcing practitioners to automate and outsource the curation of training data in order to achieve state-of-the-art performance. The absence of trustworthy human supervision over the data collection process exposes organizations to security vulnerabilities; training data can be manipulated to control and degrade the downstream behaviors of learned models. The goal of this work is to systematically categorize and discuss a wide range of dataset vulnerabilities and exploits, approaches for defending against these threats, and an array of open problems in this space.},
	number = {2},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Goldblum, Micah and Tsipras, Dimitris and Xie, Chulin and Chen, Xinyun and Schwarzschild, Avi and Song, Dawn and Mądry, Aleksander and Li, Bo and Goldstein, Tom},
	month = feb,
	year = {2023},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Data models, Data poisoning, Security, Servers, Toxicology, Training, Training data, Unsolicited e-mail, backdoor attacks, dataset security},
	pages = {1563--1580},
}

@article{khan_transformers_2022,
	title = {Transformers in {Vision}: {A} {Survey}},
	volume = {54},
	issn = {0360-0300},
	shorttitle = {Transformers in {Vision}},
	url = {https://doi.org/10.1145/3505244},
	doi = {10.1145/3505244},
	abstract = {Astounding results from Transformer models on natural language tasks have intrigued the vision community to study their application to computer vision problems. Among their salient benefits, Transformers enable modeling long dependencies between input sequence elements and support parallel processing of sequence as compared to recurrent networks, e.g., Long short-term memory. Different from convolutional networks, Transformers require minimal inductive biases for their design and are naturally suited as set-functions. Furthermore, the straightforward design of Transformers allows processing multiple modalities (e.g., images, videos, text, and speech) using similar processing blocks and demonstrates excellent scalability to very large capacity networks and huge datasets. These strengths have led to exciting progress on a number of vision tasks using Transformer networks. This survey aims to provide a comprehensive overview of the Transformer models in the computer vision discipline. We start with an introduction to fundamental concepts behind the success of Transformers, i.e., self-attention, large-scale pre-training, and bidirectional feature encoding. We then cover extensive applications of transformers in vision including popular recognition tasks (e.g., image classification, object detection, action recognition, and segmentation), generative modeling, multi-modal tasks (e.g., visual-question answering, visual reasoning, and visual grounding), video processing (e.g., activity recognition, video forecasting), low-level vision (e.g., image super-resolution, image enhancement, and colorization), and three-dimensional analysis (e.g., point cloud classification and segmentation). We compare the respective advantages and limitations of popular techniques both in terms of architectural design and their experimental value. Finally, we provide an analysis on open research directions and possible future works. We hope this effort will ignite further interest in the community to solve current challenges toward the application of transformer models in computer vision.},
	number = {10s},
	urldate = {2023-02-14},
	journal = {ACM Computing Surveys},
	author = {Khan, Salman and Naseer, Muzammal and Hayat, Munawar and Zamir, Syed Waqas and Khan, Fahad Shahbaz and Shah, Mubarak},
	month = sep,
	year = {2022},
	keywords = {Self-attention, bidirectional encoders, convolutional networks, deep neural networks, literature survey, self-supervision, transformers},
	pages = {200:1--200:41},
}

@article{schramowski_large_2022,
	title = {Large pre-trained language models contain human-like biases of what is right and wrong to do},
	volume = {4},
	copyright = {2022 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {2522-5839},
	url = {https://www.nature.com/articles/s42256-022-00458-8},
	doi = {10.1038/s42256-022-00458-8},
	abstract = {Artificial writing is permeating our lives due to recent advances in large-scale, transformer-based language models (LMs) such as BERT, GPT-2 and GPT-3. Using them as pre-trained models and fine-tuning them for specific tasks, researchers have extended the state of the art for many natural language processing tasks and shown that they capture not only linguistic knowledge but also retain general knowledge implicitly present in the data. Unfortunately, LMs trained on unfiltered text corpora suffer from degenerated and biased behaviour. While this is well established, we show here that recent LMs also contain human-like biases of what is right and wrong to do, reflecting existing ethical and moral norms of society. We show that these norms can be captured geometrically by a ‘moral direction’ which can be computed, for example, by a PCA, in the embedding space. The computed ‘moral direction’ can rate the normativity (or non-normativity) of arbitrary phrases without explicitly training the LM for this task, reflecting social norms well. We demonstrate that computing the ’moral direction’ can provide a path for attenuating or even preventing toxic degeneration in LMs, showcasing this capability on the RealToxicityPrompts testbed.},
	language = {en},
	number = {3},
	urldate = {2023-02-14},
	journal = {Nature Machine Intelligence},
	author = {Schramowski, Patrick and Turan, Cigdem and Andersen, Nico and Rothkopf, Constantin A. and Kersting, Kristian},
	month = mar,
	year = {2022},
	note = {Number: 3
Publisher: Nature Publishing Group},
	keywords = {Computer science, Language and linguistics},
	pages = {258--268},
}

@article{han_survey_2023,
	title = {A {Survey} on {Vision} {Transformer}},
	volume = {45},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2022.3152247},
	abstract = {Transformer, first applied to the field of natural language processing, is a type of deep neural network mainly based on the self-attention mechanism. Thanks to its strong representation capabilities, researchers are looking at ways to apply transformer to computer vision tasks. In a variety of visual benchmarks, transformer-based models perform similar to or better than other types of networks such as convolutional and recurrent neural networks. Given its high performance and less need for vision-specific inductive bias, transformer is receiving more and more attention from the computer vision community. In this paper, we review these vision transformer models by categorizing them in different tasks and analyzing their advantages and disadvantages. The main categories we explore include the backbone network, high/mid-level vision, low-level vision, and video processing. We also include efficient transformer methods for pushing transformer into real device-based applications. Furthermore, we also take a brief look at the self-attention mechanism in computer vision, as it is the base component in transformer. Toward the end of this paper, we discuss the challenges and provide several further research directions for vision transformers.},
	number = {1},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Han, Kai and Wang, Yunhe and Chen, Hanting and Chen, Xinghao and Guo, Jianyuan and Liu, Zhenhua and Tang, Yehui and Xiao, An and Xu, Chunjing and Xu, Yixing and Yang, Zhaohui and Zhang, Yiman and Tao, Dacheng},
	month = jan,
	year = {2023},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Computational modeling, Computer vision, Encoding, Object detection, Task analysis, Transformers, Visualization, high-level vision, low-level vision, self-attention, transformer, video},
	pages = {87--110},
}

@inproceedings{riquelme_scaling_2021,
	title = {Scaling {Vision} with {Sparse} {Mixture} of {Experts}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper/2021/hash/48237d9f2dea8c74c2a72126cf63d933-Abstract.html},
	abstract = {Sparsely-gated Mixture of Experts networks (MoEs) have demonstrated excellent scalability in Natural Language Processing. In Computer Vision, however, almost all performant networks are "dense", that is, every input is processed by every parameter. We present a Vision MoE (V-MoE), a sparse version of the Vision Transformer, that is scalable and competitive with the largest dense networks. When applied to image recognition, V-MoE matches the performance of state-of-the-art networks, while requiring as little as half of the compute at inference time. Further, we propose an extension to the routing algorithm that can prioritize subsets of each input across the entire batch, leading to adaptive per-image compute. This allows V-MoE to trade-off performance and compute smoothly at test-time. Finally, we demonstrate the potential of V-MoE to scale vision models, and train a 15B parameter model that attains 90.35\% on ImageNet.},
	urldate = {2023-02-14},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Riquelme, Carlos and Puigcerver, Joan and Mustafa, Basil and Neumann, Maxim and Jenatton, Rodolphe and Susano Pinto, André and Keysers, Daniel and Houlsby, Neil},
	year = {2021},
	pages = {8583--8595},
}

@misc{yuan_florence_2021,
	title = {Florence: {A} {New} {Foundation} {Model} for {Computer} {Vision}},
	shorttitle = {Florence},
	url = {http://arxiv.org/abs/2111.11432},
	doi = {10.48550/arXiv.2111.11432},
	abstract = {Automated visual understanding of our diverse and open world demands computer vision models to generalize well with minimal customization for specific tasks, similar to human vision. Computer vision foundation models, which are trained on diverse, large-scale dataset and can be adapted to a wide range of downstream tasks, are critical for this mission to solve real-world computer vision applications. While existing vision foundation models such as CLIP, ALIGN, and Wu Dao 2.0 focus mainly on mapping images and textual representations to a cross-modal shared representation, we introduce a new computer vision foundation model, Florence, to expand the representations from coarse (scene) to fine (object), from static (images) to dynamic (videos), and from RGB to multiple modalities (caption, depth). By incorporating universal visual-language representations from Web-scale image-text data, our Florence model can be easily adapted for various computer vision tasks, such as classification, retrieval, object detection, VQA, image caption, video retrieval and action recognition. Moreover, Florence demonstrates outstanding performance in many types of transfer learning: fully sampled fine-tuning, linear probing, few-shot transfer and zero-shot transfer for novel images and objects. All of these properties are critical for our vision foundation model to serve general purpose vision tasks. Florence achieves new state-of-the-art results in majority of 44 representative benchmarks, e.g., ImageNet-1K zero-shot classification with top-1 accuracy of 83.74 and the top-5 accuracy of 97.18, 62.4 mAP on COCO fine tuning, 80.36 on VQA, and 87.8 on Kinetics-600.},
	urldate = {2023-02-14},
	publisher = {arXiv},
	author = {Yuan, Lu and Chen, Dongdong and Chen, Yi-Ling and Codella, Noel and Dai, Xiyang and Gao, Jianfeng and Hu, Houdong and Huang, Xuedong and Li, Boxin and Li, Chunyuan and Liu, Ce and Liu, Mengchen and Liu, Zicheng and Lu, Yumao and Shi, Yu and Wang, Lijuan and Wang, Jianfeng and Xiao, Bin and Xiao, Zhen and Yang, Jianwei and Zeng, Michael and Zhou, Luowei and Zhang, Pengchuan},
	month = nov,
	year = {2021},
	note = {arXiv:2111.11432 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{liu_adversarial_2020,
	title = {Adversarial {Training} for {Large} {Neural} {Language} {Models}},
	url = {http://arxiv.org/abs/2004.08994},
	doi = {10.48550/arXiv.2004.08994},
	abstract = {Generalization and robustness are both key desiderata for designing machine learning methods. Adversarial training can enhance robustness, but past work often finds it hurts generalization. In natural language processing (NLP), pre-training large neural language models such as BERT have demonstrated impressive gain in generalization for a variety of tasks, with further improvement from adversarial fine-tuning. However, these models are still vulnerable to adversarial attacks. In this paper, we show that adversarial pre-training can improve both generalization and robustness. We propose a general algorithm ALUM (Adversarial training for large neural LangUage Models), which regularizes the training objective by applying perturbations in the embedding space that maximizes the adversarial loss. We present the first comprehensive study of adversarial training in all stages, including pre-training from scratch, continual pre-training on a well-trained model, and task-specific fine-tuning. ALUM obtains substantial gains over BERT on a wide range of NLP tasks, in both regular and adversarial scenarios. Even for models that have been well trained on extremely large text corpora, such as RoBERTa, ALUM can still produce significant gains from continual pre-training, whereas conventional non-adversarial methods can not. ALUM can be further combined with task-specific fine-tuning to attain additional gains. The ALUM code is publicly available at https://github.com/namisan/mt-dnn.},
	urldate = {2023-02-14},
	publisher = {arXiv},
	author = {Liu, Xiaodong and Cheng, Hao and He, Pengcheng and Chen, Weizhu and Wang, Yu and Poon, Hoifung and Gao, Jianfeng},
	month = apr,
	year = {2020},
	note = {arXiv:2004.08994 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{mao_towards_2022,
	title = {Towards {Robust} {Vision} {Transformer}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Mao_Towards_Robust_Vision_Transformer_CVPR_2022_paper.html},
	language = {en},
	urldate = {2023-02-13},
	author = {Mao, Xiaofeng and Qi, Gege and Chen, Yuefeng and Li, Xiaodan and Duan, Ranjie and Ye, Shaokai and He, Yuan and Xue, Hui},
	year = {2022},
	pages = {12042--12051},
}

@misc{min_recent_2021,
	title = {Recent {Advances} in {Natural} {Language} {Processing} via {Large} {Pre}-{Trained} {Language} {Models}: {A} {Survey}},
	shorttitle = {Recent {Advances} in {Natural} {Language} {Processing} via {Large} {Pre}-{Trained} {Language} {Models}},
	url = {http://arxiv.org/abs/2111.01243},
	abstract = {Large, pre-trained transformer-based language models such as BERT have drastically changed the Natural Language Processing (NLP) ﬁeld. We present a survey of recent work that uses these large language models to solve NLP tasks via pre-training then ﬁne-tuning, prompting, or text generation approaches. We also present approaches that use pre-trained language models to generate data for training augmentation or other purposes. We conclude with discussions on limitations and suggested directions for future research.},
	language = {en},
	urldate = {2023-02-12},
	publisher = {arXiv},
	author = {Min, Bonan and Ross, Hayley and Sulem, Elior and Veyseh, Amir Pouran Ben and Nguyen, Thien Huu and Sainz, Oscar and Agirre, Eneko and Heinz, Ilana and Roth, Dan},
	month = nov,
	year = {2021},
	note = {arXiv:2111.01243 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{akhtar_advances_2021,
	title = {Advances in {Adversarial} {Attacks} and {Defenses} in {Computer} {Vision}: {A} {Survey}},
	volume = {9},
	issn = {2169-3536},
	shorttitle = {Advances in {Adversarial} {Attacks} and {Defenses} in {Computer} {Vision}},
	doi = {10.1109/ACCESS.2021.3127960},
	abstract = {Deep Learning is the most widely used tool in the contemporary field of computer vision. Its ability to accurately solve complex problems is employed in vision research to learn deep neural models for a variety of tasks, including security critical applications. However, it is now known that deep learning is vulnerable to adversarial attacks that can manipulate its predictions by introducing visually imperceptible perturbations in images and videos. Since the discovery of this phenomenon in 2013, it has attracted significant attention of researchers from multiple sub-fields of machine intelligence. In 2018, we published the first-ever review of the contributions made by the computer vision community in adversarial attacks on deep learning (and their defenses). Many of those contributions have inspired new directions in this area, which has matured significantly since witnessing the first generation methods. Hence, as a legacy sequel of our first literature survey, this review article focuses on the advances in this area since 2018. We thoroughly discuss the first generation attacks and comprehensively cover the modern attacks and their defenses appearing in the prestigious sources of computer vision and machine learning research. Besides offering the most comprehensive literature review of adversarial attacks and defenses to date, the article also provides concise definitions of technical terminologies for the non-experts. Finally, it discusses challenges and future outlook of this direction based on the literature since the advent of this research direction.},
	journal = {IEEE Access},
	author = {Akhtar, Naveed and Mian, Ajmal and Kardan, Navid and Shah, Mubarak},
	year = {2021},
	note = {Conference Name: IEEE Access},
	keywords = {Adversarial examples, Computational modeling, Computer vision, Data models, Deep learning, Perturbation methods, Predictive models, Training, adversarial defense, adversarial machine learning, black-box attack, deep learning, perturbation, white-box attack},
	pages = {155161--155196},
}

@article{meng_adversarial_2022,
	title = {Adversarial {Robustness} of {Deep} {Neural} {Networks}: {A} {Survey} from a {Formal} {Verification} {Perspective}},
	issn = {1941-0018},
	shorttitle = {Adversarial {Robustness} of {Deep} {Neural} {Networks}},
	doi = {10.1109/TDSC.2022.3179131},
	abstract = {Neural networks have been widely applied in security applications such as spam and phishing detection, intrusion prevention, and malware detection. This black-box method, however, often has uncertainty and poor explainability in applications. Furthermore, neural networks themselves are often vulnerable to adversarial attacks. For those reasons, there is a high demand for trustworthy and rigorous methods to verify the robustness of neural network models. Adversarial robustness, which concerns the reliability of a neural network when dealing with maliciously manipulated inputs, is one of the hottest topics in cybersecurity and machine learning. In this work, we survey existing literature in adversarial robustness verification for neural networks and collect 39 diversified research works across machine learning, security, and software engineering domains. We systematically analyze their approaches, including how robustness is formulated, what verification techniques are used, and the strengths and limitations of each technique. We provide a taxonomy from a formal verification perspective for a comprehensive understanding of this topic. We classify the existing techniques based on property specification, problem reduction, and reasoning strategies. We also demonstrate representative techniques that have been applied in existing studies with a sample model. Finally, we discuss open questions for future research.},
	journal = {IEEE Transactions on Dependable and Secure Computing},
	author = {Meng, Mark Huasong and Bai, Guangdong and Teo, Sin Gee and Hou, Zhe and Xiao, Yan and Lin, Yun and Dong, Jin Song},
	year = {2022},
	note = {Conference Name: IEEE Transactions on Dependable and Secure Computing},
	keywords = {Biological neural networks, Cognition, Deep learning, Model checking, Robustness, Taxonomy, Training, adversarial machine learning, deep learning, neural networks, security, verification},
	pages = {1--1},
}

@article{aldahdooh_adversarial_2022,
	title = {Adversarial example detection for {DNN} models: a review and experimental comparison},
	volume = {55},
	issn = {1573-7462},
	shorttitle = {Adversarial example detection for {DNN} models},
	url = {https://doi.org/10.1007/s10462-021-10125-w},
	doi = {10.1007/s10462-021-10125-w},
	abstract = {Deep learning (DL) has shown great success in many human-related tasks, which has led to its adoption in many computer vision based applications, such as security surveillance systems, autonomous vehicles and healthcare. Such safety-critical applications have to draw their path to success deployment once they have the capability to overcome safety-critical challenges. Among these challenges are the defense against or/and the detection of the adversarial examples (AEs). Adversaries can carefully craft small, often imperceptible, noise called perturbations to be added to the clean image to generate the AE. The aim of AE is to fool the DL model which makes it a potential risk for DL applications. Many test-time evasion attacks and countermeasures, i.e., defense or detection methods, are proposed in the literature. Moreover, few reviews and surveys were published and theoretically showed the taxonomy of the threats and the countermeasure methods with little focus in AE detection methods. In this paper, we focus on image classification task and attempt to provide a survey for detection methods of test-time evasion attacks on neural network classifiers. A detailed discussion for such methods is provided with experimental results for eight state-of-the-art detectors under different scenarios on four datasets. We also provide potential challenges and future perspectives for this research direction.},
	language = {en},
	number = {6},
	urldate = {2023-01-30},
	journal = {Artificial Intelligence Review},
	author = {Aldahdooh, Ahmed and Hamidouche, Wassim and Fezza, Sid Ahmed and Déforges, Olivier},
	month = aug,
	year = {2022},
	pages = {4403--4462},
}

@inproceedings{kotyan_towards_2020,
	address = {New York, NY, USA},
	series = {{GECCO} '20},
	title = {Towards evolving robust neural architectures to defend from adversarial attacks},
	isbn = {978-1-4503-7127-8},
	url = {https://doi.org/10.1145/3377929.3389962},
	doi = {10.1145/3377929.3389962},
	abstract = {Neural networks are known to misclassify a class of subtly modified images known as adversarial samples. Recently, numerous defences have been proposed against these adversarial samples; however, none have improved the robustness of neural networks consistently. Here, we propose to use adversarial samples as a function evaluation to explore for robust neural architectures that can resist such attacks. Experiments on existing neural architecture search algorithms from the literature reveal that although accurate, they are not able to find robust architectures. An essential cause for this lies in their confined search space. We were able to evolve an architecture that is intrinsically accurate on adversarial samples by creating a novel neural architecture search. Thus, the results here demonstrate that more robust architectures exist as well as opens up a new range of possibilities for the development and exploration of neural networks using neural architecture search.},
	urldate = {2023-01-27},
	booktitle = {Proceedings of the 2020 {Genetic} and {Evolutionary} {Computation} {Conference} {Companion}},
	publisher = {Association for Computing Machinery},
	author = {Kotyan, Shashank and Vargas, Danilo Vasconcellos},
	month = jul,
	year = {2020},
	keywords = {adversarial machine learning, deep learning, evolutionary computation, neuroevolution},
	pages = {135--136},
}

@inproceedings{chen_anti-bandit_2020,
	address = {Cham},
	title = {Anti-bandit {Neural} {Architecture} {Search} for {Model} {Defense}},
	volume = {12358},
	isbn = {978-3-030-58600-3 978-3-030-58601-0},
	url = {https://link.springer.com/10.1007/978-3-030-58601-0_5},
	doi = {10.1007/978-3-030-58601-0_5},
	abstract = {Deep convolutional neural networks (DCNNs) have dominated as the best performers in machine learning, but can be challenged by adversarial attacks. In this paper, we defend against adversarial attacks using neural architecture search (NAS) which is based on a comprehensive search of denoising blocks, weight-free operations, Gabor ﬁlters and convolutions. The resulting anti-bandit NAS (ABanditNAS) incorporates a new operation evaluation measure and search process based on the lower and upper conﬁdence bounds (LCB and UCB). Unlike the conventional bandit algorithm using UCB for evaluation only, we use UCB to abandon arms for search eﬃciency and LCB for a fair competition between arms. Extensive experiments demonstrate that ABanditNAS is about twice as fast as the state-of-the-art NAS method, while achieving an 8.73\% improvement over prior arts on CIFAR-10 under PGD-7.},
	language = {en},
	urldate = {2023-01-27},
	booktitle = {{ECCV}},
	publisher = {Springer International Publishing},
	author = {Chen, Hanlin and Zhang, Baochang and Xue, Song and Gong, Xuan and Liu, Hong and Ji, Rongrong and Doermann, David},
	year = {2020},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {70--85},
}

@inproceedings{chen_stabilizing_2020,
	title = {Stabilizing {Differentiable} {Architecture} {Search} via  {Perturbation}-based {Regularization}},
	abstract = {Differentiable architecture search (DARTS) is a prevailing NAS solution to identify architectures. Based on the continuous relaxation of the architecture space, DARTS learns a differentiable architecture weight and largely reduces the search cost. However, its stability has been challenged for yielding deteriorating architectures as the search proceeds. We ﬁnd that the precipitous validation loss landscape, which leads to a dramatic performance drop when distilling the ﬁnal architecture, is an essential factor that causes instability. Based on this observation, we propose a perturbationbased regularization - SmoothDARTS (SDARTS), to smooth the loss landscape and improve the generalizability of DARTS-based methods. In particular, our new formulations stabilize DARTSbased methods by either random smoothing or adversarial attack. The search trajectory on NASBench-1Shot1 demonstrates the effectiveness of our approach and due to the improved stability, we achieve performance gain across various search spaces on 4 datasets. Furthermore, we mathematically show that SDARTS implicitly regularizes the Hessian norm of the validation loss, which accounts for a smoother loss landscape and improved performance.},
	language = {en},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Chen, Xiangning and Hsieh, Cho-Jui},
	year = {2020},
}

@misc{dong_adversarially_2020,
	title = {Adversarially {Robust} {Neural} {Architectures}},
	url = {http://arxiv.org/abs/2009.00902},
	doi = {10.48550/arXiv.2009.00902},
	abstract = {Deep Neural Network (DNN) are vulnerable to adversarial attack. Existing methods are devoted to developing various robust training strategies or regularizations to update the weights of the neural network. But beyond the weights, the overall structure and information flow in the network are explicitly determined by the neural architecture, which remains unexplored. This paper thus aims to improve the adversarial robustness of the network from the architecture perspective with NAS framework. We explore the relationship among adversarial robustness, Lipschitz constant, and architecture parameters and show that an appropriate constraint on architecture parameters could reduce the Lipschitz constant to further improve the robustness. For NAS framework, all the architecture parameters are equally treated when the discrete architecture is sampled from supernet. However, the importance of architecture parameters could vary from operation to operation or connection to connection, which is not explored and might reduce the confidence of robust architecture sampling. Thus, we propose to sample architecture parameters from trainable multivariate log-normal distributions, with which the Lipschitz constant of entire network can be approximated using a univariate log-normal distribution with mean and variance related to architecture parameters. Compared with adversarially trained neural architectures searched by various NAS algorithms as well as efficient human-designed models, our algorithm empirically achieves the best performance among all the models under various attacks on different datasets.},
	urldate = {2023-01-27},
	publisher = {arXiv},
	author = {Dong, Minjing and Li, Yanxi and Wang, Yunhe and Xu, Chang},
	month = sep,
	year = {2020},
	note = {arXiv:2009.00902 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{li_neural_2021,
	title = {Neural {Architecture} {Dilation} for {Adversarial} {Robustness}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper/2021/hash/f7664060cc52bc6f3d620bcedc94a4b6-Abstract.html},
	abstract = {With the tremendous advances in the architecture and scale of convolutional neural networks (CNNs) over the past few decades, they can easily reach or even exceed the performance of humans in certain tasks. However, a recently discovered shortcoming of CNNs is that they are vulnerable to adversarial attacks. Although the adversarial robustness of CNNs can be improved by adversarial training, there is a trade-off between standard accuracy and adversarial robustness. From the neural architecture perspective, this paper aims to improve the adversarial robustness of the backbone CNNs that have a satisfactory accuracy. Under a minimal computational overhead, the introduction of a dilation architecture is expected to be friendly with the standard performance of the backbone CNN while pursuing adversarial robustness. Theoretical analyses on the standard and adversarial error bounds naturally motivate the proposed neural architecture dilation algorithm. Experimental results on real-world datasets and benchmark neural networks demonstrate the effectiveness of the proposed algorithm to balance the accuracy and adversarial robustness.},
	urldate = {2023-01-27},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Li, Yanxi and Yang, Zhaohui and Wang, Yunhe and Xu, Chang},
	year = {2021},
	pages = {29578--29589},
}

@inproceedings{devaguptapu_adversarial_2021,
	title = {On {Adversarial} {Robustness}: {A} {Neural} {Architecture} {Search} {Perspective}},
	shorttitle = {On {Adversarial} {Robustness}},
	url = {https://openaccess.thecvf.com/content/ICCV2021W/AROW/html/Devaguptapu_On_Adversarial_Robustness_A_Neural_Architecture_Search_Perspective_ICCVW_2021_paper.html},
	language = {en},
	urldate = {2023-01-27},
	booktitle = {Proceedings of the {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision}},
	author = {Devaguptapu, Chaitanya and Agarwal, Devansh and Mittal, Gaurav and Gopalani, Pulkit and Balasubramanian, Vineeth N.},
	year = {2021},
	pages = {152--161},
}

@inproceedings{mok_advrush_2021,
	address = {Montreal, QC, Canada},
	title = {{AdvRush}: {Searching} for {Adversarially} {Robust} {Neural} {Architectures}},
	isbn = {978-1-66542-812-5},
	shorttitle = {{AdvRush}},
	url = {https://ieeexplore.ieee.org/document/9710932/},
	doi = {10.1109/ICCV48922.2021.01210},
	abstract = {Deep neural networks continue to awe the world with their remarkable performance. Their predictions, however, are prone to be corrupted by adversarial examples that are imperceptible to humans. Current efforts to improve the robustness of neural networks against adversarial examples are focused on developing robust training methods, which update the weights of a neural network in a more robust direction. In this work, we take a step beyond training of the weight parameters and consider the problem of designing an adversarially robust neural architecture with high intrinsic robustness. We propose AdvRush, a novel adversarial robustness-aware neural architecture search algorithm, based upon a ﬁnding that independent of the training method, the intrinsic robustness of a neural network can be represented with the smoothness of its input loss landscape. Through a regularizer that favors a candidate architecture with a smoother input loss landscape, AdvRush successfully discovers an adversarially robust neural architecture. Along with a comprehensive theoretical motivation for AdvRush, we conduct an extensive amount of experiments to demonstrate the efﬁcacy of AdvRush on various benchmark datasets. Notably, on CIFAR-10, AdvRush achieves 55.91\% robust accuracy under FGSM attack after standard training and 50.04\% robust accuracy under AutoAttack after 7-step PGD adversarial training.},
	language = {en},
	urldate = {2023-01-27},
	booktitle = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Mok, Jisoo and Na, Byunggook and Choe, Hyeokjun and Yoon, Sungroh},
	month = oct,
	year = {2021},
	pages = {12302--12312},
}

@misc{ning_discovering_2021,
	title = {Discovering {Robust} {Convolutional} {Architecture} at {Targeted} {Capacity}: {A} {Multi}-{Shot} {Approach}},
	shorttitle = {Discovering {Robust} {Convolutional} {Architecture} at {Targeted} {Capacity}},
	url = {http://arxiv.org/abs/2012.11835},
	doi = {10.48550/arXiv.2012.11835},
	abstract = {Convolutional neural networks (CNNs) are vulnerable to adversarial examples, and studies show that increasing the model capacity of an architecture topology (e.g., width expansion) can bring consistent robustness improvements. This reveals a clear robustness-efficiency trade-off that should be considered in architecture design. In this paper, considering scenarios with capacity budget, we aim to discover adversarially robust architecture at targeted capacities. Recent studies employed one-shot neural architecture search (NAS) to discover robust architectures. However, since the capacities of different topologies cannot be aligned in the search process, one-shot NAS methods favor topologies with larger capacities in the supernet. And the discovered topology might be suboptimal when augmented to the targeted capacity. We propose a novel multi-shot NAS method to address this issue and explicitly search for robust architectures at targeted capacities. At the targeted FLOPs of 2000M, the discovered MSRobNet-2000 outperforms the recent NAS-discovered architecture RobNet-large under various criteria by a large margin of 4\%-7\%. And at the targeted FLOPs of 1560M, MSRobNet-1560 surpasses another NAS-discovered architecture RobNet-free by 2.3\% and 1.3\% in the clean and PGD-7 accuracies, respectively. All codes are available at https://github.com/walkerning/aw{\textbackslash}\_nas.},
	urldate = {2023-01-27},
	publisher = {arXiv},
	author = {Ning, Xuefei and Zhao, Junbo and Li, Wenshuo and Zhao, Tianchen and Zheng, Yin and Yang, Huazhong and Wang, Yu},
	month = mar,
	year = {2021},
	note = {arXiv:2012.11835 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
}

@inproceedings{guo_when_2020,
	address = {Seattle, WA, USA},
	title = {When {NAS} {Meets} {Robustness}: {In} {Search} of {Robust} {Architectures} {Against} {Adversarial} {Attacks}},
	isbn = {978-1-72817-168-5},
	shorttitle = {When {NAS} {Meets} {Robustness}},
	url = {https://ieeexplore.ieee.org/document/9156305/},
	doi = {10.1109/CVPR42600.2020.00071},
	abstract = {Recent advances in adversarial attacks uncover the intrinsic vulnerability of modern deep neural networks. Since then, extensive efforts have been devoted to enhancing the robustness of deep networks via specialized learning algorithms and loss functions. In this work, we take an architectural perspective and investigate the patterns of network architectures that are resilient to adversarial attacks. To obtain the large number of networks needed for this study, we adopt one-shot neural architecture search, training a large network for once and then ﬁnetuning the sub-networks sampled therefrom. The sampled architectures together with the accuracies they achieve provide a rich basis for our study. Our “robust architecture Odyssey” reveals several valuable observations: 1) densely connected patterns result in improved robustness; 2) under computational budget, adding convolution operations to direct connection edge is effective; 3) ﬂow of solution procedure (FSP) matrix is a good indicator of network robustness. Based on these observations, we discover a family of robust architectures (RobNets). On various datasets, including CIFAR, SVHN, Tiny-ImageNet, and ImageNet, RobNets exhibit superior robustness performance to other widely used architectures. Notably, RobNets substantially improve the robust accuracy (∼5\% absolute gains) under both white-box and blackbox attacks, even with fewer parameter numbers. Code is available at https://github.com/gmh14/RobNets.},
	language = {en},
	urldate = {2023-01-27},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Guo, Minghao and Yang, Yuzhe and Xu, Rui and Liu, Ziwei and Lin, Dahua},
	month = jun,
	year = {2020},
	pages = {628--637},
}

@inproceedings{tang_onlineaugment_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{OnlineAugment}: {Online} {Data} {Augmentation} with {Less} {Domain} {Knowledge}},
	isbn = {978-3-030-58571-6},
	shorttitle = {{OnlineAugment}},
	doi = {10.1007/978-3-030-58571-6_19},
	abstract = {Data augmentation is one of the most important tools in training modern deep neural networks. Recently, great advances have been made in searching for optimal augmentation policies in the image classification domain. However, two key points related to data augmentation remain uncovered by the current methods. First is that most if not all modern augmentation search methods are offline and learning policies are isolated from their usage. The learned policies are mostly constant throughout the training process and are not adapted to the current training model state. Second, the policies rely on class-preserving image processing functions. Hence applying current offline methods to new tasks may require domain knowledge to specify such kind of operations. In this work, we offer an orthogonal online data augmentation scheme together with three new augmentation networks, co-trained with the target learning task. It is both more efficient, in the sense that it does not require expensive offline training when entering a new domain, and more adaptive as it adapts to the learner state. Our augmentation networks require less domain knowledge and are easily applicable to new tasks. Extensive experiments demonstrate that the proposed scheme alone performs on par with the state-of-the-art offline data augmentation methods, as well as improving upon the state-of-the-art in combination with those methods.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Tang, Zhiqiang and Gao, Yunhe and Karlinsky, Leonid and Sattigeri, Prasanna and Feris, Rogerio and Metaxas, Dimitris},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	pages = {313--329},
}

@article{hall_introduction_nodate,
	title = {An {Introduction} to {Machine} {Learning} {Interpretability}, {Second} {Edition}},
	language = {en},
	author = {Hall, Patrick and Gill, Navdeep},
}

@misc{fuster_predictably_2021,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Predictably {Unequal}? {The} {Effects} of {Machine} {Learning} on {Credit} {Markets}},
	shorttitle = {Predictably {Unequal}?},
	url = {https://papers.ssrn.com/abstract=3072038},
	doi = {10.2139/ssrn.3072038},
	abstract = {Innovations in statistical technology, including in predicting creditworthiness, have sparked concerns about distributional impacts across categories such as race. Theoretically, distributional consequences of better statistical technology can come from greater flexibility to uncover structural relationships, or from triangulation of otherwise excluded characteristics. Using data on US mortgages, we predict default using traditional and machine learning models. We find that Black and Hispanic borrowers are disproportionately less likely to gain from the introduction of machine learning. In a simple equilibrium credit market model, machine learning increases disparity in rates between and within groups; these changes are primarily attributable to greater flexibility.},
	language = {en},
	urldate = {2023-01-18},
	author = {Fuster, Andreas and Goldsmith-Pinkham, Paul S. and Ramadorai, Tarun and Walther, Ansgar},
	month = jun,
	year = {2021},
	keywords = {credit, disparate impact, machine learning, mortgages, race},
}

@article{gallagher_investigating_2022,
	title = {Investigating machine learning attacks on financial time series models},
	volume = {123},
	issn = {0167-4048},
	url = {https://www.sciencedirect.com/science/article/pii/S016740482200325X},
	doi = {10.1016/j.cose.2022.102933},
	abstract = {Machine learning and Artificial Intelligence (AI) already support human decision-making and complement professional roles, and are expected in the future to be sufficiently trusted to make autonomous decisions. To trust AI systems with such tasks, a high degree of confidence in their behaviour is needed. However, such systems can make drastically different decisions if the input data is modified, in a way that would be imperceptible to humans. The field of Adversarial Machine Learning studies how this feature could be exploited by an attacker and the countermeasures to defend against them. This work examines the Fast Gradient Signed Method (FGSM) attack, a novel Single Value attack and the Label Flip attack on a trending architecture, namely a 1-Dimensional Convolutional Neural Network model used for time series classification. The results show that the architecture was susceptible to these attacks and that, in their face, the classifier accuracy was significantly impacted.},
	language = {en},
	urldate = {2023-01-18},
	journal = {Computers \& Security},
	author = {Gallagher, Michael and Pitropakis, Nikolaos and Chrysoulas, Christos and Papadopoulos, Pavlos and Mylonas, Alexios and Katsikas, Sokratis},
	month = dec,
	year = {2022},
	keywords = {Adversarial machine learning, financial time-series models, neural networks},
	pages = {102933},
}

@misc{blitz_how_2023,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {How {Can} {Machine} {Learning} {Advance} {Quantitative} {Asset} {Management}?},
	url = {https://papers.ssrn.com/abstract=4321398},
	abstract = {The emerging literature suggests that machine learning (ML) is beneficial in many asset pricing applications because of its ability to detect and exploit nonlinearities and interaction effects that tend to go unnoticed with simpler modelling approaches. In this paper, we discuss the promises and pitfalls of applying machine learning to asset management, by reviewing the existing ML literature from the perspective of a prudent practitioner. The focus is on the methodological design choices that can critically affect predictive outcomes and on an evaluation of the frequent claim that ML gives spectacular performance improvements. In light of the practical considerations, the apparent advantage of ML is reduced, but still likely to make a difference for investors who adhere to a sound research protocol to navigate the intrinsic pitfalls of ML.},
	language = {en},
	urldate = {2023-01-18},
	author = {Blitz, David and Hoogteijling, Tobias and Lohre, Harald and Messow, Philip},
	month = jan,
	year = {2023},
	keywords = {asset management, factor investing, machine learning, portfolio management},
}

@inproceedings{goldblum_adversarial_2022,
	address = {New York, NY, USA},
	series = {{ICAIF} '21},
	title = {Adversarial attacks on machine learning systems for high-frequency trading},
	isbn = {978-1-4503-9148-1},
	url = {https://doi.org/10.1145/3490354.3494367},
	doi = {10.1145/3490354.3494367},
	abstract = {Algorithmic trading systems are often completely automated, and deep learning is increasingly receiving attention in this domain. Nonetheless, little is known about the robustness properties of these models. We study valuation models for algorithmic trading from the perspective of adversarial machine learning. We introduce new attacks specific to this domain with size constraints that minimize attack costs. We further discuss how these attacks can be used as an analysis tool to study and evaluate the robustness properties of financial models. Finally, we investigate the feasibility of realistic adversarial attacks in which an adversarial trader fools automated trading systems into making inaccurate predictions.},
	urldate = {2023-01-18},
	booktitle = {Proceedings of the {Second} {ACM} {International} {Conference} on {AI} in {Finance}},
	publisher = {Association for Computing Machinery},
	author = {Goldblum, Micah and Schwarzschild, Avi and Patel, Ankit and Goldstein, Tom},
	month = may,
	year = {2022},
	keywords = {HFT, adversarial attack, finance, machine learning, trading},
	pages = {1--9},
}

@inproceedings{fursov_adversarial_2021,
	address = {New York, NY, USA},
	series = {{KDD} '21},
	title = {Adversarial {Attacks} on {Deep} {Models} for {Financial} {Transaction} {Records}},
	isbn = {978-1-4503-8332-5},
	url = {https://doi.org/10.1145/3447548.3467145},
	doi = {10.1145/3447548.3467145},
	abstract = {Machine learning models using transaction records as inputs are popular among financial institutions. The most efficient models use deep-learning architectures similar to those in the NLP community, posing a challenge due to their tremendous number of parameters and limited robustness. In particular, deep-learning models are vulnerable to adversarial attacks: a little change in the input harms the model's output. In this work, we examine adversarial attacks on transaction records data and defenses from these attacks. The transaction records data have a different structure than the canonical NLP or time-series data, as neighboring records are less connected than words in sentences, and each record consists of both discrete merchant code and continuous transaction amount. We consider a black-box attack scenario, where the attack doesn't know the true decision model and pay special attention to adding transaction tokens to the end of a sequence. These limitations provide a more realistic scenario, previously unexplored in the NLP world. The proposed adversarial attacks and the respective defenses demonstrate remarkable performance using relevant datasets from the financial industry. Our results show that a couple of generated transactions are sufficient to fool a deep-learning model. Further, we improve model robustness via adversarial training or separate adversarial examples detection. This work shows that embedding protection from adversarial attacks improves model robustness, allowing a wider adoption of deep models for transaction records in banking and finance.},
	urldate = {2023-01-18},
	booktitle = {Proceedings of the 27th {ACM} {SIGKDD} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Fursov, Ivan and Morozov, Matvey and Kaploukhaya, Nina and Kovtun, Elizaveta and Rivera-Castro, Rodrigo and Gusev, Gleb and Babaev, Dmitry and Kireev, Ivan and Zaytsev, Alexey and Burnaev, Evgeny},
	month = aug,
	year = {2021},
	keywords = {adversarial attack, adversarial robustness, deep learning, generative models, transactions data},
	pages = {2868--2878},
}

@inproceedings{cohen_certified_2019,
	title = {Certified {Adversarial} {Robustness} via {Randomized} {Smoothing}},
	url = {https://proceedings.mlr.press/v97/cohen19c.html},
	abstract = {We show how to turn any classifier that classifies well under Gaussian noise into a new classifier that is certifiably robust to adversarial perturbations under the L2 norm. While this "randomized smoothing" technique has been proposed before in the literature, we are the first to provide a tight analysis, which establishes a close connection between L2 robustness and Gaussian noise. We use the technique to train an ImageNet classifier with e.g. a certified top-1 accuracy of 49\% under adversarial perturbations with L2 norm less than 0.5 (=127/255). Smoothing is the only approach to certifiably robust classification which has been shown feasible on full-resolution ImageNet. On smaller-scale datasets where competing approaches to certified L2 robustness are viable, smoothing delivers higher certified accuracies. The empirical success of the approach suggests that provable methods based on randomization at prediction time are a promising direction for future research into adversarially robust classification.},
	language = {en},
	urldate = {2022-11-12},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Cohen, Jeremy and Rosenfeld, Elan and Kolter, Zico},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {1310--1320},
}

@inproceedings{jorge_make_2022,
	title = {Make {Some} {Noise}: {Reliable} and {Efficient} {Single}-{Step} {Adversarial} {Training}},
	shorttitle = {Make {Some} {Noise}},
	url = {https://openreview.net/forum?id=NENo__bExYu},
	abstract = {Recently, Wong et al. (2020) showed that adversarial training with single-step FGSM leads to a characteristic failure mode named catastrophic overfitting (CO), in which a model becomes suddenly vulnerable to multi-step attacks. Experimentally they showed that simply adding a random perturbation prior to FGSM (RS-FGSM) could prevent CO. However, Andriushchenko \& Flammarion (2020) observed that RS-FGSM still leads to CO for larger perturbations, and proposed a computationally expensive regularizer (GradAlign) to avoid it. In this work, we methodically revisit the role of noise and clipping in single-step adversarial training. Contrary to previous intuitions, we find that using a stronger noise around the clean sample combined with {\textbackslash}textit\{not clipping\} is highly effective in avoiding CO for large perturbation radii. We then propose Noise-FGSM (N-FGSM) that, while providing the benefits of single-step adversarial training, does not suffer from CO. Empirical analyses on a large suite of experiments show that N-FGSM is able to match or surpass the performance of previous state of-the-art GradAlign while achieving 3\${\textbackslash}times\$ speed-up.},
	language = {en},
	urldate = {2023-01-16},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Jorge, Pau de and Bibi, Adel and Volpi, Riccardo and Sanyal, Amartya and Torr, Philip and Rogez, Grégory and Dokania, Puneet K.},
	month = oct,
	year = {2022},
}

@inproceedings{schulman_gradient_2015,
	title = {Gradient {Estimation} {Using} {Stochastic} {Computation} {Graphs}},
	volume = {28},
	url = {https://proceedings.neurips.cc/paper/2015/hash/de03beffeed9da5f3639a621bcab5dd4-Abstract.html},
	abstract = {In a variety of problems originating in supervised, unsupervised, and reinforcement learning, the loss function is defined by an expectation over a collection of random variables, which might be part of a probabilistic model or the external world. Estimating the gradient of this loss function, using samples, lies at the core of gradient-based learning algorithms for these problems. We introduce the formalism of stochastic computation graphs--directed acyclic graphs that include both deterministic functions and conditional probability distributions and describe how to easily and automatically derive an unbiased estimator of the loss function's gradient. The resulting algorithm for computing the gradient estimator is a simple modification of the standard backpropagation algorithm. The generic scheme we propose unifies estimators derived in variety of prior work, along with variance-reduction techniques therein. It could assist researchers in developing intricate models involving a combination of stochastic and deterministic operations, enabling, for example, attention, memory, and control actions.},
	urldate = {2023-01-14},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Schulman, John and Heess, Nicolas and Weber, Theophane and Abbeel, Pieter},
	year = {2015},
}

@inproceedings{zhang_towards_2021,
	title = {Towards {Better} {Robust} {Generalization} with {Shift} {Consistency} {Regularization}},
	url = {https://proceedings.mlr.press/v139/zhang21p.html},
	abstract = {While adversarial training becomes one of the most promising defending approaches against adversarial attacks for deep neural networks, the conventional wisdom through robust optimization may usually not guarantee good generalization for robustness. Concerning with robust generalization over unseen adversarial data, this paper investigates adversarial training from a novel perspective of shift consistency in latent space. We argue that the poor robust generalization of adversarial training is owing to the significantly dispersed latent representations generated by training and test adversarial data, as the adversarial perturbations push the latent features of natural examples in the same class towards diverse directions. This is underpinned by the theoretical analysis of the robust generalization gap, which is upper-bounded by the standard one over the natural data and a term of feature inconsistent shift caused by adversarial perturbation \{–\} a measure of latent dispersion. Towards better robust generalization, we propose a new regularization method \{–\} shift consistency regularization (SCR) \{–\} to steer the same-class latent features of both natural and adversarial data into a common direction during adversarial training. The effectiveness of SCR in adversarial training is evaluated through extensive experiments over different datasets, such as CIFAR-10, CIFAR-100, and SVHN, against several competitive methods.},
	language = {en},
	urldate = {2023-01-14},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Zhang, Shufei and Qian, Zhuang and Huang, Kaizhu and Wang, Qiufeng and Zhang, Rui and Yi, Xinping},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {12524--12534},
}

@inproceedings{jin_enhancing_2022,
	address = {New Orleans, LA, USA},
	title = {Enhancing {Adversarial} {Training} with {Second}-{Order} {Statistics} of {Weights}},
	isbn = {978-1-66546-946-3},
	url = {https://ieeexplore.ieee.org/document/9878970/},
	doi = {10.1109/CVPR52688.2022.01484},
	abstract = {Adversarial training has been shown to be one of the most effective approaches to improve the robustness of deep neural networks. It is formalized as a min-max optimization over model weights and adversarial perturbations, where the weights can be optimized through gradient descent methods like SGD. In this paper, we show that treating model weights as random variables allows for enhancing adversarial training through Second-Order Statistics Optimization (S2O) with respect to the weights. By relaxing a common (but unrealistic) assumption of previous PAC-Bayesian frameworks that all weights are statistically independent, we derive an improved PAC-Bayesian adversarial generalization bound, which suggests that optimizing second-order statistics of weights can effectively tighten the bound. In addition to this theoretical insight, we conduct an extensive set of experiments, which show that S2O not only improves the robustness and generalization of the trained neural networks when used in isolation, but also integrates easily in state-of-the-art adversarial training techniques like TRADES, AWP, MART, and AVMixup, leading to a measurable improvement of these techniques. The code is available at https://github.com/Alexkael/S2O.},
	language = {en},
	urldate = {2023-01-14},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Jin, Gaojie and Yi, Xinping and Huang, Wei and Schewe, Sven and Huang, Xiaowei},
	month = jun,
	year = {2022},
	pages = {15252--15262},
}

@inproceedings{xiong_improved_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Improved {Adversarial} {Training} via {Learned} {Optimizer}},
	isbn = {978-3-030-58598-3},
	doi = {10.1007/978-3-030-58598-3_6},
	abstract = {Adversarial attack has recently become a tremendous threat to deep learning models. To improve the robustness of machine learning models, adversarial training, formulated as a minimax optimization problem, has been recognized as one of the most effective defense mechanisms. However, the non-convex and non-concave property poses a great challenge to the minimax training. In this paper, we empirically demonstrate that the commonly used PGD attack may not be optimal for inner maximization, and improved inner optimizer can lead to a more robust model. Then we leverage a learning-to-learn (L2L) framework to train an optimizer with recurrent neural networks, providing update directions and steps adaptively for the inner problem. By co-training optimizer’s parameters and model’s weights, the proposed framework consistently improves over PGD-based adversarial training and TRADES.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Xiong, Yuanhao and Hsieh, Cho-Jui},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	keywords = {Adversarial training, Learning to learn, Optimization},
	pages = {85--100},
}

@inproceedings{qian_robust_2022,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Robust {Network} {Architecture} {Search} via {Feature} {Distortion} {Restraining}},
	isbn = {978-3-031-20065-6},
	doi = {10.1007/978-3-031-20065-6_8},
	abstract = {The vulnerability of Deep Neural Networks, i.e., susceptibility to adversarial attacks, severely limits the application of DNNs in security-sensitive domains. Most of existing methods improve model robustness from weight optimization, such as adversarial training. However, the architecture of DNNs is also a key factor to robustness, which is often neglected or underestimated. We propose Robust Network Architecture Search (RNAS) to obtain a robust network against adversarial attacks. We observe that an adversarial perturbation distorting the non-robust features in latent feature space can further aggravate misclassification. Based on this observation, we search the robust architecture through restricting feature distortion in the search process. Specifically, we define a network vulnerability metric based on feature distortion as a constraint in the search process. This process is modeled as a multi-objective bilevel optimization problem and a novel algorithm is proposed to solve this optimization. Extensive experiments conducted on CIFAR-10/100 and SVHN show that RNAS achieves the best robustness under various adversarial attacks compared with extensive baselines and SOTA methods.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2022},
	publisher = {Springer Nature Switzerland},
	author = {Qian, Yaguan and Huang, Shenghui and Wang, Bin and Ling, Xiang and Guan, Xiaohui and Gu, Zhaoquan and Zeng, Shaoning and Zhou, Wujie and Wang, Haijiang},
	editor = {Avidan, Shai and Brostow, Gabriel and Cissé, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
	year = {2022},
	keywords = {Adversarial examples, Network architecture search, Roubst architecture},
	pages = {122--138},
}

@inproceedings{hosseini_dsrna_2021,
	title = {{DSRNA}: {Differentiable} {Search} of {Robust} {Neural} {Architectures}},
	shorttitle = {{DSRNA}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Hosseini_DSRNA_Differentiable_Search_of_Robust_Neural_Architectures_CVPR_2021_paper.html},
	language = {en},
	urldate = {2023-01-10},
	author = {Hosseini, Ramtin and Yang, Xingyi and Xie, Pengtao},
	year = {2021},
	pages = {6196--6205},
}

@inproceedings{hataya_meta_2022,
	title = {Meta {Approach} to {Data} {Augmentation} {Optimization}},
	url = {https://openaccess.thecvf.com/content/WACV2022/html/Hataya_Meta_Approach_to_Data_Augmentation_Optimization_WACV_2022_paper.html},
	language = {en},
	urldate = {2022-04-01},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Winter} {Conference} on {Applications} of {Computer} {Vision}},
	author = {Hataya, Ryuichiro and Zdenek, Jan and Yoshizoe, Kazuki and Nakayama, Hideki},
	year = {2022},
	pages = {2574--2583},
}

@inproceedings{liu_direct_2021,
	title = {Direct {Differentiable} {Augmentation} {Search}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Liu_Direct_Differentiable_Augmentation_Search_ICCV_2021_paper.html},
	language = {en},
	urldate = {2022-04-04},
	booktitle = {Proceedings of the {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision}},
	author = {Liu, Aoming and Huang, Zehao and Huang, Zhiwu and Wang, Naiyan},
	year = {2021},
	pages = {12219--12228},
}

@inproceedings{zoph_neural_2022,
	title = {Neural {Architecture} {Search} with {Reinforcement} {Learning}},
	url = {https://openreview.net/forum?id=r1Ue8Hcxg},
	abstract = {Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines. Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214.},
	language = {en},
	urldate = {2023-01-09},
	author = {Zoph, Barret and Le, Quoc},
	month = jul,
	year = {2022},
}

@article{williams_simple_1992,
	title = {Simple statistical gradient-following algorithms for connectionist reinforcement learning},
	volume = {8},
	issn = {1573-0565},
	url = {https://doi.org/10.1007/BF00992696},
	doi = {10.1007/BF00992696},
	abstract = {This article presents a general class of associative reinforcement learning algorithms for connectionist networks containing stochastic units. These algorithms, called REINFORCE algorithms, are shown to make weight adjustments in a direction that lies along the gradient of expected reinforcement in both immediate-reinforcement tasks and certain limited forms of delayed-reinforcement tasks, and they do this without explicitly computing gradient estimates or even storing information from which such estimates could be computed. Specific examples of such algorithms are presented, some of which bear a close relationship to certain existing algorithms while others are novel but potentially interesting in their own right. Also given are results that show how such algorithms can be naturally integrated with backpropagation. We close with a brief discussion of a number of additional issues surrounding the use of such algorithms, including what is known about their limiting behaviors as well as further considerations that might be used to help develop similar but potentially more powerful reinforcement learning algorithms.},
	language = {en},
	number = {3},
	urldate = {2023-01-08},
	journal = {Machine Learning},
	author = {Williams, Ronald J.},
	month = may,
	year = {1992},
	keywords = {Reinforcement learning, connectionist networks, gradient descent, mathematical analysis},
	pages = {229--256},
}

@inproceedings{zhang_adversarial_2020,
	title = {Adversarial {AutoAugment}},
	url = {https://openreview.net/forum?id=ByxdUySKvS},
	abstract = {We introduce the idea of adversarial learning into automatic data augmentation to improve the generalization  of a targe network.},
	language = {en},
	urldate = {2021-12-20},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Zhang, Xinyu and Wang, Qiang and Zhang, Jian and Zhong, Zhao},
	year = {2020},
}

@inproceedings{zheng_deep_2022,
	title = {Deep {AutoAugment}},
	url = {https://openreview.net/forum?id=St-53J9ZARf},
	abstract = {While recent automated data augmentation methods lead to state-of-the-art results, their design spaces and the derived data augmentation strategies still incorporate strong human priors. In this...},
	language = {en},
	urldate = {2022-04-07},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Zheng, Yu and Zhang, Zhi and Yan, Shen and Zhang, Mi},
	year = {2022},
}

@inproceedings{cheung_modals_2021,
	title = {{MODALS}: {Modality}-agnostic {Automated} {Data} {Augmentation} in the {Latent} {Space}},
	shorttitle = {{MODALS}},
	url = {https://openreview.net/forum?id=XjYgR6gbCEc},
	abstract = {Data augmentation is an efficient way to expand a training dataset by creating additional artificial data. While data augmentation is found to be effective in improving the generalization...},
	language = {en},
	urldate = {2022-08-03},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Cheung, Tsz-Him and Yeung, Dit-Yan},
	year = {2021},
}

@misc{antoniou_alternating_2022,
	title = {Alternating {Objectives} {Generates} {Stronger} {PGD}-{Based} {Adversarial} {Attacks}},
	url = {http://arxiv.org/abs/2212.07992},
	doi = {10.48550/arXiv.2212.07992},
	abstract = {Designing powerful adversarial attacks is of paramount importance for the evaluation of \${\textbackslash}ell\_p\$-bounded adversarial defenses. Projected Gradient Descent (PGD) is one of the most effective and conceptually simple algorithms to generate such adversaries. The search space of PGD is dictated by the steepest ascent directions of an objective. Despite the plethora of objective function choices, there is no universally superior option and robustness overestimation may arise from ill-suited objective selection. Driven by this observation, we postulate that the combination of different objectives through a simple loss alternating scheme renders PGD more robust towards design choices. We experimentally verify this assertion on a synthetic-data example and by evaluating our proposed method across 25 different \${\textbackslash}ell\_\{{\textbackslash}infty\}\$-robust models and 3 datasets. The performance improvement is consistent, when compared to the single loss counterparts. In the CIFAR-10 dataset, our strongest adversarial attack outperforms all of the white-box components of AutoAttack (AA) ensemble, as well as the most powerful attacks existing on the literature, achieving state-of-the-art results in the computational budget of our study (\$T=100\$, no restarts).},
	urldate = {2023-01-04},
	publisher = {arXiv},
	author = {Antoniou, Nikolaos and Georgiou, Efthymios and Potamianos, Alexandros},
	month = dec,
	year = {2022},
	note = {arXiv:2212.07992 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@article{torralba_80_2008,
	title = {80 {Million} {Tiny} {Images}: {A} {Large} {Data} {Set} for {Nonparametric} {Object} and {Scene} {Recognition}},
	volume = {30},
	issn = {1939-3539},
	shorttitle = {80 {Million} {Tiny} {Images}},
	doi = {10.1109/TPAMI.2008.128},
	abstract = {With the advent of the Internet, billions of images are now freely available online and constitute a dense sampling of the visual world. Using a variety of non-parametric methods, we explore this world with the aid of a large dataset of 79,302,017 images collected from the Internet. Motivated by psychophysical results showing the remarkable tolerance of the human visual system to degradations in image resolution, the images in the dataset are stored as 32 x 32 color images. Each image is loosely labeled with one of the 75,062 non-abstract nouns in English, as listed in the Wordnet lexical database. Hence the image database gives a comprehensive coverage of all object categories and scenes. The semantic information from Wordnet can be used in conjunction with nearest-neighbor methods to perform object classification over a range of semantic levels minimizing the effects of labeling noise. For certain classes that are particularly prevalent in the dataset, such as people, we are able to demonstrate a recognition performance comparable to class-specific Viola-Jones style detectors.},
	number = {11},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Torralba, Antonio and Fergus, Rob and Freeman, William T.},
	month = nov,
	year = {2008},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Computer vision, Degradation, Humans, Image databases, Image recognition, Image resolution, Image sampling, Internet, Layout, Object recognition, Psychology, Visual system, large datasets, nearest-neighbor methods},
	pages = {1958--1970},
}

@inproceedings{huang_exploring_2022,
	title = {Exploring {Architectural} {Ingredients} of {Adversarially} {Robust} {Deep} {Neural} {Networks}},
	url = {https://openreview.net/forum?id=OdklztJBBYH},
	abstract = {Deep neural networks (DNNs) are known to be vulnerable to adversarial attacks. A range of defense methods have been proposed to train adversarially robust DNNs, among which adversarial training has demonstrated promising results. However, despite preliminary understandings developed for adversarial training, it is still not clear, from the architectural perspective, what configurations can lead to more robust DNNs. In this paper, we address this gap via a comprehensive investigation on the impact of network width and depth on the robustness of adversarially trained DNNs. Specifically, we make the following key observations: 1) more parameters (higher model capacity) does not necessarily help adversarial robustness; 2) reducing capacity at the last stage (the last group of blocks) of the network can actually improve adversarial robustness; and 3) under the same parameter budget, there exists an optimal architectural configuration for adversarial robustness. We also provide a theoretical analysis explaning why such network configuration can help robustness. These architectural insights can help design adversarially robust DNNs.},
	language = {en},
	urldate = {2022-12-31},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Huang, Hanxun and Wang, Yisen and Erfani, Sarah Monazam and Gu, Quanquan and Bailey, James and Ma, Xingjun},
	month = jan,
	year = {2022},
}

@inproceedings{yu_robust_2022,
	title = {Robust {Weight} {Perturbation} for {Adversarial} {Training}},
	volume = {4},
	url = {https://www.ijcai.org/proceedings/2022/512},
	doi = {10.24963/ijcai.2022/512},
	abstract = {Electronic proceedings of IJCAI 2022},
	language = {en},
	urldate = {2022-10-02},
	booktitle = {Thirty-{First} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	author = {Yu, Chaojian and Han, Bo and Gong, Mingming and Shen, Li and Ge, Shiming and Bo, Du and Liu, Tongliang},
	month = jul,
	year = {2022},
	note = {ISSN: 1045-0823},
	pages = {3688--3694},
}

@inproceedings{rade_reducing_2022,
	title = {Reducing {Excessive} {Margin} to {Achieve} a {Better} {Accuracy} vs. {Robustness} {Trade}-off},
	url = {https://openreview.net/forum?id=Azh9QBQ4tR7},
	abstract = {While adversarial training has become the de facto approach for training robust classifiers, it leads to a drop in accuracy. This has led to prior works postulating that accuracy is inherently at...},
	language = {en},
	urldate = {2022-10-02},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Rade, Rahul and Moosavi-Dezfooli, Seyed-Mohsen},
	month = mar,
	year = {2022},
}

@inproceedings{wang_self-ensemble_2022,
	title = {Self-ensemble {Adversarial} {Training} for {Improved} {Robustness}},
	url = {https://openreview.net/forum?id=oU3aTsmeRQV},
	abstract = {Due to numerous breakthroughs in real-world applications brought by machine intelligence, deep neural networks (DNNs) are widely employed in critical applications. However, predictions of DNNs are...},
	language = {en},
	urldate = {2022-10-02},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Wang, Hongjun and Wang, Yisen},
	month = may,
	year = {2022},
}

@inproceedings{smith_cyclical_2017,
	title = {Cyclical {Learning} {Rates} for {Training} {Neural} {Networks}},
	doi = {10.1109/WACV.2017.58},
	abstract = {It is known that the learning rate is the most important hyper-parameter to tune for training deep neural networks. This paper describes a new method for setting the learning rate, named cyclical learning rates, which practically eliminates the need to experimentally find the best values and schedule for the global learning rates. Instead of monotonically decreasing the learning rate, this method lets the learning rate cyclically vary between reasonable boundary values. Training with cyclical learning rates instead of fixed values achieves improved classification accuracy without a need to tune and often in fewer iterations. This paper also describes a simple way to estimate "reasonable bounds" - linearly increasing the learning rate of the network for a few epochs. In addition, cyclical learning rates are demonstrated on the CIFAR-10 and CIFAR-100 datasets with ResNets, Stochastic Depth networks, and DenseNets, and the ImageNet dataset with the AlexNet and GoogLeNet architectures. These are practical tools for everyone who trains neural networks.},
	booktitle = {2017 {IEEE} {Winter} {Conference} on {Applications} of {Computer} {Vision} ({WACV})},
	author = {Smith, Leslie N.},
	month = mar,
	year = {2017},
	keywords = {Computational efficiency, Computer architecture, Neural networks, Schedules, Training, Tuning},
	pages = {464--472},
}

@inproceedings{arjovsky_towards_2017,
	title = {Towards {Principled} {Methods} for {Training} {Generative} {Adversarial} {Networks}},
	url = {https://openreview.net/forum?id=Hk4_qw5xe},
	abstract = {The goal of this paper is not to introduce a single algorithm or method, but to make theoretical steps towards fully understanding the training dynamics of gen- erative adversarial networks. In order to substantiate our theoretical analysis, we perform targeted experiments to verify our assumptions, illustrate our claims, and quantify the phenomena. This paper is divided into three sections. The first sec- tion introduces the problem at hand. The second section is dedicated to studying and proving rigorously the problems including instability and saturation that arize when training generative adversarial networks. The third section examines a prac- tical and theoretically grounded direction towards solving these problems, while introducing new tools to study them.},
	language = {en},
	urldate = {2022-12-18},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Arjovsky, Martin and Bottou, Leon},
	year = {2017},
}

@inproceedings{croce_robustbench_2021,
	title = {{RobustBench}: a standardized adversarial robustness benchmark},
	shorttitle = {{RobustBench}},
	url = {https://openreview.net/forum?id=SSKZPJCt7B},
	abstract = {As a research community, we are still lacking a systematic understanding of the progress on adversarial robustness which often makes it hard to identify the most promising ideas in training robust models. A key challenge in benchmarking robustness is that its evaluation is often error-prone leading to robustness overestimation. Our goal is to establish a standardized benchmark of adversarial robustness, which as accurately as possible reflects the robustness of the considered models within a reasonable computational budget. To this end, we start by considering the image classification task and introduce restrictions (possibly loosened in the future) on the allowed models. We evaluate adversarial robustness with AutoAttack, an ensemble of white- and black-box attacks, which was recently shown in a large-scale study to improve almost all robustness evaluations compared to the original publications. To prevent overadaptation of new defenses to AutoAttack, we welcome external evaluations based on adaptive attacks, especially where AutoAttack flags a potential overestimation of robustness. Our leaderboard, hosted at https://robustbench.github.io/, contains evaluations of 120+ models and aims at reflecting the current state of the art in image classification on a set of well-defined tasks in \${\textbackslash}ell\_{\textbackslash}infty\$- and \${\textbackslash}ell\_2\$-threat models and on common corruptions, with possible extensions in the future. Additionally, we open-source the library https://github.com/RobustBench/robustbench that provides unified access to 80+ robust models to facilitate their downstream applications. Finally, based on the collected models, we analyze the impact of robustness on the performance on distribution shifts, calibration, out-of-distribution detection, fairness, privacy leakage, smoothness, and transferability.},
	language = {en},
	urldate = {2022-12-14},
	booktitle = {Thirty-fifth {Conference} on {Neural} {Information} {Processing} {Systems} {Datasets} and {Benchmarks} {Track} ({Round} 2)},
	author = {Croce, Francesco and Andriushchenko, Maksym and Sehwag, Vikash and Debenedetti, Edoardo and Flammarion, Nicolas and Chiang, Mung and Mittal, Prateek and Hein, Matthias},
	month = oct,
	year = {2021},
}

@article{li_understanding_2023,
	title = {Understanding and combating robust overfitting via input loss landscape analysis and regularization},
	volume = {136},
	copyright = {All rights reserved},
	issn = {0031-3203},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320322007087},
	doi = {10.1016/j.patcog.2022.109229},
	abstract = {Adversarial training is widely used to improve the robustness of deep neural networks to adversarial attack. However, adversarial training is prone to overfitting, and the cause is far from clear. This work sheds light on the mechanisms underlying overfitting through analyzing the loss landscape w.r.t. the input. We find that robust overfitting results from standard training, specifically the minimization of the clean loss, and can be mitigated by regularization of the loss gradients. Moreover, we find that robust overfitting turns severer during adversarial training partially because the gradient regularization effect of adversarial training becomes weaker due to the increase in the loss landscape’s curvature. To improve robust generalization, we propose a new regularizer to smooth the loss landscape by penalizing the weighted logits variation along the adversarial direction. Our method significantly mitigates robust overfitting and achieves the highest robustness and efficiency compared to similar previous methods. Code is available at https://github.com/TreeLLi/Combating-RO-AdvLC.},
	language = {en},
	urldate = {2022-12-13},
	journal = {Pattern Recognition},
	author = {Li, Lin and Spratling, Michael},
	month = apr,
	year = {2023},
	keywords = {Adversarial robustness, Adversarial training, Logit regularization, Loss landscape analysis, Robust overfitting},
	pages = {109229},
}

@article{obaid_picture_2022,
	title = {A picture is worth a thousand words: {Measuring} investor sentiment by combining machine learning and photos from news},
	volume = {144},
	issn = {0304-405X},
	shorttitle = {A picture is worth a thousand words},
	url = {https://www.sciencedirect.com/science/article/pii/S0304405X21002683},
	doi = {10.1016/j.jfineco.2021.06.002},
	abstract = {By applying machine learning to the accurate and cost-effective classification of photos based on sentiment, we introduce a daily market-level investor sentiment index (Photo Pessimism) obtained from a large sample of news photos. Consistent with behavioral models, Photo Pessimism predicts market return reversals and trading volume. The relation is strongest among stocks with high limits to arbitrage and during periods of elevated fear. We examine whether Photo Pessimism and pessimism embedded in news text act as complements or substitutes for each other in predicting stock returns and find evidence that the two are substitutes.},
	language = {en},
	number = {1},
	urldate = {2022-12-08},
	journal = {Journal of Financial Economics},
	author = {Obaid, Khaled and Pukthuanthong, Kuntara},
	month = apr,
	year = {2022},
	keywords = {Behavioral finance, Big data, Deep learning, Investor sentiment, Machine learning, Return predictability},
	pages = {273--297},
}

@inproceedings{zhang_rethinking_2022,
	title = {Rethinking {Lipschitz} {Neural} {Networks} and {Certified} {Robustness}: {A} {Boolean} {Function} {Perspective}},
	shorttitle = {Rethinking {Lipschitz} {Neural} {Networks} and {Certified} {Robustness}},
	url = {https://openreview.net/forum?id=xaWO6bAY0xM},
	abstract = {Designing neural networks with bounded Lipschitz constant is a promising way to obtain certifiably robust classifiers against adversarial examples. However, the relevant progress for the important \${\textbackslash}ell\_{\textbackslash}infty\$ perturbation setting is rather limited, and a principled understanding of how to design expressive \${\textbackslash}ell\_{\textbackslash}infty\$ Lipschitz networks is still lacking. In this paper, we bridge the gap by studying certified \${\textbackslash}ell\_{\textbackslash}infty\$ robustness from a novel perspective of representing Boolean functions. We derive two fundamental impossibility results that hold for any standard Lipschitz network: one for robust classification on finite datasets, and the other for Lipschitz function approximation. These results identify that networks built upon norm-bounded affine layers and Lipschitz activations intrinsically lose expressive power even in the two-dimensional case, and shed light on how recently proposed Lipschitz networks (e.g., GroupSort and \${\textbackslash}ell\_{\textbackslash}infty\$-distance nets) bypass these impossibilities by leveraging order statistic functions. Finally, based on these insights, we develop a unified Lipschitz network that generalizes prior works, and design a practical version that can be efficiently trained (making certified robust training free). Extensive experiments show that our approach is scalable, efficient, and consistently yields better certified robustness across multiple datasets and perturbation radii than prior Lipschitz networks.},
	language = {en},
	urldate = {2022-12-06},
	author = {Zhang, Bohang and Jiang, Du and He, Di and Wang, Liwei},
	month = oct,
	year = {2022},
}

@article{gu_empirical_2020,
	title = {Empirical {Asset} {Pricing} via {Machine} {Learning}},
	volume = {33},
	issn = {0893-9454},
	url = {https://doi.org/10.1093/rfs/hhaa009},
	doi = {10.1093/rfs/hhaa009},
	abstract = {We perform a comparative analysis of machine learning methods for the canonical problem of empirical asset pricing: measuring asset risk premiums. We demonstrate large economic gains to investors using machine learning forecasts, in some cases doubling the performance of leading regression-based strategies from the literature. We identify the best-performing methods (trees and neural networks) and trace their predictive gains to allowing nonlinear predictor interactions missed by other methods. All methods agree on the same set of dominant predictive signals, a set that includes variations on momentum, liquidity, and volatility.Authors have furnished an Internet Appendix, which is available on the Oxford University Press Web site next to the link to the final published paper online.},
	number = {5},
	urldate = {2022-12-05},
	journal = {The Review of Financial Studies},
	author = {Gu, Shihao and Kelly, Bryan and Xiu, Dacheng},
	month = may,
	year = {2020},
	pages = {2223--2273},
}

@article{leippold_machine_2022,
	title = {Machine learning in the {Chinese} stock market},
	volume = {145},
	issn = {0304-405X},
	url = {https://www.sciencedirect.com/science/article/pii/S0304405X21003743},
	doi = {10.1016/j.jfineco.2021.08.017},
	abstract = {We add to the emerging literature on empirical asset pricing in the Chinese stock market by building and analyzing a comprehensive set of return prediction factors using various machine learning algorithms. Contrasting previous studies for the US market, liquidity emerges as the most important predictor, leading us to closely examine the impact of transaction costs. The retail investors’ dominating presence positively affects short-term predictability, particularly for small stocks. Another feature that distinguishes the Chinese market from the US market is the high predictability of large stocks and state-owned enterprises over longer horizons. The out-of-sample performance remains economically significant after transaction costs.},
	language = {en},
	number = {2, Part A},
	urldate = {2022-12-05},
	journal = {Journal of Financial Economics},
	author = {Leippold, Markus and Wang, Qian and Zhou, Wenyu},
	month = aug,
	year = {2022},
	keywords = {Chinese stock market, Factor investing, Machine learning, Model selection},
	pages = {64--82},
}

@article{bianchi_bond_2021,
	title = {Bond {Risk} {Premiums} with {Machine} {Learning}},
	volume = {34},
	issn = {0893-9454},
	url = {https://doi.org/10.1093/rfs/hhaa062},
	doi = {10.1093/rfs/hhaa062},
	abstract = {We show that machine learning methods, in particular, extreme trees and neural networks (NNs), provide strong statistical evidence in favor of bond return predictability. NN forecasts based on macroeconomic and yield information translate into economic gains that are larger than those obtained using yields alone. Interestingly, the nature of unspanned factors changes along the yield curve: stock- and labor-market-related variables are more relevant for short-term maturities, whereas output and income variables matter more for longer maturities. Finally, NN forecasts correlate with proxies for time-varying risk aversion and uncertainty, lending support to models featuring both channels.},
	number = {2},
	urldate = {2022-12-05},
	journal = {The Review of Financial Studies},
	author = {Bianchi, Daniele and Büchner, Matthias and Tamoni, Andrea},
	month = feb,
	year = {2021},
	pages = {1046--1089},
}

@misc{chen_deep_2021,
	title = {Deep {Learning} in {Asset} {Pricing}},
	url = {http://arxiv.org/abs/1904.00745},
	doi = {10.48550/arXiv.1904.00745},
	abstract = {We use deep neural networks to estimate an asset pricing model for individual stock returns that takes advantage of the vast amount of conditioning information, while keeping a fully flexible form and accounting for time-variation. The key innovations are to use the fundamental no-arbitrage condition as criterion function, to construct the most informative test assets with an adversarial approach and to extract the states of the economy from many macroeconomic time series. Our asset pricing model outperforms out-of-sample all benchmark approaches in terms of Sharpe ratio, explained variation and pricing errors and identifies the key factors that drive asset prices.},
	urldate = {2022-12-05},
	publisher = {arXiv},
	author = {Chen, Luyang and Pelger, Markus and Zhu, Jason},
	month = aug,
	year = {2021},
	note = {arXiv:1904.00745 [q-fin, stat]},
	keywords = {Quantitative Finance - Statistical Finance, Statistics - Methodology},
}

@inproceedings{cui_learnable_2021,
	title = {Learnable {Boundary} {Guided} {Adversarial} {Training}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Cui_Learnable_Boundary_Guided_Adversarial_Training_ICCV_2021_paper.html},
	language = {en},
	urldate = {2022-12-01},
	author = {Cui, Jiequan and Liu, Shu and Wang, Liwei and Jia, Jiaya},
	year = {2021},
	pages = {15721--15730},
}

@inproceedings{cazenavette_architectural_2021,
	title = {Architectural {Adversarial} {Robustness}: {The} {Case} for {Deep} {Pursuit}},
	shorttitle = {Architectural {Adversarial} {Robustness}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Cazenavette_Architectural_Adversarial_Robustness_The_Case_for_Deep_Pursuit_CVPR_2021_paper.html},
	language = {en},
	urldate = {2022-11-28},
	author = {Cazenavette, George and Murdock, Calvin and Lucey, Simon},
	year = {2021},
	pages = {7150--7158},
}

@article{zeng_are_2021,
	title = {Are {Adversarial} {Examples} {Created} {Equal}? {A} {Learnable} {Weighted} {Minimax} {Risk} for {Robustness} under {Non}-uniform {Attacks}},
	volume = {35},
	copyright = {Copyright (c) 2021 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	shorttitle = {Are {Adversarial} {Examples} {Created} {Equal}?},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/17292},
	doi = {10.1609/aaai.v35i12.17292},
	abstract = {Adversarial Training is proved to be an efficient method to defend against adversarial examples, being one of the few defenses that withstand strong attacks. However, traditional defense mechanisms assume a uniform attack over the examples according to the underlying data distribution, which is apparently unrealistic as the attacker could choose to focus on more vulnerable examples. We present a weighted minimax risk optimization that defends against non-uniform attacks, achieving robustness against adversarial examples under perturbed test data distributions. Our modified risk considers importance weights of different adversarial examples and focuses adaptively on harder examples that are wrongly classified or at higher risk of being classified incorrectly. The designed risk allows the training process to learn a strong defense through optimizing the importance weights. The experiments show that our model significantly improves state-of-the-art adversarial accuracy under non-uniform attacks without a significant drop under uniform attacks.},
	language = {en},
	number = {12},
	urldate = {2022-11-28},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Zeng, Huimin and Zhu, Chen and Goldstein, Tom and Huang, Furong},
	month = may,
	year = {2021},
	note = {Number: 12},
	keywords = {Adversarial Learning \& Robustness},
	pages = {10815--10823},
}

@inproceedings{mao_enhance_2022,
	title = {Enhance the {Visual} {Representation} via {Discrete} {Adversarial} {Training}},
	url = {https://openreview.net/forum?id=qtZac7A3-F},
	abstract = {Adversarial Training (AT), which is commonly accepted as one of the most effective approaches defending against adversarial examples, can largely harm the standard performance, thus has limited usefulness on industrial-scale production and applications. Surprisingly, this phenomenon is totally opposite in Natural Language Processing (NLP) task, where AT can even benefit for generalization. We notice the merit of AT in NLP tasks could derive from the discrete and symbolic input space. For borrowing the advantage from NLP-style AT, we propose Discrete Adversarial Training (DAT). DAT leverages VQGAN to reform the image data to discrete text-like inputs, i.e. visual words. Then it minimizes the maximal risk on such discrete images with symbolic adversarial perturbations. We further give an explanation from the perspective of distribution to demonstrate the effectiveness of DAT. As a plug-and-play technique for enhancing the visual representation, DAT achieves significant improvement on multiple tasks including image classification, object detection and self-supervised learning. Especially, the model pre-trained with Masked Auto-Encoding (MAE) and fine-tuned by our DAT without extra data can get 31.40 mCE on ImageNet-C and 32.77\% top-1 accuracy on Stylized-ImageNet, building the new state-of-the-art. The code will be available at https://github.com/alibaba/easyrobust.},
	language = {en},
	urldate = {2022-11-28},
	author = {Mao, Xiaofeng and Chen, YueFeng and Duan, Ranjie and Zhu, Yao and Qi, Gege and Ye, Shaokai and Li, Xiaodan and Zhang, Rong and Xue', Hui},
	month = oct,
	year = {2022},
}

@inproceedings{gan_large-scale_2020,
	title = {Large-{Scale} {Adversarial} {Training} for {Vision}-and-{Language} {Representation} {Learning}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/49562478de4c54fafd4ec46fdb297de5-Abstract.html},
	abstract = {We present VILLA, the first known effort on large-scale adversarial training for vision-and-language (V+L) representation learning. VILLA consists of two training stages: (i) task-agnostic adversarial pre-training; followed by (ii) task-specific adversarial finetuning. Instead of adding adversarial perturbations on image pixels and textual tokens, we propose to perform adversarial training in the embedding space of each modality. To enable large-scale training, we adopt the free'' adversarial training strategy, and combine it with KL-divergence-based regularization to promote higher invariance in the embedding space. We apply VILLA to current best-performing V+L models, and achieve new state of the art on a wide range of tasks, including Visual Question Answering, Visual Commonsense Reasoning, Image-Text Retrieval, Referring Expression Comprehension, Visual Entailment, and NLVR2.},
	urldate = {2022-11-28},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Gan, Zhe and Chen, Yen-Chun and Li, Linjie and Zhu, Chen and Cheng, Yu and Liu, Jingjing},
	year = {2020},
	pages = {6616--6628},
}

@inproceedings{modas_prime_2022,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{PRIME}: {A} {Few} {Primitives} {Can} {Boost} {Robustness} to {Common} {Corruptions}},
	isbn = {978-3-031-19806-9},
	shorttitle = {{PRIME}},
	doi = {10.1007/978-3-031-19806-9_36},
	abstract = {Despite their impressive performance on image classification tasks, deep networks have a hard time generalizing to unforeseen corruptions of their data. To fix this vulnerability, prior works have built complex data augmentation strategies, combining multiple methods to enrich the training data. However, introducing intricate design choices or heuristics makes it hard to understand which elements of these methods are indeed crucial for improving robustness. In this work, we take a step back and follow a principled approach to achieve robustness to common corruptions. We propose PRIME, a general data augmentation scheme that relies on simple yet rich families of max-entropy image transformations. PRIME outperforms the prior art in terms of corruption robustness, while its simplicity and plug-and-play nature enable combination with other methods to further boost their robustness. We analyze PRIME to shed light on the importance of the mixing strategy on synthesizing corrupted images, and to reveal the robustness-accuracy trade-offs arising in the context of common corruptions. Finally, we show that the computational efficiency of our method allows it to be easily used in both on-line and off-line data augmentation schemes. Our code is available at https://github.com/amodas/PRIME-augmentations.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2022},
	publisher = {Springer Nature Switzerland},
	author = {Modas, Apostolos and Rade, Rahul and Ortiz-Jiménez, Guillermo and Moosavi-Dezfooli, Seyed-Mohsen and Frossard, Pascal},
	editor = {Avidan, Shai and Brostow, Gabriel and Cissé, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
	year = {2022},
	pages = {623--640},
}

@article{karim_adversarial_2021,
	title = {Adversarial {Attacks} on {Time} {Series}},
	volume = {43},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2020.2986319},
	abstract = {Time series classification models have been garnering significant importance in the research community. However, not much research has been done on generating adversarial samples for these models. These adversarial samples can become a security concern. In this paper, we propose utilizing an adversarial transformation network (ATN) on a distilled model to attack various time series classification models. The proposed attack on the classification model utilizes a distilled model as a surrogate that mimics the behavior of the attacked classical time series classification models. Our proposed methodology is applied onto 1-nearest neighbor dynamic time warping (1-NN DTW) and a fully convolutional network (FCN), all of which are trained on 42 University of California Riverside (UCR) datasets. In this paper, we show both models were susceptible to attacks on all 42 datasets. When compared to Fast Gradient Sign Method, the proposed attack generates a larger faction of successful adversarial black-box attacks. A simple defense mechanism is successfully devised to reduce the fraction of successful adversarial samples. Finally, we recommend future researchers that develop time series classification models to incorporating adversarial data samples into their training data sets to improve resilience on adversarial samples.},
	number = {10},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Karim, Fazle and Majumdar, Somshubra and Darabi, Houshang},
	month = oct,
	year = {2021},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Computational modeling, Computer vision, Data models, Machine learning, Neural networks, Time series analysis, Time series classification, Training, adversarial machine learning, deep learning, perturbation methods},
	pages = {3309--3320},
}

@inproceedings{li_why_2022,
	title = {Why {Robust} {Generalization} in {Deep} {Learning} is {Difficult}: {Perspective} of {Expressive} {Power}},
	shorttitle = {Why {Robust} {Generalization} in {Deep} {Learning} is {Difficult}},
	url = {https://openreview.net/forum?id=Z26xiZkbjgE},
	abstract = {It is well-known that modern neural networks are vulnerable to adversarial examples. To mitigate this problem, a series of robust learning algorithms have been proposed. However, although the robust training error can be near zero via some methods, all existing algorithms lead to a high robust generalization error. In this paper, we provide a theoretical understanding of this puzzling phenomenon from the perspective of expressive power for deep neural networks. Specifically, for binary classification problems with well-separated data, we show that, for ReLU networks, while mild over-parameterization is sufficient for high robust training accuracy, there exists a constant robust generalization gap unless the size of the neural network is exponential in the data dimension \$d\$. This result holds even if the data is linear separable (which means achieving standard generalization is easy), and more generally for any parameterized function classes as long as their VC dimension is at most polynomial in the number of parameters. Moreover, we establish an improved upper bound of \${\textbackslash}exp(\{{\textbackslash}mathcal\{O\}\}(k))\$ for the network size to achieve low robust generalization error when the data lies on a manifold with intrinsic dimension \$k\$ (\$k {\textbackslash}ll d\$). Nonetheless, we also have a lower bound that grows exponentially with respect to \$k\$ --- the curse of dimensionality is inevitable. By demonstrating an exponential separation between the network size for achieving low robust training and generalization error, our results reveal that the hardness of robust generalization may stem from the expressive power of practical models.},
	language = {en},
	urldate = {2022-11-24},
	author = {Li, Binghui and Jin, Jikai and Zhong, Han and Hopcroft, John E. and Wang, Liwei},
	month = oct,
	year = {2022},
}

@inproceedings{jia_-at_2022,
	title = {{LAS}-{AT}: {Adversarial} {Training} {With} {Learnable} {Attack} {Strategy}},
	shorttitle = {{LAS}-{AT}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Jia_LAS-AT_Adversarial_Training_With_Learnable_Attack_Strategy_CVPR_2022_paper.html},
	language = {en},
	urldate = {2022-10-02},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Jia, Xiaojun and Zhang, Yong and Wu, Baoyuan and Ma, Ke and Wang, Jue and Cao, Xiaochun},
	year = {2022},
	pages = {13398--13408},
}

@inproceedings{zhang_geometry-aware_2021,
	title = {Geometry-aware {Instance}-reweighted {Adversarial} {Training}},
	url = {https://openreview.net/forum?id=iAX0l6Cz8ub},
	abstract = {In adversarial machine learning, there was a common belief that robustness and accuracy hurt each other. The belief was challenged by recent studies where we can maintain the robustness and improve...},
	language = {en},
	urldate = {2021-09-10},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Zhang, Jingfeng and Zhu, Jianing and Niu, Gang and Han, Bo and Sugiyama, Masashi and Kankanhalli, Mohan},
	year = {2021},
}

@inproceedings{tsiligkaridis_understanding_2022,
	title = {Understanding and {Increasing} {Efficiency} of {Frank}-{Wolfe} {Adversarial} {Training}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Tsiligkaridis_Understanding_and_Increasing_Efficiency_of_Frank-Wolfe_Adversarial_Training_CVPR_2022_paper.html},
	language = {en},
	urldate = {2022-11-19},
	author = {Tsiligkaridis, Theodoros and Roberts, Jay},
	year = {2022},
	pages = {50--59},
}

@inproceedings{zhang_towards_2022,
	title = {Towards {Efficient} {Data} {Free} {Black}-{Box} {Adversarial} {Attack}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Zhang_Towards_Efficient_Data_Free_Black-Box_Adversarial_Attack_CVPR_2022_paper.html},
	language = {en},
	urldate = {2022-11-19},
	author = {Zhang, Jie and Li, Bo and Xu, Jianghe and Wu, Shuang and Ding, Shouhong and Zhang, Lei and Wu, Chao},
	year = {2022},
	pages = {15115--15125},
}

@inproceedings{li_visualizing_2018,
	title = {Visualizing the {Loss} {Landscape} of {Neural} {Nets}},
	volume = {31},
	url = {https://papers.nips.cc/paper/2018/hash/a41b3bb3e6b050b6c9067c67f663b915-Abstract.html},
	abstract = {Neural network training relies on our ability to find "good" minimizers of highly non-convex loss functions. It is well known that certain network architecture designs (e.g., skip connections) produce loss functions that train easier, and well-chosen training parameters (batch size, learning rate, optimizer) produce minimizers that generalize better. However, the reasons for these differences, and their effect on the underlying loss landscape, is not well understood. In this paper, we explore the structure of neural loss functions, and the effect of loss landscapes on generalization, using a range of visualization methods. First, we introduce a simple "filter normalization" method that helps us visualize loss function curvature, and make meaningful side-by-side comparisons between loss functions. Then, using a variety of visualizations, we explore how network architecture affects the loss landscape, and how training parameters affect the shape of minimizers.},
	urldate = {2022-11-18},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
	year = {2018},
}

@article{zhao_well-classified_2022,
	title = {Well-{Classified} {Examples} {Are} {Underestimated} in {Classification} with {Deep} {Neural} {Networks}},
	volume = {36},
	copyright = {Copyright (c) 2022 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/20904},
	doi = {10.1609/aaai.v36i8.20904},
	abstract = {The conventional wisdom behind learning deep classification models is to focus on bad-classified examples and ignore well-classified examples that are far from the decision boundary. For instance, when training with cross-entropy loss, examples with higher likelihoods (i.e., well-classified examples) contribute smaller gradients in back-propagation. However, we theoretically show that this common practice hinders representation learning, energy optimization, and margin growth. To counteract this deficiency, we propose to reward well-classified examples with additive bonuses to revive their contribution to the learning process. This counterexample theoretically addresses these three issues. We empirically support this claim by directly verifying the theoretical results or significant performance improvement with our counterexample on diverse tasks, including image classification, graph classification, and machine translation. Furthermore, this paper shows that we can deal with complex scenarios, such as imbalanced classification, OOD detection, and applications under adversarial attacks because our idea can solve these three issues. Code is available at https://github.com/lancopku/well-classified-examples-are-underestimated.},
	language = {en},
	number = {8},
	urldate = {2022-11-16},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Zhao, Guangxiang and Yang, Wenkai and Ren, Xuancheng and Li, Lei and Wu, Yunfang and Sun, Xu},
	month = jun,
	year = {2022},
	note = {Number: 8},
	keywords = {Computer Vision (CV)},
	pages = {9180--9189},
}

@misc{smith_super-convergence_2018,
	title = {Super-{Convergence}: {Very} {Fast} {Training} of {Neural} {Networks} {Using} {Large} {Learning} {Rates}},
	shorttitle = {Super-{Convergence}},
	url = {http://arxiv.org/abs/1708.07120},
	doi = {10.48550/arXiv.1708.07120},
	abstract = {In this paper, we describe a phenomenon, which we named "super-convergence", where neural networks can be trained an order of magnitude faster than with standard training methods. The existence of super-convergence is relevant to understanding why deep networks generalize well. One of the key elements of super-convergence is training with one learning rate cycle and a large maximum learning rate. A primary insight that allows super-convergence training is that large learning rates regularize the training, hence requiring a reduction of all other forms of regularization in order to preserve an optimal regularization balance. We also derive a simplification of the Hessian Free optimization method to compute an estimate of the optimal learning rate. Experiments demonstrate super-convergence for Cifar-10/100, MNIST and Imagenet datasets, and resnet, wide-resnet, densenet, and inception architectures. In addition, we show that super-convergence provides a greater boost in performance relative to standard training when the amount of labeled training data is limited. The architectures and code to replicate the figures in this paper are available at github.com/lnsmith54/super-convergence. See http://www.fast.ai/2018/04/30/dawnbench-fastai/ for an application of super-convergence to win the DAWNBench challenge (see https://dawn.cs.stanford.edu/benchmark/).},
	urldate = {2022-11-14},
	publisher = {arXiv},
	author = {Smith, Leslie N. and Topin, Nicholay},
	month = may,
	year = {2018},
	note = {arXiv:1708.07120 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@inproceedings{huang_calibrated_2021,
	title = {calibrated adversarial training},
	url = {https://proceedings.mlr.press/v157/huang21a.html},
	abstract = {Adversarial training is an approach of increasing the robustness of models to adversarial attacks by including adversarial examples in the training set. One major challenge of producing adversarial examples is to contain sufficient perturbation in the example to flip the model’s output while not making severe changes in the example’s semantical content. Exuberant change in the semantical content could also change the true label of the example. Adding such examples to the training set results in adverse effects. In this paper, we present the Calibrated Adversarial Training, a method that reduces the adverse effects of semantic perturbations in adversarial training. The method produces pixel-level adaptations to the perturbations based on novel calibrated robust error. We provide theoretical analysis on the calibrated robust error and derive an upper bound for it. Our empirical results show a superior performance of the Calibrated Adversarial Training over a number of public datasets.},
	language = {en},
	urldate = {2022-10-30},
	booktitle = {Proceedings of {The} 13th {Asian} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Huang, Tianjin and Menkovski, Vlado and Pei, Yulong and Pechenizkiy, Mykola},
	month = nov,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {626--641},
}

@article{biggio_wild_2018,
	title = {Wild patterns: {Ten} years after the rise of adversarial machine learning},
	volume = {84},
	issn = {0031-3203},
	shorttitle = {Wild patterns},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320318302565},
	doi = {10.1016/j.patcog.2018.07.023},
	abstract = {Learning-based pattern classifiers, including deep networks, have shown impressive performance in several application domains, ranging from computer vision to cybersecurity. However, it has also been shown that adversarial input perturbations carefully crafted either at training or at test time can easily subvert their predictions. The vulnerability of machine learning to such wild patterns (also referred to as adversarial examples), along with the design of suitable countermeasures, have been investigated in the research field of adversarial machine learning. In this work, we provide a thorough overview of the evolution of this research area over the last ten years and beyond, starting from pioneering, earlier work on the security of non-deep learning algorithms up to more recent work aimed to understand the security properties of deep learning algorithms, in the context of computer vision and cybersecurity tasks. We report interesting connections between these apparently-different lines of work, highlighting common misconceptions related to the security evaluation of machine-learning algorithms. We review the main threat models and attacks defined to this end, and discuss the main limitations of current work, along with the corresponding future challenges towards the design of more secure learning algorithms.},
	language = {en},
	urldate = {2022-10-28},
	journal = {Pattern Recognition},
	author = {Biggio, Battista and Roli, Fabio},
	month = dec,
	year = {2018},
	keywords = {Adversarial examples, Adversarial machine learning, Deep learning, Evasion attacks, Poisoning attacks, Secure learning},
	pages = {317--331},
}

@article{yu_improving_2023,
	title = {Improving adversarial robustness by learning shared information},
	volume = {134},
	issn = {0031-3203},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320322005349},
	doi = {10.1016/j.patcog.2022.109054},
	abstract = {We consider the problem of improving the adversarial robustness of neural networks while retaining natural accuracy. Motivated by the multi-view information bottleneck formalism, we seek to learn a representation that captures the shared information between clean samples and their corresponding adversarial samples while discarding these samples’ view-specific information. We show that this approach leads to a novel multi-objective loss function, and we provide mathematical motivation for its components towards improving the robust vs. natural accuracy tradeoff. We demonstrate enhanced tradeoff compared to current state-of-the-art methods with extensive evaluation on various benchmark image datasets and architectures. Ablation studies indicate that learning shared representations is key to improving performance.},
	language = {en},
	urldate = {2022-10-28},
	journal = {Pattern Recognition},
	author = {Yu, Xi and Smedemark-Margulies, Niklas and Aeron, Shuchin and Koike-Akino, Toshiaki and Moulin, Pierre and Brand, Matthew and Parsons, Kieran and Wang, Ye},
	month = feb,
	year = {2023},
	keywords = {Adversarial robustness, Information bottleneck, Multi-view learning, Shared information,},
	pages = {109054},
}

@misc{machiraju_bio-inspired_2021,
	title = {Bio-inspired {Robustness}: {A} {Review}},
	shorttitle = {Bio-inspired {Robustness}},
	url = {http://arxiv.org/abs/2103.09265},
	doi = {10.48550/arXiv.2103.09265},
	abstract = {Deep convolutional neural networks (DCNNs) have revolutionized computer vision and are often advocated as good models of the human visual system. However, there are currently many shortcomings of DCNNs, which preclude them as a model of human vision. For example, in the case of adversarial attacks, where adding small amounts of noise to an image, including an object, can lead to strong misclassification of that object. But for humans, the noise is often invisible. If vulnerability to adversarial noise cannot be fixed, DCNNs cannot be taken as serious models of human vision. Many studies have tried to add features of the human visual system to DCNNs to make them robust against adversarial attacks. However, it is not fully clear whether human vision inspired components increase robustness because performance evaluations of these novel components in DCNNs are often inconclusive. We propose a set of criteria for proper evaluation and analyze different models according to these criteria. We finally sketch future efforts to make DCCNs one step closer to the model of human vision.},
	urldate = {2022-10-27},
	publisher = {arXiv},
	author = {Machiraju, Harshitha and Choung, Oh-Hyeon and Frossard, Pascal and Herzog, Michael H.},
	month = mar,
	year = {2021},
	note = {arXiv:2103.09265 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@inproceedings{vuyyuru_biologically_2020,
	title = {Biologically {Inspired} {Mechanisms} for {Adversarial} {Robustness}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/17256f049f1e3fede17c7a313f7657f4-Abstract.html},
	abstract = {A convolutional neural network strongly robust to adversarial perturbations at reasonable computational and performance cost has not yet been demonstrated. The primate visual ventral stream seems to be robust to small perturbations in visual stimuli but the underlying mechanisms that give rise to this robust perception are not understood. In this work, we investigate the role of two biologically plausible mechanisms in adversarial robustness. We demonstrate that the non-uniform sampling performed by the primate retina and the presence of multiple receptive fields with a range of receptive field sizes at each eccentricity improve the robustness of neural networks to small adversarial perturbations. We verify that these two mechanisms do not suffer from gradient obfuscation and study their contribution to adversarial robustness through ablation studies.},
	urldate = {2022-10-26},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Vuyyuru, Manish Reddy and Banburski, Andrzej and Pant, Nishka and Poggio, Tomaso},
	year = {2020},
	pages = {2135--2146},
}

@inproceedings{tramer_fundamental_2020,
	title = {Fundamental {Tradeoffs} between {Invariance} and {Sensitivity} to {Adversarial} {Perturbations}},
	url = {https://proceedings.mlr.press/v119/tramer20a.html},
	abstract = {Adversarial examples are malicious inputs crafted to induce misclassification. Commonly studied {\textbackslash}emph\{sensitivity-based\} adversarial examples introduce semantically-small changes to an input that result in a different model prediction. This paper studies a complementary failure mode, {\textbackslash}emph\{invariance-based\} adversarial examples, that introduce minimal semantic changes that modify an input’s true label yet preserve the model’s prediction. We demonstrate fundamental tradeoffs between these two types of adversarial examples. We show that defenses against sensitivity-based attacks actively harm a model’s accuracy on invariance-based attacks, and that new approaches are needed to resist both attack types. In particular, we break state-of-the-art adversarially-trained and {\textbackslash}emph\{certifiably-robust\} models by generating small perturbations that the models are (provably) robust to, yet that change an input’s class according to human labelers. Finally, we formally show that the existence of excessively invariant classifiers arises from the presence of {\textbackslash}emph\{overly-robust\} predictive features in standard datasets.},
	language = {en},
	urldate = {2022-10-26},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Tramer, Florian and Behrmann, Jens and Carlini, Nicholas and Papernot, Nicolas and Jacobsen, Joern-Henrik},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {9561--9571},
}

@inproceedings{yang_one_2022,
	title = {One {Size} {Does} {NOT} {Fit} {All}: {Data}-{Adaptive} {Adversarial} {Training}},
	abstract = {Adversarial robustness is critical for deep learning models to defend against adversarial attacks. Although adversarial training is considered to be one of the most effective ways to improve the model’s adversarial robustness, it usually yields models with lower natural accuracy. In this paper, we argue that, for the attackable examples, traditional adversarial training which utilizes a fixed size perturbation ball can create adversarial examples that deviate far away from the original class towards the target class. Thus, the model’s performance on the natural target class will drop drastically, which leads to the decline of natural accuracy. To this end, we propose the Data-Adaptive Adversarial Training (DAAT) which adaptively adjusts the perturbation ball to a proper size for each of the natural examples with the help of a natural trained calibration network. Besides, a dynamic training strategy empowers the DAAT models with impressive robustness while retaining remarkable natural accuracy. Based on a toy example, we theoretically prove the recession of the natural accuracy caused by adversarial training and show how the data-adaptive perturbation size helps the model resist it. Finally, empirical experiments on benchmark datasets demonstrate the significant improvement of DAAT models on natural accuracy compared with strong baselines.},
	language = {en},
	booktitle = {European {Conference} on {Computer} {Vision}},
	author = {Yang, Shuo and Xu, Chang},
	year = {2022},
	pages = {16},
}

@article{zhang_towards_2020,
	title = {Towards {Robust} {Pattern} {Recognition}: {A} {Review}},
	volume = {108},
	issn = {1558-2256},
	shorttitle = {Towards {Robust} {Pattern} {Recognition}},
	doi = {10.1109/JPROC.2020.2989782},
	abstract = {The accuracies for many pattern recognition tasks have increased rapidly year by year, achieving or even outperforming human performance. From the perspective of accuracy, pattern recognition seems to be a nearly solved problem. However, once launched in real applications, the high-accuracy pattern recognition systems may become unstable and unreliable due to the lack of robustness in open and changing environments. In this article, we present a comprehensive review of research toward robust pattern recognition from the perspective of breaking three basic and implicit assumptions: closed-world assumption, independent and identically distributed assumption, and clean and big data assumption, which form the foundation of most pattern recognition models. Actually, our brain is robust at learning concepts continually and incrementally, in complex, open, and changing environments, with different contexts, modalities, and tasks, by showing only a few examples, under weak or noisy supervision. These are the major differences between human intelligence and machine intelligence, which are closely related to the above three assumptions. After witnessing the significant progress in accuracy improvement nowadays, this review paper will enable us to analyze the shortcomings and limitations of current methods and identify future research directions for robust pattern recognition.},
	number = {6},
	journal = {Proceedings of the IEEE},
	author = {Zhang, Xu-Yao and Liu, Cheng-Lin and Suen, Ching Y.},
	month = jun,
	year = {2020},
	note = {Conference Name: Proceedings of the IEEE},
	keywords = {Big Data, Clean and big data, Distributed control, Machine intelligence, Neural networks, Pattern recognition, Robustness, Task analysis, closed world, independent and identically distributed, robust pattern recognition},
	pages = {894--922},
}

@article{russakovsky_imagenet_2015,
	title = {{ImageNet} {Large} {Scale} {Visual} {Recognition} {Challenge}},
	volume = {115},
	issn = {1573-1405},
	url = {https://doi.org/10.1007/s11263-015-0816-y},
	doi = {10.1007/s11263-015-0816-y},
	abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the 5 years of the challenge, and propose future directions and improvements.},
	language = {en},
	number = {3},
	urldate = {2022-10-19},
	journal = {International Journal of Computer Vision},
	author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
	month = dec,
	year = {2015},
	keywords = {Benchmark, Dataset, Large-scale, Object detection, Object recognition},
	pages = {211--252},
}

@article{qian_survey_2022,
	title = {A survey of robust adversarial training in pattern recognition: {Fundamental}, theory, and methodologies},
	volume = {131},
	issn = {0031-3203},
	shorttitle = {A survey of robust adversarial training in pattern recognition},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320322003703},
	doi = {10.1016/j.patcog.2022.108889},
	abstract = {Deep neural networks have achieved remarkable success in machine learning, computer vision, and pattern recognition in the last few decades. Recent studies, however, show that neural networks (both shallow and deep) may be easily fooled by certain imperceptibly perturbed input samples called adversarial examples. Such security vulnerability has resulted in a large body of research in recent years because real-world threats could be introduced due to the vast applications of neural networks. To address the robustness issue to adversarial examples particularly in pattern recognition, robust adversarial training has become one mainstream. Various ideas, methods, and applications have boomed in the field. Yet, a deep understanding of adversarial training including characteristics, interpretations, theories, and connections among different models has remained elusive. This paper presents a comprehensive survey trying to offer a systematic and structured investigation on robust adversarial training in pattern recognition. We start with fundamentals including definition, notations, and properties of adversarial examples. We then introduce a general theoretical framework with gradient regularization for defending against adversarial samples - robust adversarial training with visualizations and interpretations on why adversarial training can lead to model robustness. Connections will also be established between adversarial training and other traditional learning theories. After that, we summarize, review, and discuss various methodologies with defense/training algorithms in a structured way. Finally, we present analysis, outlook, and remarks on adversarial training.},
	language = {en},
	urldate = {2022-10-19},
	journal = {Pattern Recognition},
	author = {Qian, Zhuang and Huang, Kaizhu and Wang, Qiu-Feng and Zhang, Xu-Yao},
	month = nov,
	year = {2022},
	keywords = {Adversarial examples, Adversarial training, Robust learning},
	pages = {108889},
}

@article{ning_improving_2021,
	title = {Improving {Model} {Robustness} by {Adaptively} {Correcting} {Perturbation} {Levels} with {Active} {Queries}},
	volume = {35},
	copyright = {Copyright (c) 2021 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/17106},
	doi = {10.1609/aaai.v35i10.17106},
	abstract = {In addition to high accuracy, robustness is becoming increasingly important for machine learning models in various applications. Recently, much research has been devoted to improving the model robustness by training with noise perturbations. Most existing studies assume a fixed perturbation level for all training examples, which however hardly holds in real tasks. In fact, excessive perturbations may destroy the discriminative content of an example, while deficient perturbations may fail to provide helpful information for improving the robustness. Motivated by this observation, we propose to adaptively adjust the perturbation levels for each example in the training process. Specifically, a novel active learning framework is proposed to allow the model interactively querying the correct perturbation level from human experts. By designing a cost-effective sampling strategy along with a new query type, the robustness can be significantly improved with a few queries. Both theoretical analysis and experimental studies validate the effectiveness of the proposed approach.},
	language = {en},
	number = {10},
	urldate = {2022-10-16},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Ning, Kun-Peng and Tao, Lue and Chen, Songcan and Huang, Sheng-Jun},
	month = may,
	year = {2021},
	note = {Number: 10},
	keywords = {Adversarial Learning \& Robustness},
	pages = {9161--9169},
}

@misc{yu_strength-adaptive_2022,
	title = {Strength-{Adaptive} {Adversarial} {Training}},
	url = {http://arxiv.org/abs/2210.01288},
	doi = {10.48550/arXiv.2210.01288},
	abstract = {Adversarial training (AT) is proved to reliably improve network's robustness against adversarial data. However, current AT with a pre-specified perturbation budget has limitations in learning a robust network. Firstly, applying a pre-specified perturbation budget on networks of various model capacities will yield divergent degree of robustness disparity between natural and robust accuracies, which deviates from robust network's desideratum. Secondly, the attack strength of adversarial training data constrained by the pre-specified perturbation budget fails to upgrade as the growth of network robustness, which leads to robust overfitting and further degrades the adversarial robustness. To overcome these limitations, we propose {\textbackslash}emph\{Strength-Adaptive Adversarial Training\} (SAAT). Specifically, the adversary employs an adversarial loss constraint to generate adversarial training data. Under this constraint, the perturbation budget will be adaptively adjusted according to the training state of adversarial data, which can effectively avoid robust overfitting. Besides, SAAT explicitly constrains the attack strength of training data through the adversarial loss, which manipulates model capacity scheduling during training, and thereby can flexibly control the degree of robustness disparity and adjust the tradeoff between natural accuracy and robustness. Extensive experiments show that our proposal boosts the robustness of adversarial training.},
	urldate = {2022-10-15},
	publisher = {arXiv},
	author = {Yu, Chaojian and Zhou, Dawei and Shen, Li and Yu, Jun and Han, Bo and Gong, Mingming and Wang, Nannan and Liu, Tongliang},
	month = oct,
	year = {2022},
	note = {arXiv:2210.01288 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@inproceedings{yang_one_nodate,
	title = {One {Size} {Does} {NOT} {Fit} {All}: {Data}-{Adaptive} {Adversarial} {Training}},
	abstract = {Adversarial robustness is critical for deep learning models to defend against adversarial attacks. Although adversarial training is considered to be one of the most effective ways to improve the model’s adversarial robustness, it usually yields models with lower natural accuracy. In this paper, we argue that, for the attackable examples, traditional adversarial training which utilizes a fixed size perturbation ball can create adversarial examples that deviate far away from the original class towards the target class. Thus, the model’s performance on the natural target class will drop drastically, which leads to the decline of natural accuracy. To this end, we propose the Data-Adaptive Adversarial Training (DAAT) which adaptively adjusts the perturbation ball to a proper size for each of the natural examples with the help of a natural trained calibration network. Besides, a dynamic training strategy empowers the DAAT models with impressive robustness while retaining remarkable natural accuracy. Based on a toy example, we theoretically prove the recession of the natural accuracy caused by adversarial training and show how the data-adaptive perturbation size helps the model resist it. Finally, empirical experiments on benchmark datasets demonstrate the significant improvement of DAAT models on natural accuracy compared with strong baselines.},
	language = {en},
	author = {Yang, Shuo and Xu, Chang},
	pages = {16},
}

@inproceedings{sarkar_adversarial_2021,
	title = {Adversarial {Robustness} without {Adversarial} {Training}: {A} {Teacher}-{Guided} {Curriculum} {Learning} {Approach}},
	shorttitle = {Adversarial {Robustness} without {Adversarial} {Training}},
	url = {https://openreview.net/forum?id=MqCzSKCQ1QB},
	abstract = {Current SOTA adversarially robust models are mostly based on adversarial training (AT) and differ only by some regularizers either at inner maximization or outer minimization steps. Being repetitive in nature during the inner maximization step, they take a huge time to train. We propose a non-iterative method that enforces the following ideas during training. Attribution maps are more aligned to the actual object in the image for adversarially robust models compared to naturally trained models. Also, the allowed set of pixels to perturb an image (that changes model decision) should be restricted to the object pixels only, which reduces the attack strength by limiting the attack space. Our method achieves significant performance gains with a little extra effort (10-20\%) over existing AT models and outperforms all other methods in terms of adversarial as well as natural accuracy. We have performed extensive experimentation with CIFAR-10, CIFAR-100, and TinyImageNet datasets and reported results against many popular strong adversarial attacks to prove the effectiveness of our method.},
	language = {en},
	urldate = {2022-10-15},
	author = {Sarkar, Anindya and Sarkar, Anirban and Gali, Sowrya and Balasubramanian, Vineeth N.},
	month = oct,
	year = {2021},
}

@misc{gowal_alternative_2019,
	title = {An {Alternative} {Surrogate} {Loss} for {PGD}-based {Adversarial} {Testing}},
	url = {http://arxiv.org/abs/1910.09338},
	doi = {10.48550/arXiv.1910.09338},
	abstract = {Adversarial testing methods based on Projected Gradient Descent (PGD) are widely used for searching norm-bounded perturbations that cause the inputs of neural networks to be misclassified. This paper takes a deeper look at these methods and explains the effect of different hyperparameters (i.e., optimizer, step size and surrogate loss). We introduce the concept of MultiTargeted testing, which makes clever use of alternative surrogate losses, and explain when and how MultiTargeted is guaranteed to find optimal perturbations. Finally, we demonstrate that MultiTargeted outperforms more sophisticated methods and often requires less iterative steps than other variants of PGD found in the literature. Notably, MultiTargeted ranks first on MadryLab's white-box MNIST and CIFAR-10 leaderboards, reducing the accuracy of their MNIST model to 88.36\% (with \${\textbackslash}ell\_{\textbackslash}infty\$ perturbations of \${\textbackslash}epsilon = 0.3\$) and the accuracy of their CIFAR-10 model to 44.03\% (at \${\textbackslash}epsilon = 8/255\$). MultiTargeted also ranks first on the TRADES leaderboard reducing the accuracy of their CIFAR-10 model to 53.07\% (with \${\textbackslash}ell\_{\textbackslash}infty\$ perturbations of \${\textbackslash}epsilon = 0.031\$).},
	urldate = {2022-10-15},
	publisher = {arXiv},
	author = {Gowal, Sven and Uesato, Jonathan and Qin, Chongli and Huang, Po-Sen and Mann, Timothy and Kohli, Pushmeet},
	month = oct,
	year = {2019},
	note = {arXiv:1910.09338 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{tashiro_diversity_2020,
	title = {Diversity can be {Transferred}: {Output} {Diversification} for {White}- and {Black}-box {Attacks}},
	volume = {33},
	shorttitle = {Diversity can be {Transferred}},
	url = {https://proceedings.neurips.cc/paper/2020/hash/30da227c6b5b9e2482b6b221c711edfd-Abstract.html},
	abstract = {Adversarial attacks often involve random perturbations of the inputs drawn from uniform or Gaussian distributions, e.g. to initialize optimization-based white-box attacks or generate update directions in black-box attacks. These simple perturbations, however, could be sub-optimal as they are agnostic to the model being attacked. To improve the efficiency of these attacks, we propose Output Diversified Sampling (ODS), a novel sampling strategy that attempts to maximize diversity in the target model's outputs among the generated samples. While ODS is a gradient-based strategy, the diversity offered by ODS is transferable and can be helpful for both white-box and black-box attacks via surrogate models. Empirically, we demonstrate that ODS significantly improves the performance of existing white-box and black-box attacks. In particular, ODS reduces the number of queries needed for state-of-the-art black-box attacks on ImageNet by a factor of two.},
	urldate = {2022-10-14},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Tashiro, Yusuke and Song, Yang and Ermon, Stefano},
	year = {2020},
	pages = {4536--4548},
}

@inproceedings{croce_provable_2019,
	title = {Provable {Robustness} of {ReLU} networks via {Maximization} of {Linear} {Regions}},
	url = {https://proceedings.mlr.press/v89/croce19a.html},
	abstract = {It has been shown that neural network classifiers are not robust. This raises concerns about their usage in safety-critical systems. We propose in this paper a regularization scheme for ReLU networks which provably improves the robustness of the classifier by maximizing the linear regions of the classifier as well as the distance to the decision boundary. Using our regularization we can even find the minimal adversarial perturbation for a certain fraction of test points for large networks. In the experiments we show that our approach improves upon pure adversarial training both in terms of lower and upper bounds on the robustness and is comparable or better than the state of the art in terms of test error and robustness.},
	language = {en},
	urldate = {2022-10-14},
	booktitle = {Proceedings of the {Twenty}-{Second} {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Croce, Francesco and Andriushchenko, Maksym and Hein, Matthias},
	month = apr,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {2057--2066},
}

@article{jiang_imbalanced_2021,
	title = {Imbalanced {Gradients}: {A} {New} {Cause} of {Overestimated} {Adversarial} {Robustness}},
	shorttitle = {Imbalanced {Gradients}},
	url = {https://openreview.net/forum?id=8SP2-AiWttb},
	abstract = {Evaluating the robustness of a defense model is a challenging task in adversarial robustness research. Obfuscated gradients, a type of gradient masking, have previously been found to exist in many defense methods and cause a false signal of robustness. In this paper, we identify a more subtle situation called {\textbackslash}emph\{Imbalanced Gradients\} that can also cause overestimated adversarial robustness. The phenomenon of imbalanced gradients occurs when the gradient of one term of the margin loss dominates and pushes the attack towards to a suboptimal direction. To exploit imbalanced gradients, we formulate a {\textbackslash}emph\{Margin Decomposition (MD)\} attack that decomposes a margin loss into individual terms and then explores the attackability of these terms separately via a two-stage process. We examine 12 state-of-the-art defense models, and find that models exploiting label smoothing easily cause imbalanced gradients, and on which our MD attacks can decrease their PGD robustness (evaluated by PGD attack) by over 23{\textbackslash}\%. For 6 out of the 12 defenses, our attack can reduce their PGD robustness by at least 9{\textbackslash}\%. The results suggest that imbalanced gradients need to be carefully addressed for more reliable adversarial robustness.},
	language = {en},
	urldate = {2022-10-14},
	author = {Jiang, Linxi and Ma, Xingjun and Weng, Zejia and Bailey, James and Jiang, Yu-Gang},
	month = mar,
	year = {2021},
}

@article{xu_towards_2021,
	title = {Towards evaluating the robustness of deep diagnostic models by adversarial attack},
	volume = {69},
	issn = {1361-8415},
	url = {https://www.sciencedirect.com/science/article/pii/S1361841521000232},
	doi = {10.1016/j.media.2021.101977},
	abstract = {Deep learning models (with neural networks) have been widely used in challenging tasks such as computer-aided disease diagnosis based on medical images. Recent studies have shown deep diagnostic models may not be robust in the inference process and may pose severe security concerns in clinical practice. Among all the factors that make the model not robust, the most serious one is adversarial examples. The so-called “adversarial example” is a well-designed perturbation that is not easily perceived by humans but results in a false output of deep diagnostic models with high confidence. In this paper, we evaluate the robustness of deep diagnostic models by adversarial attack. Specifically, we have performed two types of adversarial attacks to three deep diagnostic models in both single-label and multi-label classification tasks, and found that these models are not reliable when attacked by adversarial example. We have further explored how adversarial examples attack the models, by analyzing their quantitative classification results, intermediate features, discriminability of features and correlation of estimated labels for both original/clean images and those adversarial ones. We have also designed two new defense methods to handle adversarial examples in deep diagnostic models, i.e., Multi-Perturbations Adversarial Training (MPAdvT) and Misclassification-Aware Adversarial Training (MAAdvT). The experimental results have shown that the use of defense methods can significantly improve the robustness of deep diagnostic models against adversarial attacks.},
	language = {en},
	urldate = {2022-10-12},
	journal = {Medical Image Analysis},
	author = {Xu, Mengting and Zhang, Tao and Li, Zhongnian and Liu, Mingxia and Zhang, Daoqiang},
	month = apr,
	year = {2021},
	keywords = {Adversarial attack, Deep diagnostic models, Defense, Robustness},
	pages = {101977},
}

@article{shi_robust_2022,
	title = {Robust convolutional neural networks against adversarial attacks on medical images},
	volume = {132},
	issn = {0031-3203},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320322004046},
	doi = {10.1016/j.patcog.2022.108923},
	abstract = {Convolutional neural networks (CNNs) have been widely applied to medical images. However, medical images are vulnerable to adversarial attacks by perturbations that are undetectable to human experts. This poses significant security risks and challenges to CNN-based applications in clinic practice. In this work, we quantify the scale of adversarial perturbation imperceptible to clinical practitioners and investigate the cause of the vulnerability in CNNs. Specifically, we discover that noise (i.e., irrelevant or corrupted discriminative information) in medical images might be a key contributor to performance deterioration of CNNs against adversarial perturbations, as noisy features are learned unconsciously by CNNs in feature representations and magnified by adversarial perturbations. In response, we propose a novel defense method by embedding sparsity denoising operators in CNNs for improved robustness. Tested with various state-of-the-art attacking methods on two distinct medical image modalities, we demonstrate that the proposed method can successfully defend against those unnoticeable adversarial attacks by retaining as much as over 90\% of its original performance. We believe our findings are critical for improving and deploying CNN-based medical applications in real-world scenarios.},
	language = {en},
	urldate = {2022-10-12},
	journal = {Pattern Recognition},
	author = {Shi, Xiaoshuang and Peng, Yifan and Chen, Qingyu and Keenan, Tiarnan and Thavikulwat, Alisa T. and Lee, Sungwon and Tang, Yuxing and Chew, Emily Y. and Summers, Ronald M. and Lu, Zhiyong},
	month = dec,
	year = {2022},
	keywords = {Adversarial examples, CNNs, Sparsity denoising},
	pages = {108923},
}

@article{ma_understanding_2021,
	title = {Understanding adversarial attacks on deep learning based medical image analysis systems},
	volume = {110},
	issn = {0031-3203},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320320301357},
	doi = {10.1016/j.patcog.2020.107332},
	abstract = {Deep neural networks (DNNs) have become popular for medical image analysis tasks like cancer diagnosis and lesion detection. However, a recent study demonstrates that medical deep learning systems can be compromised by carefully-engineered adversarial examples/attacks with small imperceptible perturbations. This raises safety concerns about the deployment of these systems in clinical settings. In this paper, we provide a deeper understanding of adversarial examples in the context of medical images. We find that medical DNN models can be more vulnerable to adversarial attacks compared to models for natural images, according to two different viewpoints. Surprisingly, we also find that medical adversarial attacks can be easily detected, i.e., simple detectors can achieve over 98\% detection AUC against state-of-the-art attacks, due to fundamental feature differences compared to normal examples. We believe these findings may be a useful basis to approach the design of more explainable and secure medical deep learning systems.},
	language = {en},
	urldate = {2022-10-12},
	journal = {Pattern Recognition},
	author = {Ma, Xingjun and Niu, Yuhao and Gu, Lin and Wang, Yisen and Zhao, Yitian and Bailey, James and Lu, Feng},
	month = feb,
	year = {2021},
	keywords = {Adversarial attack, Adversarial example detection, Deep learning, Medical image analysis},
	pages = {107332},
}

@article{hirano_universal_2021,
	title = {Universal adversarial attacks on deep neural networks for medical image classification},
	volume = {21},
	issn = {1471-2342},
	url = {https://doi.org/10.1186/s12880-020-00530-y},
	doi = {10.1186/s12880-020-00530-y},
	abstract = {Deep neural networks (DNNs) are widely investigated in medical image classification to achieve automated support for clinical diagnosis. It is necessary to evaluate the robustness of medical DNN tasks against adversarial attacks, as high-stake decision-making will be made based on the diagnosis. Several previous studies have considered simple adversarial attacks. However, the vulnerability of DNNs to more realistic and higher risk attacks, such as universal adversarial perturbation (UAP), which is a single perturbation that can induce DNN failure in most classification tasks has not been evaluated yet.},
	language = {en},
	number = {1},
	urldate = {2022-10-12},
	journal = {BMC Medical Imaging},
	author = {Hirano, Hokuto and Minagi, Akinori and Takemoto, Kazuhiro},
	month = jan,
	year = {2021},
	keywords = {Adversarial attacks, Deep neural networks, Medical imaging, Security and privacy},
	pages = {9},
}

@article{apostolidis_survey_2021,
	title = {A {Survey} on {Adversarial} {Deep} {Learning} {Robustness} in {Medical} {Image} {Analysis}},
	volume = {10},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2079-9292},
	url = {https://www.mdpi.com/2079-9292/10/17/2132},
	doi = {10.3390/electronics10172132},
	abstract = {In the past years, deep neural networks (DNN) have become popular in many disciplines such as computer vision (CV), natural language processing (NLP), etc. The evolution of hardware has helped researchers to develop many powerful Deep Learning (DL) models to face numerous challenging problems. One of the most important challenges in the CV area is Medical Image Analysis in which DL models process medical images—such as magnetic resonance imaging (MRI), X-ray, computed tomography (CT), etc.—using convolutional neural networks (CNN) for diagnosis or detection of several diseases. The proper function of these models can significantly upgrade the health systems. However, recent studies have shown that CNN models are vulnerable under adversarial attacks with imperceptible perturbations. In this paper, we summarize existing methods for adversarial attacks, detections and defenses on medical imaging. Finally, we show that many attacks, which are undetectable by the human eye, can degrade the performance of the models, significantly. Nevertheless, some effective defense and attack detection methods keep the models safe to an extent. We end with a discussion on the current state-of-the-art and future challenges.},
	language = {en},
	number = {17},
	urldate = {2022-10-12},
	journal = {Electronics},
	author = {Apostolidis, Kyriakos D. and Papakostas, George A.},
	month = jan,
	year = {2021},
	note = {Number: 17
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {adversarial attack, computer vision, convolutional neural networks, deep learning, medical image analysis},
	pages = {2132},
}

@article{finlayson_adversarial_2019,
	title = {Adversarial attacks on medical machine learning},
	volume = {363},
	url = {https://www.science.org/doi/10.1126/science.aaw4399},
	doi = {10.1126/science.aaw4399},
	number = {6433},
	urldate = {2022-10-11},
	journal = {Science},
	author = {Finlayson, Samuel G. and Bowers, John D. and Ito, Joichi and Zittrain, Jonathan L. and Beam, Andrew L. and Kohane, Isaac S.},
	month = mar,
	year = {2019},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {1287--1289},
}

@article{bortsova_adversarial_2021,
	title = {Adversarial attack vulnerability of medical image analysis systems: {Unexplored} factors},
	volume = {73},
	issn = {1361-8415},
	shorttitle = {Adversarial attack vulnerability of medical image analysis systems},
	url = {https://www.sciencedirect.com/science/article/pii/S1361841521001870},
	doi = {10.1016/j.media.2021.102141},
	abstract = {Adversarial attacks are considered a potentially serious security threat for machine learning systems. Medical image analysis (MedIA) systems have recently been argued to be vulnerable to adversarial attacks due to strong financial incentives and the associated technological infrastructure. In this paper, we study previously unexplored factors affecting adversarial attack vulnerability of deep learning MedIA systems in three medical domains: ophthalmology, radiology, and pathology. We focus on adversarial black-box settings, in which the attacker does not have full access to the target model and usually uses another model, commonly referred to as surrogate model, to craft adversarial examples that are then transferred to the target model. We consider this to be the most realistic scenario for MedIA systems. Firstly, we study the effect of weight initialization (pre-training on ImageNet or random initialization) on the transferability of adversarial attacks from the surrogate model to the target model, i.e., how effective attacks crafted using the surrogate model are on the target model. Secondly, we study the influence of differences in development (training and validation) data between target and surrogate models. We further study the interaction of weight initialization and data differences with differences in model architecture. All experiments were done with a perturbation degree tuned to ensure maximal transferability at minimal visual perceptibility of the attacks. Our experiments show that pre-training may dramatically increase the transferability of adversarial examples, even when the target and surrogate’s architectures are different: the larger the performance gain using pre-training, the larger the transferability. Differences in the development data between target and surrogate models considerably decrease the performance of the attack; this decrease is further amplified by difference in the model architecture. We believe these factors should be considered when developing security-critical MedIA systems planned to be deployed in clinical practice. We recommend avoiding using only standard components, such as pre-trained architectures and publicly available datasets, as well as disclosure of design specifications, in addition to using adversarial defense methods. When evaluating the vulnerability of MedIA systems to adversarial attacks, various attack scenarios and target-surrogate differences should be simulated to achieve realistic robustness estimates. The code and all trained models used in our experiments are publicly available.3},
	language = {en},
	urldate = {2022-10-11},
	journal = {Medical Image Analysis},
	author = {Bortsova, Gerda and González-Gonzalo, Cristina and Wetstein, Suzanne C. and Dubost, Florian and Katramados, Ioannis and Hogeweg, Laurens and Liefers, Bart and van Ginneken, Bram and Pluim, Josien P. W. and Veta, Mitko and Sánchez, Clara I. and de Bruijne, Marleen},
	month = oct,
	year = {2021},
	keywords = {Adversarial attacks, Cybersecurity, Deep learning, Medical imaging},
	pages = {102141},
}

@article{zhang_adversarial_2019,
	title = {Adversarial {Interpolation} {Training}: {A} {Simple} {Approach} for {Improving} {Model} {Robustness}},
	shorttitle = {Adversarial {Interpolation} {Training}},
	url = {https://openreview.net/forum?id=Syejj0NYvr},
	abstract = {adversarial interpolation training: a simple, intuitive and effective approach for improving model robustness},
	language = {en},
	urldate = {2022-10-09},
	author = {Zhang, Haichao and Xu, Wei},
	month = dec,
	year = {2019},
}

@article{perrault-archambault_mixup_2019,
	title = {{MixUp} as {Directional} {Adversarial} {Training}},
	url = {https://openreview.net/forum?id=SkgjKR4YwH},
	abstract = {We present a novel interpretation of MixUp as belonging to a class highly analogous to adversarial training, and on this basis we introduce a simple generalization which outperforms MixUp},
	language = {en},
	urldate = {2022-10-09},
	author = {Perrault-Archambault, Guillaume and Mao, Yongyi and Guo, Hongyu and Zhang, Richong},
	month = dec,
	year = {2019},
}

@inproceedings{song_robust_2020,
	title = {Robust {Local} {Features} for {Improving} the {Generalization} of {Adversarial} {Training}},
	url = {https://openreview.net/forum?id=H1lZJpVFvr},
	abstract = {We propose a new stream of adversarial training approach called Robust Local Features for Adversarial Training (RLFAT) that significantly improves both the adversarially robust generalization and...},
	language = {en},
	urldate = {2022-10-09},
	author = {Song, Chuanbiao and He, Kun and Lin, Jiadong and Wang, Liwei and Hopcroft, John E.},
	month = mar,
	year = {2020},
}

@inproceedings{muller_when_2019,
	title = {When does label smoothing help?},
	volume = {32},
	url = {https://papers.nips.cc/paper/2019/hash/f1748d6b0fd9d439f71450117eba2725-Abstract.html},
	abstract = {The generalization and learning speed of a multi-class neural network can often be significantly improved by using soft targets that are a weighted average of the hard targets and the uniform distribution over labels. Smoothing the labels in this way prevents the network from becoming over-confident and label smoothing has been used in many state-of-the-art models, including image classification, language translation and speech recognition. Despite its widespread use, label smoothing is still poorly understood. Here we show empirically that in addition to improving generalization, label smoothing improves model calibration which can significantly improve beam search. However, we also observe that if a teacher network is trained with label smoothing, knowledge distillation into a student network is much less effective.  To explain these observations, we visualize how label smoothing changes the  representations learned by the penultimate layer of the network. We show that label smoothing encourages the representations of training examples from the same class to group in tight clusters. This results in loss of information in the logits about resemblances between instances of different classes, which is necessary for distillation, but does not hurt generalization or calibration of the model's predictions.},
	urldate = {2022-10-06},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Müller, Rafael and Kornblith, Simon and Hinton, Geoffrey E},
	year = {2019},
}

@inproceedings{li_what_2022,
	title = {What {Happens} after {SGD} {Reaches} {Zero} {Loss}? --{A} {Mathematical} {Framework}},
	shorttitle = {What {Happens} after {SGD} {Reaches} {Zero} {Loss}?},
	url = {https://openreview.net/forum?id=siCt4xZn5Ve},
	abstract = {Understanding the implicit bias of Stochastic Gradient Descent (SGD) is one of the key challenges in deep learning, especially for overparametrized models, where the local minimizers of the loss...},
	language = {en},
	urldate = {2022-10-03},
	author = {Li, Zhiyuan and Wang, Tianhao and Arora, Sanjeev},
	month = mar,
	year = {2022},
}

@inproceedings{kong_resolving_2022,
	title = {Resolving {Training} {Biases} via {Influence}-based {Data} {Relabeling}},
	url = {https://openreview.net/forum?id=EskfH0bwNVn},
	abstract = {The performance of supervised learning methods easily suffers from the training bias issue caused by train-test distribution mismatch or label noise. Influence function is a  technique that...},
	language = {en},
	urldate = {2022-10-03},
	author = {Kong, Shuming and Shen, Yanyan and Huang, Linpeng},
	month = mar,
	year = {2022},
}

@inproceedings{zhao_comparing_2022,
	title = {Comparing {Distributions} by {Measuring} {Differences} that {Affect} {Decision} {Making}},
	url = {https://openreview.net/forum?id=KB5onONJIAU},
	abstract = {Measuring the discrepancy between two probability distributions is a fundamental problem in machine learning and statistics. We propose a new class of discrepancies based on the optimal loss for a...},
	language = {en},
	urldate = {2022-10-03},
	author = {Zhao, Shengjia and Sinha, Abhishek and He, Yutong and Perreault, Aidan and Song, Jiaming and Ermon, Stefano},
	month = mar,
	year = {2022},
}

@inproceedings{singla_improved_2022,
	title = {Improved deterministic l2 robustness on {CIFAR}-10 and {CIFAR}-100},
	url = {https://openreview.net/forum?id=tD7eCtaSkR},
	abstract = {Training convolutional neural networks (CNNs) with a strict Lipschitz constraint under the \$l\_\{2\}\$ norm is useful for provable adversarial robustness, interpretable gradients and stable training....},
	language = {en},
	urldate = {2022-10-03},
	author = {Singla, Sahil and Singla, Surbhi and Feizi, Soheil},
	month = mar,
	year = {2022},
}

@inproceedings{zhang_boosting_2022,
	title = {Boosting the {Certified} {Robustness} of {L}-infinity {Distance} {Nets}},
	url = {https://openreview.net/forum?id=Q76Y7wkiji},
	abstract = {Recently, Zhang et al. (2021) developed a new neural network architecture based on \${\textbackslash}ell\_{\textbackslash}infty\$-distance functions, which naturally possesses certified \${\textbackslash}ell\_{\textbackslash}infty\$ robustness by its...},
	language = {en},
	urldate = {2022-10-03},
	author = {Zhang, Bohang and Jiang, Du and He, Di and Wang, Liwei},
	month = mar,
	year = {2022},
}

@inproceedings{harrington_finding_2022,
	title = {Finding {Biological} {Plausibility} for {Adversarially} {Robust} {Features} via {Metameric} {Tasks}},
	url = {https://openreview.net/forum?id=yeP_zx9vqNm},
	abstract = {Recent work suggests that feature constraints in the training datasets of deep neural networks (DNNs) drive robustness to adversarial noise (Ilyas et al., 2019). The representations learned by such...},
	language = {en},
	urldate = {2022-10-03},
	author = {Harrington, Anne and Deza, Arturo},
	month = feb,
	year = {2022},
}

@inproceedings{fiez_minimax_2022,
	title = {Minimax {Optimization} with {Smooth} {Algorithmic} {Adversaries}},
	url = {https://openreview.net/forum?id=UdxJ2fJx7N0},
	abstract = {This paper considers minimax optimization \${\textbackslash}min\_x {\textbackslash}max\_y f(x, y)\$ in the challenging setting where \$f\$ can be both nonconvex in \$x\$ and nonconcave in \$y\$. Though such optimization problems arise in...},
	language = {en},
	urldate = {2022-10-03},
	author = {Fiez, Tanner and Jin, Chi and Netrapalli, Praneeth and Ratliff, Lillian J.},
	month = mar,
	year = {2022},
}

@misc{tishby_information_2000,
	title = {The information bottleneck method},
	url = {http://arxiv.org/abs/physics/0004057},
	doi = {10.48550/arXiv.physics/0004057},
	abstract = {We define the relevant information in a signal \$x{\textbackslash}in X\$ as being the information that this signal provides about another signal \$y{\textbackslash}in {\textbackslash}Y\$. Examples include the information that face images provide about the names of the people portrayed, or the information that speech sounds provide about the words spoken. Understanding the signal \$x\$ requires more than just predicting \$y\$, it also requires specifying which features of \${\textbackslash}X\$ play a role in the prediction. We formalize this problem as that of finding a short code for \${\textbackslash}X\$ that preserves the maximum information about \${\textbackslash}Y\$. That is, we squeeze the information that \${\textbackslash}X\$ provides about \${\textbackslash}Y\$ through a `bottleneck' formed by a limited set of codewords \${\textbackslash}tX\$. This constrained optimization problem can be seen as a generalization of rate distortion theory in which the distortion measure \$d(x,{\textbackslash}x)\$ emerges from the joint statistics of \${\textbackslash}X\$ and \${\textbackslash}Y\$. This approach yields an exact set of self consistent equations for the coding rules \$X {\textbackslash}to {\textbackslash}tX\$ and \${\textbackslash}tX {\textbackslash}to {\textbackslash}Y\$. Solutions to these equations can be found by a convergent re-estimation method that generalizes the Blahut-Arimoto algorithm. Our variational principle provides a surprisingly rich framework for discussing a variety of problems in signal processing and learning, as will be described in detail elsewhere.},
	urldate = {2022-10-03},
	publisher = {arXiv},
	author = {Tishby, Naftali and Pereira, Fernando C. and Bialek, William},
	month = apr,
	year = {2000},
	note = {arXiv:physics/0004057},
	keywords = {Computer Science - Machine Learning, Condensed Matter - Disordered Systems and Neural Networks, Nonlinear Sciences - Adaptation and Self-Organizing Systems, Physics - Data Analysis, Statistics and Probability},
}

@article{xu_infoat_2022,
	title = {{InfoAT}: {Improving} {Adversarial} {Training} {Using} the {Information} {Bottleneck} {Principle}},
	issn = {2162-2388},
	shorttitle = {{InfoAT}},
	doi = {10.1109/TNNLS.2022.3183095},
	abstract = {Adversarial training (AT) has shown excellent high performance in defending against adversarial examples. Recent studies demonstrate that examples are not equally important to the final robustness of models during AT, that is, the so-called hard examples that can be attacked easily exhibit more influence than robust examples on the final robustness. Therefore, guaranteeing the robustness of hard examples is crucial for improving the final robustness of the model. However, defining effective heuristics to search for hard examples is still difficult. In this article, inspired by the information bottleneck (IB) principle, we uncover that an example with high mutual information of the input and its associated latent representation is more likely to be attacked. Based on this observation, we propose a novel and effective adversarial training method (InfoAT). InfoAT is encouraged to find examples with high mutual information and exploit them efficiently to improve the final robustness of models. Experimental results show that InfoAT achieves the best robustness among different datasets and models in comparison with several state-of-the-art methods.},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Xu, Mengting and Zhang, Tao and Li, Zhongnian and Zhang, Daoqiang},
	year = {2022},
	note = {Conference Name: IEEE Transactions on Neural Networks and Learning Systems},
	keywords = {Adversarial training (AT), Linear programming, Mutual information, Neural networks, Perturbation methods, Robustness, Standards, Training, information bottleneck (IB), mutual information, robustness},
	pages = {1--10},
}

@inproceedings{zhang_attacks_2020,
	title = {Attacks {Which} {Do} {Not} {Kill} {Training} {Make} {Adversarial} {Learning} {Stronger}},
	url = {https://proceedings.mlr.press/v119/zhang20z.html},
	abstract = {Adversarial training based on the minimax formulation is necessary for obtaining adversarial robustness of trained models. However, it is conservative or even pessimistic so that it sometimes hurts the natural generalization. In this paper, we raise a fundamental question\{—\}do we have to trade off natural generalization for adversarial robustness? We argue that adversarial training is to employ confident adversarial data for updating the current model. We propose a novel formulation of friendly adversarial training (FAT): rather than employing most adversarial data maximizing the loss, we search for least adversarial data (i.e., friendly adversarial data) minimizing the loss, among the adversarial data that are confidently misclassified. Our novel formulation is easy to implement by just stopping the most adversarial data searching algorithms such as PGD (projected gradient descent) early, which we call early-stopped PGD. Theoretically, FAT is justified by an upper bound of the adversarial risk. Empirically, early-stopped PGD allows us to answer the earlier question negatively\{—\}adversarial robustness can indeed be achieved without compromising the natural generalization.},
	language = {en},
	urldate = {2022-10-02},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Zhang, Jingfeng and Xu, Xilie and Han, Bo and Niu, Gang and Cui, Lizhen and Sugiyama, Masashi and Kankanhalli, Mohan},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {11278--11287},
}

@inproceedings{zhou_enhancing_2022,
	title = {Enhancing {Adversarial} {Robustness} for {Deep} {Metric} {Learning}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Zhou_Enhancing_Adversarial_Robustness_for_Deep_Metric_Learning_CVPR_2022_paper.html},
	language = {en},
	urldate = {2022-10-02},
	author = {Zhou, Mo and Patel, Vishal M.},
	year = {2022},
	pages = {15325--15334},
}

@misc{yang_adaptive_2022,
	title = {Adaptive {Regularization} for {Adversarial} {Training}},
	url = {http://arxiv.org/abs/2206.03353},
	doi = {10.48550/arXiv.2206.03353},
	abstract = {Adversarial training, which is to enhance robustness against adversarial attacks, has received much attention because it is easy to generate human-imperceptible perturbations of data to deceive a given deep neural network. In this paper, we propose a new adversarial training algorithm that is theoretically well motivated and empirically superior to other existing algorithms. A novel feature of the proposed algorithm is to use a data-adaptive regularization for robustifying a prediction model. We apply more regularization to data which are more vulnerable to adversarial attacks and vice versa. Even though the idea of data-adaptive regularization is not new, our data-adaptive regularization has a firm theoretical base of reducing an upper bound of the robust risk. Numerical experiments illustrate that our proposed algorithm improves the generalization (accuracy on clean samples) and robustness (accuracy on adversarial attacks) simultaneously to achieve the state-of-the-art performance.},
	urldate = {2022-10-02},
	publisher = {arXiv},
	author = {Yang, Dongyoon and Kong, Insung and Kim, Yongdai},
	month = jun,
	year = {2022},
	note = {arXiv:2206.03353 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{modas_prime_2022,
	title = {{PRIME}: {A} few primitives can boost robustness to common corruptions},
	shorttitle = {{PRIME}},
	url = {http://arxiv.org/abs/2112.13547},
	doi = {10.48550/arXiv.2112.13547},
	abstract = {Despite their impressive performance on image classification tasks, deep networks have a hard time generalizing to unforeseen corruptions of their data. To fix this vulnerability, prior works have built complex data augmentation strategies, combining multiple methods to enrich the training data. However, introducing intricate design choices or heuristics makes it hard to understand which elements of these methods are indeed crucial for improving robustness. In this work, we take a step back and follow a principled approach to achieve robustness to common corruptions. We propose PRIME, a general data augmentation scheme that relies on simple yet rich families of max-entropy image transformations. PRIME outperforms the prior art in terms of corruption robustness, while its simplicity and plug-and-play nature enable combination with other methods to further boost their robustness. We analyze PRIME to shed light on the importance of the mixing strategy on synthesizing corrupted images, and to reveal the robustness-accuracy trade-offs arising in the context of common corruptions. Finally, we show that the computational efficiency of our method allows it to be easily used in both on-line and off-line data augmentation schemes.},
	urldate = {2022-10-02},
	publisher = {arXiv},
	author = {Modas, Apostolos and Rade, Rahul and Ortiz-Jiménez, Guillermo and Moosavi-Dezfooli, Seyed-Mohsen and Frossard, Pascal},
	month = mar,
	year = {2022},
	note = {arXiv:2112.13547 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@inproceedings{cheng_cat_2022,
	address = {Vienna, Austria},
	title = {{CAT}: {Customized} {Adversarial} {Training} for {Improved} {Robustness}},
	isbn = {978-1-956792-00-3},
	shorttitle = {{CAT}},
	url = {https://www.ijcai.org/proceedings/2022/95},
	doi = {10.24963/ijcai.2022/95},
	abstract = {Adversarial training has become one of the most effective methods for improving robustness of neural networks. However, it often suffers from poor generalization on both clean and perturbed data. Current robust training method always use a uniformed perturbation strength for every samples to generate adversarial examples during model training for improving adversarial robustness. However, we show it would lead worse training and generalizaiton error and forcing the prediction to match one-hot label. In this paper, therefore, we propose a new algorithm, named Customized Adversarial Training (CAT), which adaptively customizes the perturbation level and the corresponding label for each training sample in adversarial training. We ﬁrst show theoretically the CAT scheme improves the generalization. Also, through extensive experiments, we show that the proposed algorithm achieves better clean and robust accuracy than previous adversarial training methods. The full version of this paper is available at https://arxiv.org/abs/2002.06789.},
	language = {en},
	urldate = {2022-10-01},
	booktitle = {Proceedings of the {Thirty}-{First} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Cheng, Minhao and Lei, Qi and Chen, Pin-Yu and Dhillon, Inderjit and Hsieh, Cho-Jui},
	month = jul,
	year = {2022},
	pages = {673--679},
}

@inproceedings{wang_improving_2020,
	title = {Improving {Adversarial} {Robustness} {Requires} {Revisiting} {Misclassified} {Examples}},
	abstract = {Deep neural networks (DNNs) are vulnerable to adversarial examples crafted by imperceptible perturbations. A range of defense techniques have been proposed to improve DNN robustness to adversarial examples, among which adversarial training has been demonstrated to be the most effective. Adversarial training is often formulated as a min-max optimization problem, with the inner maximization for generating adversarial examples. However, there exists a simple, yet easily overlooked fact that adversarial examples are only deﬁned on correctly classiﬁed (natural) examples, but inevitably, some (natural) examples will be misclassiﬁed during training. In this paper, we investigate the distinctive inﬂuence of misclassiﬁed and correctly classiﬁed examples on the ﬁnal robustness of adversarial training. Speciﬁcally, we ﬁnd that misclassiﬁed examples indeed have a signiﬁcant impact on the ﬁnal robustness. More surprisingly, we ﬁnd that different maximization techniques on misclassiﬁed examples may have a negligible inﬂuence on the ﬁnal robustness, while different minimization techniques are crucial. Motivated by the above discovery, we propose a new defense algorithm called Misclassiﬁcation Aware adveRsarial Training (MART), which explicitly differentiates the misclassiﬁed and correctly classiﬁed examples during the training. We also propose a semi-supervised extension of MART, which can leverage the unlabeled data to further improve the robustness. Experimental results show that MART and its variant could signiﬁcantly improve the state-of-the-art adversarial robustness.},
	language = {en},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Wang, Yisen and Zou, Difan and Yi, Jinfeng and Bailey, James and Ma, Xingjun and Gu, Quanquan},
	year = {2020},
	pages = {14},
}

@inproceedings{ding_sensitivity_2019,
	title = {On the {Sensitivity} of {Adversarial} {Robustness} to {Input} {Data} {Distributions}},
	url = {https://openreview.net/forum?id=S1xNEhR9KX},
	abstract = {Robustness performance of PGD trained models are sensitive to semantics-preserving transformation of image datasets, which implies the trickiness of evaluation of robust learning algorithms in...},
	language = {en},
	urldate = {2021-09-12},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Ding, Gavin Weiguang and Lui, Kry Yik Chau and Jin, Xiaomeng and Wang, Luyu and Huang, Ruitong},
	year = {2019},
}

@inproceedings{ding_mma_2020,
	title = {{MMA} {Training}: {Direct} {Input} {Space} {Margin} {Maximization} through {Adversarial} {Training}},
	shorttitle = {{MMA} {Training}},
	url = {https://openreview.net/forum?id=HkeryxBtPB},
	abstract = {We propose MMA training to directly maximize input space margin in order to improve adversarial robustness primarily by removing the requirement of specifying a fixed distortion bound.},
	language = {en},
	urldate = {2021-09-10},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Ding, Gavin Weiguang and Sharma, Yash and Lui, Kry Yik Chau and Huang, Ruitong},
	year = {2020},
}

@inproceedings{yun_cutmix_2019,
	title = {{CutMix}: {Regularization} {Strategy} to {Train} {Strong} {Classifiers} {With} {Localizable} {Features}},
	shorttitle = {{CutMix}},
	url = {https://openaccess.thecvf.com/content_ICCV_2019/html/Yun_CutMix_Regularization_Strategy_to_Train_Strong_Classifiers_With_Localizable_Features_ICCV_2019_paper.html},
	urldate = {2021-12-17},
	booktitle = {Proceedings of the {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision}},
	author = {Yun, Sangdoo and Han, Dongyoon and Oh, Seong Joon and Chun, Sanghyuk and Choe, Junsuk and Yoo, Youngjoon},
	year = {2019},
	pages = {6023--6032},
}

@inproceedings{singla_low_2021,
	title = {Low {Curvature} {Activations} {Reduce} {Overfitting} in {Adversarial} {Training}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Singla_Low_Curvature_Activations_Reduce_Overfitting_in_Adversarial_Training_ICCV_2021_paper.html},
	language = {en},
	urldate = {2021-11-17},
	booktitle = {Proceedings of the {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision}},
	author = {Singla, Vasu and Singla, Sahil and Feizi, Soheil and Jacobs, David},
	year = {2021},
	pages = {16423--16433},
}

@inproceedings{gowal_improving_2021,
	title = {Improving {Robustness} using {Generated} {Data}},
	abstract = {Recent work argues that robust training requires substantially larger datasets than those required for standard classiﬁcation. On CIFAR-10 and CIFAR-100, this translates into a sizable robust-accuracy gap between models trained solely on data from the original training set and those trained with additional data extracted from the “80 Million Tiny Images” dataset (80M-TI). In this paper, we explore how generative models trained solely on the original training set can be leveraged to artiﬁcially increase the size of the original training set and improve adversarial robustness to p norm-bounded perturbations. We identify the sufﬁcient conditions under which incorporating additional generated data can improve robustness, and demonstrate that it is possible to signiﬁcantly reduce the robust-accuracy gap to models trained with additional real data. Surprisingly, we show that even the addition of non-realistic random data (generated by Gaussian sampling) can improve robustness. We evaluate our approach on CIFAR-10, CIFAR-100, SVHN and TINYIMAGENET against ∞ and 2 norm-bounded perturbations of size = 8/255 and = 128/255, respectively. We show large absolute improvements in robust accuracy compared to previous state-of-the-art methods. Against ∞ normbounded perturbations of size = 8/255, our models achieve 66.10\% and 33.49\% robust accuracy on CIFAR-10 and CIFAR-100, respectively (improving upon the state-of-the-art by +8.96\% and +3.29\%). Against 2 norm-bounded perturbations of size = 128/255, our model achieves 78.31\% on CIFAR-10 (+3.81\%). These results beat most prior works that use external data.},
	language = {en},
	booktitle = {Thirty-{Fifth} {Conference} on {Neural} {Information} {Processing} {Systems}},
	author = {Gowal, Sven and Rebufﬁ, Sylvestre-Alvise and Wiles, Olivia and Stimberg, Florian and Calian, Dan and Mann, Timothy},
	year = {2021},
	pages = {16},
}

@inproceedings{chen_robust_2021,
	title = {Robust {Overfitting} may be mitigated by properly learned smoothening},
	url = {https://openreview.net/forum?id=qZzy5urZw9},
	abstract = {A recent study (Rice et al.,  2020) revealed overfitting to be a dominant phenomenon in adversarially robust training of deep networks, and that appropriate early-stopping of adversarial training...},
	language = {en},
	urldate = {2021-02-23},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Chen, Tianlong and Zhang, Zhenyu and Liu, Sijia and Chang, Shiyu and Wang, Zhangyang},
	year = {2021},
}

@inproceedings{dong_exploring_2022,
	title = {Exploring {Memorization} in {Adversarial} {Training}},
	url = {https://openreview.net/forum?id=7gE9V9GBZaI},
	abstract = {Deep learning models have a propensity for fitting the entire training set even with random labels, which requires memorization of every training sample. In this paper, we explore the memorization...},
	language = {en},
	urldate = {2022-04-06},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Dong, Yinpeng and Xu, Ke and Yang, Xiao and Pang, Tianyu and Deng, Zhijie and Su, Hang and Zhu, Jun},
	year = {2022},
}

@inproceedings{gontijo-lopes_tradeoffs_2021,
	title = {Tradeoffs in {Data} {Augmentation}: {An} {Empirical} {Study}},
	shorttitle = {Tradeoffs in {Data} {Augmentation}},
	url = {https://openreview.net/forum?id=ZcKPWuhG6wy},
	abstract = {Though data augmentation has become a standard component of deep neural network training, the underlying mechanism behind the effectiveness of these techniques remains poorly understood. In...},
	language = {en},
	urldate = {2021-12-19},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Gontijo-Lopes, Raphael and Smullin, Sylvia and Cubuk, Ekin Dogus and Dyer, Ethan},
	year = {2021},
}

@misc{wang_resmooth_2022,
	title = {{ReSmooth}: {Detecting} and {Utilizing} {OOD} {Samples} when {Training} with {Data} {Augmentation}},
	shorttitle = {{ReSmooth}},
	url = {http://arxiv.org/abs/2205.12606},
	doi = {10.48550/arXiv.2205.12606},
	abstract = {Data augmentation (DA) is a widely used technique for enhancing the training of deep neural networks. Recent DA techniques which achieve state-of-the-art performance always meet the need for diversity in augmented training samples. However, an augmentation strategy that has a high diversity usually introduces out-of-distribution (OOD) augmented samples and these samples consequently impair the performance. To alleviate this issue, we propose ReSmooth, a framework that firstly detects OOD samples in augmented samples and then leverages them. To be specific, we first use a Gaussian mixture model to fit the loss distribution of both the original and augmented samples and accordingly split these samples into in-distribution (ID) samples and OOD samples. Then we start a new training where ID and OOD samples are incorporated with different smooth labels. By treating ID samples and OOD samples unequally, we can make better use of the diverse augmented data. Further, we incorporate our ReSmooth framework with negative data augmentation strategies. By properly handling their intentionally created ODD samples, the classification performance of negative data augmentations is largely ameliorated. Experiments on several classification benchmarks show that ReSmooth can be easily extended to existing augmentation strategies (such as RandAugment, rotate, and jigsaw) and improve on them.},
	urldate = {2022-09-26},
	publisher = {arXiv},
	author = {Wang, Chenyang and Jiang, Junjun and Zhou, Xiong and Liu, Xianming},
	month = may,
	year = {2022},
	note = {arXiv:2205.12606 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{xie_intriguing_2020,
	title = {Intriguing {Properties} of {Adversarial} {Training} at {Scale}},
	url = {https://openreview.net/forum?id=HyxJhCEFDS},
	abstract = {The first rigor diagnose of large-scale adversarial training on ImageNet},
	language = {en},
	urldate = {2022-09-26},
	author = {Xie, Cihang and Yuille, Alan},
	month = mar,
	year = {2020},
}

@inproceedings{wang_once-for-all_2020,
	title = {Once-for-{All} {Adversarial} {Training}: {In}-{Situ} {Tradeoff} between {Robustness} and {Accuracy} for {Free}},
	volume = {33},
	shorttitle = {Once-for-{All} {Adversarial} {Training}},
	url = {https://proceedings.neurips.cc/paper/2020/hash/537d9b6c927223c796cac288cced29df-Abstract.html},
	abstract = {Adversarial training and its many variants substantially improve deep network robustness, yet at the cost of compromising standard accuracy. Moreover, the training process is heavy and hence it becomes impractical to thoroughly explore the trade-off between accuracy and robustness. This paper asks this new question: how to quickly calibrate a trained model in-situ, to examine the achievable trade-offs between its standard and robust accuracies, without (re-)training it many times? Our proposed framework, Once-for-all Adversarial Training (OAT), is built on an innovative model-conditional training framework, with a controlling hyper-parameter as the input. The trained model could be adjusted among different standard and robust accuracies “for free” at testing time. As an important knob, we exploit dual batch normalization to separate standard and adversarial feature statistics, so that they can be learned in one model without degrading performance. We further extend OAT to a Once-for-all Adversarial Training and Slimming (OATS) framework, that allows for the joint trade-off among accuracy, robustness and runtime efficiency. Experiments show that, without any re-training nor ensembling, OAT/OATS achieve similar or even superior performance compared to dedicatedly trained models at various configurations. Our codes and pretrained models are available at: https://github.com/VITA-Group/Once-for-All-Adversarial-Training.},
	urldate = {2022-09-26},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Wang, Haotao and Chen, Tianlong and Gui, Shupeng and Hu, TingKuei and Liu, Ji and Wang, Zhangyang},
	year = {2020},
	pages = {7449--7461},
}

@misc{merchant_does_2020,
	title = {Does {Data} {Augmentation} {Benefit} from {Split} {BatchNorms}},
	url = {http://arxiv.org/abs/2010.07810},
	doi = {10.48550/arXiv.2010.07810},
	abstract = {Data augmentation has emerged as a powerful technique for improving the performance of deep neural networks and led to state-of-the-art results in computer vision. However, state-of-the-art data augmentation strongly distorts training images, leading to a disparity between examples seen during training and inference. In this work, we explore a recently proposed training paradigm in order to correct for this disparity: using an auxiliary BatchNorm for the potentially out-of-distribution, strongly augmented images. Our experiments then focus on how to define the BatchNorm parameters that are used at evaluation. To eliminate the train-test disparity, we experiment with using the batch statistics defined by clean training images only, yet surprisingly find that this does not yield improvements in model performance. Instead, we investigate using BatchNorm parameters defined by weak augmentations and find that this method significantly improves the performance of common image classification benchmarks such as CIFAR-10, CIFAR-100, and ImageNet. We then explore a fundamental trade-off between accuracy and robustness coming from using different BatchNorm parameters, providing greater insight into the benefits of data augmentation on model performance.},
	urldate = {2022-09-26},
	publisher = {arXiv},
	author = {Merchant, Amil and Zoph, Barret and Cubuk, Ekin Dogus},
	month = oct,
	year = {2020},
	note = {arXiv:2010.07810 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@inproceedings{izmailov_averaging_2018,
	series = {34th {Conference} on {Uncertainty} in {Artificial} {Intelligence} 2018, {UAI} 2018},
	title = {Averaging weights leads to wider optima and better generalization},
	shorttitle = {Averaging weights leads to wider optima and better generalization},
	url = {http://www.scopus.com/inward/record.url?scp=85059432227&partnerID=8YFLogxK},
	abstract = {Deep neural networks are typically trained by optimizing a loss function with an SGD variant, in conjunction with a decaying learning rate, until convergence. We show that simple averaging of multiple points along the trajectory of SGD, with a cyclical or constant learning rate, leads to better generalization than conventional training. We also show that this Stochastic Weight Averaging (SWA) procedure finds much broader optima than SGD, and approximates the recent Fast Geometric Ensem-bling (FGE) approach with a single model. Using SWA we achieve notable improvement in test accuracy over conventional SGD training on a range of state-of-the-art residual networks, PyramidNets, DenseNets, and ShakeShake networks on CIFAR-10, CIFAR-100, and ImageNet. In short, SWA is extremely easy to implement, improves generalization, and has almost no computational overhead.},
	urldate = {2022-07-25},
	booktitle = {34th {Conference} on {Uncertainty} in {Artificial} {Intelligence} 2018, {UAI} 2018},
	publisher = {Association For Uncertainty in Artificial Intelligence (AUAI)},
	author = {Izmailov, Pavel and Podoprikhin, Dmitrii and Garipov, Timur and Vetrov, Dmitry and Wilson, Andrew Gordon},
	editor = {Silva, Ricardo and Globerson, Amir and Globerson, Amir},
	year = {2018},
	pages = {876--885},
}

@inproceedings{yi_reweighting_2021,
	title = {Reweighting {Augmented} {Samples} by {Minimizing} the {Maximal} {Expected} {Loss}},
	url = {https://openreview.net/forum?id=9G5MIc-goqB},
	abstract = {Data augmentation is an effective technique to improve the generalization of deep neural networks. However, previous data augmentation methods usually treat the augmented samples equally without...},
	language = {en},
	urldate = {2021-12-19},
	author = {Yi, Mingyang and Hou, Lu and Shang, Lifeng and Jiang, Xin and Liu, Qun and Ma, Zhi-Ming},
	year = {2021},
}

@inproceedings{lamb_interpolated_2019,
	address = {New York, NY, USA},
	series = {{AISec}'19},
	title = {Interpolated {Adversarial} {Training}: {Achieving} {Robust} {Neural} {Networks} {Without} {Sacrificing} {Too} {Much} {Accuracy}},
	isbn = {978-1-4503-6833-9},
	shorttitle = {Interpolated {Adversarial} {Training}},
	url = {https://doi.org/10.1145/3338501.3357369},
	doi = {10.1145/3338501.3357369},
	abstract = {Adversarial robustness has become a central goal in deep learning, both in theory and in practice. However, successful methods to improve the adversarial robustness (such as adversarial training) greatly hurt generalization performance on the unperturbed data. This could have a major impact on how achieving adversarial robustness affects real world systems (i.e. many may opt to forego robustness if it can improve accuracy on the unperturbed data). We propose Interpolated Adversarial Training, which employs recently proposed interpolation based training methods in the framework of adversarial training. On CIFAR-10, adversarial training increases the standard test error (when there is no adversary) from 4.43\% to 12.32\%, whereas with our Interpolated adversarial training we retain adversarial robustness while achieving a standard test error of only 6.45\%. With our technique, the relative increase in the standard error for the robust model is reduced from 178.1\% to just 45.5\%.},
	urldate = {2022-09-20},
	booktitle = {Proceedings of the 12th {ACM} {Workshop} on {Artificial} {Intelligence} and {Security}},
	publisher = {Association for Computing Machinery},
	author = {Lamb, Alex and Verma, Vikas and Kannala, Juho and Bengio, Yoshua},
	month = nov,
	year = {2019},
	keywords = {adversarial robustness, interpolation based training, manifold mixup, mixup, neural networks, standard test error},
	pages = {95--103},
}

@article{li_understanding_2022,
	title = {Understanding and {Combating} {Robust} {Overfitting} via {Input} {Loss} {Landscape} {Analysis} and {Regularization}},
	abstract = {Adversarial training is widely used to improve the robustness of deep neural networks to adversarial attack. However, adversarial training is prone to overfitting, and the cause is far from clear. This work sheds light on the mechanisms underlying overfitting through analyzing the loss landscape w.r.t. the input. We find that robust overfitting results from standard training, specifically the minimization of the clean loss, and can be mitigated by regularization of the loss gradients. Moreover, we find that robust overfitting turns severer during adversarial training partially because the gradient regularization effect of adversarial training becomes weaker due to the increase in the loss landscape’s curvature. To improve robust generalization, we propose a new regularizer to smooth the loss landscape by penalizing the weighted logits variation along the adversarial direction. Our method significantly mitigates robust overfitting and achieves the highest robustness and efficiency compared to similar previous methods.},
	language = {en},
	journal = {submitted},
	author = {Li, Lin and Spratling, Michael},
	month = aug,
	year = {2022},
	pages = {29},
}

@article{han_advancing_2021,
	title = {Advancing diagnostic performance and clinical usability of neural networks via adversarial training and dual batch normalization},
	volume = {12},
	copyright = {2021 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-021-24464-3},
	doi = {10.1038/s41467-021-24464-3},
	abstract = {Unmasking the decision making process of machine learning models is essential for implementing diagnostic support systems in clinical practice. Here, we demonstrate that adversarially trained models can significantly enhance the usability of pathology detection as compared to their standard counterparts. We let six experienced radiologists rate the interpretability of saliency maps in datasets of X-rays, computed tomography, and magnetic resonance imaging scans. Significant improvements are found for our adversarial models, which are further improved by the application of dual-batch normalization. Contrary to previous research on adversarially trained models, we find that accuracy of such models is equal to standard models, when sufficiently large datasets and dual batch norm training are used. To ensure transferability, we additionally validate our results on an external test set of 22,433 X-rays. These findings elucidate that different paths for adversarial and real images are needed during training to achieve state of the art results with superior clinical interpretability.},
	language = {en},
	number = {1},
	urldate = {2022-09-19},
	journal = {Nature Communications},
	author = {Han, Tianyu and Nebelung, Sven and Pedersoli, Federico and Zimmermann, Markus and Schulze-Hagen, Maximilian and Ho, Michael and Haarburger, Christoph and Kiessling, Fabian and Kuhl, Christiane and Schulz, Volkmar and Truhn, Daniel},
	month = jul,
	year = {2021},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Machine learning, Medical imaging, Predictive medicine},
	pages = {4315},
}

@inproceedings{wong_wasserstein_2019,
	title = {Wasserstein {Adversarial} {Examples} via {Projected} {Sinkhorn} {Iterations}},
	url = {https://proceedings.mlr.press/v97/wong19a.html},
	abstract = {A rapidly growing area of work has studied the existence of adversarial examples, datapoints which have been perturbed to fool a classifier, but the vast majority of these works have focused primarily on threat models defined by ℓpℓp{\textbackslash}ell\_p norm-bounded perturbations. In this paper, we propose a new threat model for adversarial attacks based on the Wasserstein distance. In the image classification setting, such distances measure the cost of moving pixel mass, which can naturally represent “standard” image manipulations such as scaling, rotation, translation, and distortion (and can potentially be applied to other settings as well). To generate Wasserstein adversarial examples, we develop a procedure for approximate projection onto the Wasserstein ball, based upon a modified version of the Sinkhorn iteration. The resulting algorithm can successfully attack image classification models, bringing traditional CIFAR10 models down to 3\% accuracy within a Wasserstein ball with radius 0.1 (i.e., moving 10\% of the image mass 1 pixel), and we demonstrate that PGD-based adversarial training can improve this adversarial accuracy to 76\%. In total, this work opens up a new direction of study in adversarial robustness, more formally considering convex metrics that accurately capture the invariances that we typically believe should exist in classifiers, and code for all experiments in the paper is available at https://github.com/locuslab/projected\_sinkhorn.},
	language = {en},
	urldate = {2022-09-18},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Wong, Eric and Schmidt, Frank and Kolter, Zico},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {6808--6817},
}

@inproceedings{wu_stronger_2020,
	title = {Stronger and {Faster} {Wasserstein} {Adversarial} {Attacks}},
	url = {https://proceedings.mlr.press/v119/wu20d.html},
	abstract = {Deep models, while being extremely flexible and accurate, are surprisingly vulnerable to “small, imperceptible” perturbations known as adversarial attacks. While the majority of existing attacks focus on measuring perturbations under the ℓpℓp{\textbackslash}ell\_p metric, Wasserstein distance, which takes geometry in pixel space into account, has long been known to be a suitable metric for measuring image quality and has recently risen as a compelling alternative to the ℓpℓp{\textbackslash}ell\_p metric in adversarial attacks. However, constructing an effective attack under the Wasserstein metric is computationally much more challenging and calls for better optimization algorithms. We address this gap in two ways: (a) we develop an exact yet efficient projection operator to enable a stronger projected gradient attack; (b) we show that the Frank-Wolfe method equipped with a suitable linear minimization oracle works extremely fast under Wasserstein constraints. Our algorithms not only converge faster but also generate much stronger attacks. For instance, we decrease the accuracy of a residual network on CIFAR-10 to 3.43.43.4\% within a Wasserstein perturbation ball of radius 0.0050.0050.005, in contrast to 65.665.665.6\% using the previous Wasserstein attack based on an {\textbackslash}emph\{approximate\} projection operator. Furthermore, employing our stronger attacks in adversarial training significantly improves the robustness of adversarially trained models.},
	language = {en},
	urldate = {2022-09-18},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Wu, Kaiwen and Wang, Allen and Yu, Yaoliang},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {10377--10387},
}

@inproceedings{nanda_measuring_2022,
	title = {Measuring {Representational} {Robustness} of {Neural} {Networks} {Through} {Shared} {Invariances}},
	url = {https://proceedings.mlr.press/v162/nanda22a.html},
	abstract = {A major challenge in studying robustness in deep learning is defining the set of “meaningless” perturbations to which a given Neural Network (NN) should be invariant. Most work on robustness implicitly uses a human as the reference model to define such perturbations. Our work offers a new view on robustness by using another reference NN to define the set of perturbations a given NN should be invariant to, thus generalizing the reliance on a reference “human NN” to any NN. This makes measuring robustness equivalent to measuring the extent to which two NNs share invariances. We propose a measure called {\textbackslash}stir, which faithfully captures the extent to which two NNs share invariances. {\textbackslash}stir re-purposes existing representation similarity measures to make them suitable for measuring shared invariances. Using our measure, we are able to gain insights about how shared invariances vary with changes in weight initialization, architecture, loss functions, and training dataset. Our implementation is available at: {\textbackslash}url\{https://github.com/nvedant07/STIR\}.},
	language = {en},
	urldate = {2022-09-16},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Nanda, Vedant and Speicher, Till and Kolling, Camila and Dickerson, John P. and Gummadi, Krishna and Weller, Adrian},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {16368--16382},
}

@inproceedings{zhou_understanding_2022,
	title = {Understanding {The} {Robustness} in {Vision} {Transformers}},
	url = {https://proceedings.mlr.press/v162/zhou22m.html},
	abstract = {Recent studies show that Vision Transformers (ViTs) exhibit strong robustness against various corruptions. Although this property is partly attributed to the self-attention mechanism, there is still a lack of an explanatory framework towards a more systematic understanding. In this paper, we examine the role of self-attention in learning robust representations. Our study is motivated by the intriguing properties of self-attention in visual grouping which indicate that self-attention could promote improved mid-level representation and robustness. We thus propose a family of fully attentional networks (FANs) that incorporate self-attention in both token mixing and channel processing. We validate the design comprehensively on various hierarchical backbones. Our model with a DeiT architecture achieves a state-of-the-art 47.6\% mCE on ImageNet-C with 29M parameters. We also demonstrate significantly improved robustness in two downstream tasks: semantic segmentation and object detection},
	language = {en},
	urldate = {2022-09-16},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Zhou, Daquan and Yu, Zhiding and Xie, Enze and Xiao, Chaowei and Anandkumar, Animashree and Feng, Jiashi and Alvarez, Jose M.},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {27378--27394},
}

@inproceedings{teti_lcanets_2022,
	title = {{LCANets}: {Lateral} {Competition} {Improves} {Robustness} {Against} {Corruption} and {Attack}},
	shorttitle = {{LCANets}},
	url = {https://proceedings.mlr.press/v162/teti22a.html},
	abstract = {Although Convolutional Neural Networks (CNNs) achieve high accuracy on image recognition tasks, they lack robustness against realistic corruptions and fail catastrophically when deliberately attacked. Previous CNNs with representations similar to primary visual cortex (V1) were more robust to adversarial attacks on images than current adversarial defense techniques, but they required training on large-scale neural recordings or handcrafting neuroscientific models. Motivated by evidence that neural activity in V1 is sparse, we develop a class of hybrid CNNs, called LCANets, which feature a frontend that performs sparse coding via local lateral competition. We demonstrate that LCANets achieve competitive clean accuracy to standard CNNs on action and image recognition tasks and significantly greater accuracy under various image corruptions. We also perform the first adversarial attacks with full knowledge of a sparse coding CNN layer by attacking LCANets with white-box and black-box attacks, and we show that, contrary to previous hypotheses, sparse coding layers are not very robust to white-box attacks. Finally, we propose a way to use sparse coding layers as a plug-and-play robust frontend by showing that they significantly increase the robustness of adversarially-trained CNNs over corruptions and attacks.},
	language = {en},
	urldate = {2022-09-16},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Teti, Michael and Kenyon, Garrett and Migliori, Ben and Moore, Juston},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {21232--21252},
}

@inproceedings{xue_investigating_2022,
	title = {Investigating {Why} {Contrastive} {Learning} {Benefits} {Robustness} against {Label} {Noise}},
	url = {https://proceedings.mlr.press/v162/xue22a.html},
	abstract = {Self-supervised Contrastive Learning (CL) has been recently shown to be very effective in preventing deep networks from overfitting noisy labels. Despite its empirical success, the theoretical understanding of the effect of contrastive learning on boosting robustness is very limited. In this work, we rigorously prove that the representation matrix learned by contrastive learning boosts robustness, by having: (i) one prominent singular value corresponding to each sub-class in the data, and significantly smaller remaining singular values; and (ii) a large alignment between the prominent singular vectors and the clean labels of each sub-class. The above properties enable a linear layer trained on such representations to effectively learn the clean labels without overfitting the noise. We further show that the low-rank structure of the Jacobian of deep networks pre-trained with contrastive learning allows them to achieve a superior performance initially, when fine-tuned on noisy labels. Finally, we demonstrate that the initial robustness provided by contrastive learning enables robust training methods to achieve state-of-the-art performance under extreme noise levels, e.g., an average of 27.18\% and 15.58\% increase in accuracy on CIFAR-10 and CIFAR-100 with 80\% symmetric noisy labels, and 4.11\% increase in accuracy on WebVision.},
	language = {en},
	urldate = {2022-09-16},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Xue, Yihao and Whitecross, Kyle and Mirzasoleiman, Baharan},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {24851--24871},
}

@inproceedings{zhang_branch_2022,
	title = {A {Branch} and {Bound} {Framework} for {Stronger} {Adversarial} {Attacks} of {ReLU} {Networks}},
	url = {https://proceedings.mlr.press/v162/zhang22ae.html},
	abstract = {Strong adversarial attacks are important for evaluating the true robustness of deep neural networks. Most existing attacks search in the input space, e.g., using gradient descent, and may miss adversarial examples due to non-convexity. In this work, we systematically search adversarial examples in the activation space of ReLU networks to tackle hard instances where none of the existing adversarial attacks succeed. Unfortunately, searching the activation space typically relies on generic mixed integer programming (MIP) solvers and is limited to small networks and easy problem instances. To improve scalability and practicability, we use branch and bound (BaB) with specialized GPU-based bound propagation methods, and propose a top-down beam-search approach to quickly identify the subspace that may contain adversarial examples. Moreover, we build an adversarial candidates pool using cheap attacks to further assist the search in activation space via diving techniques and a bottom-up large neighborhood search. Our adversarial attack framework, BaB-Attack, opens up a new opportunity for designing novel adversarial attacks not limited to searching the input space, and enables us to borrow techniques from integer programming theory and neural network verification. In experiments, we can successfully generate adversarial examples when existing attacks on input space fail. Compared to off-the-shelf MIP solver based attacks that requires significant computations, we outperform in both success rates and efficiency.},
	language = {en},
	urldate = {2022-09-16},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Zhang, Huan and Wang, Shiqi and Xu, Kaidi and Wang, Yihan and Jana, Suman and Hsieh, Cho-Jui and Kolter, Zico},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {26591--26604},
}

@inproceedings{dominguez-olmedo_adversarial_2022,
	title = {On the {Adversarial} {Robustness} of {Causal} {Algorithmic} {Recourse}},
	url = {https://proceedings.mlr.press/v162/dominguez-olmedo22a.html},
	abstract = {Algorithmic recourse seeks to provide actionable recommendations for individuals to overcome unfavorable classification outcomes from automated decision-making systems. Recourse recommendations should ideally be robust to reasonably small uncertainty in the features of the individual seeking recourse. In this work, we formulate the adversarially robust recourse problem and show that recourse methods that offer minimally costly recourse fail to be robust. We then present methods for generating adversarially robust recourse for linear and for differentiable classifiers. Finally, we show that regularizing the decision-making classifier to behave locally linearly and to rely more strongly on actionable features facilitates the existence of adversarially robust recourse.},
	language = {en},
	urldate = {2022-09-16},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Dominguez-Olmedo, Ricardo and Karimi, Amir H. and Schölkopf, Bernhard},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {5324--5342},
}

@inproceedings{guo_adversarially_2022,
	title = {Adversarially trained neural representations are already as robust as biological neural representations},
	url = {https://proceedings.mlr.press/v162/guo22d.html},
	abstract = {Visual systems of primates are the gold standard of robust perception. There is thus a general belief that mimicking the neural representations that underlie those systems will yield artificial visual systems that are adversarially robust. In this work, we develop a method for performing adversarial visual attacks directly on primate brain activity. We then leverage this method to demonstrate that the above-mentioned belief might not be well-founded. Specifically, we report that the biological neurons that make up visual systems of primates exhibit susceptibility to adversarial perturbations that is comparable in magnitude to existing (robustly trained) artificial neural networks.},
	language = {en},
	urldate = {2022-09-16},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Guo, Chong and Lee, Michael and Leclerc, Guillaume and Dapello, Joel and Rao, Yug and Madry, Aleksander and Dicarlo, James},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {8072--8081},
}

@inproceedings{gao_fast_2022,
	title = {Fast and {Reliable} {Evaluation} of {Adversarial} {Robustness} with {Minimum}-{Margin} {Attack}},
	url = {https://proceedings.mlr.press/v162/gao22i.html},
	abstract = {The AutoAttack (AA) has been the most reliable method to evaluate adversarial robustness when considerable computational resources are available. However, the high computational cost (e.g., 100 times more than that of the project gradient descent attack) makes AA infeasible for practitioners with limited computational resources, and also hinders applications of AA in the adversarial training (AT). In this paper, we propose a novel method, minimum-margin (MM) attack, to fast and reliably evaluate adversarial robustness. Compared with AA, our method achieves comparable performance but only costs 3\% of the computational time in extensive experiments. The reliability of our method lies in that we evaluate the quality of adversarial examples using the margin between two targets that can precisely identify the most adversarial example. The computational efficiency of our method lies in an effective Sequential TArget Ranking Selection (STARS) method, ensuring that the cost of the MM attack is independent of the number of classes. The MM attack opens a new way for evaluating adversarial robustness and provides a feasible and reliable way to generate high-quality adversarial examples in AT.},
	language = {en},
	urldate = {2022-09-16},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Gao, Ruize and Wang, Jiongxiao and Zhou, Kaiwen and Liu, Feng and Xie, Binghui and Niu, Gang and Han, Bo and Cheng, James},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {7144--7163},
}

@inproceedings{zhou_improving_2022,
	title = {Improving {Adversarial} {Robustness} via {Mutual} {Information} {Estimation}},
	url = {https://proceedings.mlr.press/v162/zhou22j.html},
	abstract = {Deep neural networks (DNNs) are found to be vulnerable to adversarial noise. They are typically misled by adversarial samples to make wrong predictions. To alleviate this negative effect, in this paper, we investigate the dependence between outputs of the target model and input adversarial samples from the perspective of information theory, and propose an adversarial defense method. Specifically, we first measure the dependence by estimating the mutual information (MI) between outputs and the natural patterns of inputs (called natural MI) and MI between outputs and the adversarial patterns of inputs (called adversarial MI), respectively. We find that adversarial samples usually have larger adversarial MI and smaller natural MI compared with those w.r.t. natural samples. Motivated by this observation, we propose to enhance the adversarial robustness by maximizing the natural MI and minimizing the adversarial MI during the training process. In this way, the target model is expected to pay more attention to the natural pattern that contains objective semantics. Empirical evaluations demonstrate that our method could effectively improve the adversarial accuracy against multiple attacks.},
	language = {en},
	urldate = {2022-09-16},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Zhou, Dawei and Wang, Nannan and Gao, Xinbo and Han, Bo and Wang, Xiaoyu and Zhan, Yibing and Liu, Tongliang},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {27338--27352},
}

@inproceedings{yao_improving_2022,
	title = {Improving {Out}-of-{Distribution} {Robustness} via {Selective} {Augmentation}},
	url = {https://proceedings.mlr.press/v162/yao22b.html},
	abstract = {Machine learning algorithms typically assume that training and test examples are drawn from the same distribution. However, distribution shift is a common problem in real-world applications and can cause models to perform dramatically worse at test time. In this paper, we specifically consider the problems of subpopulation shifts (e.g., imbalanced data) and domain shifts. While prior works often seek to explicitly regularize internal representations or predictors of the model to be domain invariant, we instead aim to learn invariant predictors without restricting the model’s internal representations or predictors. This leads to a simple mixup-based technique which learns invariant predictors via selective augmentation called LISA. LISA selectively interpolates samples either with the same labels but different domains or with the same domain but different labels. Empirically, we study the effectiveness of LISA on nine benchmarks ranging from subpopulation shifts to domain shifts, and we find that LISA consistently outperforms other state-of-the-art methods and leads to more invariant predictors. We further analyze a linear setting and theoretically show how LISA leads to a smaller worst-group error.},
	language = {en},
	urldate = {2022-09-16},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Yao, Huaxiu and Wang, Yu and Li, Sai and Zhang, Linjun and Liang, Weixin and Zou, James and Finn, Chelsea},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {25407--25437},
}

@inproceedings{yamamura_diversified_2022,
	title = {Diversified {Adversarial} {Attacks} based on {Conjugate} {Gradient} {Method}},
	url = {https://proceedings.mlr.press/v162/yamamura22a.html},
	abstract = {Deep learning models are vulnerable to adversarial examples, and adversarial attacks used to generate such examples have attracted considerable research interest. Although existing methods based on the steepest descent have achieved high attack success rates, ill-conditioned problems occasionally reduce their performance. To address this limitation, we utilize the conjugate gradient (CG) method, which is effective for this type of problem, and propose a novel attack algorithm inspired by the CG method, named the Auto Conjugate Gradient (ACG) attack. The results of large-scale evaluation experiments conducted on the latest robust models show that, for most models, ACG was able to find more adversarial examples with fewer iterations than the existing SOTA algorithm Auto-PGD (APGD). We investigated the difference in search performance between ACG and APGD in terms of diversification and intensification, and define a measure called Diversity Index (DI) to quantify the degree of diversity. From the analysis of the diversity using this index, we show that the more diverse search of the proposed method remarkably improves its attack success rate.},
	language = {en},
	urldate = {2022-09-16},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Yamamura, Keiichiro and Sato, Haruki and Tateiwa, Nariaki and Hata, Nozomi and Mitsutake, Toru and Oe, Issa and Ishikura, Hiroki and Fujisawa, Katsuki},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {24872--24894},
}

@inproceedings{croce_adversarial_2022,
	title = {Adversarial {Robustness} against {Multiple} and {Single} \$l\_p\$-{Threat} {Models} via {Quick} {Fine}-{Tuning} of {Robust} {Classifiers}},
	url = {https://proceedings.mlr.press/v162/croce22b.html},
	abstract = {A major drawback of adversarially robust models, in particular for large scale datasets like ImageNet, is the extremely long training time compared to standard models. Moreover, models should be robust not only to one lplpl\_p-threat model but ideally to all of them. In this paper we propose Extreme norm Adversarial Training (E-AT) for multiple-norm robustness which is based on geometric properties of lplpl\_p-balls. E-AT costs up to three times less than other adversarial training methods for multiple-norm robustness. Using E-AT we show that for ImageNet a single epoch and for CIFAR-10 three epochs are sufficient to turn any lplpl\_p-robust model into a multiple-norm robust model. In this way we get the first multiple-norm robust model for ImageNet and boost the state-of-the-art for multiple-norm robustness to more than 515151\% on CIFAR-10. Finally, we study the general transfer via fine-tuning of adversarial robustness between different individual lplpl\_p-threat models and improve the previous SOTA l1l1l\_1-robustness on both CIFAR-10 and ImageNet. Extensive experiments show that our scheme works across datasets and architectures including vision transformers.},
	language = {en},
	urldate = {2022-09-16},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Croce, Francesco and Hein, Matthias},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {4436--4454},
}

@inproceedings{dbouk_adversarial_2022,
	title = {Adversarial {Vulnerability} of {Randomized} {Ensembles}},
	url = {https://proceedings.mlr.press/v162/dbouk22a.html},
	abstract = {Despite the tremendous success of deep neural networks across various tasks, their vulnerability to imperceptible adversarial perturbations has hindered their deployment in the real world. Recently, works on randomized ensembles have empirically demonstrated significant improvements in adversarial robustness over standard adversarially trained (AT) models with minimal computational overhead, making them a promising solution for safety-critical resource-constrained applications. However, this impressive performance raises the question: Are these robustness gains provided by randomized ensembles real? In this work we address this question both theoretically and empirically. We first establish theoretically that commonly employed robustness evaluation methods such as adaptive PGD provide a false sense of security in this setting. Subsequently, we propose a theoretically-sound and efficient adversarial attack algorithm (ARC) capable of compromising random ensembles even in cases where adaptive PGD fails to do so. We conduct comprehensive experiments across a variety of network architectures, training schemes, datasets, and norms to support our claims, and empirically establish that randomized ensembles are in fact more vulnerable to ℓpℓp{\textbackslash}ell\_p-bounded adversarial perturbations than even standard AT models. Our code can be found at https://github.com/hsndbk4/ARC.},
	language = {en},
	urldate = {2022-09-16},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Dbouk, Hassan and Shanbhag, Naresh},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {4890--4917},
}

@inproceedings{kou_certified_2022,
	title = {Certified {Adversarial} {Robustness} {Under} the {Bounded} {Support} {Set}},
	url = {https://proceedings.mlr.press/v162/kou22a.html},
	abstract = {Deep neural networks (DNNs) have revealed severe vulnerability to adversarial perturbations, beside empirical adversarial training for robustness, the design of provably robust classifiers attracts more and more attention. Randomized smoothing methods provide the certified robustness with agnostic architecture, which is further extended to a provable robustness framework using f-divergence. While these methods cannot be applied to smoothing measures with bounded support set such as uniform probability measure due to the use of likelihood ratio in their certification methods. In this paper, we generalize the fff-divergence-based framework to a Wasserstein-distance-based and total-variation-distance-based framework that is first able to analyze robustness properties of bounded support set smoothing measures both theoretically and experimentally. By applying our methodology to uniform probability measures with support set lp(p=1,2,∞ and general)lp(p=1,2,∞ and general)l\_p (p=1,2,{\textbackslash}infty{\textbackslash}text\{ and general\}) ball, we prove negative certified robustness properties with respect to lq(q=1,2,∞)lq(q=1,2,∞)l\_q (q=1, 2, {\textbackslash}infty) perturbations and present experimental results on CIFAR-10 dataset with ResNet to validate our theory. And it is also worth mentioning that our certification procedure only costs constant computation time.},
	language = {en},
	urldate = {2022-09-16},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Kou, Yiwen and Zheng, Qinyuan and Wang, Yisen},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {11559--11597},
}

@inproceedings{pang_robustness_2022,
	title = {Robustness and {Accuracy} {Could} {Be} {Reconcilable} by ({Proper}) {Definition}},
	url = {https://proceedings.mlr.press/v162/pang22a.html},
	abstract = {The trade-off between robustness and accuracy has been widely studied in the adversarial literature. Although still controversial, the prevailing view is that this trade-off is inherent, either empirically or theoretically. Thus, we dig for the origin of this trade-off in adversarial training and find that it may stem from the improperly defined robust error, which imposes an inductive bias of local invariance — an overcorrection towards smoothness. Given this, we advocate employing local equivariance to describe the ideal behavior of a robust model, leading to a self-consistent robust error named SCORE. By definition, SCORE facilitates the reconciliation between robustness and accuracy, while still handling the worst-case uncertainty via robust optimization. By simply substituting KL divergence with variants of distance metrics, SCORE can be efficiently minimized. Empirically, our models achieve top-rank performance on RobustBench under AutoAttack. Besides, SCORE provides instructive insights for explaining the overfitting phenomenon and semantic input gradients observed on robust models.},
	language = {en},
	urldate = {2022-09-16},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Pang, Tianyu and Lin, Min and Yang, Xiao and Zhu, Jun and Yan, Shuicheng},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {17258--17277},
}

@inproceedings{gao_rethinking_2022,
	title = {Rethinking {Image}-{Scaling} {Attacks}: {The} {Interplay} {Between} {Vulnerabilities} in {Machine} {Learning} {Systems}},
	shorttitle = {Rethinking {Image}-{Scaling} {Attacks}},
	url = {https://proceedings.mlr.press/v162/gao22g.html},
	abstract = {As real-world images come in varying sizes, the machine learning model is part of a larger system that includes an upstream image scaling algorithm. In this paper, we investigate the interplay between vulnerabilities of the image scaling procedure and machine learning models in the decision-based black-box setting. We propose a novel sampling strategy to make a black-box attack exploit vulnerabilities in scaling algorithms, scaling defenses, and the final machine learning model in an end-to-end manner. Based on this scaling-aware attack, we reveal that most existing scaling defenses are ineffective under threat from downstream models. Moreover, we empirically observe that standard black-box attacks can significantly improve their performance by exploiting the vulnerable scaling procedure. We further demonstrate this problem on a commercial Image Analysis API with decision-based black-box attacks.},
	language = {en},
	urldate = {2022-09-16},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Gao, Yue and Shumailov, Ilia and Fawaz, Kassem},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {7102--7121},
}

@inproceedings{xu_adversarially_2022,
	title = {Adversarially {Robust} {Models} may not {Transfer} {Better}: {Sufficient} {Conditions} for {Domain} {Transferability} from the {View} of {Regularization}},
	shorttitle = {Adversarially {Robust} {Models} may not {Transfer} {Better}},
	url = {https://proceedings.mlr.press/v162/xu22n.html},
	abstract = {Machine learning (ML) robustness and domain generalization are fundamentally correlated: they essentially concern data distribution shifts under adversarial and natural settings, respectively. On one hand, recent studies show that more robust (adversarially trained) models are more generalizable. On the other hand, there is a lack of theoretical understanding of their fundamental connections. In this paper, we explore the relationship between regularization and domain transferability considering different factors such as norm regularization and data augmentations (DA). We propose a general theoretical framework proving that factors involving the model function class regularization are sufficient conditions for relative domain transferability. Our analysis implies that “robustness" is neither necessary nor sufficient for transferability; rather, regularization is a more fundamental perspective for understanding domain transferability. We then discuss popular DA protocols (including adversarial training) and show when they can be viewed as the function class regularization under certain conditions and therefore improve generalization. We conduct extensive experiments to verify our theoretical findings and show several counterexamples where robustness and generalization are negatively correlated on different datasets.},
	language = {en},
	urldate = {2022-09-16},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Xu, Xiaojun and Zhang, Jacky Y. and Ma, Evelyn and Son, Hyun Ho and Koyejo, Sanmi and Li, Bo},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {24770--24802},
}

@inproceedings{li_double_2022,
	title = {Double {Sampling} {Randomized} {Smoothing}},
	url = {https://proceedings.mlr.press/v162/li22aa.html},
	abstract = {Neural networks (NNs) are known to be vulnerable against adversarial perturbations, and thus there is a line of work aiming to provide robustness certification for NNs, such as randomized smoothing, which samples smoothing noises from a certain distribution to certify the robustness for a smoothed classifier. However, as previous work shows, the certified robust radius in randomized smoothing suffers from scaling to large datasets ("curse of dimensionality"). To overcome this hurdle, we propose a Double Sampling Randomized Smoothing (DSRS) framework, which exploits the sampled probability from an additional smoothing distribution to tighten the robustness certification of the previous smoothed classifier. Theoretically, under mild assumptions, we prove that DSRS can certify Θ(d−−√)Θ(d){\textbackslash}Theta({\textbackslash}sqrt d) robust radius under ℓ2ℓ2{\textbackslash}ell\_2 norm where ddd is the input dimension, which implies that DSRS may be able to break the curse of dimensionality of randomized smoothing. We instantiate DSRS for a generalized family of Gaussian smoothing and propose an efficient and sound computing method based on customized dual optimization considering sampling error. Extensive experiments on MNIST, CIFAR-10, and ImageNet verify our theory and show that DSRS certifies larger robust radii than existing baselines consistently under different settings. Code is available at https://github.com/llylly/DSRS.},
	language = {en},
	urldate = {2022-09-16},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Li, Linyi and Zhang, Jiawei and Xie, Tao and Li, Bo},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {13163--13208},
}

@inproceedings{mustafa_generalization_2022,
	title = {On the {Generalization} {Analysis} of {Adversarial} {Learning}},
	url = {https://proceedings.mlr.press/v162/mustafa22a.html},
	abstract = {Many recent studies have highlighted the susceptibility of virtually all machine-learning models to adversarial attacks. Adversarial attacks are imperceptible changes to an input example of a given prediction model. Such changes are carefully designed to alter the otherwise correct prediction of the model. In this paper, we study the generalization properties of adversarial learning. In particular, we derive high-probability generalization bounds on the adversarial risk in terms of the empirical adversarial risk, the complexity of the function class and the adversarial noise set. Our bounds are generally applicable to many models, losses, and adversaries. We showcase its applicability by deriving adversarial generalization bounds for the multi-class classification setting and various prediction models (including linear models and Deep Neural Networks). We also derive optimistic adversarial generalization bounds for the case of smooth losses. These are the first fast-rate bounds valid for adversarial deep learning to the best of our knowledge.},
	language = {en},
	urldate = {2022-09-16},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Mustafa, Waleed and Lei, Yunwen and Kloft, Marius},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {16174--16196},
}

@inproceedings{sukenik_intriguing_2022,
	title = {Intriguing {Properties} of {Input}-{Dependent} {Randomized} {Smoothing}},
	url = {https://proceedings.mlr.press/v162/sukeni-k22a.html},
	abstract = {Randomized smoothing is currently considered the state-of-the-art method to obtain certifiably robust classifiers. Despite its remarkable performance, the method is associated with various serious problems such as “certified accuracy waterfalls”, certification vs. accuracy trade-off, or even fairness issues. Input-dependent smoothing approaches have been proposed with intention of overcoming these flaws. However, we demonstrate that these methods lack formal guarantees and so the resulting certificates are not justified. We show that in general, the input-dependent smoothing suffers from the curse of dimensionality, forcing the variance function to have low semi-elasticity. On the other hand, we provide a theoretical and practical framework that enables the usage of input-dependent smoothing even in the presence of the curse of dimensionality, under strict restrictions. We present one concrete design of the smoothing variance function and test it on CIFAR10 and MNIST. Our design mitigates some of the problems of classical smoothing and is formally underlined, yet further improvement of the design is still necessary.},
	language = {en},
	urldate = {2022-09-16},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Súkenı́k, Peter and Kuvshinov, Aleksei and Günnemann, Stephan},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {20697--20743},
}

@inproceedings{wei_smooth_2022,
	title = {To {Smooth} or {Not}? {When} {Label} {Smoothing} {Meets} {Noisy} {Labels}},
	shorttitle = {To {Smooth} or {Not}?},
	url = {https://proceedings.mlr.press/v162/wei22b.html},
	abstract = {Label smoothing (LS) is an arising learning paradigm that uses the positively weighted average of both the hard training labels and uniformly distributed soft labels. It was shown that LS serves as a regularizer for training data with hard labels and therefore improves the generalization of the model. Later it was reported LS even helps with improving robustness when learning with noisy labels. However, we observed that the advantage of LS vanishes when we operate in a high label noise regime. Intuitively speaking, this is due to the increased entropy of P(noisy label{\textbar}X) when the noise rate is high, in which case, further applying LS tends to “over-smooth” the estimated posterior. We proceeded to discover that several learning-with-noisy-labels solutions in the literature instead relate more closely to negative/not label smoothing (NLS), which acts counter to LS and defines as using a negative weight to combine the hard and soft labels! We provide understandings for the properties of LS and NLS when learning with noisy labels. Among other established properties, we theoretically show NLS is considered more beneficial when the label noise rates are high. We provide extensive experimental results on multiple benchmarks to support our findings too. Code is publicly available at https://github.com/UCSC-REAL/negative-label-smoothing.},
	language = {en},
	urldate = {2022-09-16},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Wei, Jiaheng and Liu, Hangyu and Liu, Tongliang and Niu, Gang and Sugiyama, Masashi and Liu, Yang},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {23589--23614},
}

@inproceedings{sitawarin_demystifying_2022,
	title = {Demystifying the {Adversarial} {Robustness} of {Random} {Transformation} {Defenses}},
	url = {https://proceedings.mlr.press/v162/sitawarin22a.html},
	abstract = {Neural networks’ lack of robustness against attacks raises concerns in security-sensitive settings such as autonomous vehicles. While many countermeasures may look promising, only a few withstand rigorous evaluation. Defenses using random transformations (RT) have shown impressive results, particularly BaRT (Raff et al., 2019) on ImageNet. However, this type of defense has not been rigorously evaluated, leaving its robustness properties poorly understood. Their stochastic properties make evaluation more challenging and render many proposed attacks on deterministic models inapplicable. First, we show that the BPDA attack (Athalye et al., 2018a) used in BaRT’s evaluation is ineffective and likely overestimates its robustness. We then attempt to construct the strongest possible RT defense through the informed selection of transformations and Bayesian optimization for tuning their parameters. Furthermore, we create the strongest possible attack to evaluate our RT defense. Our new attack vastly outperforms the baseline, reducing the accuracy by 83\% compared to the 19\% reduction by the commonly used EoT attack (4.3×4.3×4.3{\textbackslash}times improvement). Our result indicates that the RT defense on the Imagenette dataset (a ten-class subset of ImageNet) is not robust against adversarial examples. Extending the study further, we use our new attack to adversarially train RT defense (called AdvRT), resulting in a large robustness gain. Code is available at https://github.com/wagnergroup/demystify-random-transform.},
	language = {en},
	urldate = {2022-09-16},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Sitawarin, Chawin and Golan-Strieb, Zachary J. and Wagner, David},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {20232--20252},
}

@inproceedings{shen_data_2022,
	title = {Data {Augmentation} as {Feature} {Manipulation}},
	url = {https://proceedings.mlr.press/v162/shen22a.html},
	abstract = {Data augmentation is a cornerstone of the machine learning pipeline, yet its theoretical underpinnings remain unclear. Is it merely a way to artificially augment the data set size? Or is it about encouraging the model to satisfy certain invariances? In this work we consider another angle, and we study the effect of data augmentation on the dynamic of the learning process. We find that data augmentation can alter the relative importance of various features, effectively making certain informative but hard to learn features more likely to be captured in the learning process. Importantly, we show that this effect is more pronounced for non-linear models, such as neural networks. Our main contribution is a detailed analysis of data augmentation on the learning dynamic for a two layer convolutional neural network in the recently proposed multi-view model by Z. Allen-Zhu and Y. Li. We complement this analysis with further experimental evidence that data augmentation can be viewed as a form of feature manipulation.},
	language = {en},
	urldate = {2022-09-16},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Shen, Ruoqi and Bubeck, Sebastien and Gunasekar, Suriya},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {19773--19808},
}

@inproceedings{han_you_2022,
	title = {You {Only} {Cut} {Once}: {Boosting} {Data} {Augmentation} with a {Single} {Cut}},
	shorttitle = {You {Only} {Cut} {Once}},
	url = {https://proceedings.mlr.press/v162/han22a.html},
	abstract = {We present You Only Cut Once (YOCO) for performing data augmentations. YOCO cuts one image into two pieces and performs data augmentations individually within each piece. Applying YOCO improves the diversity of the augmentation per sample and encourages neural networks to recognize objects from partial information. YOCO enjoys the properties of parameter-free, easy usage, and boosting almost all augmentations for free. Thorough experiments are conducted to evaluate its effectiveness. We first demonstrate that YOCO can be seamlessly applied to varying data augmentations, neural network architectures, and brings performance gains on CIFAR and ImageNet classification tasks, sometimes surpassing conventional image-level augmentation by large margins. Moreover, we show YOCO benefits contrastive pre-training toward a more powerful representation that can be better transferred to multiple downstream tasks. Finally, we study a number of variants of YOCO and empirically analyze the performance for respective settings.},
	language = {en},
	urldate = {2022-09-16},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Han, Junlin and Fang, Pengfei and Li, Weihao and Hong, Jie and Armin, Mohammad Ali and Reid, Ian and Petersson, Lars and Li, Hongdong},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {8196--8212},
}

@inproceedings{zhou_modeling_2022,
	title = {Modeling {Adversarial} {Noise} for {Adversarial} {Training}},
	url = {https://proceedings.mlr.press/v162/zhou22k.html},
	abstract = {Deep neural networks have been demonstrated to be vulnerable to adversarial noise, promoting the development of defense against adversarial attacks. Motivated by the fact that adversarial noise contains well-generalizing features and that the relationship between adversarial data and natural data can help infer natural data and make reliable predictions, in this paper, we study to model adversarial noise by learning the transition relationship between adversarial labels (i.e. the flipped labels used to generate adversarial data) and natural labels (i.e. the ground truth labels of the natural data). Specifically, we introduce an instance-dependent transition matrix to relate adversarial labels and natural labels, which can be seamlessly embedded with the target model (enabling us to model stronger adaptive adversarial noise). Empirical evaluations demonstrate that our method could effectively improve adversarial accuracy.},
	language = {en},
	urldate = {2022-09-16},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Zhou, Dawei and Wang, Nannan and Han, Bo and Liu, Tongliang},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {27353--27366},
}

@inproceedings{alayrac_are_2019,
	title = {Are {Labels} {Required} for {Improving} {Adversarial} {Robustness}?},
	abstract = {Recent work has uncovered the interesting (and somewhat surprising) ﬁnding that training models to be invariant to adversarial perturbations requires substantially larger datasets than those required for standard classiﬁcation. This result is a key hurdle in the deployment of robust machine learning models in many real world applications where labeled data is expensive. Our main insight is that unlabeled data can be a competitive alternative to labeled data for training adversarially robust models. Theoretically, we show that in a simple statistical setting, the sample complexity for learning an adversarially robust model from unlabeled data matches the fully supervised case up to constant factors. On standard datasets like CIFAR10, a simple Unsupervised Adversarial Training (UAT) approach using unlabeled data improves robust accuracy by 21.7\% over using 4K supervised examples alone, and captures over 95\% of the improvement from the same number of labeled examples. Finally, we report an improvement of 4\% over the previous state-of-theart on CIFAR-10 against the strongest known attack by using additional unlabeled data from the uncurated 80 Million Tiny Images dataset. This demonstrates that our ﬁnding extends as well to the more realistic case where unlabeled data is also uncurated, therefore opening a new avenue for improving adversarial training.},
	language = {en},
	booktitle = {33rd {Conference} on {Neural} {Information} {Processing} {Systems} ({NeurIPS} 2019)},
	author = {Alayrac, Jean-Baptiste and Uesato, Jonathan and Huang, Po-Sen and Fawzi, Alhussein and Stanforth, Robert and Kohli, Pushmeet},
	year = {2019},
	pages = {10},
}

@inproceedings{sitawarin_sat_2021,
	title = {{SAT}: {Improving} {Adversarial} {Training} via {Curriculum}-{Based} {Loss} {Smoothing}},
	shorttitle = {{SAT}},
	url = {http://arxiv.org/abs/2003.09347},
	doi = {10.1145/3474369.3486878},
	abstract = {Adversarial training (AT) has become a popular choice for training robust networks. However, it tends to sacrifice clean accuracy heavily in favor of robustness and suffers from a large generalization error. To address these concerns, we propose Smooth Adversarial Training (SAT), guided by our analysis on the eigenspectrum of the loss Hessian. We find that curriculum learning, a scheme that emphasizes on starting "easy" and gradually ramping up on the "difficulty" of training, smooths the adversarial loss landscape for a suitably chosen difficulty metric. We present a general formulation for curriculum learning in the adversarial setting and propose two difficulty metrics based on the maximal Hessian eigenvalue (H-SAT) and the softmax probability (P-SA). We demonstrate that SAT stabilizes network training even for a large perturbation norm and allows the network to operate at a better clean accuracy versus robustness trade-off curve compared to AT. This leads to a significant improvement in both clean accuracy and robustness compared to AT, TRADES, and other baselines. To highlight a few results, our best model improves normal and robust accuracy by 6\% and 1\% on CIFAR-100 compared to AT, respectively. On Imagenette, a ten-class subset of ImageNet, our model outperforms AT by 23\% and 3\% on normal and robust accuracy respectively.},
	urldate = {2022-09-15},
	booktitle = {Proceedings of the 14th {ACM} {Workshop} on {Artificial} {Intelligence} and {Security}},
	author = {Sitawarin, Chawin and Chakraborty, Supriyo and Wagner, David},
	month = nov,
	year = {2021},
	note = {arXiv:2003.09347 [cs, stat]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {25--36},
}

@inproceedings{huang_transferable_2022,
	title = {Transferable {Adversarial} {Attack} based on {Integrated} {Gradients}},
	url = {https://openreview.net/forum?id=DesNW4-5ai9},
	abstract = {The vulnerability of deep neural networks to adversarial examples has drawn tremendous attention from the community. Three approaches, optimizing standard objective functions, exploiting attention...},
	language = {en},
	urldate = {2022-04-06},
	author = {Huang, Yi and Kong, Adams Wai-Kin},
	year = {2022},
}

@inproceedings{lv_implicit_2022,
	title = {Implicit {Bias} of {Adversarial} {Training} for {Deep} {Neural} {Networks}},
	url = {https://openreview.net/forum?id=l8It-0lE5e7},
	abstract = {We provide theoretical understandings of the implicit bias imposed by adversarial training for homogeneous deep neural networks without any explicit regularization. In particular, for deep linear...},
	language = {en},
	urldate = {2022-04-06},
	author = {Lv, Bochen and Zhu, Zhanxing},
	year = {2022},
}

@inproceedings{agarwal_neural_2021,
	title = {Neural {Additive} {Models}: {Interpretable} {Machine} {Learning} with {Neural} {Nets}},
	volume = {34},
	shorttitle = {Neural {Additive} {Models}},
	url = {https://papers.nips.cc/paper/2021/hash/251bd0442dfcc53b5a761e050f8022b8-Abstract.html},
	abstract = {Deep neural networks (DNNs) are powerful black-box predictors that have achieved impressive performance on a wide variety of tasks. However, their accuracy comes at the cost of intelligibility: it is usually unclear how they make their decisions. This hinders their applicability to high stakes decision-making domains such as healthcare. We propose Neural Additive Models (NAMs) which combine some of the expressivity of DNNs with the inherent intelligibility of generalized additive models. NAMs learn a linear combination of neural networks that each attend to a single input feature. These networks are trained jointly and can learn arbitrarily complex relationships between their input feature and the output. Our experiments on regression and classification datasets show that NAMs are more accurate than widely used intelligible models such as logistic regression and shallow decision trees. They perform similarly to existing state-of-the-art generalized additive models in accuracy, but are more flexible because they are based on neural nets instead of boosted trees. To demonstrate this, we show how NAMs can be used for multitask learning on synthetic data and on the COMPAS recidivism data due to their composability, and demonstrate that the differentiability of NAMs allows them to train more complex interpretable models for COVID-19.},
	urldate = {2022-09-15},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Agarwal, Rishabh and Melnick, Levi and Frosst, Nicholas and Zhang, Xuezhou and Lengerich, Ben and Caruana, Rich and Hinton, Geoffrey E},
	year = {2021},
	pages = {4699--4711},
}

@inproceedings{naseer_intriguing_2021,
	title = {Intriguing {Properties} of {Vision} {Transformers}},
	volume = {34},
	url = {https://papers.nips.cc/paper/2021/hash/c404a5adbf90e09631678b13b05d9d7a-Abstract.html},
	abstract = {Vision transformers (ViT) have demonstrated impressive performance across numerous machine vision tasks. These models are based on multi-head self-attention mechanisms that can flexibly attend to a sequence of image patches to encode contextual cues. An important question is how such flexibility (in attending image-wide context conditioned on a given patch) can facilitate handling nuisances in natural images e.g., severe occlusions, domain shifts, spatial permutations, adversarial and natural perturbations. We systematically study this question via an extensive set of experiments encompassing three ViT families and provide comparisons with a high-performing convolutional neural network (CNN). We show and analyze the following intriguing properties of ViT: (a)Transformers are highly robust to severe occlusions, perturbations and domain shifts, e.g., retain as high as 60\% top-1 accuracy on ImageNet even after randomly occluding 80\% of the image content. (b)The robustness towards occlusions is not due to texture bias, instead we show that ViTs are significantly less biased towards local textures, compared to CNNs. When properly trained to encode shape-based features, ViTs demonstrate shape recognition capability comparable to that of human visual system, previously unmatched in the literature. (c)Using ViTs to encode shape representation leads to an interesting consequence of accurate semantic segmentation without pixel-level supervision. (d)Off-the-shelf features from a single ViT model can be combined to create a feature ensemble,  leading to high accuracy rates across a range of classification datasets in both traditional and few-shot learning paradigms.  We show effective features of ViTs are due to flexible and dynamic receptive fields possible via self-attention mechanisms. Our code will be publicly released.},
	urldate = {2022-09-15},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Naseer, Muhammad Muzammal and Ranasinghe, Kanchana and Khan, Salman H and Hayat, Munawar and Shahbaz Khan, Fahad and Yang, Ming-Hsuan},
	year = {2021},
	pages = {23296--23308},
}

@article{nakkiran_deep_2021,
	title = {Deep double descent: where bigger models and more data hurt},
	volume = {2021},
	issn = {1742-5468},
	shorttitle = {Deep double descent},
	url = {https://doi.org/10.1088/1742-5468/ac3a74},
	doi = {10.1088/1742-5468/ac3a74},
	abstract = {We show that a variety of modern deep learning tasks exhibit a ‘double-descent’ phenomenon where, as we increase model size, performance first gets worse and then gets better. Moreover, we show that double descent occurs not just as a function of model size, but also as a function of the number of training epochs. We unify the above phenomena by defining a new complexity measure we call the effective model complexity and conjecture a generalized double descent with respect to this measure. Furthermore, our notion of model complexity allows us to identify certain regimes where increasing (even quadrupling) the number of train samples actually hurts test performance.},
	language = {en},
	number = {12},
	urldate = {2022-08-31},
	journal = {Journal of Statistical Mechanics: Theory and Experiment},
	author = {Nakkiran, Preetum and Kaplun, Gal and Bansal, Yamini and Yang, Tristan and Barak, Boaz and Sutskever, Ilya},
	month = dec,
	year = {2021},
	note = {Publisher: IOP Publishing},
	pages = {124003},
}

@inproceedings{dabouei_supermix_2021,
	title = {{SuperMix}: {Supervising} the {Mixing} {Data} {Augmentation}},
	shorttitle = {{SuperMix}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Dabouei_SuperMix_Supervising_the_Mixing_Data_Augmentation_CVPR_2021_paper.html},
	language = {en},
	urldate = {2022-09-02},
	author = {Dabouei, Ali and Soleymani, Sobhan and Taherkhani, Fariborz and Nasrabadi, Nasser M.},
	year = {2021},
	pages = {13794--13803},
}

@inproceedings{muller_trivialaugment_2021,
	title = {{TrivialAugment}: {Tuning}-{Free} {Yet} {State}-of-the-{Art} {Data} {Augmentation}},
	shorttitle = {{TrivialAugment}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Muller_TrivialAugment_Tuning-Free_Yet_State-of-the-Art_Data_Augmentation_ICCV_2021_paper.html},
	language = {en},
	urldate = {2022-01-08},
	booktitle = {Proceedings of the {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision}},
	author = {Müller, Samuel G. and Hutter, Frank},
	year = {2021},
	pages = {774--782},
}

@inproceedings{kuchnik_efficient_2019,
	title = {Efficient {Augmentation} via {Data} {Subsampling}},
	url = {https://openreview.net/forum?id=Byxpfh0cFm},
	abstract = {Selectively augmenting difficult to classify points results in efficient training.},
	language = {en},
	urldate = {2022-08-26},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Kuchnik, Michael and Smith, Virginia},
	year = {2019},
}

@inproceedings{chen_more_2020,
	title = {More {Data} {Can} {Expand} {The} {Generalization} {Gap} {Between} {Adversarially} {Robust} and {Standard} {Models}},
	url = {https://proceedings.mlr.press/v119/chen20q.html},
	abstract = {Despite remarkable success in practice, modern machine learning models have been found to be susceptible to adversarial attacks that make human-imperceptible perturbations to the data, but result in serious and potentially dangerous prediction errors. To address this issue, practitioners often use adversarial training to learn models that are robust against such attacks at the cost of higher generalization error on unperturbed test sets. The conventional wisdom is that more training data should shrink the gap between the generalization error of adversarially-trained models and standard models. However, we study the training of robust classifiers for both Gaussian and Bernoulli models under \${\textbackslash}ell\_{\textbackslash}infty\$ attacks, and we prove that more data may actually increase this gap. Furthermore, our theoretical results identify if and when additional data will finally begin to shrink the gap. Lastly, we experimentally demonstrate that our results also hold for linear regression models, which may indicate that this phenomenon occurs more broadly.},
	language = {en},
	urldate = {2022-07-30},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Chen, Lin and Min, Yifei and Zhang, Mingrui and Karbasi, Amin},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {1670--1680},
}

@misc{chen_guided_2021,
	title = {Guided {Interpolation} for {Adversarial} {Training}},
	url = {http://arxiv.org/abs/2102.07327},
	doi = {10.48550/arXiv.2102.07327},
	abstract = {To enhance adversarial robustness, adversarial training learns deep neural networks on the adversarial variants generated by their natural data. However, as the training progresses, the training data becomes less and less attackable, undermining the robustness enhancement. A straightforward remedy is to incorporate more training data, but sometimes incurring an unaffordable cost. In this paper, to mitigate this issue, we propose the guided interpolation framework (GIF): in each epoch, the GIF employs the previous epoch's meta information to guide the data's interpolation. Compared with the vanilla mixup, the GIF can provide a higher ratio of attackable data, which is beneficial to the robustness enhancement; it meanwhile mitigates the model's linear behavior between classes, where the linear behavior is favorable to generalization but not to the robustness. As a result, the GIF encourages the model to predict invariantly in the cluster of each class. Experiments demonstrate that the GIF can indeed enhance adversarial robustness on various adversarial training methods and various datasets.},
	urldate = {2022-08-31},
	publisher = {arXiv},
	author = {Chen, Chen and Zhang, Jingfeng and Xu, Xilie and Hu, Tianlei and Niu, Gang and Chen, Gang and Sugiyama, Masashi},
	month = feb,
	year = {2021},
	note = {arXiv:2102.07327 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@inproceedings{rebuffi_data_2021,
	title = {Data {Augmentation} {Can} {Improve} {Robustness}},
	url = {https://openreview.net/forum?id=kgVJBBThdSZ},
	abstract = {We demonstrate that, contrary to previous findings, when combined with model weight averaging, data augmentation can significantly boost robust accuracy.},
	language = {en},
	urldate = {2021-12-02},
	booktitle = {Neural {Information} {Processing} {Systems}},
	author = {Rebuffi, Sylvestre-Alvise and Gowal, Sven and Calian, Dan Andrei and Stimberg, Florian and Wiles, Olivia and Mann, Timothy},
	month = may,
	year = {2021},
}

@inproceedings{carmon_unlabeled_2019,
	title = {Unlabeled {Data} {Improves} {Adversarial} {Robustness}},
	abstract = {We demonstrate, theoretically and empirically, that adversarial robustness can signiﬁcantly beneﬁt from semisupervised learning. Theoretically, we revisit the simple Gaussian model of Schmidt et al. [41] that shows a sample complexity gap between standard and robust classiﬁcation. We prove that unlabeled data bridges this gap: a simple semisupervised learning procedure (self-training) achieves high robust accuracy using the same number of labels required for achieving high standard accuracy. Empirically, we augment CIFAR-10 with 500K unlabeled images sourced from 80 Million Tiny Images and use robust self-training to outperform state-of-the-art robust accuracies by over 5 points in (i) `1 robustness against several strong attacks via adversarial training and (ii) certiﬁed `2 and `1 robustness via randomized smoothing. On SVHN, adding the dataset’s own extra training set with the labels removed provides gains of 4 to 10 points, within 1 point of the gain from using the extra labels.},
	language = {en},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Carmon, Yair and Raghunathan, Aditi and Schmidt, Ludwig and Duchi, John C and Liang, Percy S},
	year = {2019},
	pages = {12},
}

@article{chen_decision_2022,
	title = {Decision {Boundary}-aware {Data} {Augmentation} for {Adversarial} {Training}},
	issn = {1941-0018},
	doi = {10.1109/TDSC.2022.3165889},
	abstract = {Adversarial training (AT) is a typical method to learn adversarially robust deep neural networks via training on the adversarial variants generated by their natural examples. However, as training progresses, the training data becomes less attackable, which may undermine the enhancement of model robustness. A straightforward remedy is to incorporate more training data, but it may incur an unaffordable cost. To mitigate this issue, in this paper, we propose a deCisiOn bounDary-aware data Augmentation framework (CODA): in each epoch, the CODA directly employs the meta information of the previous epoch to guide the augmentation process and generate more data that are close to the decision boundary, i.e., attackable data. Compared with the vanilla mixup, our proposed CODA can provide a higher ratio of attackable data, which is beneficial to enhance model robustness; it meanwhile mitigates the model's linear behavior between classes, where the linear behavior is favorable to the standard training for generalization but not to the adversarial training for robustness. As a result, our proposed CODA encourages the model to predict invariantly in the cluster of each class. Experiments demonstrate that our proposed CODA can indeed enhance adversarial robustness across various adversarial training methods and multiple datasets.},
	journal = {IEEE Transactions on Dependable and Secure Computing},
	author = {Chen, Chen and Zhang, Jingfeng and Xu, Xilie and Lyu, Lingjuan and Chen, Chaochao and Hu, Tianlei and Chen, Gang},
	year = {2022},
	note = {Conference Name: IEEE Transactions on Dependable and Secure Computing},
	keywords = {Data models, Predictive models, Principal component analysis, Robustness, Standards, Training, Training data, adversarial robustness, data augmentation F},
	pages = {1--1},
}

@inproceedings{min_curious_2021,
	title = {The curious case of adversarially robust models: {More} data can help, double descend, or hurt generalization},
	shorttitle = {The curious case of adversarially robust models},
	url = {https://proceedings.mlr.press/v161/min21a.html},
	abstract = {Adversarial training has shown its ability in producing models that are robust to perturbations on the input data, but usually at the expense of a decrease in the standard accuracy. To mitigate this issue, it is commonly believed that more training data will eventually help such adversarially robust models generalize better on the benign/unperturbed test data. In this paper, however, we challenge this conventional belief and show that more training data can hurt the generalization of adversarially robust models in classification problems. We first investigate the Gaussian mixture classification with a linear loss and identify three regimes based on the strength of the adversary. In the weak adversary regime, more data improves the generalization of adversarially robust models. In the medium adversary regime, with more training data, the generalization loss exhibits a double descent curve, which implies the existence of an intermediate stage where more training data hurts the generalization. In the strong adversary regime, more data almost immediately causes the generalization error to increase. Then we analyze a two-dimensional classification problem with a 0-1 loss. We prove that more data always hurts generalization of adversarially trained models with large perturbations. Empirical studies confirm our theoretical results.},
	language = {en},
	urldate = {2022-08-12},
	booktitle = {Proceedings of the {Thirty}-{Seventh} {Conference} on {Uncertainty} in {Artificial} {Intelligence}},
	publisher = {PMLR},
	author = {Min, Yifei and Chen, Lin and Karbasi, Amin},
	month = dec,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {129--139},
}

@misc{chen_benign_2021,
	title = {Benign {Overfitting} in {Adversarially} {Robust} {Linear} {Classification}},
	url = {http://arxiv.org/abs/2112.15250},
	doi = {10.48550/arXiv.2112.15250},
	abstract = {"Benign overfitting", where classifiers memorize noisy training data yet still achieve a good generalization performance, has drawn great attention in the machine learning community. To explain this surprising phenomenon, a series of works have provided theoretical justification in over-parameterized linear regression, classification, and kernel methods. However, it is not clear if benign overfitting still occurs in the presence of adversarial examples, i.e., examples with tiny and intentional perturbations to fool the classifiers. In this paper, we show that benign overfitting indeed occurs in adversarial training, a principled approach to defend against adversarial examples. In detail, we prove the risk bounds of the adversarially trained linear classifier on the mixture of sub-Gaussian data under \${\textbackslash}ell\_p\$ adversarial perturbations. Our result suggests that under moderate perturbations, adversarially trained linear classifiers can achieve the near-optimal standard and adversarial risks, despite overfitting the noisy training data. Numerical experiments validate our theoretical findings.},
	urldate = {2022-08-12},
	publisher = {arXiv},
	author = {Chen, Jinghui and Cao, Yuan and Gu, Quanquan},
	month = dec,
	year = {2021},
	note = {arXiv:2112.15250 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
}

@misc{xu_wemix_2020,
	title = {{WeMix}: {How} to {Better} {Utilize} {Data} {Augmentation}},
	shorttitle = {{WeMix}},
	url = {http://arxiv.org/abs/2010.01267},
	doi = {10.48550/arXiv.2010.01267},
	abstract = {Data augmentation is a widely used training trick in deep learning to improve the network generalization ability. Despite many encouraging results, several recent studies did point out limitations of the conventional data augmentation scheme in certain scenarios, calling for a better theoretical understanding of data augmentation. In this work, we develop a comprehensive analysis that reveals pros and cons of data augmentation. The main limitation of data augmentation arises from the data bias, i.e. the augmented data distribution can be quite different from the original one. This data bias leads to a suboptimal performance of existing data augmentation methods. To this end, we develop two novel algorithms, termed "AugDrop" and "MixLoss", to correct the data bias in the data augmentation. Our theoretical analysis shows that both algorithms are guaranteed to improve the effect of data augmentation through the bias correction, which is further validated by our empirical studies. Finally, we propose a generic algorithm "WeMix" by combining AugDrop and MixLoss, whose effectiveness is observed from extensive empirical evaluations.},
	urldate = {2022-08-12},
	publisher = {arXiv},
	author = {Xu, Yi and Noy, Asaf and Lin, Ming and Qian, Qi and Li, Hao and Jin, Rong},
	month = oct,
	year = {2020},
	note = {arXiv:2010.01267 [cs, math, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
}

@article{harris_fmix_2021,
	title = {{FMix}: {Enhancing} {Mixed} {Sample} {Data} {Augmentation}},
	shorttitle = {{FMix}},
	url = {https://openreview.net/forum?id=oev4KdikGjy},
	abstract = {Mixed Sample Data Augmentation (MSDA) has received increasing attention in recent years, with many successful variants such as MixUp and CutMix. We analyse MSDA from an information theoretic...},
	language = {en},
	urldate = {2022-08-03},
	author = {Harris, Ethan and Marcu, Antonia and Painter, Matthew and Niranjan, Mahesan and Prugel-Bennett, Adam and Hare, Jonathon},
	month = mar,
	year = {2021},
}

@inproceedings{takahashi_ricap_2018,
	title = {{RICAP}: {Random} {Image} {Cropping} and {Patching} {Data} {Augmentation} for {Deep} {CNNs}},
	shorttitle = {{RICAP}},
	url = {https://proceedings.mlr.press/v95/takahashi18a.html},
	abstract = {Deep convolutional neural networks (CNNs) have demonstrated remarkable results in image recognition owing to their rich expression ability and numerous parameters. However, an excessive expression ability compared to the variety of training images often has a risk of overfitting. Data augmentation techniques have been proposed to address this problem as they enrich datasets by flipping, cropping, resizing, and color-translating images. They enable deep CNNs to achieve an impressive performance. In this study, we propose a new data augmentation technique called {\textbackslash}emph\{random image cropping and patching\} ({\textbackslash}emph\{RICAP\}), which randomly crops four images and patches them to construct a new training image. Hence, RICAP randomly picks up subsets of original features among the four images and discard others, enriching the variety of training images. Also, RICAP mixes the class labels of the four images and enjoys a benefit similar to label smoothing. We evaluated RICAP with current state-of-the-art CNNs (e.g., shake-shake regularization model) and achieved a new state-of-the-art test error of {\textbackslash}textcolor\{red\}\{2.232.232.23\%\} on CIFAR-10 among competitive data augmentation techniques such as cutout and mixup. We also confirmed that deep CNNs with RICAP achieved better results on CIFAR-100 and ImageNet than those results obtained by other techniques.},
	language = {en},
	urldate = {2022-08-03},
	booktitle = {Proceedings of {The} 10th {Asian} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Takahashi, Ryo and Matsubara, Takashi and Uehara, Kuniaki},
	month = nov,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {786--798},
}

@misc{mounsaveng_adversarial_2019,
	title = {Adversarial {Learning} of {General} {Transformations} for {Data} {Augmentation}},
	url = {http://arxiv.org/abs/1909.09801},
	doi = {10.48550/arXiv.1909.09801},
	abstract = {Data augmentation (DA) is fundamental against overfitting in large convolutional neural networks, especially with a limited training dataset. In images, DA is usually based on heuristic transformations, like geometric or color transformations. Instead of using predefined transformations, our work learns data augmentation directly from the training data by learning to transform images with an encoder-decoder architecture combined with a spatial transformer network. The transformed images still belong to the same class but are new, more complex samples for the classifier. Our experiments show that our approach is better than previous generative data augmentation methods, and comparable to predefined transformation methods when training an image classifier.},
	urldate = {2022-08-03},
	publisher = {arXiv},
	author = {Mounsaveng, Saypraseuth and Vazquez, David and Ayed, Ismail Ben and Pedersoli, Marco},
	month = sep,
	year = {2019},
	note = {arXiv:1909.09801 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
}

@article{agarwal_cognitive_2021,
	title = {Cognitive data augmentation for adversarial defense via pixel masking},
	volume = {146},
	issn = {0167-8655},
	url = {https://www.sciencedirect.com/science/article/pii/S0167865521000519},
	doi = {10.1016/j.patrec.2021.01.032},
	abstract = {The vulnerability of deep networks towards adversarial perturbations has motivated the researchers to design detection and mitigation algorithms. Inspired by the dropout and dropconnect algorithms as well as augmentation techniques, this paper presents “PixelMask” based data augmentation as an efficient method of reducing the sensitivity of convolutional neural networks (CNNs) towards adversarial attacks. In the proposed approach, samples generated using PixelMask are used as augmented data, which helps in learning robust CNN models. Experiments performed with multiple databases and architectures show that the proposed PixelMask based data augmentation approach improves the classification performance on adversarially perturbed images. The proposed defense mechanism can be applied effectively for different adversarial attacks and can easily be combined with any deep neural network (DNN) architecture to increase the robustness. The effectiveness of the proposed defense is demonstrated in gray-box, white-box, and unseen train-test attack scenarios. For example, on the CIFAR-10 database under adaptive attack (i.e., projected gradient descent), the proposed PixelMask is able to improve the recognition performance of CNN by at-least 22.69\%. Another advantage of the proposed algorithm over several existing defense algorithms is that the proposed defense either is able to retain or increase the classification accuracy of clean examples.},
	language = {en},
	urldate = {2022-08-03},
	journal = {Pattern Recognition Letters},
	author = {Agarwal, Akshay and Vatsa, Mayank and Singh, Richa and Ratha, Nalini},
	month = jun,
	year = {2021},
	keywords = {Adversarial attacks, Data augmentation, Deep learning},
	pages = {244--251},
}

@misc{eghbal-zadeh_data_2020,
	title = {On {Data} {Augmentation} and {Adversarial} {Risk}: {An} {Empirical} {Analysis}},
	shorttitle = {On {Data} {Augmentation} and {Adversarial} {Risk}},
	url = {http://arxiv.org/abs/2007.02650},
	doi = {10.48550/arXiv.2007.02650},
	abstract = {Data augmentation techniques have become standard practice in deep learning, as it has been shown to greatly improve the generalisation abilities of models. These techniques rely on different ideas such as invariance-preserving transformations (e.g, expert-defined augmentation), statistical heuristics (e.g, Mixup), and learning the data distribution (e.g, GANs). However, in the adversarial settings it remains unclear under what conditions such data augmentation methods reduce or even worsen the misclassification risk. In this paper, we therefore analyse the effect of different data augmentation techniques on the adversarial risk by three measures: (a) the well-known risk under adversarial attacks, (b) a new measure of prediction-change stress based on the Laplacian operator, and (c) the influence of training examples on prediction. The results of our empirical analysis disprove the hypothesis that an improvement in the classification performance induced by a data augmentation is always accompanied by an improvement in the risk under adversarial attack. Further, our results reveal that the augmented data has more influence than the non-augmented data, on the resulting models. Taken together, our results suggest that general-purpose data augmentations that do not take into the account the characteristics of the data and the task, must be applied with care.},
	urldate = {2022-08-03},
	publisher = {arXiv},
	author = {Eghbal-zadeh, Hamid and Koutini, Khaled and Primus, Paul and Haunschmid, Verena and Lewandowski, Michal and Zellinger, Werner and Moser, Bernhard A. and Widmer, Gerhard},
	month = jul,
	year = {2020},
	note = {arXiv:2007.02650 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{kim_local_2021,
	title = {Local {Augment}: {Utilizing} {Local} {Bias} {Property} of {Convolutional} {Neural} {Networks} for {Data} {Augmentation}},
	volume = {9},
	issn = {2169-3536},
	shorttitle = {Local {Augment}},
	doi = {10.1109/ACCESS.2021.3050758},
	abstract = {Data augmentation is an effective way to increase the diversity of existing training datasets that result in improved generalization ability of convolutional neural networks (CNNs). The augmentation effect is usually global for the existing methods i.e., a single augmentation effect is applied to the whole image, thus limiting the diversity of local characteristics in augmented images. Moreover, the global augmentation effect does not support the most fundamental behavior of CNNs i.e., they focus more on local features (local texture, tiny noise etc.) than global shapes. We refer to this behavior as local bias property. In this paper, we propose a new data augmentation method, called Local Augment (LA), which highly alters the local bias property so that it can generate significantly diverse augmented images and offers the network with a better augmentation effect. First, we select few local patches in an image, then apply different types of augmentation strategies to each local patch. This augmentation process collapses the global structure of the object but creates locally diversified samples, which helps the network to learn the local bias property in a more generalized way. As a result, it increases the generalizability and the prediction accuracy of the network. To verify the effectiveness of the proposed method, we perform comprehensive experiments on image classification with benchmark datasets, where the proposed method outperforms the sate-of-the-art data augmentation techniques on ImageNet and STL10 and shows competitive performance on CIFAR100.},
	journal = {IEEE Access},
	author = {Kim, Youmin and Uddin, A. F. M. Shahab and Bae, Sung-Ho},
	year = {2021},
	note = {Conference Name: IEEE Access},
	keywords = {Image classification, Interpolation, Licenses, Robustness, Shape, Task analysis, Training, Training data, data augmentation, local bias property, multiple augmentation effects, overfitting},
	pages = {15191--15199},
}

@inproceedings{zhao_maximum-entropy_2020,
	title = {Maximum-{Entropy} {Adversarial} {Data} {Augmentation} for {Improved} {Generalization} and {Robustness}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/a5bfc9e07964f8dddeb95fc584cd965d-Abstract.html},
	abstract = {Adversarial data augmentation has shown promise for training robust deep neural networks against unforeseen data shifts or corruptions. However, it is difficult to define heuristics to generate effective fictitious target distributions containing "hard" adversarial perturbations that are largely different from the source distribution. In this paper, we propose a novel and effective regularization term for adversarial data augmentation. We theoretically derive it from the information bottleneck principle, which results in a maximum-entropy formulation. Intuitively, this regularization term encourages perturbing the underlying source distribution to enlarge predictive uncertainty of the current model, so that the generated "hard" adversarial perturbations can improve the model robustness during training. Experimental results on three standard benchmarks demonstrate that our method consistently outperforms the existing state of the art by a statistically significant margin.},
	urldate = {2022-08-03},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Zhao, Long and Liu, Ting and Peng, Xi and Metaxas, Dimitris},
	year = {2020},
	pages = {14435--14447},
}

@misc{yu_pda_2020,
	title = {{PDA}: {Progressive} {Data} {Augmentation} for {General} {Robustness} of {Deep} {Neural} {Networks}},
	shorttitle = {{PDA}},
	url = {http://arxiv.org/abs/1909.04839},
	doi = {10.48550/arXiv.1909.04839},
	abstract = {Adversarial images are designed to mislead deep neural networks (DNNs), attracting great attention in recent years. Although several defense strategies achieved encouraging robustness against adversarial samples, most of them fail to improve the robustness on common corruptions such as noise, blur, and weather/digital effects (e.g. frost, pixelate). To address this problem, we propose a simple yet effective method, named Progressive Data Augmentation (PDA), which enables general robustness of DNNs by progressively injecting diverse adversarial noises during training. In other words, DNNs trained with PDA are able to obtain more robustness against both adversarial attacks as well as common corruptions than the recent state-of-the-art methods. We also find that PDA is more efficient than prior arts and able to prevent accuracy drop on clean samples without being attacked. Furthermore, we theoretically show that PDA can control the perturbation bound and guarantee better generalization ability than existing work. Extensive experiments on many benchmarks such as CIFAR-10, SVHN, and ImageNet demonstrate that PDA significantly outperforms its counterparts in various experimental setups.},
	urldate = {2022-08-03},
	publisher = {arXiv},
	author = {Yu, Hang and Liu, Aishan and Liu, Xianglong and Li, Gengchao and Luo, Ping and Cheng, Ran and Yang, Jichen and Zhang, Chongzhi},
	month = feb,
	year = {2020},
	note = {arXiv:1909.04839 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{lopes_improving_2019,
	title = {Improving {Robustness} {Without} {Sacrificing} {Accuracy} with {Patch} {Gaussian} {Augmentation}},
	url = {http://arxiv.org/abs/1906.02611},
	doi = {10.48550/arXiv.1906.02611},
	abstract = {Deploying machine learning systems in the real world requires both high accuracy on clean data and robustness to naturally occurring corruptions. While architectural advances have led to improved accuracy, building robust models remains challenging. Prior work has argued that there is an inherent trade-off between robustness and accuracy, which is exemplified by standard data augment techniques such as Cutout, which improves clean accuracy but not robustness, and additive Gaussian noise, which improves robustness but hurts accuracy. To overcome this trade-off, we introduce Patch Gaussian, a simple augmentation scheme that adds noise to randomly selected patches in an input image. Models trained with Patch Gaussian achieve state of the art on the CIFAR-10 and ImageNetCommon Corruptions benchmarks while also improving accuracy on clean data. We find that this augmentation leads to reduced sensitivity to high frequency noise(similar to Gaussian) while retaining the ability to take advantage of relevant high frequency information in the image (similar to Cutout). Finally, we show that Patch Gaussian can be used in conjunction with other regularization methods and data augmentation policies such as AutoAugment, and improves performance on the COCO object detection benchmark.},
	urldate = {2022-08-03},
	publisher = {arXiv},
	author = {Lopes, Raphael Gontijo and Yin, Dong and Poole, Ben and Gilmer, Justin and Cubuk, Ekin D.},
	month = jun,
	year = {2019},
	note = {arXiv:1906.02611 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{lemley_smart_2017,
	title = {Smart {Augmentation} {Learning} an {Optimal} {Data} {Augmentation} {Strategy}},
	volume = {5},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2017.2696121},
	abstract = {A recurring problem faced when training neural networks is that there is typically not enough data to maximize the generalization capability of deep neural networks. There are many techniques to address this, including data augmentation, dropout, and transfer learning. In this paper, we introduce an additional method, which we call smart augmentation and we show how to use it to increase the accuracy and reduce over fitting on a target network. Smart augmentation works, by creating a network that learns how to generate augmented data during the training process of a target network in a way that reduces that networks loss. This allows us to learn augmentations that minimize the error of that network. Smart augmentation has shown the potential to increase accuracy by demonstrably significant measures on all data sets tested. In addition, it has shown potential to achieve similar or improved performance levels with significantly smaller network sizes in a number of tested cases.},
	journal = {IEEE Access},
	author = {Lemley, Joseph and Bazrafkan, Shabab and Corcoran, Peter},
	year = {2017},
	note = {Conference Name: IEEE Access},
	keywords = {Artificial intelligence, Biological neural networks, Data models, Electronic mail, Informatics, Machine learning, Training, artificial neural networks, computer vision supervised learning, image databases, machine learning, machine learning algorithms},
	pages = {5858--5869},
}

@inproceedings{ratner_learning_2017,
	title = {Learning to {Compose} {Domain}-{Specific} {Transformations} for {Data} {Augmentation}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper/2017/hash/f26dab9bf6a137c3b6782e562794c2f2-Abstract.html},
	abstract = {Data augmentation is a ubiquitous technique for increasing the size of labeled training sets by leveraging task-specific data transformations that preserve class labels. While it is often easy for domain experts to specify individual transformations, constructing and tuning the more sophisticated compositions typically needed to achieve state-of-the-art results is a time-consuming manual task in practice. We propose a method for automating this process by learning a generative sequence model over user-specified transformation functions using a generative adversarial approach. Our method can make use of arbitrary, non-deterministic transformation functions, is robust to misspecified user input, and is trained on unlabeled data. The learned transformation model can then be used to perform data augmentation for any end discriminative model. In our experiments, we show the efficacy of our approach on both image and text datasets, achieving improvements of 4.0 accuracy points on CIFAR-10, 1.4 F1 points on the ACE relation extraction task, and 3.4 accuracy points when using domain-specific transformation operations on a medical imaging dataset as compared to standard heuristic augmentation approaches.},
	urldate = {2022-08-03},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Ratner, Alexander J and Ehrenberg, Henry and Hussain, Zeshan and Dunnmon, Jared and Ré, Christopher},
	year = {2017},
}

@inproceedings{fawzi_adaptive_2016,
	title = {Adaptive data augmentation for image classification},
	doi = {10.1109/ICIP.2016.7533048},
	abstract = {Data augmentation is the process of generating samples by transforming training data, with the target of improving the accuracy and robustness of classifiers. In this paper, we propose a new automatic and adaptive algorithm for choosing the transformations of the samples used in data augmentation. Specifically, for each sample, our main idea is to seek a small transformation that yields maximal classification loss on the transformed sample. We employ a trust-region optimization strategy, which consists of solving a sequence of linear programs. Our data augmentation scheme is then integrated into a Stochastic Gradient Descent algorithm for training deep neural networks. We perform experiments on two datasets, and show that that the proposed scheme outperforms random data augmentation algorithms in terms of accuracy and robustness, while yielding comparable or superior results with respect to existing selective sampling approaches.},
	booktitle = {2016 {IEEE} {International} {Conference} on {Image} {Processing} ({ICIP})},
	author = {Fawzi, Alhussein and Samulowitz, Horst and Turaga, Deepak and Frossard, Pascal},
	month = sep,
	year = {2016},
	note = {ISSN: 2381-8549},
	keywords = {Approximation algorithms, Data augmentation, Neural networks, Optimization, Robustness, Training, Training data, Transforms, image robustness, transformation invariance, trust-region optimization},
	pages = {3688--3692},
}

@inproceedings{wang_removing_2022,
	title = {Removing {Batch} {Normalization} {Boosts} {Adversarial} {Training}},
	url = {https://proceedings.mlr.press/v162/wang22ap.html},
	abstract = {Adversarial training (AT) defends deep neural networks against adversarial attacks. One challenge that limits its practical application is the performance degradation on clean samples. A major bottleneck identified by previous works is the widely used batch normalization (BN), which struggles to model the different statistics of clean and adversarial training samples in AT. Although the dominant approach is to extend BN to capture this mixture of distribution, we propose to completely eliminate this bottleneck by removing all BN layers in AT. Our normalizer-free robust training (NoFrost) method extends recent advances in normalizer-free networks to AT for its unexplored advantage on handling the mixture distribution challenge. We show that NoFrost achieves adversarial robustness with only a minor sacrifice on clean sample accuracy. On ImageNet with ResNet50, NoFrost achieves \$74.06\%\$ clean accuracy, which drops merely \$2.00\%\$ from standard training. In contrast, BN-based AT obtains \$59.28\%\$ clean accuracy, suffering a significant \$16.78\%\$ drop from standard training. In addition, NoFrost achieves a \$23.56\%\$ adversarial robustness against PGD attack, which improves the \$13.57\%\$ robustness in BN-based AT. We observe better model smoothness and larger decision margins from NoFrost, which make the models less sensitive to input perturbations and thus more robust. Moreover, when incorporating more data augmentations into NoFrost, it achieves comprehensive robustness against multiple distribution shifts. Code and pre-trained models are public at https://github.com/amazon-research/normalizer-free-robust-training.},
	language = {en},
	urldate = {2022-08-02},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Wang, Haotao and Zhang, Aston and Zheng, Shuai and Shi, Xingjian and Li, Mu and Wang, Zhangyang},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {23433--23445},
}

@misc{noauthor_kings_2022,
	title = {King's {Computational} {Research}, {Engineering} and {Technology} {Environment} ({CREATE})},
	url = {https://doi.org/10.18742/rnvf-m076},
	publisher = {King's College London},
	year = {2022},
}

@inproceedings{lee_smoothmix_2020,
	title = {{SmoothMix}: {A} {Simple} {Yet} {Effective} {Data} {Augmentation} to {Train} {Robust} {Classifiers}},
	shorttitle = {{SmoothMix}},
	url = {https://openaccess.thecvf.com/content_CVPRW_2020/html/w45/Lee_SmoothMix_A_Simple_Yet_Effective_Data_Augmentation_to_Train_Robust_CVPRW_2020_paper.html},
	urldate = {2022-07-31},
	author = {Lee, Jin-Ha and Zaheer, Muhammad Zaigham and Astrid, Marcella and Lee, Seung-Ik},
	year = {2020},
	pages = {756--757},
}

@inproceedings{hong_stylemix_2021,
	title = {{StyleMix}: {Separating} {Content} and {Style} for {Enhanced} {Data} {Augmentation}},
	shorttitle = {{StyleMix}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Hong_StyleMix_Separating_Content_and_Style_for_Enhanced_Data_Augmentation_CVPR_2021_paper.html},
	language = {en},
	urldate = {2022-07-31},
	author = {Hong, Minui and Choi, Jinwoo and Kim, Gunhee},
	year = {2021},
	pages = {14862--14870},
}

@inproceedings{xie_unsupervised_2020,
	title = {Unsupervised {Data} {Augmentation} for {Consistency} {Training}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/44feb0096faa8326192570788b38c1d1-Abstract.html},
	abstract = {Semi-supervised learning lately has shown much promise in improving deep learning models when labeled data is scarce. Common among recent approaches is the use of consistency training on a large amount of unlabeled data to constrain model predictions to be invariant to input noise. In this work, we present a new perspective on how to effectively noise unlabeled examples and argue that the quality of noising, specifically those produced by advanced data augmentation methods, plays a crucial role in semi-supervised learning. By substituting simple noising operations with advanced data augmentation methods such as RandAugment and back-translation, our method brings substantial improvements across six language and three vision tasks under the same consistency training framework. On the IMDb text classification dataset, with only 20 labeled examples, our method achieves an error rate of 4.20, outperforming the state-of-the-art model trained on 25,000 labeled examples. On a standard semi-supervised learning benchmark, CIFAR-10, our method outperforms all previous approaches and achieves an error rate of 5.43 with only 250 examples. Our method also combines well with transfer learning, e.g., when finetuning from BERT, and yields improvements in high-data regime, such as ImageNet, whether when there is only 10\% labeled data or when a full labeled set with 1.3M extra unlabeled examples is used. Code is available at https://github.com/google-research/uda.},
	urldate = {2022-07-30},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Xie, Qizhe and Dai, Zihang and Hovy, Eduard and Luong, Thang and Le, Quoc},
	year = {2020},
	pages = {6256--6268},
}

@inproceedings{zhang_defense_2019,
	title = {Defense {Against} {Adversarial} {Attacks} {Using} {Feature} {Scattering}-based {Adversarial} {Training}},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper/2019/hash/d8700cbd38cc9f30cecb34f0c195b137-Abstract.html},
	abstract = {We introduce a feature scattering-based adversarial training approach for improving model robustness against adversarial attacks.
Conventional adversarial training approaches leverage a supervised scheme (either targeted or non-targeted) in generating attacks for training, which typically suffer from issues such as label leaking as noted in recent works.
Differently, the proposed approach generates adversarial images for training through feature scattering in the latent space, which is unsupervised in nature and avoids label leaking. More importantly, this new approach generates perturbed images in a collaborative fashion, taking the inter-sample relationships into consideration. We conduct analysis on model robustness and demonstrate the effectiveness of the proposed approach  through extensively experiments on different datasets compared with state-of-the-art approaches.},
	urldate = {2022-07-30},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Zhang, Haichao and Wang, Jianyu},
	year = {2019},
}

@misc{erichson_noisymix_2022,
	title = {{NoisyMix}: {Boosting} {Model} {Robustness} to {Common} {Corruptions}},
	shorttitle = {{NoisyMix}},
	url = {http://arxiv.org/abs/2202.01263},
	doi = {10.48550/arXiv.2202.01263},
	abstract = {For many real-world applications, obtaining stable and robust statistical performance is more important than simply achieving state-of-the-art predictive test accuracy, and thus robustness of neural networks is an increasingly important topic. Relatedly, data augmentation schemes have been shown to improve robustness with respect to input perturbations and domain shifts. Motivated by this, we introduce NoisyMix, a novel training scheme that promotes stability as well as leverages noisy augmentations in input and feature space to improve both model robustness and in-domain accuracy. NoisyMix produces models that are consistently more robust and that provide well-calibrated estimates of class membership probabilities. We demonstrate the benefits of NoisyMix on a range of benchmark datasets, including ImageNet-C, ImageNet-R, and ImageNet-P. Moreover, we provide theory to understand implicit regularization and robustness of NoisyMix.},
	urldate = {2022-07-28},
	publisher = {arXiv},
	author = {Erichson, N. Benjamin and Lim, Soon Hoe and Xu, Winnie and Utrera, Francisco and Cao, Ziang and Mahoney, Michael W.},
	month = may,
	year = {2022},
	note = {arXiv:2202.01263 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{yu_understanding_2022,
	title = {Understanding {Robust} {Overfitting} of {Adversarial} {Training} and {Beyond}},
	url = {https://proceedings.mlr.press/v162/yu22b.html},
	abstract = {Robust overfitting widely exists in adversarial training of deep networks. The exact underlying reasons for this are still not completely understood. Here, we explore the causes of robust overfitting by comparing the data distribution of non-overfit (weak adversary) and overfitted (strong adversary) adversarial training, and observe that the distribution of the adversarial data generated by weak adversary mainly contain small-loss data. However, the adversarial data generated by strong adversary is more diversely distributed on the large-loss data and the small-loss data. Given these observations, we further designed data ablation adversarial training and identify that some small-loss data which are not worthy of the adversary strength cause robust overfitting in the strong adversary mode. To relieve this issue, we propose minimum loss constrained adversarial training (MLCAT): in a minibatch, we learn large-loss data as usual, and adopt additional measures to increase the loss of the small-loss data. Technically, MLCAT hinders data fitting when they become easy to learn to prevent robust overfitting; philosophically, MLCAT reflects the spirit of turning waste into treasure and making the best use of each adversarial data; algorithmically, we designed two realizations of MLCAT, and extensive experiments demonstrate that MLCAT can eliminate robust overfitting and further boost adversarial robustness.},
	language = {en},
	urldate = {2022-07-27},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Yu, Chaojian and Han, Bo and Shen, Li and Yu, Jun and Gong, Chen and Gong, Mingming and Liu, Tongliang},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {25595--25610},
}

@inproceedings{park_reliably_2021,
	address = {Montreal, QC, Canada},
	title = {Reliably fast adversarial training via latent adversarial perturbation},
	isbn = {978-1-66542-812-5},
	url = {https://ieeexplore.ieee.org/document/9710792/},
	doi = {10.1109/ICCV48922.2021.00766},
	abstract = {While multi-step adversarial training is widely popular as an effective defense method against strong adversarial attacks, its computational cost is notoriously expensive, compared to standard training. Several single-step adversarial training methods have been proposed to mitigate the abovementioned overhead cost; however, their performance is not sufficiently reliable depending on the optimization setting. To overcome such limitations, we deviate from the existing input-space-based adversarial training regime and propose a single-step latent adversarial training method (SLAT), which leverages the gradients of latent representation as the latent adversarial perturbation. We demonstrate that the {\textbackslash}ell \_1 norm of feature gradients is implicitly regularized through the adopted latent perturbation, thereby recovering local linearity and ensuring reliable performance, compared to the existing single-step adversarial training methods. Because latent perturbation is based on the gradients of the latent representations which can be obtained for free in the process of input gradients computation, the proposed method costs roughly the same time as the fast gradient sign method. Experiment results demonstrate that the proposed method, despite its structural simplicity, outperforms state-of-the-art accelerated adversarial training methods.},
	language = {en},
	urldate = {2022-07-27},
	booktitle = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Park, Geon Yeong and Wan Lee, Sang},
	month = oct,
	year = {2021},
	pages = {7738--7747},
}

@inproceedings{zhang_towards_2021,
	title = {Towards {Better} {Robust} {Generalization} with {Shift} {Consistency} {Regularization}},
	url = {https://proceedings.mlr.press/v139/zhang21p.html},
	abstract = {While adversarial training becomes one of the most promising defending approaches against adversarial attacks for deep neural networks, the conventional wisdom through robust optimization may usually not guarantee good generalization for robustness. Concerning with robust generalization over unseen adversarial data, this paper investigates adversarial training from a novel perspective of shift consistency in latent space. We argue that the poor robust generalization of adversarial training is owing to the significantly dispersed latent representations generated by training and test adversarial data, as the adversarial perturbations push the latent features of natural examples in the same class towards diverse directions. This is underpinned by the theoretical analysis of the robust generalization gap, which is upper-bounded by the standard one over the natural data and a term of feature inconsistent shift caused by adversarial perturbation \{–\} a measure of latent dispersion. Towards better robust generalization, we propose a new regularization method \{–\} shift consistency regularization (SCR) \{–\} to steer the same-class latent features of both natural and adversarial data into a common direction during adversarial training. The effectiveness of SCR in adversarial training is evaluated through extensive experiments over different datasets, such as CIFAR-10, CIFAR-100, and SVHN, against several competitive methods.},
	language = {en},
	urldate = {2022-07-27},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Zhang, Shufei and Qian, Zhuang and Huang, Kaizhu and Wang, Qiufeng and Zhang, Rui and Yi, Xinping},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {12524--12534},
}

@article{montavon_explaining_2017,
	title = {Explaining nonlinear classification decisions with deep {Taylor} decomposition},
	volume = {65},
	issn = {0031-3203},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320316303582},
	doi = {10.1016/j.patcog.2016.11.008},
	abstract = {Nonlinear methods such as Deep Neural Networks (DNNs) are the gold standard for various challenging machine learning problems such as image recognition. Although these methods perform impressively well, they have a significant disadvantage, the lack of transparency, limiting the interpretability of the solution and thus the scope of application in practice. Especially DNNs act as black boxes due to their multilayer nonlinear structure. In this paper we introduce a novel methodology for interpreting generic multilayer neural networks by decomposing the network classification decision into contributions of its input elements. Although our focus is on image classification, the method is applicable to a broad set of input data, learning tasks and network architectures. Our method called deep Taylor decomposition efficiently utilizes the structure of the network by backpropagating the explanations from the output to the input layer. We evaluate the proposed method empirically on the MNIST and ILSVRC data sets.},
	language = {en},
	urldate = {2022-07-26},
	journal = {Pattern Recognition},
	author = {Montavon, Grégoire and Lapuschkin, Sebastian and Binder, Alexander and Samek, Wojciech and Müller, Klaus-Robert},
	month = may,
	year = {2017},
	keywords = {Deep neural networks, Heatmapping, Image recognition, Relevance propagation, Taylor decomposition},
	pages = {211--222},
}

@article{le_tiny_2015,
	title = {Tiny imagenet visual recognition challenge},
	volume = {7},
	number = {7},
	journal = {CS 231N},
	author = {Le, Ya and Yang, Xuan},
	year = {2015},
	pages = {3},
}

@misc{kim_torchattacks_2021,
	title = {Torchattacks: {A} {PyTorch} {Repository} for {Adversarial} {Attacks}},
	shorttitle = {Torchattacks},
	url = {http://arxiv.org/abs/2010.01950},
	doi = {10.48550/arXiv.2010.01950},
	abstract = {Torchattacks is a PyTorch library that contains adversarial attacks to generate adversarial examples and to verify the robustness of deep learning models. The code can be found at https://github.com/Harry24k/adversarial-attacks-pytorch.},
	urldate = {2022-07-09},
	publisher = {arXiv},
	author = {Kim, Hoki},
	month = feb,
	year = {2021},
	note = {arXiv:2010.01950 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@article{chen_efficient_2022,
	title = {Efficient {Robust} {Training} via {Backward} {Smoothing}},
	volume = {36},
	copyright = {Copyright (c) 2022 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/20571},
	doi = {10.1609/aaai.v36i6.20571},
	abstract = {Adversarial training is so far the most effective strategy in defending against adversarial examples. However, it suffers from high computational costs due to the iterative adversarial attacks in each training step. Recent studies show that it is possible to achieve fast Adversarial Training by performing a single-step attack with random initialization. However, such an approach still lags behind state-of-the-art adversarial training algorithms on both stability and model robustness. In this work, we develop a new understanding towards Fast Adversarial Training, by viewing random initialization as performing randomized smoothing for better optimization of the inner maximization problem. Following this new perspective, we also propose a new initialization strategy, backward smoothing, to further improve the stability and model robustness over single-step robust training methods.
 Experiments on multiple benchmarks demonstrate that our method achieves similar model robustness as the original TRADES method while using much less training time ({\textasciitilde}3x improvement with the same training schedule).},
	language = {en},
	number = {6},
	urldate = {2022-07-06},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Chen, Jinghui and Cheng, Yu and Gan, Zhe and Gu, Quanquan and Liu, Jingjing},
	month = jun,
	year = {2022},
	note = {Number: 6},
	keywords = {Machine Learning (ML)},
	pages = {6222--6230},
}

@article{mandery_unifying_2016,
	title = {Unifying {Representations} and {Large}-{Scale} {Whole}-{Body} {Motion} {Databases} for {Studying} {Human} {Motion}},
	volume = {32},
	issn = {1941-0468},
	doi = {10.1109/TRO.2016.2572685},
	abstract = {Large-scale human motion databases are key for research questions ranging from human motion analysis and synthesis, biomechanics of human motion, data-driven learning of motion primitives, and rehabilitation robotics to the design of humanoid robots and wearable robots such as exoskeletons. In this paper we present a large-scale database of whole-body human motion with methods and tools, which allows a unifying representation of captured human motion, and efficient search in the database, as well as the transfer of subject-specific motions to robots with different embodiments. To this end, captured subject-specific motion is normalized regarding the subject's height and weight by using a reference kinematics and dynamics model of the human body, the master motor map (MMM). In contrast with previous approaches and human motion databases, the motion data in our database consider not only the motions of the human subject but the position and motion of objects with which the subject is interacting as well. In addition to the description of the MMM reference model, we present procedures and techniques for the systematic recording, labeling, and organization of human motion capture data, object motions as well as the subject–object relations. To allow efficient search for certain motion types in the database, motion recordings are manually annotated with motion description tags organized in a tree structure. We demonstrate the transfer of human motion to humanoid robots and provide several examples of motion analysis using the database.},
	number = {4},
	journal = {IEEE Transactions on Robotics},
	author = {Mandery, Christian and Terlemez, Ömer and Do, Martin and Vahrenkamp, Nikolaus and Asfour, Tamim},
	month = aug,
	year = {2016},
	note = {Conference Name: IEEE Transactions on Robotics},
	keywords = {Animation, Data models, Databases, Humanoid robots, Legged locomotion, Motion measurement, models of the human body, whole-body human motion databases},
	pages = {796--809},
}

@inproceedings{cubuk_autoaugment_2019,
	address = {Long Beach, CA, USA},
	title = {{AutoAugment}: {Learning} {Augmentation} {Strategies} {From} {Data}},
	isbn = {978-1-72813-293-8},
	shorttitle = {{AutoAugment}},
	url = {https://ieeexplore.ieee.org/document/8953317/},
	doi = {10.1109/CVPR.2019.00020},
	abstract = {Data augmentation is an effective technique for improving the accuracy of modern image classiﬁers. However, current data augmentation implementations are manually designed. In this paper, we describe a simple procedure called AutoAugment to automatically search for improved data augmentation policies. In our implementation, we have designed a search space where a policy consists of many subpolicies, one of which is randomly chosen for each image in each mini-batch. A sub-policy consists of two operations, each operation being an image processing function such as translation, rotation, or shearing, and the probabilities and magnitudes with which the functions are applied. We use a search algorithm to ﬁnd the best policy such that the neural network yields the highest validation accuracy on a target dataset. Our method achieves state-of-the-art accuracy on CIFAR-10, CIFAR-100, SVHN, and ImageNet (without additional data). On ImageNet, we attain a Top-1 accuracy of 83.5\% which is 0.4\% better than the previous record of 83.1\%. On CIFAR-10, we achieve an error rate of 1.5\%, which is 0.6\% better than the previous state-of-theart. Augmentation policies we ﬁnd are transferable between datasets. The policy learned on ImageNet transfers well to achieve signiﬁcant improvements on other datasets, such as Oxford Flowers, Caltech-101, Oxford-IIT Pets, FGVC Aircraft, and Stanford Cars.},
	language = {en},
	urldate = {2022-06-09},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Cubuk, Ekin D. and Zoph, Barret and Mane, Dandelion and Vasudevan, Vijay and Le, Quoc V.},
	month = jun,
	year = {2019},
	pages = {113--123},
}

@inproceedings{bs_regularizer_2019,
	address = {Long Beach, CA, USA},
	title = {Regularizer to {Mitigate} {Gradient} {Masking} {Effect} {During} {Single}-{Step} {Adversarial} {Training}},
	isbn = {978-1-72812-506-0},
	url = {https://ieeexplore.ieee.org/document/9025502/},
	doi = {10.1109/CVPRW.2019.00014},
	abstract = {Neural networks are susceptible to adversarial samples: samples with imperceptible noise, crafted to manipulate network’s prediction. In order to learn robust models, a training procedure, called Adversarial Training has been introduced. During adversarial training, models are trained with mini-batch containing adversarial samples. In order to scale adversarial training for large datasets and networks, fast and simple methods (e.g., FGSM:Fast Gradient Sign Method) of generating adversarial samples are used while training. It has been shown that models trained using single-step adversarial training methods (i.e., adversarial samples generated using non-iterative methods such as FGSM) are not robust, instead they learn to generate weaker adversaries by masking the gradients. In this work, we propose a regularization term in the training loss, to mitigate the effect of gradient masking during single-step adversarial training. The proposed regularization term causes training loss to increase when the distance between logits (i.e., pre-softmax output of a classiﬁer) for FGSM and R-FGSM (small random noise is added to the clean sample before computing its FGSM sample) adversaries of a clean sample becomes large. The proposed single-step adversarial training is faster than computationally expensive state-of-the-art PGD adversarial training method, and also achieves on par results.},
	language = {en},
	urldate = {2022-06-07},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops} ({CVPRW})},
	publisher = {IEEE},
	author = {B.S., Vivek and Baburaj, Arya and Babu, R. Venkatesh},
	month = jun,
	year = {2019},
	pages = {66--73},
}

@inproceedings{sriramanan_guided_2020,
	title = {Guided {Adversarial} {Attack} for {Evaluating} and {Enhancing} {Adversarial} {Defenses}},
	volume = {33},
	url = {https://papers.nips.cc/paper/2020/hash/ea3ed20b6b101a09085ef09c97da1597-Abstract.html},
	abstract = {Advances in the development of adversarial attacks have been fundamental to the progress of adversarial defense research. Efficient and effective attacks are crucial for reliable evaluation of defenses, and also for developing robust models. Adversarial attacks are often generated by maximizing standard losses such as the cross-entropy loss or maximum-margin loss within a constraint set using Projected Gradient Descent (PGD). In this work, we introduce a relaxation term to the standard loss, that finds more suitable gradient-directions, increases attack efficacy and leads to more efficient adversarial training. We propose Guided Adversarial Margin Attack (GAMA), which utilizes function mapping of the clean image to guide the generation of adversaries, thereby resulting in stronger attacks. We evaluate our attack against multiple defenses and show improved performance when compared to existing attacks. Further, we propose Guided Adversarial Training (GAT), which achieves state-of-the-art performance amongst single-step defenses by utilizing the proposed relaxation term for both attack generation and training.},
	urldate = {2022-06-07},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Sriramanan, Gaurang and Addepalli, Sravanti and Baburaj, Arya and R, Venkatesh Babu},
	year = {2020},
	pages = {20297--20308},
}

@inproceedings{chen_sparsity_2022,
	title = {Sparsity {Winning} {Twice}: {Better} {Robust} {Generalization} from {More} {Efficient} {Training}},
	shorttitle = {Sparsity {Winning} {Twice}},
	url = {https://openreview.net/forum?id=SYuJXrXq8tw},
	abstract = {Recent studies demonstrate the deep networks, even robustified by the state-of-the-art adversarial training (AT), still suffer from large robust generalization gaps, in addition to the much more...},
	language = {en},
	urldate = {2022-04-06},
	author = {Chen, Tianlong and Zhang, Zhenyu and Wang, Pengjun and Balachandra, Santosh and Ma, Haoyu and Wang, Zehao and Wang, Zhangyang},
	year = {2022},
}

@techreport{rebuffi_fixing_2021,
	title = {Fixing {Data} {Augmentation} to {Improve} {Adversarial} {Robustness}},
	url = {http://arxiv.org/abs/2103.01946},
	abstract = {Adversarial training suffers from robust overfitting, a phenomenon where the robust test accuracy starts to decrease during training. In this paper, we focus on both heuristics-driven and data-driven augmentations as a means to reduce robust overfitting. First, we demonstrate that, contrary to previous findings, when combined with model weight averaging, data augmentation can significantly boost robust accuracy. Second, we explore how state-of-the-art generative models can be leveraged to artificially increase the size of the training set and further improve adversarial robustness. Finally, we evaluate our approach on CIFAR-10 against \${\textbackslash}ell\_{\textbackslash}infty\$ and \${\textbackslash}ell\_2\$ norm-bounded perturbations of size \${\textbackslash}epsilon = 8/255\$ and \${\textbackslash}epsilon = 128/255\$, respectively. We show large absolute improvements of +7.06\% and +5.88\% in robust accuracy compared to previous state-of-the-art methods. In particular, against \${\textbackslash}ell\_{\textbackslash}infty\$ norm-bounded perturbations of size \${\textbackslash}epsilon = 8/255\$, our model reaches 64.20\% robust accuracy without using any external data, beating most prior works that use external data.},
	number = {arXiv:2103.01946},
	urldate = {2022-06-05},
	institution = {arXiv},
	author = {Rebuffi, Sylvestre-Alvise and Gowal, Sven and Calian, Dan A. and Stimberg, Florian and Wiles, Olivia and Mann, Timothy},
	month = oct,
	year = {2021},
	doi = {10.48550/arXiv.2103.01946},
	note = {arXiv:2103.01946 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@inproceedings{dong_boosting_2018,
	address = {Salt Lake City, UT},
	title = {Boosting {Adversarial} {Attacks} with {Momentum}},
	isbn = {978-1-5386-6420-9},
	url = {https://ieeexplore.ieee.org/document/8579055/},
	doi = {10.1109/CVPR.2018.00957},
	abstract = {Deep neural networks are vulnerable to adversarial examples, which poses security concerns on these algorithms due to the potentially severe consequences. Adversarial attacks serve as an important surrogate to evaluate the robustness of deep learning models before they are deployed. However, most of existing adversarial attacks can only fool a black-box model with a low success rate. To address this issue, we propose a broad class of momentum-based iterative algorithms to boost adversarial attacks. By integrating the momentum term into the iterative process for attacks, our methods can stabilize update directions and escape from poor local maxima during the iterations, resulting in more transferable adversarial examples. To further improve the success rates for black-box attacks, we apply momentum iterative algorithms to an ensemble of models, and show that the adversarially trained models with a strong defense ability are also vulnerable to our black-box attacks. We hope that the proposed methods will serve as a benchmark for evaluating the robustness of various deep models and defense methods. With this method, we won the ﬁrst places in NIPS 2017 Non-targeted Adversarial Attack and Targeted Adversarial Attack competitions.},
	language = {en},
	urldate = {2022-05-30},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Dong, Yinpeng and Liao, Fangzhou and Pang, Tianyu and Su, Hang and Zhu, Jun and Hu, Xiaolin and Li, Jianguo},
	month = jun,
	year = {2018},
	pages = {9185--9193},
}

@techreport{yao_taking_2019,
	title = {Taking {Human} out of {Learning} {Applications}: {A} {Survey} on {Automated} {Machine} {Learning}},
	shorttitle = {Taking {Human} out of {Learning} {Applications}},
	url = {http://arxiv.org/abs/1810.13306},
	abstract = {Machine learning techniques have deeply rooted in our everyday life. However, since it is knowledge- and labor-intensive to pursue good learning performance, human experts are heavily involved in every aspect of machine learning. In order to make machine learning techniques easier to apply and reduce the demand for experienced human experts, automated machine learning (AutoML) has emerged as a hot topic with both industrial and academic interest. In this paper, we provide an up to date survey on AutoML. First, we introduce and define the AutoML problem, with inspiration from both realms of automation and machine learning. Then, we propose a general AutoML framework that not only covers most existing approaches to date but also can guide the design for new methods. Subsequently, we categorize and review the existing works from two aspects, i.e., the problem setup and the employed techniques. Finally, we provide a detailed analysis of AutoML approaches and explain the reasons underneath their successful applications. We hope this survey can serve as not only an insightful guideline for AutoML beginners but also an inspiration for future research.},
	number = {arXiv:1810.13306},
	urldate = {2022-05-26},
	institution = {arXiv},
	author = {Yao, Quanming and Wang, Mengshuo and Chen, Yuqiang and Dai, Wenyuan and Li, Yu-Feng and Tu, Wei-Wei and Yang, Qiang and Yu, Yang},
	month = dec,
	year = {2019},
	doi = {10.48550/arXiv.1810.13306},
	note = {arXiv:1810.13306 [cs, stat]
type: article},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@techreport{bischl_hyperparameter_2021,
	title = {Hyperparameter {Optimization}: {Foundations}, {Algorithms}, {Best} {Practices} and {Open} {Challenges}},
	shorttitle = {Hyperparameter {Optimization}},
	url = {http://arxiv.org/abs/2107.05847},
	abstract = {Most machine learning algorithms are configured by one or several hyperparameters that must be carefully chosen and often considerably impact performance. To avoid a time consuming and unreproducible manual trial-and-error process to find well-performing hyperparameter configurations, various automatic hyperparameter optimization (HPO) methods, e.g., based on resampling error estimation for supervised machine learning, can be employed. After introducing HPO from a general perspective, this paper reviews important HPO methods such as grid or random search, evolutionary algorithms, Bayesian optimization, Hyperband and racing. It gives practical recommendations regarding important choices to be made when conducting HPO, including the HPO algorithms themselves, performance evaluation, how to combine HPO with ML pipelines, runtime improvements, and parallelization. This work is accompanied by an appendix that contains information on specific software packages in R and Python, as well as information and recommended hyperparameter search spaces for specific learning algorithms. We also provide notebooks that demonstrate concepts from this work as supplementary files.},
	number = {arXiv:2107.05847},
	urldate = {2022-05-26},
	institution = {arXiv},
	author = {Bischl, Bernd and Binder, Martin and Lang, Michel and Pielok, Tobias and Richter, Jakob and Coors, Stefan and Thomas, Janek and Ullmann, Theresa and Becker, Marc and Boulesteix, Anne-Laure and Deng, Difan and Lindauer, Marius},
	month = nov,
	year = {2021},
	doi = {10.48550/arXiv.2107.05847},
	note = {arXiv:2107.05847 [cs, stat]
type: article},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{elsken_neural_2019,
	title = {Neural architecture search: a survey},
	volume = {20},
	issn = {1532-4435},
	shorttitle = {Neural architecture search},
	abstract = {Deep Learning has enabled remarkable progress over the last years on a variety of tasks, such as image recognition, speech recognition, and machine translation. One crucial aspect for this progress are novel neural architectures. Currently employed architectures have mostly been developed manually by human experts, which is a time-consuming and error-prone process. Because of this, there is growing interest in automated neural architecture search methods. We provide an overview of existing work in this field of research and categorize them according to three dimensions: search space, search strategy, and performance estimation strategy.},
	number = {1},
	journal = {The Journal of Machine Learning Research},
	author = {Elsken, Thomas and Metzen, Jan Hendrik and Hutter, Frank},
	month = jan,
	year = {2019},
	keywords = {autoDL, autoML, neural architecture search, performance estimation strategy, search space design, search strategy},
	pages = {1997--2017},
}

@incollection{feragen_enabling_2021,
	address = {Cham},
	title = {Enabling {Data} {Diversity}: {Efficient} {Automatic} {Augmentation} via {Regularized} {Adversarial} {Training}},
	volume = {12729},
	isbn = {978-3-030-78190-3 978-3-030-78191-0},
	shorttitle = {Enabling {Data} {Diversity}},
	url = {https://link.springer.com/10.1007/978-3-030-78191-0_7},
	abstract = {Data augmentation has proved extremely useful by increasing training data variance to alleviate overﬁtting and improve deep neural networks’ generalization performance. In medical image analysis, a welldesigned augmentation policy usually requires much expert knowledge and is diﬃcult to generalize to multiple tasks due to the vast discrepancies among pixel intensities, image appearances, and object shapes in diﬀerent medical tasks. To automate medical data augmentation, we propose a regularized adversarial training framework via two min-max objectives and three diﬀerentiable augmentation models covering aﬃne transformation, deformation, and appearance changes. Our method is more automatic and eﬃcient than previous automatic augmentation methods, which still rely on pre-deﬁned operations with human-speciﬁed ranges and costly bi-level optimization. Extensive experiments demonstrated that our approach, with less training overhead, achieves superior performance over state-of-the-art auto-augmentation methods on both tasks of 2D skin cancer classiﬁcation and 3D organs-at-risk segmentation.},
	language = {en},
	urldate = {2022-05-26},
	booktitle = {Information {Processing} in {Medical} {Imaging}},
	publisher = {Springer International Publishing},
	author = {Gao, Yunhe and Tang, Zhiqiang and Zhou, Mu and Metaxas, Dimitris},
	editor = {Feragen, Aasa and Sommer, Stefan and Schnabel, Julia and Nielsen, Mads},
	year = {2021},
	doi = {10.1007/978-3-030-78191-0_7},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {85--97},
}

@inproceedings{peng_jointly_2018,
	address = {Salt Lake City, UT},
	title = {Jointly {Optimize} {Data} {Augmentation} and {Network} {Training}: {Adversarial} {Data} {Augmentation} in {Human} {Pose} {Estimation}},
	isbn = {978-1-5386-6420-9},
	shorttitle = {Jointly {Optimize} {Data} {Augmentation} and {Network} {Training}},
	url = {https://ieeexplore.ieee.org/document/8578335/},
	doi = {10.1109/CVPR.2018.00237},
	language = {en},
	urldate = {2022-05-26},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Peng, Xi and Tang, Zhiqiang and Yang, Fei and Feris, Rogerio S. and Metaxas, Dimitris},
	month = jun,
	year = {2018},
	pages = {2226--2234},
}

@inproceedings{zoph_learning_2018,
	title = {Learning {Transferable} {Architectures} for {Scalable} {Image} {Recognition}},
	url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Zoph_Learning_Transferable_Architectures_CVPR_2018_paper.html},
	urldate = {2022-05-25},
	author = {Zoph, Barret and Vasudevan, Vijay and Shlens, Jonathon and Le, Quoc V.},
	year = {2018},
	pages = {8697--8710},
}

@article{tkach_online_2017,
	title = {Online generative model personalization for hand tracking},
	volume = {36},
	issn = {0730-0301},
	url = {https://doi.org/10.1145/3130800.3130830},
	doi = {10.1145/3130800.3130830},
	abstract = {We present a new algorithm for real-time hand tracking on commodity depth-sensing devices. Our method does not require a user-specific calibration session, but rather learns the geometry as the user performs live in front of the camera, thus enabling seamless virtual interaction at the consumer level. The key novelty in our approach is an online optimization algorithm that jointly estimates pose and shape in each frame, and determines the uncertainty in such estimates. This knowledge allows the algorithm to integrate per-frame estimates over time, and build a personalized geometric model of the captured user. Our approach can easily be integrated in state-of-the-art continuous generative motion tracking software. We provide a detailed evaluation that shows how our approach achieves accurate motion tracking for real-time applications, while significantly simplifying the workflow of accurate hand performance capture. We also provide quantitative evaluation datasets at http://gfx.uvic.ca/datasets/handy},
	number = {6},
	urldate = {2022-04-19},
	journal = {ACM Transactions on Graphics},
	author = {Tkach, Anastasia and Tagliasacchi, Andrea and Remelli, Edoardo and Pauly, Mark and Fitzgibbon, Andrew},
	month = nov,
	year = {2017},
	keywords = {articulated registration, generative tracking, motion capture, real-time calibration, real-time hand tracking},
	pages = {243:1--243:11},
}

@article{glauser_deformation_2019,
	title = {Deformation {Capture} via {Soft} and {Stretchable} {Sensor} {Arrays}},
	volume = {38},
	issn = {0730-0301},
	url = {https://doi.org/10.1145/3311972},
	doi = {10.1145/3311972},
	abstract = {We propose a hardware and software pipeline to fabricate flexible wearable sensors and use them to capture deformations without line-of-sight. Our first contribution is a low-cost fabrication pipeline to embed multiple aligned conductive layers with complex geometries into silicone compounds. Overlapping conductive areas from separate layers form local capacitors that measure dense area changes. Contrary to existing fabrication methods, the proposed technique only requires hardware that is readily available in modern fablabs. While area measurements alone are not enough to reconstruct the full 3D deformation of a surface, they become sufficient when paired with a data-driven prior. A novel semi-automatic tracking algorithm, based on an elastic surface geometry deformation, allows us to capture ground-truth data with an optical mocap system, even under heavy occlusions or partially unobservable markers. The resulting dataset is used to train a regressor based on deep neural networks, directly mapping the area readings to global positions of surface vertices. We demonstrate the flexibility and accuracy of the proposed hardware and software in a series of controlled experiments and design a prototype of wearable wrist, elbow, and biceps sensors, which do not require line-of-sight and can be worn below regular clothing.},
	number = {2},
	urldate = {2022-04-19},
	journal = {ACM Transactions on Graphics},
	author = {Glauser, Oliver and Panozzo, Daniele and Hilliges, Otmar and Sorkine-Hornung, Olga},
	month = mar,
	year = {2019},
	keywords = {Deformation capture, capacitive, sensor array, stretchable},
	pages = {16:1--16:16},
}

@article{glauser_interactive_2019,
	title = {Interactive hand pose estimation using a stretch-sensing soft glove},
	volume = {38},
	issn = {0730-0301},
	url = {https://doi.org/10.1145/3306346.3322957},
	doi = {10.1145/3306346.3322957},
	abstract = {We propose a stretch-sensing soft glove to interactively capture hand poses with high accuracy and without requiring an external optical setup. We demonstrate how our device can be fabricated and calibrated at low cost, using simple tools available in most fabrication labs. To reconstruct the pose from the capacitive sensors embedded in the glove, we propose a deep network architecture that exploits the spatial layout of the sensor itself. The network is trained only once, using an inexpensive off-the-shelf hand pose reconstruction system to gather the training data. The per-user calibration is then performed on-the-fly using only the glove. The glove's capabilities are demonstrated in a series of ablative experiments, exploring different models and calibration methods. Comparing against commercial data gloves, we achieve a 35\% improvement in reconstruction accuracy.},
	number = {4},
	urldate = {2022-04-19},
	journal = {ACM Transactions on Graphics},
	author = {Glauser, Oliver and Wu, Shihao and Panozzo, Daniele and Hilliges, Otmar and Sorkine-Hornung, Olga},
	month = jul,
	year = {2019},
	keywords = {data glove, hand tracking, sensor array, stretch-sensing},
	pages = {41:1--41:15},
}

@article{chen_survey_2020,
	title = {A {Survey} on {Hand} {Pose} {Estimation} with {Wearable} {Sensors} and {Computer}-{Vision}-{Based} {Methods}},
	volume = {20},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/20/4/1074},
	doi = {10.3390/s20041074},
	abstract = {Real-time sensing and modeling of the human body, especially the hands, is an important research endeavor for various applicative purposes such as in natural human computer interactions. Hand pose estimation is a big academic and technical challenge due to the complex structure and dexterous movement of human hands. Boosted by advancements from both hardware and artificial intelligence, various prototypes of data gloves and computer-vision-based methods have been proposed for accurate and rapid hand pose estimation in recent years. However, existing reviews either focused on data gloves or on vision methods or were even based on a particular type of camera, such as the depth camera. The purpose of this survey is to conduct a comprehensive and timely review of recent research advances in sensor-based hand pose estimation, including wearable and vision-based solutions. Hand kinematic models are firstly discussed. An in-depth review is conducted on data gloves and vision-based sensor systems with corresponding modeling methods. Particularly, this review also discusses deep-learning-based methods, which are very promising in hand pose estimation. Moreover, the advantages and drawbacks of the current hand gesture estimation methods, the applicative scope, and related challenges are also discussed.},
	language = {en},
	number = {4},
	urldate = {2022-04-19},
	journal = {Sensors},
	author = {Chen, Weiya and Yu, Chenchen and Tu, Chenyu and Lyu, Zehua and Tang, Jing and Ou, Shiqi and Fu, Yan and Xue, Zhidong},
	month = jan,
	year = {2020},
	note = {Number: 4
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {computer vision, data gloves, deep learning, hand pose estimation, human–computer interaction, wearable devices},
	pages = {1074},
}

@inproceedings{yuan_bighand22m_2017,
	address = {Honolulu, HI},
	title = {{BigHand2}.{2M} {Benchmark}: {Hand} {Pose} {Dataset} and {State} of the {Art} {Analysis}},
	isbn = {978-1-5386-0457-1},
	shorttitle = {{BigHand2}.{2M} {Benchmark}},
	url = {http://ieeexplore.ieee.org/document/8099762/},
	doi = {10.1109/CVPR.2017.279},
	abstract = {In this paper we introduce a large-scale hand pose dataset, collected using a novel capture method. Existing datasets are either generated synthetically or captured using depth sensors: synthetic datasets exhibit a certain level of appearance difference from real depth images, and real datasets are limited in quantity and coverage, mainly due to the difﬁculty to annotate them. We propose a tracking system with six 6D magnetic sensors and inverse kinematics to automatically obtain 21-joints hand pose annotations of depth maps captured with minimal restriction on the range of motion. The capture protocol aims to fully cover the natural hand pose space. As shown in embedding plots, the new dataset exhibits a signiﬁcantly wider and denser range of hand poses compared to existing benchmarks. Current state-of-the-art methods are evaluated on the dataset, and we demonstrate signiﬁcant improvements in cross-benchmark performance. We also show signiﬁcant improvements in egocentric hand pose estimation with a CNN trained on the new dataset.},
	language = {en},
	urldate = {2022-04-19},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Yuan, Shanxin and Ye, Qi and Stenger, Bjorn and Jain, Siddhant and Kim, Tae-Kyun},
	month = jul,
	year = {2017},
	pages = {2605--2613},
}

@inproceedings{zhao_differentiable_2020,
	title = {Differentiable {Augmentation} for {Data}-{Efficient} {GAN} {Training}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/55479c55ebd1efd3ff125f1337100388-Abstract.html},
	abstract = {The performance of generative adversarial networks (GANs) heavily deteriorates given a limited amount of training data. This is mainly because the discriminatorsis memorizing the exact training set. To combat it, we propose Differentiable Augmentation (DiffAugment), a simple method that improves the data efficiency of GANs by imposing various types of differentiable augmentations on both real and fake samples. Previous attempts to directly augment the training data manipulate the distribution of real images, yielding little benefit; DiffAugment enables us to adopt the differentiable augmentation for the generated samples, effectively stabilizes training, and leads to better convergence. Experiments demonstrate consistent gains of our method over a variety of GAN architectures and loss functions for both unconditional and class-conditional generation. With DiffAugment, we achieve astate-of-the-art FID of 6.80 with an IS of 100.8 on ImageNet 128×128 and 2-4× reductions of FID given 1,000 images on FFHQ and LSUN. Furthermore, with only 20\% training data, we can match the top performance on CIFAR-10 and CIFAR-100. Finally, our method can generate high-fidelity images using only 100 images without pre-training, while being on par with existing transfer learning algorithms. Code is available at https://github.com/mit-han-lab/data-efficient-gans.},
	urldate = {2022-04-09},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Zhao, Shengyu and Liu, Zhijian and Lin, Ji and Zhu, Jun-Yan and Han, Song},
	year = {2020},
	pages = {7559--7570},
}

@article{antoniou_data_2018,
	title = {Data {Augmentation} {Generative} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1711.04340},
	abstract = {Effective training of neural networks requires much data. In the low-data regime, parameters are underdetermined, and learnt networks generalise poorly. Data Augmentation alleviates this by using existing data more effectively. However standard data augmentation produces only limited plausible alternative data. Given there is potential to generate a much broader set of augmentations, we design and train a generative model to do data augmentation. The model, based on image conditional Generative Adversarial Networks, takes data from a source domain and learns to take any data item and generalise it to generate other within-class data items. As this generative process does not depend on the classes themselves, it can be applied to novel unseen classes of data. We show that a Data Augmentation Generative Adversarial Network (DAGAN) augments standard vanilla classifiers well. We also show a DAGAN can enhance few-shot learning systems such as Matching Networks. We demonstrate these approaches on Omniglot, on EMNIST having learnt the DAGAN on Omniglot, and VGG-Face data. In our experiments we can see over 13\% increase in accuracy in the low-data regime experiments in Omniglot (from 69\% to 82\%), EMNIST (73.9\% to 76\%) and VGG-Face (4.5\% to 12\%); in Matching Networks for Omniglot we observe an increase of 0.5\% (from 96.9\% to 97.4\%) and an increase of 1.8\% in EMNIST (from 59.5\% to 61.3\%).},
	urldate = {2022-04-09},
	journal = {arXiv:1711.04340 [cs, stat]},
	author = {Antoniou, Antreas and Storkey, Amos and Edwards, Harrison},
	month = mar,
	year = {2018},
	note = {arXiv: 1711.04340},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{ortiz_depth_2018,
	title = {Depth {Data} {Error} {Modeling} of the {ZED} {3D} {Vision} {Sensor} from {Stereolabs}},
	volume = {17},
	copyright = {Copyright (c)},
	issn = {1577-5097},
	url = {https://raco.cat/index.php/ELCVIA/article/view/v17-n1-ortiz},
	abstract = {The ZED camera is binocular vision system that can be used to provide a 3D perception of the world. It can be applied in autonomous robot navigation, virtual reality, tracking, motion analysis and so on. This paper proposes a mathematical error model for depth data estimated by the ZED camera with its several resolutions of operation. For doing that, the ZED is attached to a Nvidia Jetson TK1 board providing an embedded system that is used for processing raw data acquired by ZED from a 3D checkerboard. Corners are extracted from the checkerboard using RGB data, and a 3D reconstruction is done for these points using disparity data calculated from the ZED camera, coming up with a partially ordered, and regularly distributed (in 3D space) point cloud of corners with given coordinates, which are computed by the device software. These corners also have their ideal world (3D) positions known with respect to the coordinate frame origin that is empirically set in the pattern. Both given (computed)  coordinates from the camera’s data and known (ideal) coordinates of a corner can, thus, be compared for estimating the error between the given and ideal point locations of the detected corner cloud. Subsequently, using a curve fitting technique, we obtain the equations that model the RMS (Root Mean Square) error. This procedure is repeated for several resolutions of the ZED sensor, and at several distances. Results showed its best effectiveness with a maximum distance of approximately sixteen meters, in real time, which allows its use in robotic or other online applications.},
	language = {eng},
	number = {1},
	urldate = {2022-04-09},
	journal = {ELCVIA: electronic letters on computer vision and image analysis},
	author = {Ortiz, Luis Enrique and Cabrera, Viviana Elizabeth and Goncalves, Luiz M. G.},
	month = jun,
	year = {2018},
	note = {Number: 1},
	keywords = {3D and Stereo, Sensor Systems},
	pages = {1--15},
}

@article{tadic_perspectives_2022,
	title = {Perspectives of {RealSense} and {ZED} {Depth} {Sensors} for {Robotic} {Vision} {Applications}},
	volume = {10},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2075-1702},
	url = {https://www.mdpi.com/2075-1702/10/3/183},
	doi = {10.3390/machines10030183},
	abstract = {This review paper presents an overview of depth cameras. Our goal is to describe the features and capabilities of the introduced depth sensors in order to determine their possibilities in robotic applications, focusing on objects that might appear in applications with high accuracy requirements. A series of experiments was conducted, and various depth measuring conditions were examined in order to compare the measurement results of all the depth cameras. Based on the results, all the examined depth sensors were appropriate for applications where obstacle avoidance and robot spatial orientation were required in coexistence with image vision algorithms. In robotic vision applications where high accuracy and precision were obligatory, the ZED depth sensors achieved better measurement results.},
	language = {en},
	number = {3},
	urldate = {2022-04-09},
	journal = {Machines},
	author = {Tadic, Vladimir and Toth, Attila and Vizvari, Zoltan and Klincsik, Mihaly and Sari, Zoltan and Sarcevic, Peter and Sarosi, Jozsef and Biro, Istvan},
	month = mar,
	year = {2022},
	note = {Number: 3
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {RealSense, ZED, ZED 2i, depth map, depth sensors, robotic applications},
	pages = {183},
}

@article{caeiro-rodriguez_systematic_2021,
	title = {A {Systematic} {Review} of {Commercial} {Smart} {Gloves}: {Current} {Status} and {Applications}},
	volume = {21},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1424-8220},
	shorttitle = {A {Systematic} {Review} of {Commercial} {Smart} {Gloves}},
	url = {https://www.mdpi.com/1424-8220/21/8/2667},
	doi = {10.3390/s21082667},
	abstract = {Smart gloves have been under development during the last 40 years to support human-computer interaction based on hand and finger movement. Despite the many devoted efforts and the multiple advances in related areas, these devices have not become mainstream yet. Nevertheless, during recent years, new devices with improved features have appeared, being used for research purposes too. This paper provides a review of current commercial smart gloves focusing on three main capabilities: (i) hand and finger pose estimation and motion tracking, (ii) kinesthetic feedback, and (iii) tactile feedback. For the first capability, a detailed reference model of the hand and finger basic movements (known as degrees of freedom) is proposed. Based on the PRISMA guidelines for systematic reviews for the period 2015–2021, 24 commercial smart gloves have been identified, while many others have been discarded because they did not meet the inclusion criteria: currently active commercial and fully portable smart gloves providing some of the three main capabilities for the whole hand. The paper reviews the technologies involved, main applications and it discusses about the current state of development. Reference models to support end users and researchers comparing and selecting the most appropriate devices are identified as a key need.},
	language = {en},
	number = {8},
	urldate = {2022-04-09},
	journal = {Sensors},
	author = {Caeiro-Rodríguez, Manuel and Otero-González, Iván and Mikic-Fonte, Fernando A. and Llamas-Nistal, Martín},
	month = jan,
	year = {2021},
	note = {Number: 8
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {extended reality, hand and finger pose estimation and motion tracking, haptic feedback, kinesthetic feedback, smart gloves, tactile feedback},
	pages = {2667},
}

@article{bergstra_random_2012,
	title = {Random {Search} for {Hyper}-{Parameter} {Optimization}},
	abstract = {Grid search and manual search are the most widely used strategies for hyper-parameter optimization. This paper shows empirically and theoretically that randomly chosen trials are more efﬁcient for hyper-parameter optimization than trials on a grid. Empirical evidence comes from a comparison with a large previous study that used grid search and manual search to conﬁgure neural networks and deep belief networks. Compared with neural networks conﬁgured by a pure grid search, we ﬁnd that random search over the same domain is able to ﬁnd models that are as good or better within a small fraction of the computation time. Granting random search the same computational budget, random search ﬁnds better models by effectively searching a larger, less promising conﬁguration space. Compared with deep belief networks conﬁgured by a thoughtful combination of manual search and grid search, purely random search over the same 32-dimensional conﬁguration space found statistically equal performance on four of seven data sets, and superior performance on one of seven. A Gaussian process analysis of the function from hyper-parameters to validation set performance reveals that for most data sets only a few of the hyper-parameters really matter, but that different hyper-parameters are important on different data sets. This phenomenon makes grid search a poor choice for conﬁguring algorithms for new data sets. Our analysis casts some light on why recent “High Throughput” methods achieve surprising success—they appear to search through a large number of hyper-parameters because most hyper-parameters do not matter much. We anticipate that growing interest in large hierarchical models will place an increasing burden on techniques for hyper-parameter optimization; this work shows that random search is a natural baseline against which to judge progress in the development of adaptive (sequential) hyper-parameter optimization algorithms.},
	language = {en},
	author = {Bergstra, James and Bengio, Yoshua},
	year = {2012},
	pages = {25},
}

@inproceedings{franceschi_forward_2017,
	title = {Forward and {Reverse} {Gradient}-{Based} {Hyperparameter} {Optimization}},
	url = {https://proceedings.mlr.press/v70/franceschi17a.html},
	abstract = {We study two procedures (reverse-mode and forward-mode) for computing the gradient of the validation error with respect to the hyperparameters of any iterative learning algorithm such as stochastic gradient descent. These procedures mirror two ways of computing gradients for recurrent neural networks and have different trade-offs in terms of running time and space requirements. Our formulation of the reverse-mode procedure is linked to previous work by Maclaurin et al (2015) but does not require reversible dynamics. Additionally, we explore the use of constraints on the hyperparameters. The forward-mode procedure is suitable for real-time hyperparameter updates, which may significantly speedup hyperparameter optimization on large datasets. We present a series of experiments on image and phone classification tasks. In the second task, previous gradient-based approaches are prohibitive. We show that our real-time algorithm yields state-of-the-art results in affordable time.},
	language = {en},
	urldate = {2022-04-09},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Franceschi, Luca and Donini, Michele and Frasconi, Paolo and Pontil, Massimiliano},
	month = jul,
	year = {2017},
	note = {ISSN: 2640-3498},
	pages = {1165--1173},
}

@inproceedings{snoek_practical_2012,
	title = {Practical {Bayesian} {Optimization} of {Machine} {Learning} {Algorithms}},
	volume = {25},
	url = {https://papers.nips.cc/paper/2012/hash/05311655a15b75fab86956663e1819cd-Abstract.html},
	abstract = {The use of machine learning algorithms frequently involves careful tuning of learning parameters and model hyperparameters. Unfortunately, this tuning is often a “black art” requiring expert experience, rules of thumb, or sometimes brute-force search. There is therefore great appeal for automatic approaches that can optimize the performance of any given learning algorithm to the problem at hand. In this work, we consider this problem through the framework of Bayesian optimization, in which a learning algorithm’s generalization performance is modeled as a sample from a Gaussian process (GP). We show that certain choices for the nature of the GP, such as the type of kernel and the treatment of its hyperparameters, can play a crucial role in obtaining a good optimizer that can achieve expert-level performance. We describe new algorithms that take into account the variable cost (duration) of learning algorithm experiments and that can leverage the presence of multiple cores for parallel experimentation. We show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization for many algorithms including Latent Dirichlet Allocation, Structured SVMs and convolutional neural networks.},
	urldate = {2022-04-08},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P},
	year = {2012},
}

@article{zhou_metaaugment_2021,
	title = {{MetaAugment}: {Sample}-{Aware} {Data} {Augmentation} {Policy} {Learning}},
	volume = {35},
	copyright = {Copyright (c) 2021 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	shorttitle = {{MetaAugment}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/17324},
	abstract = {Automated data augmentation has shown superior performance in image recognition. Existing works search for dataset-level augmentation policies without considering individual sample variations, which are likely to be sub-optimal. On the other hand, learning different policies for different samples naively could greatly increase the computing cost. In this paper, we learn a sample-aware data augmentation policy efficiently by formulating it as a sample reweighting problem. Specifically, an augmentation policy network takes a transformation and the corresponding augmented image as inputs, and outputs a weight to adjust the augmented image loss computed by a task network. At training stage, the task network minimizes the weighted losses of augmented training images, while the policy network minimizes the loss of the task network on a validation set via meta-learning. We theoretically prove the convergence of the training procedure and further derive the exact convergence rate. Superior performance is achieved on widely-used benchmarks including CIFAR-10/100, Omniglot, and ImageNet.},
	language = {en},
	number = {12},
	urldate = {2022-04-08},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Zhou, Fengwei and Li, Jiawei and Xie, Chuanlong and Chen, Fei and Hong, Lanqing and Sun, Rui and Li, Zhenguo},
	month = may,
	year = {2021},
	note = {Number: 12},
	keywords = {(Deep) Neural Network Algorithms},
	pages = {11097--11105},
}

@article{chen_group-theoretic_2020,
	title = {A {Group}-{Theoretic} {Framework} for {Data} {Augmentation}},
	volume = {21},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v21/20-163.html},
	abstract = {Data augmentation is a widely used trick when training deep neural networks: in addition to the original data, properly transformed data are also added to the training set. However, to the best of our knowledge, a clear mathematical framework to explain the performance benefits of data augmentation is not available. In this paper, we develop such a theoretical framework. We show data augmentation is equivalent to an averaging operation over the orbits of a certain group that keeps the data distribution approximately invariant. We prove that it leads to variance reduction. We study empirical risk minimization, and the examples of exponential families, linear regression, and certain two-layer neural networks. We also discuss how data augmentation could be used in problems with symmetry where other approaches are prevalent, such as in cryo-electron microscopy (cryo-EM).},
	number = {245},
	urldate = {2022-04-08},
	journal = {Journal of Machine Learning Research},
	author = {Chen, Shuxiao and Dobriban, Edgar and Lee, Jane H.},
	year = {2020},
	pages = {1--71},
}

@inproceedings{hoffer_augment_2020,
	address = {Seattle, WA, USA},
	title = {Augment {Your} {Batch}: {Improving} {Generalization} {Through} {Instance} {Repetition}},
	isbn = {978-1-72817-168-5},
	shorttitle = {Augment {Your} {Batch}},
	url = {https://ieeexplore.ieee.org/document/9157738/},
	doi = {10.1109/CVPR42600.2020.00815},
	abstract = {Large-batch SGD is important for scaling training of deep neural networks. However, without ﬁne-tuning hyperparameter schedules, the generalization of the model may be hampered. We propose to use batch augmentation: replicating instances of samples within the same batch with different data augmentations. Batch augmentation acts as a regularizer and an accelerator, increasing both generalization and performance scaling for a ﬁxed budget of optimization steps. We analyze the effect of batch augmentation on gradient variance and show that it empirically improves convergence for a wide variety of networks and datasets. Our results show that batch augmentation reduces the number of necessary SGD updates to achieve the same accuracy as the state-of-the-art. Overall, this simple yet effective method enables faster training and better generalization by allowing more computational resources to be used concurrently.},
	language = {en},
	urldate = {2022-04-08},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Hoffer, Elad and Ben-Nun, Tal and Hubara, Itay and Giladi, Niv and Hoefler, Torsten and Soudry, Daniel},
	month = jun,
	year = {2020},
	pages = {8126--8135},
}

@inproceedings{jaderberg_spatial_2015,
	title = {Spatial {Transformer} {Networks}},
	volume = {28},
	url = {https://proceedings.neurips.cc/paper/2015/hash/33ceb07bf4eeb3da587e268d663aba1a-Abstract.html},
	abstract = {Convolutional Neural Networks define an exceptionallypowerful class of model, but are still limited by the lack of abilityto be spatially invariant to the input data in a computationally and parameterefficient manner. In this work we introduce a new learnable module, theSpatial Transformer, which explicitly allows the spatial manipulation ofdata within the network. This differentiable module can be insertedinto existing convolutional architectures, giving neural networks the ability toactively spatially transform feature maps, conditional on the feature map itself,without any extra training supervision or modification to the optimisation process. We show that the useof spatial transformers results in models which learn invariance to translation,scale, rotation and more generic warping, resulting in state-of-the-artperformance on several benchmarks, and for a numberof classes of transformations.},
	urldate = {2022-04-08},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Jaderberg, Max and Simonyan, Karen and Zisserman, Andrew and kavukcuoglu, koray},
	year = {2015},
}

@inproceedings{li_pointaugment_2020,
	address = {Seattle, WA, USA},
	title = {{PointAugment}: {An} {Auto}-{Augmentation} {Framework} for {Point} {Cloud} {Classification}},
	isbn = {978-1-72817-168-5},
	shorttitle = {{PointAugment}},
	url = {https://ieeexplore.ieee.org/document/9156333/},
	doi = {10.1109/CVPR42600.2020.00641},
	abstract = {We present PointAugment1, a new auto-augmentation framework that automatically optimizes and augments point cloud samples to enrich the data diversity when we train a classiﬁcation network. Different from existing autoaugmentation methods for 2D images, PointAugment is sample-aware and takes an adversarial learning strategy to jointly optimize an augmentor network and a classiﬁer network, such that the augmentor can learn to produce augmented samples that best ﬁt the classiﬁer. Moreover, we formulate a learnable point augmentation function with a shape-wise transformation and a point-wise displacement, and carefully design loss functions to adopt the augmented samples based on the learning progress of the classiﬁer. Extensive experiments also conﬁrm PointAugment’s effectiveness and robustness to improve the performance of various networks on shape classiﬁcation and retrieval.},
	language = {en},
	urldate = {2022-04-08},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Li, Ruihui and Li, Xianzhi and Heng, Pheng-Ann and Fu, Chi-Wing},
	month = jun,
	year = {2020},
	pages = {6377--6386},
}

@inproceedings{xiong_explore_2021,
	title = {Explore {Visual} {Concept} {Formation} for {Image} {Classification}},
	url = {https://proceedings.mlr.press/v139/xiong21a.html},
	abstract = {Human beings acquire the ability of image classification through visual concept learning, in which the process of concept formation involves intertwined searches of common properties and concept descriptions. However, in most image classification algorithms using deep convolutional neural network (ConvNet), the representation space is constructed under the premise that concept descriptions are fixed as one-hot codes, which limits the mining of properties and the ability of identifying unseen samples. Inspired by this, we propose a learning strategy of visual concept formation (LSOVCF) based on the ConvNet, in which the two intertwined parts of concept formation, i.e. feature extraction and concept description, are learned together. First, LSOVCF takes sample response in the last layer of ConvNet to induct concept description being assumed as Gaussian distribution, which is part of the training process. Second, the exploration and experience loss is designed for optimization, which adopts experience cache pool to speed up convergence. Experiments show that LSOVCF improves the ability of identifying unseen samples on cifar10, STL10, flower17 and ImageNet based on several backbones, from the classic VGG to the SOTA Ghostnet. The code is available at {\textbackslash}url\{https://github.com/elvintanhust/LSOVCF\}.},
	language = {en},
	urldate = {2022-04-07},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Xiong, Shengzhou and Tan, Yihua and Wang, Guoyou},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {11470--11479},
}

@inproceedings{ren_interpreting_2021,
	title = {Interpreting and {Disentangling} {Feature} {Components} of {Various} {Complexity} from {DNNs}},
	url = {https://proceedings.mlr.press/v139/ren21b.html},
	abstract = {This paper aims to define, visualize, and analyze the feature complexity that is learned by a DNN. We propose a generic definition for the feature complexity. Given the feature of a certain layer in the DNN, our method decomposes and visualizes feature components of different complexity orders from the feature. The feature decomposition enables us to evaluate the reliability, the effectiveness, and the significance of over-fitting of these feature components. Furthermore, such analysis helps to improve the performance of DNNs. As a generic method, the feature complexity also provides new insights into existing deep-learning techniques, such as network compression and knowledge distillation.},
	language = {en},
	urldate = {2022-04-07},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Ren, Jie and Li, Mingjie and Liu, Zexu and Zhang, Quanshi},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {8971--8981},
}

@article{schneider_explaining_2022,
	title = {Explaining classifiers by constructing familiar concepts},
	issn = {1573-0565},
	url = {https://doi.org/10.1007/s10994-022-06157-0},
	doi = {10.1007/s10994-022-06157-0},
	abstract = {Interpreting a large number of neurons in deep learning is difficult. Our proposed ‘CLAssifier-DECoder’ architecture (ClaDec) facilitates the understanding of the output of an arbitrary layer of neurons or subsets thereof. It uses a decoder that transforms the incomprehensible representation of the given neurons to a representation that is more similar to the domain a human is familiar with. In an image recognition problem, one can recognize what information (or concepts) a layer maintains by contrasting reconstructed images of ClaDec with those of a conventional auto-encoder(AE) serving as reference. An extension of ClaDec allows trading comprehensibility and fidelity. We evaluate our approach for image classification using convolutional neural networks. We show that reconstructed visualizations using encodings from a classifier capture more relevant classification information than conventional AEs. This holds although AEs contain more information on the original input. Our user study highlights that even non-experts can identify a diverse set of concepts contained in images that are relevant (or irrelevant) for the classifier. We also compare against saliency based methods that focus on pixel relevance rather than concepts. We show that ClaDec tends to highlight more relevant input areas to classification though outcomes depend on classifier architecture. Code is at https://github.com/JohnTailor/ClaDec},
	language = {en},
	urldate = {2022-04-06},
	journal = {Machine Learning},
	author = {Schneider, Johannes and Vlachos, Michalis},
	month = mar,
	year = {2022},
}

@inproceedings{ghandeharioun_dissect_2021,
	title = {{DISSECT}: {Disentangled} {Simultaneous} {Explanations} via {Concept} {Traversals}},
	shorttitle = {{DISSECT}},
	url = {https://openreview.net/forum?id=qY79G8jGsep},
	abstract = {Explaining deep learning model inferences is a promising venue for scientific understanding, improving safety, uncovering hidden biases, evaluating fairness, and beyond, as argued by many scholars....},
	language = {en},
	urldate = {2022-04-06},
	author = {Ghandeharioun, Asma and Kim, Been and Li, Chun-Liang and Jou, Brendan and Eoff, Brian and Picard, Rosalind},
	month = sep,
	year = {2021},
}

@inproceedings{ivankay_fooling_2021,
	title = {Fooling {Explanations} in {Text} {Classifiers}},
	url = {https://openreview.net/forum?id=j3krplz_4w6},
	abstract = {State-of-the-art text classification models are becoming increasingly reliant on deep neural networks (DNNs). Due to their black-box nature, faithful and robust explanation methods need to...},
	language = {en},
	urldate = {2022-04-06},
	author = {Ivankay, Adam and Girardi, Ivan and Marchiori, Chiara and Frossard, Pascal},
	month = sep,
	year = {2021},
}

@inproceedings{lim_noisy_2021,
	title = {Noisy {Feature} {Mixup}},
	url = {https://openreview.net/forum?id=vJb4I2ANmy},
	abstract = {We introduce Noisy Feature Mixup (NFM), an inexpensive yet effective method for data augmentation that combines the best of interpolation based training and noise injection schemes. Rather than...},
	language = {en},
	urldate = {2022-04-06},
	author = {Lim, Soon Hoe and Erichson, N. Benjamin and Utrera, Francisco and Xu, Winnie and Mahoney, Michael W.},
	month = sep,
	year = {2021},
}

@inproceedings{cheung_adaaug_2021,
	title = {{AdaAug}: {Learning} {Class}- and {Instance}-adaptive {Data} {Augmentation} {Policies}},
	shorttitle = {{AdaAug}},
	url = {https://openreview.net/forum?id=rWXfFogxRJN},
	abstract = {Data augmentation is an effective way to improve the generalization capability of modern deep learning models. However, the underlying augmentation methods mostly rely on handcrafted operations....},
	language = {en},
	urldate = {2022-04-06},
	author = {Cheung, Tsz-Him and Yeung, Dit-Yan},
	month = sep,
	year = {2021},
}

@inproceedings{rigotti_attention-based_2021,
	title = {Attention-based {Interpretability} with {Concept} {Transformers}},
	url = {https://openreview.net/forum?id=kAa9eDS0RdO},
	abstract = {Attention is a mechanism that has been instrumental in driving remarkable performance gains of deep neural network models in a host of visual, NLP and multimodal tasks.
One additional notable...},
	language = {en},
	urldate = {2022-04-06},
	author = {Rigotti, Mattia and Miksovic, Christoph and Giurgiu, Ioana and Gschwind, Thomas and Scotton, Paolo},
	month = sep,
	year = {2021},
}

@inproceedings{zhang_adversarial_2021,
	title = {Adversarial {Robustness} {Through} the {Lens} of {Causality}},
	url = {https://openreview.net/forum?id=cZAi1yWpiXQ},
	abstract = {The adversarial vulnerability of deep neural networks has attracted signiﬁcant attention in machine learning. As causal reasoning has an instinct for modeling distribution change, it is essential...},
	language = {en},
	urldate = {2022-04-06},
	author = {Zhang, Yonggang and Gong, Mingming and Liu, Tongliang and Niu, Gang and Tian, Xinmei and Han, Bo and Schölkopf, Bernhard and Zhang, Kun},
	month = sep,
	year = {2021},
}

@inproceedings{he_gda-am_2021,
	title = {{GDA}-{AM}: {ON} {THE} {EFFECTIVENESS} {OF} {SOLVING} {MIN}-{IMAX} {OPTIMIZATION} {VIA} {ANDERSON} {MIXING}},
	shorttitle = {{GDA}-{AM}},
	url = {https://openreview.net/forum?id=3YqeuCVwy1d},
	abstract = {Many modern machine learning algorithms such as generative adversarial networks (GANs) and adversarial training can be formulated as minimax optimization.Gradient descent ascent (GDA) is the most...},
	language = {en},
	urldate = {2022-04-06},
	author = {He, Huan and Zhao, Shifan and Xi, Yuanzhe and Ho, Joyce and Saad, Yousef},
	month = sep,
	year = {2021},
}

@inproceedings{singla_salient_2021,
	title = {Salient {ImageNet}: {How} to discover spurious features in {Deep} {Learning}?},
	shorttitle = {Salient {ImageNet}},
	url = {https://openreview.net/forum?id=XVPqLyNxSyh},
	abstract = {Deep neural networks can be unreliable in the real world especially when they heavily use \{{\textbackslash}it spurious\} features for their predictions. Focusing on image classifications, we define \{\vphantom{\}}{\textbackslash}it core...},
	language = {en},
	urldate = {2022-04-06},
	author = {Singla, Sahil and Feizi, Soheil},
	month = sep,
	year = {2021},
}

@inproceedings{antverg_pitfalls_2021,
	title = {On the {Pitfalls} of {Analyzing} {Individual} {Neurons} in {Language} {Models}},
	url = {https://openreview.net/forum?id=8uz0EWPQIMu},
	abstract = {While many studies have shown that linguistic information is encoded in hidden word representations, few have studied individual neurons, to show how and in which neurons it is encoded.
Among these...},
	language = {en},
	urldate = {2022-04-06},
	author = {Antverg, Omer and Belinkov, Yonatan},
	month = sep,
	year = {2021},
}

@inproceedings{liang_metashift_2021,
	title = {{MetaShift}: {A} {Dataset} of {Datasets} for {Evaluating} {Contextual} {Distribution} {Shifts} and {Training} {Conflicts}},
	shorttitle = {{MetaShift}},
	url = {https://openreview.net/forum?id=MTex8qKavoS},
	abstract = {Understanding the performance of machine learning models across diverse data distributions is critically important for reliable applications. Motivated by this, there is a growing focus on curating...},
	language = {en},
	urldate = {2022-04-06},
	author = {Liang, Weixin and Zou, James},
	month = sep,
	year = {2021},
}

@inproceedings{schott_visual_2021,
	title = {Visual {Representation} {Learning} {Does} {Not} {Generalize} {Strongly} {Within} the {Same} {Domain}},
	url = {https://openreview.net/forum?id=9RUHPlladgh},
	abstract = {An important component for generalization in machine learning is to uncover underlying latent factors of variation as well as the mechanism through which each factor acts in the world.
In this...},
	language = {en},
	urldate = {2022-04-06},
	author = {Schott, Lukas and Kügelgen, Julius Von and Träuble, Frederik and Gehler, Peter Vincent and Russell, Chris and Bethge, Matthias and Schölkopf, Bernhard and Locatello, Francesco and Brendel, Wieland},
	month = sep,
	year = {2021},
}

@inproceedings{scimeca_which_2021,
	title = {Which {Shortcut} {Cues} {Will} {DNNs} {Choose}? {A} {Study} from the {Parameter}-{Space} {Perspective}},
	shorttitle = {Which {Shortcut} {Cues} {Will} {DNNs} {Choose}?},
	url = {https://openreview.net/forum?id=qRDQi3ocgR3},
	abstract = {Deep neural networks (DNNs) often rely on easy–to–learn discriminatory features, or cues, that are not necessarily essential to the problem at hand. For example, ducks in an image may be recognized...},
	language = {en},
	urldate = {2022-04-06},
	author = {Scimeca, Luca and Oh, Seong Joon and Chun, Sanghyuk and Poli, Michael and Yun, Sangdoo},
	month = sep,
	year = {2021},
}

@inproceedings{harris_joint_2021,
	title = {Joint {Shapley} values: a measure of joint feature importance},
	shorttitle = {Joint {Shapley} values},
	url = {https://openreview.net/forum?id=vcUmUvQCloe},
	abstract = {The Shapley value is one of the most widely used measures of feature importance partly as it measures a feature's average effect on a model's prediction.  We introduce joint Shapley values, which...},
	language = {en},
	urldate = {2022-04-06},
	author = {Harris, Chris and Pymar, Richard and Rowat, Colin},
	month = sep,
	year = {2021},
}

@inproceedings{hernandez_natural_2021,
	title = {Natural {Language} {Descriptions} of {Deep} {Features}},
	url = {https://openreview.net/forum?id=NudBMY-tzDr},
	abstract = {Some neurons in deep networks specialize in recognizing highly specific perceptual, structural, or semantic features of inputs. In computer vision, techniques exist for identifying neurons that...},
	language = {en},
	urldate = {2022-04-06},
	author = {Hernandez, Evan and Schwettmann, Sarah and Bau, David and Bagashvili, Teona and Torralba, Antonio and Andreas, Jacob},
	month = sep,
	year = {2021},
}

@inproceedings{deng_discovering_2021,
	title = {{DISCOVERING} {AND} {EXPLAINING} {THE} {REPRESENTATION} {BOTTLENECK} {OF} {DNNS}},
	url = {https://openreview.net/forum?id=iRCUlgmdfHJ},
	abstract = {This paper explores the bottleneck of feature representations of deep neural networks (DNNs), from the perspective of the complexity of interactions between input variables encoded in DNNs. To this...},
	language = {en},
	urldate = {2022-04-06},
	author = {Deng, Huiqi and Ren, Qihan and Zhang, Hao and Zhang, Quanshi},
	month = sep,
	year = {2021},
}

@inproceedings{wang_hidden_2021,
	title = {The {Hidden} {Convex} {Optimization} {Landscape} of {Regularized} {Two}-{Layer} {ReLU} {Networks}: an {Exact} {Characterization} of {Optimal} {Solutions}},
	shorttitle = {The {Hidden} {Convex} {Optimization} {Landscape} of {Regularized} {Two}-{Layer} {ReLU} {Networks}},
	url = {https://openreview.net/forum?id=Z7Lk2cQEG8a},
	abstract = {We prove that finding all globally optimal two-layer ReLU neural networks can be performed by solving a convex optimization program with cone constraints. Our analysis is novel, characterizes all...},
	language = {en},
	urldate = {2022-04-06},
	author = {Wang, Yifei and Lacotte, Jonathan and Pilanci, Mert},
	month = sep,
	year = {2021},
}

@inproceedings{chang_node-gam_2021,
	title = {{NODE}-{GAM}: {Neural} {Generalized} {Additive} {Model} for {Interpretable} {Deep} {Learning}},
	shorttitle = {{NODE}-{GAM}},
	url = {https://openreview.net/forum?id=g8NJR6fCCl8},
	abstract = {Deployment of machine learning models in real high-risk settings (e.g. healthcare) often depends not only on the model's accuracy but also on its fairness, robustness, and interpretability....},
	language = {en},
	urldate = {2022-04-06},
	author = {Chang, Chun-Hao and Caruana, Rich and Goldenberg, Anna},
	month = sep,
	year = {2021},
}

@inproceedings{masoomi_explanations_2021,
	title = {Explanations of {Black}-{Box} {Models} based on {Directional} {Feature} {Interactions}},
	url = {https://openreview.net/forum?id=45Mr7LeKR9},
	abstract = {As machine learning algorithms are deployed ubiquitously to a variety of domains, it is imperative to make these often black-box models transparent.  Several recent works explain black-box models...},
	language = {en},
	urldate = {2022-04-06},
	author = {Masoomi, Aria and Hill, Davin and Xu, Zhonghui and Hersh, Craig P. and Silverman, Edwin K. and Castaldi, Peter J. and Ioannidis, Stratis and Dy, Jennifer},
	month = sep,
	year = {2021},
}

@inproceedings{chidambaram_towards_2021,
	title = {Towards {Understanding} the {Data} {Dependency} of {Mixup}-style {Training}},
	url = {https://openreview.net/forum?id=ieNJYujcGDO},
	abstract = {In the Mixup training paradigm, a model is trained using convex combinations of data points and their associated labels. Despite seeing very few true data points during training, models trained...},
	language = {en},
	urldate = {2022-04-06},
	author = {Chidambaram, Muthu and Wang, Xiang and Hu, Yuzheng and Wu, Chenwei and Ge, Rong},
	month = sep,
	year = {2021},
}

@inproceedings{harrington_finding_2021,
	title = {Finding {Biological} {Plausibility} for {Adversarially} {Robust} {Features} via {Metameric} {Tasks}},
	url = {https://openreview.net/forum?id=yeP_zx9vqNm},
	abstract = {Recent work suggests that feature constraints in the training datasets of deep neural networks (DNNs) drive robustness to adversarial noise (Ilyas et al., 2019). The representations learned by such...},
	language = {en},
	urldate = {2022-04-06},
	author = {Harrington, Anne and Deza, Arturo},
	month = sep,
	year = {2021},
}

@inproceedings{arbel_amortized_2021,
	title = {Amortized {Implicit} {Differentiation} for {Stochastic} {Bilevel} {Optimization}},
	url = {https://openreview.net/forum?id=3PN4iyXBeF},
	abstract = {We study a class of algorithms for solving bilevel optimization problems in both stochastic and deterministic settings when the inner-level objective is strongly convex. Specifically, we consider...},
	language = {en},
	urldate = {2022-04-06},
	author = {Arbel, Michael and Mairal, Julien},
	month = sep,
	year = {2021},
}

@inproceedings{zhou_deep_2021,
	title = {Do deep networks transfer invariances across classes?},
	url = {https://openreview.net/forum?id=Fn7i_r5rR0q},
	abstract = {In order to generalize well, classifiers must learn to be invariant to nuisance transformations that do not alter an input's class. Many problems have "class-agnostic" nuisance transformations that...},
	language = {en},
	urldate = {2022-04-06},
	author = {Zhou, Allan and Tajwar, Fahim and Robey, Alexander and Knowles, Tom and Pappas, George J. and Hassani, Hamed and Finn, Chelsea},
	month = sep,
	year = {2021},
}

@inproceedings{calian_defending_2021,
	title = {Defending {Against} {Image} {Corruptions} {Through} {Adversarial} {Augmentations}},
	url = {https://openreview.net/forum?id=jJOjjiZHy3h},
	abstract = {Modern neural networks excel at image classification, yet they remain vulnerable to common image corruptions such as blur, speckle noise or fog. Recent methods that focus on this problem, such as...},
	language = {en},
	urldate = {2022-04-06},
	author = {Calian, Dan Andrei and Stimberg, Florian and Wiles, Olivia and Rebuffi, Sylvestre-Alvise and György, András and Mann, Timothy A. and Gowal, Sven},
	month = sep,
	year = {2021},
}

@inproceedings{sixt_users_2021,
	title = {Do {Users} {Benefit} {From} {Interpretable} {Vision}? {A} {User} {Study}, {Baseline}, {And} {Dataset}},
	shorttitle = {Do {Users} {Benefit} {From} {Interpretable} {Vision}?},
	url = {https://openreview.net/forum?id=v6s3HVjPerv},
	abstract = {A variety of methods exist to explain image classification models. However, whether they provide any benefit to users over simply comparing various inputs and the model’s respective predictions...},
	language = {en},
	urldate = {2022-04-06},
	author = {Sixt, Leon and Schuessler, Martin and Popescu, Oana-Iuliana and Weiß, Philipp and Landgraf, Tim},
	month = sep,
	year = {2021},
}

@inproceedings{larsen_how_2021,
	title = {How many degrees of freedom do we need to train deep networks: a loss landscape perspective},
	shorttitle = {How many degrees of freedom do we need to train deep networks},
	url = {https://openreview.net/forum?id=ChMLTGRjFcU},
	abstract = {A variety of recent works, spanning pruning, lottery tickets, and training within random subspaces, have shown that deep neural networks can be trained using far fewer degrees of freedom than the...},
	language = {en},
	urldate = {2022-04-06},
	author = {Larsen, Brett W. and Fort, Stanislav and Becker, Nic and Ganguli, Surya},
	month = sep,
	year = {2021},
}

@inproceedings{black_consistent_2021,
	title = {Consistent {Counterfactuals} for {Deep} {Models}},
	url = {https://openreview.net/forum?id=St6eyiTEHnG},
	abstract = {Counterfactual examples are one of the most commonly-cited methods for explaining the predictions of machine learning models in key areas such as finance and medical diagnosis. Counterfactuals are...},
	language = {en},
	urldate = {2022-04-06},
	author = {Black, Emily and Wang, Zifan and Fredrikson, Matt},
	month = sep,
	year = {2021},
}

@inproceedings{jethani_fastshap_2021,
	title = {{FastSHAP}: {Real}-{Time} {Shapley} {Value} {Estimation}},
	shorttitle = {{FastSHAP}},
	url = {https://openreview.net/forum?id=Zq2G_VTV53T},
	abstract = {Although Shapley values are theoretically appealing for explaining black-box models, they are costly to calculate and thus impractical in settings that involve large, high-dimensional models. To...},
	language = {en},
	urldate = {2022-04-06},
	author = {Jethani, Neil and Sudarshan, Mukund and Covert, Ian Connick and Lee, Su-In and Ranganath, Rajesh},
	month = sep,
	year = {2021},
}

@inproceedings{mei_falcon_2021,
	title = {{FALCON}: {Fast} {Visual} {Concept} {Learning} by {Integrating} {Images}, {Linguistic} descriptions, and {Conceptual} {Relations}},
	shorttitle = {{FALCON}},
	url = {https://openreview.net/forum?id=htWIlvDcY8},
	abstract = {We present a meta-learning framework for learning new visual concepts quickly, from just one or a few examples, guided by multiple naturally occurring data streams: simultaneously looking at images...},
	language = {en},
	urldate = {2022-04-06},
	author = {Mei, Lingjie and Mao, Jiayuan and Wang, Ziqi and Gan, Chuang and Tenenbaum, Joshua B.},
	month = sep,
	year = {2021},
}

@inproceedings{hamilton_axiomatic_2021,
	title = {Axiomatic {Explanations} for {Visual} {Search}, {Retrieval}, and {Similarity} {Learning}},
	url = {https://openreview.net/forum?id=TqNsv1TuCX9},
	abstract = {Visual search, recommendation, and contrastive similarity learning power technologies that impact billions of users worldwide. Modern model architectures can be complex and difficult to interpret...},
	language = {en},
	urldate = {2022-04-06},
	author = {Hamilton, Mark and Lundberg, Scott and Fu, Stephanie and Zhang, Lei and Freeman, William T.},
	month = sep,
	year = {2021},
}

@inproceedings{kim_what_2021,
	title = {What {Makes} {Better} {Augmentation} {Strategies}? {Augment} {Difficult} but {Not} too {Different}},
	shorttitle = {What {Makes} {Better} {Augmentation} {Strategies}?},
	url = {https://openreview.net/forum?id=Ucx3DQbC9GH},
	abstract = {The practice of data augmentation has been extensively used to boost the performance of deep neural networks for various NLP tasks. It is more effective when only a limited number of labeled...},
	language = {en},
	urldate = {2022-04-06},
	author = {Kim, Jaehyung and Kang, Dongyeop and Ahn, Sungsoo and Shin, Jinwoo},
	month = sep,
	year = {2021},
}

@inproceedings{chen_recursive_2022,
	title = {Recursive {Disentanglement} {Network}},
	url = {https://openreview.net/forum?id=CSfcOznpDY},
	abstract = {Disentangled feature representation is essential for data-efficient learning. The feature space of deep models is inherently compositional. Existing \${\textbackslash}beta\$-VAE-based methods, which only apply...},
	language = {en},
	urldate = {2022-04-06},
	author = {Chen, Yixuan and Shi, Yubin and Li, Dongsheng and Wang, Yujiang and Dong, Mingzhi and Zhao, Yingying and Dick, Robert and Lv, Qin and Yang, Fan and Shang, Li},
	year = {2022},
}

@inproceedings{adebayo_post_2022,
	title = {Post hoc {Explanations} may be {Ineffective} for {Detecting} {Unknown} {Spurious} {Correlation}},
	url = {https://openreview.net/forum?id=xNOVfCCvDpM},
	abstract = {We investigate whether three types of post hoc model explanations--feature attribution, concept activation, and training point ranking--are effective for detecting a model's reliance on spurious...},
	language = {en},
	urldate = {2022-04-06},
	author = {Adebayo, Julius and Muelly, Michael and Abelson, Harold and Kim, Been},
	year = {2022},
}

@inproceedings{sreedharan_bridging_2021,
	title = {Bridging the {Gap}: {Providing} {Post}-{Hoc} {Symbolic} {Explanations} for {Sequential} {Decision}-{Making} {Problems} with {Inscrutable} {Representations}},
	shorttitle = {Bridging the {Gap}},
	url = {https://openreview.net/forum?id=o-1v9hdSult},
	abstract = {As increasingly complex AI systems are introduced into our daily lives, it becomes important for such systems to be capable of explaining the rationale for their decisions and allowing users to...},
	language = {en},
	urldate = {2022-04-06},
	author = {Sreedharan, Sarath and Soni, Utkarsh and Verma, Mudit and Srivastava, Siddharth and Kambhampati, Subbarao},
	month = sep,
	year = {2021},
}

@inproceedings{zhao_quantitative_2021,
	title = {Quantitative {Performance} {Assessment} of {CNN} {Units} via {Topological} {Entropy} {Calculation}},
	url = {https://openreview.net/forum?id=xFOyMwWPkz},
	abstract = {Identifying the status of individual network units is critical for understanding the mechanism of convolutional neural networks (CNNs). However, it is still challenging to reliably give a general...},
	language = {en},
	urldate = {2022-04-06},
	author = {Zhao, Yang and Zhang, Hao},
	month = sep,
	year = {2021},
}

@inproceedings{jia_zest_2021,
	title = {A {Zest} of {LIME}: {Towards} {Architecture}-{Independent} {Model} {Distances}},
	shorttitle = {A {Zest} of {LIME}},
	url = {https://openreview.net/forum?id=OUz_9TiTv9j},
	abstract = {Definitions of the distance between two machine learning models either characterize the similarity of the models' predictions or of their weights. While similarity of weights is attractive because...},
	language = {en},
	urldate = {2022-04-06},
	author = {Jia, Hengrui and Chen, Hongyu and Guan, Jonas and Shamsabadi, Ali Shahin and Papernot, Nicolas},
	month = sep,
	year = {2021},
}

@inproceedings{lord_attacking_2021,
	title = {Attacking deep networks with surrogate-based adversarial black-box methods is easy},
	url = {https://openreview.net/forum?id=Zf4ZdI4OQPV},
	abstract = {A recent line of work on black-box adversarial attacks has revived the use of transfer from surrogate models by integrating it into query-based search. However, we find that existing approaches of...},
	language = {en},
	urldate = {2022-04-06},
	author = {Lord, Nicholas A. and Mueller, Romain and Bertinetto, Luca},
	month = sep,
	year = {2021},
}

@inproceedings{gontijo-lopes_no_2021,
	title = {No {One} {Representation} to {Rule} {Them} {All}: {Overlapping} {Features} of {Training} {Methods}},
	shorttitle = {No {One} {Representation} to {Rule} {Them} {All}},
	url = {https://openreview.net/forum?id=BK-4qbGgIE3},
	abstract = {Despite being able to capture a range of features of the data, high accuracy models trained with supervision tend to make similar predictions. This seemingly implies that high-performing models...},
	language = {en},
	urldate = {2022-04-06},
	author = {Gontijo-Lopes, Raphael and Dauphin, Yann and Cubuk, Ekin Dogus},
	month = sep,
	year = {2021},
}

@inproceedings{lin_online_2019,
	title = {Online {Hyper}-{Parameter} {Learning} for {Auto}-{Augmentation} {Strategy}},
	doi = {10.1109/ICCV.2019.00668},
	abstract = {Data augmentation is critical to the success of modern deep learning techniques. In this paper, we propose Online Hyper-parameter Learning for Auto-Augmentation (OHL-Auto-Aug), an economical solution that learns the augmentation policy distribution along with network training. Unlike previous methods on auto-augmentation that search augmentation strategies in an offline manner, our method formulates the augmentation policy as a parameterized probability distribution, thus allowing its parameters to be optimized jointly with network parameters. Our proposed OHL-Auto-Aug eliminates the need of re-training and dramatically reduces the cost of the overall search process, while establishes significantly accuracy improvements over baseline models. On both CIFAR-10 and ImageNet, our method achieves remarkable on search accuracy, 60x faster on CIFAR-10 and 24x faster on ImageNet, while maintaining competitive accuracies.},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Lin, Chen and Guo, Minghao and Li, Chuming and Yuan, Xin and Wu, Wei and Yan, Junjie and Lin, Dahua and Ouyang, Wanli},
	month = oct,
	year = {2019},
	note = {ISSN: 2380-7504},
	keywords = {Biological system modeling, Computational modeling, Computer architecture, Data models, Optimization, Probability distribution, Training},
	pages = {6578--6587},
}

@inproceedings{tian_improving_2020,
	title = {Improving {Auto}-{Augment} via {Augmentation}-{Wise} {Weight} {Sharing}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/dc49dfebb0b00fd44aeff5c60cc1f825-Abstract.html},
	abstract = {The recent progress on automatically searching augmentation policies has boosted the performance substantially for various tasks.
A key component of automatic augmentation search is the evaluation process for a particular augmentation policy, which is utilized to return reward and usually runs thousands of times.
A plain evaluation process, which includes full model training and validation, would be time-consuming.
To achieve efficiency, many choose to sacrifice evaluation reliability for speed.
In this paper, we dive into the dynamics of augmented training of the model.
This inspires us to design a powerful and efficient proxy task based on the Augmentation-Wise Weight Sharing (AWS) to form a fast yet accurate evaluation process in an elegant way.
Comprehensive analysis verifies the superiority of this approach in terms of effectiveness and efficiency.
The augmentation policies found by our method achieve superior accuracies compared with existing auto-augmentation search methods.
On CIFAR-10, we achieve a top-1 error rate of 1.24\%, which is currently the best performing single model without extra training data.
On ImageNet, we get a top-1 error rate of 20.36\% for ResNet-50, which leads to 3.34\% absolute error rate reduction over the baseline augmentation.},
	urldate = {2022-04-05},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Tian, Keyu and Lin, Chen and Sun, Ming and Zhou, Luping and Yan, Junjie and Ouyang, Wanli},
	year = {2020},
	pages = {19088--19098},
}

@article{zhang_noise_2022,
	title = {Noise {Augmentation} {Is} {All} {You} {Need} {For} {FGSM} {Fast} {Adversarial} {Training}: {Catastrophic} {Overfitting} {And} {Robust} {Overfitting} {Require} {Different} {Augmentation}},
	shorttitle = {Noise {Augmentation} {Is} {All} {You} {Need} {For} {FGSM} {Fast} {Adversarial} {Training}},
	url = {http://arxiv.org/abs/2202.05488},
	abstract = {Adversarial training (AT) and its variants are the most effective approaches for obtaining adversarially robust models. A unique characteristic of AT is that an inner maximization problem needs to be solved repeatedly before the model weights can be updated, which makes the training slow. FGSM AT significantly improves its efficiency but it fails when the step size grows. The SOTA GradAlign makes FGSM AT compatible with a higher step size, however, its regularization on input gradient makes it 3 to 4 times slower than FGSM AT. Our proposed NoiseAug removes the extra computation overhead by directly regularizing on the input itself. The key contribution of this work lies in an empirical finding that single-step FGSM AT is not as hard as suggested in the past line of work: noise augmentation is all you need for (FGSM) fast AT. Towards understanding the success of our NoiseAug, we perform an extensive analysis and find that mitigating Catastrophic Overfitting (CO) and Robust Overfitting (RO) need different augmentations. Instead of more samples caused by data augmentation, we identify what makes NoiseAug effective for preventing CO might lie in its improved local linearity.},
	urldate = {2022-04-04},
	journal = {arXiv:2202.05488 [cs]},
	author = {Zhang, Chaoning and Zhang, Kang and Niu, Axi and Zhang, Chenshuang and Feng, Jiu and Yoo, Chang D. and Kweon, In So},
	month = feb,
	year = {2022},
	note = {arXiv: 2202.05488},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@inproceedings{donhauser_interpolation_2021,
	title = {Interpolation can hurt robust generalization even when there is no noise},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper/2021/hash/c4f2c88e16a579900657c18726641c81-Abstract.html},
	abstract = {Numerous recent works show that overparameterization implicitly reduces variance for min-norm interpolators and max-margin classifiers. These findings suggest that ridge regularization has vanishing benefits in high dimensions.  We challenge this narrative by showing that, even in the absence of noise, avoiding interpolation through ridge regularization can significantly improve generalization.  We prove this phenomenon for the robust risk of both linear regression and classification, and hence provide the first theoretical result on {\textbackslash}emph\{robust overfitting\}.},
	urldate = {2022-04-04},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Donhauser, Konstantin and Tifrea, Alexandru and Aerni, Michael and Heckel, Reinhard and Yang, Fanny},
	year = {2021},
	pages = {23465--23477},
}

@inproceedings{yang_boundary_2020,
	title = {Boundary thickness and robustness in learning models},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/44e76e99b5e194377e955b13fb12f630-Abstract.html},
	abstract = {Robustness of machine learning models to various adversarial and non-adversarial corruptions continues to be of interest. In this paper, we introduce the notion of the boundary thickness of a classifier, and we describe its connection with and usefulness for model robustness. Thick decision boundaries lead to improved performance, while thin decision boundaries lead to overfitting (e.g., measured by the robust generalization gap between training and testing) and lower robustness. We show that a thicker boundary helps improve robustness against adversarial examples (e.g., improving the robust test accuracy of adversarial training), as well as so-called out-of-distribution (OOD) transforms, and we show that many commonly-used regularization and data augmentation procedures can increase boundary thickness. On the theoretical side, we establish that maximizing boundary thickness is akin to minimizing the so-called mixup loss. Using these observations, we can show that noise-augmentation on mixup training further increases boundary thickness, thereby combating vulnerability to various forms of adversarial attacks and OOD transforms. We can also show that the performance improvement in several recent lines of work happens in conjunction with a thicker boundary.},
	urldate = {2022-04-04},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Yang, Yaoqing and Khanna, Rajiv and Yu, Yaodong and Gholami, Amir and Keutzer, Kurt and Gonzalez, Joseph E and Ramchandran, Kannan and Mahoney, Michael W},
	year = {2020},
	pages = {6223--6234},
}

@article{bunk_adversarially_2021,
	title = {Adversarially {Optimized} {Mixup} for {Robust} {Classification}},
	url = {http://arxiv.org/abs/2103.11589},
	abstract = {Mixup is a procedure for data augmentation that trains networks to make smoothly interpolated predictions between datapoints. Adversarial training is a strong form of data augmentation that optimizes for worst-case predictions in a compact space around each data-point, resulting in neural networks that make much more robust predictions. In this paper, we bring these ideas together by adversarially probing the space between datapoints, using projected gradient descent (PGD). The fundamental approach in this work is to leverage backpropagation through the mixup interpolation during training to optimize for places where the network makes unsmooth and incongruous predictions. Additionally, we also explore several modifications and nuances, like optimization of the mixup ratio and geometrical label assignment, and discuss their impact on enhancing network robustness. Through these ideas, we have been able to train networks that robustly generalize better; experiments on CIFAR-10 and CIFAR-100 demonstrate consistent improvements in accuracy against strong adversaries, including the recent strong ensemble attack AutoAttack. Our source code would be released for reproducibility.},
	urldate = {2022-04-04},
	journal = {arXiv:2103.11589 [cs]},
	author = {Bunk, Jason and Chattopadhyay, Srinjoy and Manjunath, B. S. and Chandrasekaran, Shivkumar},
	month = mar,
	year = {2021},
	note = {arXiv: 2103.11589},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{suzuki_teachaugment_2022,
	title = {{TeachAugment}: {Data} {Augmentation} {Optimization} {Using} {Teacher} {Knowledge}},
	shorttitle = {{TeachAugment}},
	url = {http://arxiv.org/abs/2202.12513},
	abstract = {Optimization of image transformation functions for the purpose of data augmentation has been intensively studied. In particular, adversarial data augmentation strategies, which search augmentation maximizing task loss, show significant improvement in the model generalization for many tasks. However, the existing methods require careful parameter tuning to avoid excessively strong deformations that take away image features critical for acquiring generalization. In this paper, we propose a data augmentation optimization method based on the adversarial strategy called TeachAugment, which can produce informative transformed images to the model without requiring careful tuning by leveraging a teacher model. Specifically, the augmentation is searched so that augmented images are adversarial for the target model and recognizable for the teacher model. We also propose data augmentation using neural networks, which simplifies the search space design and allows for updating of the data augmentation using the gradient method. We show that TeachAugment outperforms existing methods in experiments of image classification, semantic segmentation, and unsupervised representation learning tasks.},
	urldate = {2022-04-04},
	journal = {arXiv:2202.12513 [cs]},
	author = {Suzuki, Teppei},
	month = mar,
	year = {2022},
	note = {arXiv: 2202.12513},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{gavrikov_cnn_2022,
	title = {{CNN} {Filter} {DB}: {An} {Empirical} {Investigation} of {Trained} {Convolutional} {Filters}},
	shorttitle = {{CNN} {Filter} {DB}},
	url = {http://arxiv.org/abs/2203.15331},
	abstract = {Currently, many theoretical as well as practically relevant questions towards the transferability and robustness of Convolutional Neural Networks (CNNs) remain unsolved. While ongoing research efforts are engaging these problems from various angles, in most computer vision related cases these approaches can be generalized to investigations of the effects of distribution shifts in image data. In this context, we propose to study the shifts in the learned weights of trained CNN models. Here we focus on the properties of the distributions of dominantly used 3x3 convolution filter kernels. We collected and publicly provide a dataset with over 1.4 billion filters from hundreds of trained CNNs, using a wide range of datasets, architectures, and vision tasks. In a first use case of the proposed dataset, we can show highly relevant properties of many publicly available pre-trained models for practical applications: I) We analyze distribution shifts (or the lack thereof) between trained filters along different axes of meta-parameters, like visual category of the dataset, task, architecture, or layer depth. Based on these results, we conclude that model pre-training can succeed on arbitrary datasets if they meet size and variance conditions. II) We show that many pre-trained models contain degenerated filters which make them less robust and less suitable for fine-tuning on target applications. Data \& Project website: https://github.com/paulgavrikov/cnn-filter-db},
	urldate = {2022-04-04},
	journal = {arXiv:2203.15331 [cs]},
	author = {Gavrikov, Paul and Keuper, Janis},
	month = mar,
	year = {2022},
	note = {arXiv: 2203.15331},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@inproceedings{ho_population_2019,
	title = {Population {Based} {Augmentation}: {Efficient} {Learning} of {Augmentation} {Policy} {Schedules}},
	shorttitle = {Population {Based} {Augmentation}},
	url = {https://proceedings.mlr.press/v97/ho19b.html},
	abstract = {A key challenge in leveraging data augmentation for neural network training is choosing an effective augmentation policy from a large search space of candidate operations. Properly chosen augmentation policies can lead to significant generalization improvements; however, state-of-the-art approaches such as AutoAugment are computationally infeasible to run for the ordinary user. In this paper, we introduce a new data augmentation algorithm, Population Based Augmentation (PBA), which generates nonstationary augmentation policy schedules instead of a fixed augmentation policy. We show that PBA can match the performance of AutoAugment on CIFAR-10, CIFAR-100, and SVHN, with three orders of magnitude less overall compute. On CIFAR-10 we achieve a mean test error of 1.46\%, which is a slight improvement upon the current state-of-the-art. The code for PBA is open source and is available at https://github.com/arcelien/pba.},
	language = {en},
	urldate = {2022-04-03},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Ho, Daniel and Liang, Eric and Chen, Xi and Stoica, Ion and Abbeel, Pieter},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {2731--2741},
}

@inproceedings{liu_data_2020,
	address = {Online},
	title = {Data {Boost}: {Text} {Data} {Augmentation} {Through} {Reinforcement} {Learning} {Guided} {Conditional} {Generation}},
	shorttitle = {Data {Boost}},
	url = {https://aclanthology.org/2020.emnlp-main.726},
	doi = {10.18653/v1/2020.emnlp-main.726},
	abstract = {Data augmentation is proven to be effective in many NLU tasks, especially for those suffering from data scarcity. In this paper, we present a powerful and easy to deploy text augmentation framework, Data Boost, which augments data through reinforcement learning guided conditional generation. We evaluate Data Boost on three diverse text classification tasks under five different classifier architectures. The result shows that Data Boost can boost the performance of classifiers especially in low-resource data scenarios. For instance, Data Boost improves F1 for the three tasks by 8.7\% on average when given only 10\% of the whole data for training. We also compare Data Boost with six prior text augmentation methods. Through human evaluations (N=178), we confirm that Data Boost augmentation has comparable quality as the original data with respect to readability and class consistency.},
	urldate = {2022-04-01},
	booktitle = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Liu, Ruibo and Xu, Guangxuan and Jia, Chenyan and Ma, Weicheng and Wang, Lili and Vosoughi, Soroush},
	month = nov,
	year = {2020},
	pages = {9031--9041},
}

@inproceedings{li_differentiable_2020,
	address = {Cham},
	title = {Differentiable {Automatic} {Data} {Augmentation}},
	isbn = {978-3-030-58542-6},
	doi = {10.1007/978-3-030-58542-6_35},
	abstract = {Data augmentation (DA) techniques aim to increase data variability, and thus train deep networks with better generalisation. The pioneering AutoAugment automated the search for optimal DA policies with reinforcement learning. However, AutoAugment is extremely computationally expensive, limiting its wide applicability. Followup works such as Population Based Augmentation (PBA) and Fast AutoAugment improved efficiency, but their optimization speed remains a bottleneck. In this paper, we propose Differentiable Automatic Data Augmentation (DADA) which dramatically reduces the cost. DADA relaxes the discrete DA policy selection to a differentiable optimization problem via Gumbel-Softmax. In addition, we introduce an unbiased gradient estimator, RELAX, leading to an efficient and effective one-pass optimization strategy to learn an efficient and accurate DA policy. We conduct extensive experiments on CIFAR-10, CIFAR-100, SVHN, and ImageNet datasets. Furthermore, we demonstrate the value of Auto DA in pre-training for downstream detection problems. Results show our DADA is at least one order of magnitude faster than the state-of-the-art while achieving very comparable accuracy. The code is available at https://github.com/VDIGPKU/DADA.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Li, Yonggang and Hu, Guosheng and Wang, Yongtao and Hospedales, Timothy and Robertson, Neil M. and Yang, Yongxin},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	pages = {580--595},
}

@article{machkour_classical_2021,
	title = {Classical and {Deep} {Learning} based {Visual} {Servoing} {Systems}: a {Survey} on {State} of the {Art}},
	volume = {104},
	issn = {1573-0409},
	shorttitle = {Classical and {Deep} {Learning} based {Visual} {Servoing} {Systems}},
	url = {https://doi.org/10.1007/s10846-021-01540-w},
	doi = {10.1007/s10846-021-01540-w},
	abstract = {Computer vision, together with bayesian estimation algorithms, sensors, and actuators, are used in robotics to solve a variety of critical tasks such as localization, obstacle avoidance, and navigation. Classical approaches in visual servoing systems relied on extracting features from images to control robot movements. Now, state of the art computer vision systems use deep neural networks in tasks such as object recognition, detection, segmentation, and tracking. These networks and specialized controllers play a predominant role in the design and implementation of modern visual servoing systems due to their accuracy, flexibility, and adaptability. Recent research in direct systems for visual servoing has created robotic systems capable of relying only on the information contained in the whole image. Furthermore, end-to-end systems learn the control laws during training, eliminating entirely the controller. This paper presents a comprehensive survey on the state of the art in visual servoing systems, discussing the latest classical methods not included in other surveys but emphasizing the new approaches based on deep neural networks and their applications in a broad variety of applications within robotics.},
	language = {en},
	number = {1},
	urldate = {2022-03-31},
	journal = {Journal of Intelligent \& Robotic Systems},
	author = {Machkour, Zakariae and Ortiz-Arroyo, Daniel and Durdevic, Petar},
	month = dec,
	year = {2021},
	pages = {11},
}

@article{wu_survey_2022,
	title = {A survey {Of} learning-{Based} control of robotic visual servoing systems},
	volume = {359},
	issn = {0016-0032},
	url = {https://www.sciencedirect.com/science/article/pii/S0016003221006621},
	doi = {10.1016/j.jfranklin.2021.11.009},
	abstract = {Major difficulties and challenges of modern robotics systems focus on how to give robots self-learning and self-decision-making ability. Visual servoing control strategy is an important strategy of robotic systems to perceive the environment via the vision. The vision can guide new robotic systems to complete more complicated tasks in complex working environments. This survey aims at describing the state-of-the-art learning-based algorithms, especially those algorithms that combine with model predictive control (MPC) used in visual servoing systems, and providing some pioneering and advanced references with several numerical simulations. The general modeling methods of visual servo and the influence of traditional control strategies on robotic visual servoing systems are introduced. The advantages of introducing neural-network-based algorithms and reinforcement-learning-based algorithms into the systems are discussed. Finally, according to the existing research progress and references, the future directions of robotic visual servoing systems are summarized and prospected.},
	language = {en},
	number = {1},
	urldate = {2022-03-31},
	journal = {Journal of the Franklin Institute},
	author = {Wu, Jinhui and Jin, Zhehao and Liu, Andong and Yu, Li and Yang, Fuwen},
	month = jan,
	year = {2022},
	pages = {556--577},
}

@inproceedings{kwon_h2o_2021,
	title = {{H2O}: {Two} {Hands} {Manipulating} {Objects} for {First} {Person} {Interaction} {Recognition}},
	shorttitle = {{H2O}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Kwon_H2O_Two_Hands_Manipulating_Objects_for_First_Person_Interaction_Recognition_ICCV_2021_paper.html},
	language = {en},
	urldate = {2022-03-31},
	author = {Kwon, Taein and Tekin, Bugra and Stühmer, Jan and Bogo, Federica and Pollefeys, Marc},
	year = {2021},
	pages = {10138--10148},
}

@inproceedings{maqueda_event-based_2018,
	title = {Event-{Based} {Vision} {Meets} {Deep} {Learning} on {Steering} {Prediction} for {Self}-{Driving} {Cars}},
	url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Maqueda_Event-Based_Vision_Meets_CVPR_2018_paper.html},
	urldate = {2022-03-22},
	author = {Maqueda, Ana I. and Loquercio, Antonio and Gallego, Guillermo and García, Narciso and Scaramuzza, Davide},
	year = {2018},
	pages = {5419--5427},
}

@article{gallego_event-based_2022,
	title = {Event-{Based} {Vision}: {A} {Survey}},
	volume = {44},
	issn = {1939-3539},
	shorttitle = {Event-{Based} {Vision}},
	doi = {10.1109/TPAMI.2020.3008413},
	abstract = {Event cameras are bio-inspired sensors that differ from conventional frame cameras: Instead of capturing images at a fixed rate, they asynchronously measure per-pixel brightness changes, and output a stream of events that encode the time, location and sign of the brightness changes. Event cameras offer attractive properties compared to traditional cameras: high temporal resolution (in the order of μμs), very high dynamic range (140 dB versus 60 dB), low power consumption, and high pixel bandwidth (on the order of kHz) resulting in reduced motion blur. Hence, event cameras have a large potential for robotics and computer vision in challenging scenarios for traditional cameras, such as low-latency, high speed, and high dynamic range. However, novel methods are required to process the unconventional output of these sensors in order to unlock their potential. This paper provides a comprehensive overview of the emerging field of event-based vision, with a focus on the applications and the algorithms developed to unlock the outstanding properties of event cameras. We present event cameras from their working principle, the actual sensors that are available and the tasks that they have been used for, from low-level vision (feature detection and tracking, optic flow, etc.) to high-level vision (reconstruction, segmentation, recognition). We also discuss the techniques developed to process events, including learning-based techniques, as well as specialized processors for these novel sensors, such as spiking neural networks. Additionally, we highlight the challenges that remain to be tackled and the opportunities that lie ahead in the search for a more efficient, bio-inspired way for machines to perceive and interact with the world.},
	number = {1},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Gallego, Guillermo and Delbrück, Tobi and Orchard, Garrick and Bartolozzi, Chiara and Taba, Brian and Censi, Andrea and Leutenegger, Stefan and Davison, Andrew J. and Conradt, Jörg and Daniilidis, Kostas and Scaramuzza, Davide},
	month = jan,
	year = {2022},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Brightness, Cameras, Event cameras, Retina, Robot vision systems, Voltage control, asynchronous sensor, bio-inspired vision, high dynamic range, low latency, low power},
	pages = {154--180},
}

@inproceedings{gong_maxup_2021,
	title = {{MaxUp}: {Lightweight} {Adversarial} {Training} {With} {Data} {Augmentation} {Improves} {Neural} {Network} {Training}},
	shorttitle = {{MaxUp}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Gong_MaxUp_Lightweight_Adversarial_Training_With_Data_Augmentation_Improves_Neural_Network_CVPR_2021_paper.html?utm_campaign=Akira%27s%20Machine%20Learning%20News%20%28ja%29&utm_medium=email&utm_source=Revue%20newsletter},
	language = {en},
	urldate = {2022-02-11},
	author = {Gong, Chengyue and Ren, Tongzheng and Ye, Mao and Liu, Qiang},
	year = {2021},
	pages = {2474--2483},
}

@article{yuan_adaptive_2021,
	title = {Adaptive {Image} {Transformations} for {Transfer}-based {Adversarial} {Attack}},
	url = {http://arxiv.org/abs/2111.13844},
	abstract = {Adversarial attacks provide a good way to study the robustness of deep learning models. One category of methods in transfer-based black-box attack utilizes several image transformation operations to improve the transferability of adversarial examples, which is effective, but fails to take the specific characteristic of the input image into consideration. In this work, we propose a novel architecture, called Adaptive Image Transformation Learner (AITL), which incorporates different image transformation operations into a unified framework to further improve the transferability of adversarial examples. Unlike the fixed combinational transformations used in existing works, our elaborately designed transformation learner adaptively selects the most effective combination of image transformations specific to the input image. Extensive experiments on ImageNet demonstrate that our method significantly improves the attack success rates on both normally trained models and defense models under various settings.},
	urldate = {2022-02-08},
	journal = {arXiv:2111.13844 [cs]},
	author = {Yuan, Zheng and Zhang, Jie and Shan, Shiguang},
	month = nov,
	year = {2021},
	note = {arXiv: 2111.13844},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{shu_adversarial_2021,
	title = {Adversarial {Differentiable} {Data} {Augmentation} for {Autonomous} {Systems}},
	doi = {10.1109/ICRA48506.2021.9561205},
	abstract = {Autonomous systems often rely on neural networks to achieve high performance on planning and control problems. Unfortunately, neural networks suffer severely when input images become degraded in ways that are not reflected in the training data. This is particularly problematic for robotic systems like autonomous vehicles (AV) for which reliability is paramount. In this work, we consider robust optimization methods for hardening control systems against image corruptions and other unexpected domain shifts. Recent work on robust optimization for neural nets has been focused largely on combating adversarial attacks. In this work, we borrow ideas from the adversarial training and data augmentation literature to enhance robustness to image corruptions and domain shifts. To this end, we train networks while augmenting image data with a battery of image degradations. Unlike traditional augmentation methods, we choose the parameters for each degradation adversarially so as to maximize system performance. By formulating image degradations in a way that is differentiable with respect to degradation parameters, we enable the use of efficient optimization methods (PGD) for choosing worst-case augmentation parameters. We demonstrate the efficacy of this method on the learning to steer task for AVs. By adversarially training against image corruptions, we produce networks that are highly robust to image corruptions. We show that the proposed differentiable augmentation schemes result in higher levels of robustness and accuracy for a range of settings as compared to baseline and state-of-the-art augmentation methods.},
	booktitle = {2021 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Shu, Manli and Shen, Yu and Lin, Ming C. and Goldstein, Tom},
	month = may,
	year = {2021},
	note = {ISSN: 2577-087X},
	keywords = {Data models, Degradation, Neural networks, Optimization methods, Robustness, Training, Training data},
	pages = {14069--14075},
}

@inproceedings{perez_enhancing_2021,
	title = {Enhancing {Adversarial} {Robustness} via {Test}-{Time} {Transformation} {Ensembling}},
	url = {https://openaccess.thecvf.com/content/ICCV2021W/AROW/html/Perez_Enhancing_Adversarial_Robustness_via_Test-Time_Transformation_Ensembling_ICCVW_2021_paper.html},
	language = {en},
	urldate = {2022-02-08},
	author = {Pérez, Juan C. and Alfarra, Motasem and Jeanneret, Guillaume and Rueda, Laura and Thabet, Ali and Ghanem, Bernard and Arbeláez, Pablo},
	year = {2021},
	pages = {81--91},
}

@inproceedings{tang_onlineaugment_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{OnlineAugment}: {Online} {Data} {Augmentation} with {Less} {Domain} {Knowledge}},
	isbn = {978-3-030-58571-6},
	shorttitle = {{OnlineAugment}},
	doi = {10.1007/978-3-030-58571-6_19},
	abstract = {Data augmentation is one of the most important tools in training modern deep neural networks. Recently, great advances have been made in searching for optimal augmentation policies in the image classification domain. However, two key points related to data augmentation remain uncovered by the current methods. First is that most if not all modern augmentation search methods are offline and learning policies are isolated from their usage. The learned policies are mostly constant throughout the training process and are not adapted to the current training model state. Second, the policies rely on class-preserving image processing functions. Hence applying current offline methods to new tasks may require domain knowledge to specify such kind of operations. In this work, we offer an orthogonal online data augmentation scheme together with three new augmentation networks, co-trained with the target learning task. It is both more efficient, in the sense that it does not require expensive offline training when entering a new domain, and more adaptive as it adapts to the learner state. Our augmentation networks require less domain knowledge and are easily applicable to new tasks. Extensive experiments demonstrate that the proposed scheme alone performs on par with the state-of-the-art offline data augmentation methods, as well as improving upon the state-of-the-art in combination with those methods.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Tang, Zhiqiang and Gao, Yunhe and Karlinsky, Leonid and Sattigeri, Prasanna and Feris, Rogerio and Metaxas, Dimitris},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	pages = {313--329},
}

@inproceedings{volpi_generalizing_2018,
	title = {Generalizing to {Unseen} {Domains} via {Adversarial} {Data} {Augmentation}},
	volume = {31},
	url = {https://proceedings.neurips.cc/paper/2018/hash/1d94108e907bb8311d8802b48fd54b4a-Abstract.html},
	urldate = {2022-02-08},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Volpi, Riccardo and Namkoong, Hongseok and Sener, Ozan and Duchi, John C and Murino, Vittorio and Savarese, Silvio},
	year = {2018},
}

@article{guo_connections_2021,
	title = {On {Connections} {Between} {Regularizations} for {Improving} {DNN} {Robustness}},
	volume = {43},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2020.3006917},
	abstract = {This paper analyzes regularization terms proposed recently for improving the adversarial robustness of deep neural networks (DNNs), from a theoretical point of view. Specifically, we study possible connections between several effective methods, including input-gradient regularization, Jacobian regularization, curvature regularization, and a cross-Lipschitz functional. We investigate them on DNNs with general rectified linear activations, which constitute one of the most prevalent families of models for image classification and a host of other machine learning applications. We shed light on essential ingredients of these regularizations and re-interpret their functionality. Through the lens of our study, more principled and efficient regularizations can possibly be invented in the near future.},
	number = {12},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Guo, Yiwen and Chen, Long and Chen, Yurong and Zhang, Changshui},
	month = dec,
	year = {2021},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Computational modeling, Deep neural networks, Jacobian matrices, Neural networks, Perturbation methods, Robustness, Task analysis, Training data, adversarial robustness, network property, regularizations},
	pages = {4469--4476},
}

@inproceedings{cisse_parseval_2017,
	title = {Parseval {Networks}: {Improving} {Robustness} to {Adversarial} {Examples}},
	shorttitle = {Parseval {Networks}},
	url = {https://proceedings.mlr.press/v70/cisse17a.html},
	abstract = {We introduce Parseval networks, a form of deep neural networks in which the Lipschitz constant of linear, convolutional and aggregation layers is constrained to be smaller than \$1\$. Parseval networks are empirically and theoretically motivated by an analysis of the robustness of the predictions made by deep neural networks when their input is subject to an adversarial perturbation. The most important feature of Parseval networks is to maintain weight matrices of linear and convolutional layers to be (approximately) Parseval tight frames, which are extensions of orthogonal matrices to non-square matrices. We describe how these constraints can be maintained efficiently during SGD. We show that Parseval networks match the state-of-the-art regarding accuracy on CIFAR-10/100 and Street View House Numbers (SVHN), while being more robust than their vanilla counterpart against adversarial examples. Incidentally, Parseval networks also tend to train faster and make a better usage of the full capacity of the networks.},
	language = {en},
	urldate = {2022-01-25},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Cisse, Moustapha and Bojanowski, Piotr and Grave, Edouard and Dauphin, Yann and Usunier, Nicolas},
	month = jul,
	year = {2017},
	note = {ISSN: 2640-3498},
	pages = {854--863},
}

@inproceedings{kaushik_explaining_2021,
	title = {Explaining the {Efficacy} of {Counterfactually} {Augmented} {Data}},
	url = {https://openreview.net/forum?id=HHiiQKWsOcV},
	abstract = {In attempts to produce machine learning models less reliant on spurious patterns in NLP datasets, researchers have recently proposed curating counterfactually augmented data (CAD) via a...},
	language = {en},
	urldate = {2021-12-19},
	author = {Kaushik, Divyansh and Setlur, Amrith and Hovy, Eduard H. and Lipton, Zachary Chase},
	year = {2021},
}

@article{wang_image_2004,
	title = {Image quality assessment: from error visibility to structural similarity},
	volume = {13},
	issn = {1941-0042},
	shorttitle = {Image quality assessment},
	doi = {10.1109/TIP.2003.819861},
	abstract = {Objective methods for assessing perceptual image quality traditionally attempted to quantify the visibility of errors (differences) between a distorted image and a reference image using a variety of known properties of the human visual system. Under the assumption that human visual perception is highly adapted for extracting structural information from a scene, we introduce an alternative complementary framework for quality assessment based on the degradation of structural information. As a specific example of this concept, we develop a structural similarity index and demonstrate its promise through a set of intuitive examples, as well as comparison to both subjective ratings and state-of-the-art objective methods on a database of images compressed with JPEG and JPEG2000. A MATLAB implementation of the proposed algorithm is available online at http://www.cns.nyu.edu//spl sim/lcv/ssim/.},
	number = {4},
	journal = {IEEE Transactions on Image Processing},
	author = {Wang, Zhou and Bovik, A.C. and Sheikh, H.R. and Simoncelli, E.P.},
	month = apr,
	year = {2004},
	note = {Conference Name: IEEE Transactions on Image Processing},
	keywords = {Data mining, Degradation, Humans, Image quality, Indexes, Layout, Quality assessment, Transform coding, Visual perception, Visual system},
	pages = {600--612},
}

@article{inoue_data_2018,
	title = {Data {Augmentation} by {Pairing} {Samples} for {Images} {Classification}},
	url = {http://arxiv.org/abs/1801.02929},
	abstract = {Data augmentation is a widely used technique in many machine learning tasks, such as image classification, to virtually enlarge the training dataset size and avoid overfitting. Traditional data augmentation techniques for image classification tasks create new samples from the original training data by, for example, flipping, distorting, adding a small amount of noise to, or cropping a patch from an original image. In this paper, we introduce a simple but surprisingly effective data augmentation technique for image classification tasks. With our technique, named SamplePairing, we synthesize a new sample from one image by overlaying another image randomly chosen from the training data (i.e., taking an average of two images for each pixel). By using two images randomly selected from the training set, we can generate \$N{\textasciicircum}2\$ new samples from \$N\$ training samples. This simple data augmentation technique significantly improved classification accuracy for all the tested datasets; for example, the top-1 error rate was reduced from 33.5\% to 29.0\% for the ILSVRC 2012 dataset with GoogLeNet and from 8.22\% to 6.93\% in the CIFAR-10 dataset. We also show that our SamplePairing technique largely improved accuracy when the number of samples in the training set was very small. Therefore, our technique is more valuable for tasks with a limited amount of training data, such as medical imaging tasks.},
	urldate = {2022-01-10},
	journal = {arXiv:1801.02929 [cs, stat]},
	author = {Inoue, Hiroshi},
	month = apr,
	year = {2018},
	note = {arXiv: 1801.02929},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{sun_can_2021,
	title = {Can {Shape} {Structure} {Features} {Improve} {Model} {Robustness} {Under} {Diverse} {Adversarial} {Settings}?},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Sun_Can_Shape_Structure_Features_Improve_Model_Robustness_Under_Diverse_Adversarial_ICCV_2021_paper.html},
	language = {en},
	urldate = {2022-01-08},
	author = {Sun, Mingjie and Li, Zichao and Xiao, Chaowei and Qiu, Haonan and Kailkhura, Bhavya and Liu, Mingyan and Li, Bo},
	year = {2021},
	pages = {7526--7535},
}

@inproceedings{li_towards_2021,
	title = {Towards {Robustness} of {Deep} {Neural} {Networks} via {Regularization}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Li_Towards_Robustness_of_Deep_Neural_Networks_via_Regularization_ICCV_2021_paper.html},
	language = {en},
	urldate = {2022-01-08},
	author = {Li, Yao and Min, Martin Renqiang and Lee, Thomas and Yu, Wenchao and Kruus, Erik and Wang, Wei and Hsieh, Cho-Jui},
	year = {2021},
	pages = {7496--7505},
}

@inproceedings{huang_stochastic_2021,
	title = {Stochastic {Partial} {Swap}: {Enhanced} {Model} {Generalization} and {Interpretability} for {Fine}-{Grained} {Recognition}},
	shorttitle = {Stochastic {Partial} {Swap}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Huang_Stochastic_Partial_Swap_Enhanced_Model_Generalization_and_Interpretability_for_Fine-Grained_ICCV_2021_paper.html},
	language = {en},
	urldate = {2022-01-08},
	author = {Huang, Shaoli and Wang, Xinchao and Tao, Dacheng},
	year = {2021},
	pages = {620--629},
}

@inproceedings{li_scouter_2021,
	title = {{SCOUTER}: {Slot} {Attention}-{Based} {Classifier} for {Explainable} {Image} {Recognition}},
	shorttitle = {{SCOUTER}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Li_SCOUTER_Slot_Attention-Based_Classifier_for_Explainable_Image_Recognition_ICCV_2021_paper.html},
	language = {en},
	urldate = {2022-01-07},
	author = {Li, Liangzhi and Wang, Bowen and Verma, Manisha and Nakashima, Yuta and Kawasaki, Ryo and Nagahara, Hajime},
	year = {2021},
	pages = {1046--1055},
}

@inproceedings{islam_global_2021,
	title = {Global {Pooling}, {More} {Than} {Meets} the {Eye}: {Position} {Information} {Is} {Encoded} {Channel}-{Wise} in {CNNs}},
	shorttitle = {Global {Pooling}, {More} {Than} {Meets} the {Eye}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Islam_Global_Pooling_More_Than_Meets_the_Eye_Position_Information_Is_ICCV_2021_paper.html},
	language = {en},
	urldate = {2022-01-07},
	author = {Islam, Md Amirul and Kowal, Matthew and Jia, Sen and Derpanis, Konstantinos G. and Bruce, Neil D. B.},
	year = {2021},
	pages = {793--801},
}

@inproceedings{zhu_toward_2021,
	title = {Toward {Human}-{Like} {Grasp}: {Dexterous} {Grasping} via {Semantic} {Representation} of {Object}-{Hand}},
	shorttitle = {Toward {Human}-{Like} {Grasp}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Zhu_Toward_Human-Like_Grasp_Dexterous_Grasping_via_Semantic_Representation_of_Object-Hand_ICCV_2021_paper.html},
	language = {en},
	urldate = {2022-01-05},
	author = {Zhu, Tianqiang and Wu, Rina and Lin, Xiangbo and Sun, Yi},
	year = {2021},
	pages = {15741--15751},
}

@inproceedings{ruiz_generating_2021,
	title = {Generating {Attribution} {Maps} {With} {Disentangled} {Masked} {Backpropagation}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Ruiz_Generating_Attribution_Maps_With_Disentangled_Masked_Backpropagation_ICCV_2021_paper.html},
	language = {en},
	urldate = {2022-01-05},
	author = {Ruiz, Adria and Agudo, Antonio and Moreno-Noguer, Francesc},
	year = {2021},
	pages = {905--914},
}

@inproceedings{lang_explaining_2021,
	title = {Explaining in {Style}: {Training} a {GAN} {To} {Explain} a {Classifier} in {StyleSpace}},
	shorttitle = {Explaining in {Style}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Lang_Explaining_in_Style_Training_a_GAN_To_Explain_a_Classifier_ICCV_2021_paper.html},
	language = {en},
	urldate = {2022-01-05},
	author = {Lang, Oran and Gandelsman, Yossi and Yarom, Michal and Wald, Yoav and Elidan, Gal and Hassidim, Avinatan and Freeman, William T. and Isola, Phillip and Globerson, Amir and Irani, Michal and Mosseri, Inbar},
	year = {2021},
	pages = {693--702},
}

@article{chen_deep_2021,
	title = {Deep reinforcement learning based moving object grasping},
	volume = {565},
	issn = {0020-0255},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025521001158},
	doi = {10.1016/j.ins.2021.01.077},
	abstract = {Traditional grasping methods for locating unpredictable positions of moving objects under an unstructured environment cannot achieve good performance. This paper studies the utilization of deep reinforcement learning (DRL) with a Kinect depth sensor to resolve this challenging problem. The proposed grasping system integrates the DRL algorithm, Soft-Actor-Critic, and object detection techniques to implement an approaching-tracking-grasping scheme. Considering the state and action space for the high-degree-of-freedom manipulator, we employ an improved Soft-Actor-Critic algorithm to speed up the learning process. The proposed system can decouple object detection from the DRL control, which allows us to generalize the framework from a simulation environment to a real robot. Experimental results demonstrate that the developed system can autonomously grasp a moving object with different moving trajectories.},
	language = {en},
	urldate = {2022-01-05},
	journal = {Information Sciences},
	author = {Chen, Pengzhan and Lu, Weiqing},
	month = jul,
	year = {2021},
	keywords = {Grasping planning, Moving object, Object detection, Soft-Actor-Critic algorithm},
	pages = {62--76},
}

@article{carneiro_robot_2021,
	title = {Robot {Anticipation} {Learning} {System} for {Ball} {Catching}},
	volume = {10},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/2218-6581/10/4/113},
	doi = {10.3390/robotics10040113},
	abstract = {Catching flying objects is a challenging task in human–robot interaction. Traditional techniques predict the intersection position and time using the information obtained during the free-flying ball motion. A common pain point in these systems is the short ball flight time and uncertainties in the ball’s trajectory estimation. In this paper, we present the Robot Anticipation Learning System (RALS) that accounts for the information obtained from observation of the thrower’s hand motion before the ball is released. RALS takes extra time for the robot to start moving in the direction of the target before the opponent finishes throwing. To the best of our knowledge, this is the first robot control system for ball-catching with anticipation skills. Our results show that the information fused from both throwing and flying motions improves the ball-catching rate by up to 20\% compared to the baseline approach, with the predictions relying only on the information acquired during the flight phase.},
	language = {en},
	number = {4},
	urldate = {2022-01-05},
	journal = {Robotics},
	author = {Carneiro, Diogo and Silva, Filipe and Georgieva, Petia},
	month = dec,
	year = {2021},
	note = {Number: 4
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {anticipation learning, ball catching, human–robot interaction, neural network, trajectory prediction},
	pages = {113},
}

@inproceedings{yu_neural_2021,
	title = {Neural {Motion} {Prediction} for {In}-flight {Uneven} {Object} {Catching}},
	doi = {10.1109/IROS51168.2021.9635983},
	abstract = {In-flight objects capture is extremely challenging. The robot is required to complete trajectory prediction, interception position calculation and motion planning within tens of milliseconds. As in-flight uneven objects are affected by various kinds of forces, which leads to the time-varying acceleration, motion prediction for them is difficult. In order to compensate the system’s non-linearity, we propose using a recurrent neural network model, which we call the Neural Acceleration Estimator (NAE), to estimate the varying acceleration by observing a small fragment of previous deflected trajectory without any prior information. Moreover, end-to-end training with Differantiable Filter (NAE-DF) gives a supervision for measurement uncertainty and further improves the prediction accuracy. Experimental results show that motion prediction with NAE and NAE-DF is superior to other methods and has a good generalization performance on unseen objects. We test our methods on a robot, performing velocity control in real world and respectively achieve 83.3\% and 86.7\% success rate on a ploy urethane banana and a gourd. We also release an object in-flight dataset containing 1,500 trajectorys for uneven objects, which can be found on the project website:https://sites.google.com/view/neural-motion-prediction.},
	booktitle = {2021 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Yu, Hongxiang and Guo, Dashun and Yin, Huan and Chen, Anzhe and Xu, Kechun and Chen, Zexi and Wang, Minhang and Tan, Qimeng and Wang, Yue and Xiong, Rong},
	month = sep,
	year = {2021},
	note = {ISSN: 2153-0866},
	keywords = {Accelerometers, Measurement uncertainty, Predictive models, Robot kinematics, Training, Uncertainty, Velocity control},
	pages = {4662--4669},
}

@article{kim_catching_2014,
	title = {Catching {Objects} in {Flight}},
	volume = {30},
	issn = {1941-0468},
	doi = {10.1109/TRO.2014.2316022},
	abstract = {We address the difficult problem of catching in-flight objects with uneven shapes. This requires the solution of three complex problems: accurate prediction of the trajectory of fastmoving objects, predicting the feasible catching configuration, and planning the arm motion, and all within milliseconds. We follow a programming-by-demonstration approach in order to learn, from throwing examples, models of the object dynamics and arm movement. We propose a new methodology to find a feasible catching configuration in a probabilistic manner. We use the dynamical systems approach to encode motion from several demonstrations. This enables a rapid and reactive adaptation of the arm motion in the presence of sensor uncertainty. We validate the approach in simulation with the iCub humanoid robot and in real-world experiments with the KUKA LWR 4+ (7-degree-of-freedom arm robot) to catch a hammer, a tennis racket, an empty bottle, a partially filled bottle, and a cardboard box.},
	number = {5},
	journal = {IEEE Transactions on Robotics},
	author = {Kim, Seungsu and Shukla, Ashwini and Billard, Aude},
	month = oct,
	year = {2014},
	note = {Conference Name: IEEE Transactions on Robotics},
	keywords = {Aerospace electronics, Catching, Dynamics, Gaussian mixture model, Grasping, Robot kinematics, Robot sensing systems, Trajectory, machine learning, robot control, support vector machines},
	pages = {1049--1065},
}

@inproceedings{luo_catching_2021,
	address = {New York, NY, USA},
	series = {{MIG} '21},
	title = {Catching and {Throwing} {Control} of a {Physically} {Simulated} {Hand}},
	isbn = {978-1-4503-9131-3},
	url = {https://doi.org/10.1145/3487983.3488300},
	doi = {10.1145/3487983.3488300},
	abstract = {We design a nominal controller for animating an articulated physics-based human arm model, including the hands and fingers, to catch and throw objects. The controller is based on a finite state machine that defines the target poses for proportional-derivative control of the hand, as well as the orientation and position of the center of the palm using the solution of an inverse kinematics solver. We then use reinforcement learning to train agents to improve the robustness of the nominal controller for achieving many different goals. Imitation learning based on trajectories output by a numerical optimization is used to accelerate the training process. The success of our controllers is demonstrated by a variety of throwing and catching tasks, including flipping objects, hitting targets, and throwing objects to a desired height, and for several different objects, such as cans, spheres, and rods. We also discuss ways to extend our approach so that more challenging tasks, such as juggling, may be accomplished.},
	urldate = {2022-01-04},
	booktitle = {Motion, {Interaction} and {Games}},
	publisher = {Association for Computing Machinery},
	author = {Luo, Yunhao and Xie, Kaixiang and Andrews, Sheldon and Kry, Paul},
	month = nov,
	year = {2021},
	keywords = {catching, grasping, hand simulation, physics-based animation, throwing},
	pages = {1--7},
}

@inproceedings{baxter_exploring_2021,
	title = {Exploring {Learning} for {Intercepting} {Projectiles} with a {Robot}-{Held} {Stick}},
	doi = {10.1109/IROS51168.2021.9635993},
	abstract = {For many tasks, including table tennis, catching, and sword fighting, a critical step is intercepting the incoming object with a robot arm or held tool. Solutions to robot arm interception via learning, specifically reinforcement learning (RL), have become prevalent, as they provide robust solutions to the robot arm interception problem, even for high degree of freedom robotic systems. Despite numerous solutions, there has been little exploration into the factors of learning that impact solution quality. Thus, there is little insight into what problem features lead to better learning success. In this paper, we explore the parameters that impact solution quality. We find that link position observations outperform joint angle observations in terms of learning speed, performance, ability to utilize more than one frame of observation, and generalization to situations not trained for. These results are immediately applicable to RL for robot arm interception tasks.},
	booktitle = {2021 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Baxter, John E. G. and Adamson, Torin and Sugaya, Satomi and Tapia, Lydia},
	month = sep,
	year = {2021},
	note = {ISSN: 2153-0866},
	keywords = {Arms, Kinematics, Manipulators, Neurons, Projectiles, Reinforcement learning, Task analysis},
	pages = {369--376},
}

@article{maurice_human_2019,
	title = {Human movement and ergonomics: {An} industry-oriented dataset for collaborative robotics},
	abstract = {Improving work conditions in industry is a major challenge that can be addressed with new emerging technologies such as collaborative robots. Machine learning techniques can improve the performance of those robots, by endowing them with a degree of awareness of the human state and ergonomics condition. The availability of appropriate datasets to learn models and test prediction and control algorithms, however, remains an issue. This article presents a dataset of human motions in industry-like activities, fully labeled according to the ergonomics assessment worksheet EAWS, widely used in industries such as car manufacturing. Thirteen participants performed several series of activities, such as screwing and manipulating loads under different conditions, resulting in more than 5 hours of data. The dataset contains the participants’ whole-body kinematics recorded both with wearable inertial sensors and marker-based optical motion capture, finger pressure force, video recordings, and annotations by three independent annotators of the performed action and the adopted posture following the EAWS postural grid. Sensor data are available in different formats to facilitate their reuse. The dataset is intended for use by researchers developing algorithms for classifying, predicting, or evaluating human motion in industrial settings, as well as researchers developing collaborative robotics solutions that aim at improving the workers’ ergonomics. The annotation of the whole dataset following an ergonomics standard makes it valuable for ergonomics-related applications, but we expect its use to be broader in the robotics, machine learning, and human movement communities.},
	language = {en},
	journal = {The International Journal of Robotics Research},
	author = {Maurice, Pauline and Malaisé, Adrien and Amiot, Clélie and Paris, Nicolas and Richard, Guy-Junior and Rochel, Olivier and Ivaldi, Serena},
	year = {2019},
	pages = {9},
}

@inproceedings{zeng_visual_2020,
	title = {Visual {Reaction}: {Learning} to {Play} {Catch} {With} {Your} {Drone}},
	shorttitle = {Visual {Reaction}},
	url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Zeng_Visual_Reaction_Learning_to_Play_Catch_With_Your_Drone_CVPR_2020_paper.html},
	urldate = {2022-01-05},
	author = {Zeng, Kuo-Hao and Mottaghi, Roozbeh and Weihs, Luca and Farhadi, Ali},
	year = {2020},
	pages = {11573--11582},
}

@inproceedings{kim_keep_2021,
	title = {Keep {CALM} and {Improve} {Visual} {Feature} {Attribution}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Kim_Keep_CALM_and_Improve_Visual_Feature_Attribution_ICCV_2021_paper.html},
	language = {en},
	urldate = {2022-01-04},
	author = {Kim, Jae Myung and Choe, Junsuk and Akata, Zeynep and Oh, Seong Joon},
	year = {2021},
	pages = {8350--8360},
}

@inproceedings{jung_towards_2021,
	title = {Towards {Better} {Explanations} of {Class} {Activation} {Mapping}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Jung_Towards_Better_Explanations_of_Class_Activation_Mapping_ICCV_2021_paper.html},
	language = {en},
	urldate = {2022-01-04},
	author = {Jung, Hyungsik and Oh, Youngrock},
	year = {2021},
	pages = {1336--1344},
}

@inproceedings{benz_batch_2021,
	title = {Batch {Normalization} {Increases} {Adversarial} {Vulnerability} and {Decreases} {Adversarial} {Transferability}: {A} {Non}-{Robust} {Feature} {Perspective}},
	shorttitle = {Batch {Normalization} {Increases} {Adversarial} {Vulnerability} and {Decreases} {Adversarial} {Transferability}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Benz_Batch_Normalization_Increases_Adversarial_Vulnerability_and_Decreases_Adversarial_Transferability_A_ICCV_2021_paper.html},
	language = {en},
	urldate = {2022-01-04},
	author = {Benz, Philipp and Zhang, Chaoning and Kweon, In So},
	year = {2021},
	pages = {7818--7827},
}

@inproceedings{stutz_relating_2021,
	title = {Relating {Adversarially} {Robust} {Generalization} to {Flat} {Minima}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Stutz_Relating_Adversarially_Robust_Generalization_to_Flat_Minima_ICCV_2021_paper.html},
	language = {en},
	urldate = {2022-01-04},
	author = {Stutz, David and Hein, Matthias and Schiele, Bernt},
	year = {2021},
	pages = {7807--7817},
}

@inproceedings{chockler_explanations_2021,
	title = {Explanations for {Occluded} {Images}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Chockler_Explanations_for_Occluded_Images_ICCV_2021_paper.html},
	language = {en},
	urldate = {2022-01-04},
	author = {Chockler, Hana and Kroening, Daniel and Sun, Youcheng},
	year = {2021},
	pages = {1234--1243},
}

@inproceedings{li_discover_2021,
	title = {Discover the {Unknown} {Biased} {Attribute} of an {Image} {Classifier}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Li_Discover_the_Unknown_Biased_Attribute_of_an_Image_Classifier_ICCV_2021_paper.html},
	language = {en},
	urldate = {2022-01-04},
	author = {Li, Zhiheng and Xu, Chenliang},
	year = {2021},
	pages = {14970--14979},
}

@inproceedings{lerman_explaining_2021,
	title = {Explaining {Local}, {Global}, and {Higher}-{Order} {Interactions} in {Deep} {Learning}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Lerman_Explaining_Local_Global_and_Higher-Order_Interactions_in_Deep_Learning_ICCV_2021_paper.html},
	language = {en},
	urldate = {2022-01-04},
	author = {Lerman, Samuel and Venuto, Charles and Kautz, Henry and Xu, Chenliang},
	year = {2021},
	pages = {1224--1233},
}

@inproceedings{rodriguez_beyond_2021,
	title = {Beyond {Trivial} {Counterfactual} {Explanations} {With} {Diverse} {Valuable} {Explanations}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Rodriguez_Beyond_Trivial_Counterfactual_Explanations_With_Diverse_Valuable_Explanations_ICCV_2021_paper.html},
	language = {en},
	urldate = {2022-01-04},
	author = {Rodríguez, Pau and Caccia, Massimo and Lacoste, Alexandre and Zamparo, Lee and Laradji, Issam and Charlin, Laurent and Vazquez, David},
	year = {2021},
	pages = {1056--1065},
}

@inproceedings{zhu_towards_2021,
	title = {Towards {Understanding} the {Generative} {Capability} of {Adversarially} {Robust} {Classifiers}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Zhu_Towards_Understanding_the_Generative_Capability_of_Adversarially_Robust_Classifiers_ICCV_2021_paper.html},
	language = {en},
	urldate = {2022-01-04},
	author = {Zhu, Yao and Ma, Jiacheng and Sun, Jiacheng and Chen, Zewei and Jiang, Rongxin and Chen, Yaowu and Li, Zhenguo},
	year = {2021},
	pages = {7728--7737},
}

@inproceedings{sariyildiz_concept_2021,
	title = {Concept {Generalization} in {Visual} {Representation} {Learning}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Sariyildiz_Concept_Generalization_in_Visual_Representation_Learning_ICCV_2021_paper.html},
	language = {en},
	urldate = {2022-01-04},
	author = {Sariyildiz, Mert Bulent and Kalantidis, Yannis and Larlus, Diane and Alahari, Karteek},
	year = {2021},
	pages = {9629--9639},
}

@inproceedings{wang_causal_2021,
	title = {Causal {Attention} for {Unbiased} {Visual} {Recognition}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Wang_Causal_Attention_for_Unbiased_Visual_Recognition_ICCV_2021_paper.html},
	language = {en},
	urldate = {2022-01-04},
	author = {Wang, Tan and Zhou, Chang and Sun, Qianru and Zhang, Hanwang},
	year = {2021},
	pages = {3091--3100},
}

@article{lingchen_uniformaugment_2020,
	title = {{UniformAugment}: {A} {Search}-free {Probabilistic} {Data} {Augmentation} {Approach}},
	shorttitle = {{UniformAugment}},
	url = {http://arxiv.org/abs/2003.14348},
	abstract = {Augmenting training datasets has been shown to improve the learning effectiveness for several computer vision tasks. A good augmentation produces an augmented dataset that adds variability while retaining the statistical properties of the original dataset. Some techniques, such as AutoAugment and Fast AutoAugment, have introduced a search phase to find a set of suitable augmentation policies for a given model and dataset. This comes at the cost of great computational overhead, adding up to several thousand GPU hours. More recently RandAugment was proposed to substantially speedup the search phase by approximating the search space by a couple of hyperparameters, but still incurring non-negligible cost for tuning those. In this paper we show that, under the assumption that the augmentation space is approximately distribution invariant, a uniform sampling over the continuous space of augmentation transformations is sufficient to train highly effective models. Based on that result we propose UniformAugment, an automated data augmentation approach that completely avoids a search phase. In addition to discussing the theoretical underpinning supporting our approach, we also use the standard datasets, as well as established models for image classification, to show that UniformAugment's effectiveness is comparable to the aforementioned methods, while still being highly efficient by virtue of not requiring any search.},
	urldate = {2022-01-04},
	journal = {arXiv:2003.14348 [cs]},
	author = {LingChen, Tom Ching and Khonsari, Ava and Lashkari, Amirreza and Nazari, Mina Rafi and Sambee, Jaspreet Singh and Nascimento, Mario A.},
	month = mar,
	year = {2020},
	note = {arXiv: 2003.14348},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@inproceedings{tran_bayesian_2017,
	title = {A {Bayesian} {Data} {Augmentation} {Approach} for {Learning} {Deep} {Models}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper/2017/hash/076023edc9187cf1ac1f1163470e479a-Abstract.html},
	urldate = {2021-12-24},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Tran, Toan and Pham, Trung and Carneiro, Gustavo and Palmer, Lyle and Reid, Ian},
	year = {2017},
}

@inproceedings{ye_h2o_2021,
	title = {{H2O}: {A} {Benchmark} for {Visual} {Human}-human {Object} {Handover} {Analysis}},
	shorttitle = {{H2O}},
	url = {http://arxiv.org/abs/2104.11466},
	abstract = {Object handover is a common human collaboration behavior that attracts attention from researchers in Robotics and Cognitive Science. Though visual perception plays an important role in the object handover task, the whole handover process has been specifically explored. In this work, we propose a novel rich-annotated dataset, H2O, for visual analysis of human-human object handovers. The H2O, which contains 18K video clips involving 15 people who hand over 30 objects to each other, is a multi-purpose benchmark. It can support several vision-based tasks, from which, we specifically provide a baseline method, RGPNet, for a less-explored task named Receiver Grasp Prediction. Extensive experiments show that the RGPNet can produce plausible grasps based on the giver's hand-object states in the pre-handover phase. Besides, we also report the hand and object pose errors with existing baselines and show that the dataset can serve as the video demonstrations for robot imitation learning on the handover task. Dataset, model and code will be made public.},
	urldate = {2021-12-24},
	booktitle = {International {Conference} on {Computer} {Vision}},
	author = {Ye, Ruolin and Xu, Wenqiang and Xue, Zhendong and Tang, Tutian and Wang, Yanfeng and Lu, Cewu},
	month = apr,
	year = {2021},
	note = {arXiv: 2104.11466},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@inproceedings{katharopoulos_not_2018,
	title = {Not {All} {Samples} {Are} {Created} {Equal}: {Deep} {Learning} with {Importance} {Sampling}},
	shorttitle = {Not {All} {Samples} {Are} {Created} {Equal}},
	url = {https://proceedings.mlr.press/v80/katharopoulos18a.html},
	abstract = {Deep Neural Network training spends most of the computation on examples that are properly handled, and could be ignored. We propose to mitigate this phenomenon with a principled importance sampling scheme that focuses computation on "informative" examples, and reduces the variance of the stochastic gradients during training. Our contribution is twofold: first, we derive a tractable upper bound to the per-sample gradient norm, and second we derive an estimator of the variance reduction achieved with importance sampling, which enables us to switch it on when it will result in an actual speedup. The resulting scheme can be used by changing a few lines of code in a standard SGD procedure, and we demonstrate experimentally on image classification, CNN fine-tuning, and RNN training, that for a fixed wall-clock time budget, it provides a reduction of the train losses of up to an order of magnitude and a relative improvement of test errors between 5\% and 17\%.},
	language = {en},
	urldate = {2021-12-20},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Katharopoulos, Angelos and Fleuret, Francois},
	month = jul,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {2525--2534},
}

@article{he_data_2019,
	title = {Data {Augmentation} {Revisited}: {Rethinking} the {Distribution} {Gap} between {Clean} and {Augmented} {Data}},
	shorttitle = {Data {Augmentation} {Revisited}},
	url = {http://arxiv.org/abs/1909.09148},
	abstract = {Data augmentation has been widely applied as an effective methodology to improve generalization in particular when training deep neural networks. Recently, researchers proposed a few intensive data augmentation techniques, which indeed improved accuracy, yet we notice that these methods augment data have also caused a considerable gap between clean and augmented data. In this paper, we revisit this problem from an analytical perspective, for which we estimate the upper-bound of expected risk using two terms, namely, empirical risk and generalization error, respectively. We develop an understanding of data augmentation as regularization, which highlights the major features. As a result, data augmentation significantly reduces the generalization error, but meanwhile leads to a slightly higher empirical risk. On the assumption that data augmentation helps models converge to a better region, the model can benefit from a lower empirical risk achieved by a simple method, i.e., using less-augmented data to refine the model trained on fully-augmented data. Our approach achieves consistent accuracy gain on a few standard image classification benchmarks, and the gain transfers to object detection.},
	urldate = {2021-12-20},
	journal = {arXiv:1909.09148 [cs, stat]},
	author = {He, Zhuoxun and Xie, Lingxi and Chen, Xin and Zhang, Ya and Wang, Yanfeng and Tian, Qi},
	month = nov,
	year = {2019},
	note = {arXiv: 1909.09148},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{hataya_faster_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Faster {AutoAugment}: {Learning} {Augmentation} {Strategies} {Using} {Backpropagation}},
	isbn = {978-3-030-58595-2},
	shorttitle = {Faster {AutoAugment}},
	doi = {10.1007/978-3-030-58595-2_1},
	abstract = {Data augmentation methods are indispensable heuristics to boost the performance of deep neural networks, especially in image recognition tasks. Recently, several studies have shown that augmentation strategies found by search algorithms outperform hand-made strategies. Such methods employ black-box search algorithms over image transformations with continuous or discrete parameters and require a long time to obtain better strategies. In this paper, we propose a differentiable policy search pipeline for data augmentation, which is much faster than previous methods. We introduce approximate gradients for several transformation operations with discrete parameters as well as a differentiable mechanism for selecting operations. As the objective of training, we minimize the distance between the distributions of augmented and original data, which can be differentiated. We show that our method, Faster AutoAugment, achieves significantly faster searching than prior methods without a performance drop.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Hataya, Ryuichiro and Zdenek, Jan and Yoshizoe, Kazuki and Nakayama, Hideki},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	pages = {1--16},
}

@inproceedings{lim_fast_2019,
	title = {Fast {AutoAugment}},
	volume = {32},
	url = {https://papers.nips.cc/paper/2019/hash/6add07cf50424b14fdf649da87843d01-Abstract.html},
	urldate = {2021-12-20},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Lim, Sungbin and Kim, Ildoo and Kim, Taesup and Kim, Chiheon and Kim, Sungwoong},
	year = {2019},
}

@article{inoue_data_2018-1,
	title = {Data {Augmentation} by {Pairing} {Samples} for {Images} {Classification}},
	url = {https://arxiv.org/abs/1801.02929v2},
	abstract = {Data augmentation is a widely used technique in many machine learning tasks, such as image classification, to virtually enlarge the training dataset size and avoid overfitting. Traditional data augmentation techniques for image classification tasks create new samples from the original training data by, for example, flipping, distorting, adding a small amount of noise to, or cropping a patch from an original image. In this paper, we introduce a simple but surprisingly effective data augmentation technique for image classification tasks. With our technique, named SamplePairing, we synthesize a new sample from one image by overlaying another image randomly chosen from the training data (i.e., taking an average of two images for each pixel). By using two images randomly selected from the training set, we can generate \$N{\textasciicircum}2\$ new samples from \$N\$ training samples. This simple data augmentation technique significantly improved classification accuracy for all the tested datasets; for example, the top-1 error rate was reduced from 33.5\% to 29.0\% for the ILSVRC 2012 dataset with GoogLeNet and from 8.22\% to 6.93\% in the CIFAR-10 dataset. We also show that our SamplePairing technique largely improved accuracy when the number of samples in the training set was very small. Therefore, our technique is more valuable for tasks with a limited amount of training data, such as medical imaging tasks.},
	language = {en},
	urldate = {2021-12-20},
	author = {Inoue, Hiroshi},
	month = jan,
	year = {2018},
}

@inproceedings{zhong_random_2020,
	title = {Random {Erasing} {Data} {Augmentation}},
	volume = {34},
	copyright = {Copyright (c) 2020 Association for the Advancement of Artificial Intelligence},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/7000},
	doi = {10.1609/aaai.v34i07.7000},
	abstract = {In this paper, we introduce Random Erasing, a new data augmentation method for training the convolutional neural network (CNN). In training, Random Erasing randomly selects a rectangle region in an image and erases its pixels with random values. In this process, training images with various levels of occlusion are generated, which reduces the risk of over-fitting and makes the model robust to occlusion. Random Erasing is parameter learning free, easy to implement, and can be integrated with most of the CNN-based recognition models. Albeit simple, Random Erasing is complementary to commonly used data augmentation techniques such as random cropping and flipping, and yields consistent improvement over strong baselines in image classification, object detection and person re-identification. Code is available at: https://github.com/zhunzhong07/Random-Erasing.},
	language = {en},
	urldate = {2021-12-20},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Zhong, Zhun and Zheng, Liang and Kang, Guoliang and Li, Shaozi and Yang, Yi},
	month = apr,
	year = {2020},
	note = {Number: 07},
	pages = {13001--13008},
}

@inproceedings{marques-silva_explanations_2021,
	title = {Explanations for {Monotonic} {Classifiers}},
	url = {https://proceedings.mlr.press/v139/marques-silva21a.html},
	abstract = {In many classification tasks there is a requirement of monotonicity. Concretely, if all else remains constant, increasing (resp. decreasing) the value of one or more features must not decrease (resp. increase) the value of the prediction. Despite comprehensive efforts on learning monotonic classifiers, dedicated approaches for explaining monotonic classifiers are scarce and classifier-specific. This paper describes novel algorithms for the computation of one formal explanation of a (black-box) monotonic classifier. These novel algorithms are polynomial (indeed linear) in the run time complexity of the classifier. Furthermore, the paper presents a practically efficient model-agnostic algorithm for enumerating formal explanations.},
	language = {en},
	urldate = {2021-12-19},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Marques-Silva, Joao and Gerspacher, Thomas and Cooper, Martin C. and Ignatiev, Alexey and Narodytska, Nina},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {7469--7479},
}

@inproceedings{xu_be_2021,
	title = {To be {Robust} or to be {Fair}: {Towards} {Fairness} in {Adversarial} {Training}},
	language = {en},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	author = {Xu, Han and Liu, Xiaorui and Li, Yaxin and Jain, Anil K and Tang, Jiliang},
	year = {2021},
	pages = {10},
}

@inproceedings{trauble_disentangled_2021,
	title = {On {Disentangled} {Representations} {Learned} from {Correlated} {Data}},
	language = {en},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	author = {Träuble, Frederik and Creager, Elliot and Kilbertus, Niki and Locatello, Francesco and Dittadi, Andrea and Goyal, Anirudh and Schölkopf, Bernhard and Bauer, Stefan},
	year = {2021},
	pages = {12},
}

@inproceedings{oberst_regularizing_2021,
	title = {Regularizing towards {Causal} {Invariance}: {Linear} {Models} with {Proxies}},
	abstract = {We propose a method for learning linear models whose predictive performance is robust to causal interventions on unobserved variables, when noisy proxies of those variables are available. Our approach takes the form of a regularization term that trades off between in-distribution performance and robustness to interventions. Under the assumption of a linear structural causal model, we show that a single proxy can be used to create estimators that are prediction optimal under interventions of bounded strength. This strength depends on the magnitude of the measurement noise in the proxy, which is, in general, not identiﬁable. In the case of two proxy variables, we propose a modiﬁed estimator that is prediction optimal under interventions up to a known strength. We further show how to extend these estimators to scenarios where additional information about the “test time” intervention is available during training. We evaluate our theoretical ﬁndings in synthetic experiments and using real data of hourly pollution levels across several cities in China.},
	language = {en},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	author = {Oberst, Michael and Thams, Nikolaj and Peters, Jonas and Sontag, David},
	year = {2021},
	pages = {11},
}

@inproceedings{moshkovitz_connecting_2021,
	title = {Connecting {Interpretability} and {Robustness} in {Decision} {Trees} through {Separation}},
	abstract = {Recent research has recognized interpretability and robustness as essential properties of trustworthy classiﬁcation. Curiously, a connection between robustness and interpretability was empirically observed, but the theoretical reasoning behind it remained elusive. In this paper, we rigorously investigate this connection. Speciﬁcally, we focus on interpretation using decision trees and robustness to l1-perturbation. Previous works deﬁned the notion of r-separation as a sufﬁcient condition for robustness. We prove upper and lower bounds on the tree size in case the data is r-separated. We then show that a tighter bound on the size is possible when the data is linearly separated. We provide the ﬁrst algorithm with provable guarantees both on robustness, interpretability, and accuracy in the context of decision trees. Experiments conﬁrm that our algorithm yields classiﬁers that are both interpretable and robust and have high accuracy. The code for the experiments is available at https://github.com/yangarbiter/ interpretable-robust-trees.},
	language = {en},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	author = {Moshkovitz, Michal and Yang, Yao-Yuan and Chaudhuri, Kamalika},
	year = {2021},
	pages = {11},
}

@inproceedings{mahajan_domain_2021,
	title = {Domain {Generalization} using {Causal} {Matching}},
	abstract = {In the domain generalization literature, a common objective is to learn representations independent of the domain after conditioning on the class label. We show that this objective is not sufﬁcient: there exist counter-examples where a model fails to generalize to unseen domains even after satisfying class-conditional domain invariance. We formalize this observation through a structural causal model and show the importance of modeling within-class variations for generalization. Speciﬁcally, classes contain objects that characterize speciﬁc causal features, and domains can be interpreted as interventions on these objects that change non-causal features. We highlight an alternative condition: inputs across domains should have the same representation if they are derived from the same object. Based on this objective, we propose matching-based algorithms when base objects are observed (e.g., through data augmentation) and approximate the objective when objects are not observed (MatchDG). Our simple matching-based algorithms are competitive to prior work on out-of-domain accuracy for rotated MNIST, Fashion-MNIST, PACS, and Chest-Xray datasets. Our method MatchDG also recovers ground-truth object matches: on MNIST and Fashion-MNIST, top-10 matches from MatchDG have over 50\% overlap with ground-truth matches.},
	language = {en},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	author = {Mahajan, Divyat and Tople, Shruti and Sharma, Amit},
	year = {2021},
	pages = {12},
}

@inproceedings{laber_price_2021,
	title = {On the price of explainability for some clustering problems},
	abstract = {The price of explainability for a clustering task can be deﬁned as the unavoidable loss, in terms of the objective function, if we force the ﬁnal partition to be explainable. Here, we study this price for the following clustering problems: k-means, k-medians, k-centers and maximum-spacing. We provide upper and lower bounds for a natural model where explainability is achieved via decision trees. For the k-means and k-medians problems our upper bounds improve those obtained by [Dasgupta et. al, ICML 20] for low dimensions. Another contribution is a simple and efﬁcient algorithm for building explainable clusterings for the k-means problem. We provide empirical evidence that its performance is better than the current state of the art for decision-tree based explainable clustering.},
	language = {en},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	author = {Laber, Eduardo and Murtinho, Lucas},
	year = {2021},
	pages = {11},
}

@inproceedings{gurel_knowledge_2021,
	title = {Knowledge {Enhanced} {Machine} {Learning} {Pipeline}  against {Diverse} {Adversarial} {Attacks}},
	abstract = {Despite the great successes achieved by deep neural networks (DNNs), recent studies show that they are vulnerable against adversarial examples, which aim to mislead DNNs by adding small adversarial perturbations. Several defenses have been proposed against such attacks, while many of them have been adaptively attacked. In this work, we aim to enhance the ML robustness from a different perspective by leveraging domain knowledge: We propose a Knowledge Enhanced Machine Learning Pipeline (KEMLP) to integrate domain knowledge (i.e., logic relationships among different predictions) into a probabilistic graphical model via ﬁrst-order logic rules. In particular, we develop KEMLP by integrating a diverse set of weak auxiliary models based on their logical relationships to the main DNN model that performs the target task. Theoretically, we provide convergence results and prove that, under mild conditions, the prediction of KEMLP is more robust than that of the main DNN model. Empirically, we take road sign recognition as an example and leverage the relationships between road signs and their shapes and contents as domain knowledge. We show that compared with adversarial training and other baselines, KEMLP achieves higher robustness against physical attacks, Lp bounded attacks, unforeseen attacks, and natural corruptions under both whitebox and blackbox settings, while still maintaining high clean accuracy.},
	language = {en},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	author = {Gürel, Nezihe Merve and Qi, Xiangyu and Rimanic, Luka and Zhang, Ce and Li, Bo},
	year = {2021},
	pages = {12},
}

@inproceedings{garreau_what_2021,
	title = {What {Does} {LIME} {Really} {See} in {Images}?},
	abstract = {The performance of modern algorithms on certain computer vision tasks such as object recognition is now close to that of humans. This success was achieved at the price of complicated architectures depending on millions of parameters and it has become quite challenging to understand how particular predictions are made. Interpretability methods propose to give us this understanding. In this paper, we study LIME, perhaps one of the most popular. On the theoretical side, we show that when the number of generated examples is large, LIME explanations are concentrated around a limit explanation for which we give an explicit expression. We further this study for elementary shape detectors and linear models. As a consequence of this analysis, we uncover a connection between LIME and integrated gradients, another explanation method. More precisely, the LIME explanations are similar to the sum of integrated gradients over the superpixels used in the preprocessing step of LIME.},
	language = {en},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	author = {Garreau, Damien and Mardaoui, Dina},
	year = {2021},
	pages = {10},
}

@inproceedings{caucheteux_disentangling_2021,
	title = {Disentangling {Syntax} and {Semantics} in the {Brain} with {Deep} {Networks}},
	abstract = {The activations of language transformers like GPT-2 have been shown to linearly map onto brain activity during speech comprehension. However, the nature of these activations remains largely unknown and presumably conﬂate distinct linguistic classes. Here, we propose a taxonomy to factorize the high-dimensional activations of language models into four combinatorial classes: lexical, compositional, syntactic, and semantic representations. We then introduce a statistical method to decompose, through the lens of GPT-2’s activations, the brain activity of 345 subjects recorded with functional magnetic resonance imaging (fMRI) during the listening of 4.6 hours of narrated text. The results highlight two ﬁndings. First, compositional representations recruit a more widespread cortical network than lexical ones, and encompass the bilateral temporal, parietal and prefrontal cortices. Second, contrary to previous claims, syntax and semantics are not associated with separated modules, but, instead, appear to share a common and distributed neural substrate. Overall, this study introduces a versatile framework to isolate, in the brain activity, the distributed representations of linguistic constructs.},
	language = {en},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	author = {Caucheteux, Charlotte and Gramfort, Alexandre and King, Jean-Remi},
	year = {2021},
	pages = {13},
}

@inproceedings{biggs_model_2021,
	title = {Model {Distillation} for {Revenue} {Optimization}: {Interpretable} {Personalized} {Pricing}},
	abstract = {Data-driven pricing strategies are becoming increasingly common, where customers are offered a personalized price based on features that are predictive of their valuation of a product. It is desirable for this pricing policy to be simple and interpretable, so it can be veriﬁed, checked for fairness, and easily implemented. However, efforts to incorporate machine learning into a pricing framework often lead to complex pricing policies which are not interpretable, resulting in slow adoption in practice. We present a customized, prescriptive tree-based algorithm that distills knowledge from a complex black-box machine learning algorithm, segments customers with similar valuations and prescribes prices in such a way that maximizes revenue while maintaining interpretability. We quantify the regret of a resulting policy and demonstrate its efﬁcacy in applications with both synthetic and real-world datasets.},
	language = {en},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	author = {Biggs, Max and Sun, Wei and Ettl, Markus},
	year = {2021},
	pages = {11},
}

@inproceedings{wong_leveraging_2021,
	title = {Leveraging {Sparse} {Linear} {Layers} for {Debuggable} {Deep} {Networks}},
	url = {https://proceedings.mlr.press/v139/wong21b.html},
	abstract = {We show how fitting sparse linear models over learned deep feature representations can lead to more debuggable neural networks. These networks remain highly accurate while also being more amenable to human interpretation, as we demonstrate quantitatively and via human experiments. We further illustrate how the resulting sparse explanations can help to identify spurious correlations, explain misclassifications, and diagnose model biases in vision and language tasks.},
	language = {en},
	urldate = {2021-12-19},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Wong, Eric and Santurkar, Shibani and Madry, Aleksander},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {11205--11216},
}

@inproceedings{afchar_towards_2021,
	title = {Towards {Rigorous} {Interpretations}: a {Formalisation} of {Feature} {Attribution}},
	shorttitle = {Towards {Rigorous} {Interpretations}},
	url = {https://proceedings.mlr.press/v139/afchar21a.html},
	abstract = {Feature attribution is often loosely presented as the process of selecting a subset of relevant features as a rationale of a prediction. Task-dependent by nature, precise definitions of "relevance" encountered in the literature are however not always consistent. This lack of clarity stems from the fact that we usually do not have access to any notion of ground-truth attribution and from a more general debate on what good interpretations are. In this paper we propose to formalise feature selection/attribution based on the concept of relaxed functional dependence. In particular, we extend our notions to the instance-wise setting and derive necessary properties for candidate selection solutions, while leaving room for task-dependence. By computing ground-truth attributions on synthetic datasets, we evaluate many state-of-the-art attribution methods and show that, even when optimised, some fail to verify the proposed properties and provide wrong solutions.},
	language = {en},
	urldate = {2021-12-19},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Afchar, Darius and Guigue, Vincent and Hennequin, Romain},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {76--86},
}

@inproceedings{catav_marginal_2021,
	title = {Marginal {Contribution} {Feature} {Importance} - an {Axiomatic} {Approach} for {Explaining} {Data}},
	url = {https://proceedings.mlr.press/v139/catav21a.html},
	abstract = {In recent years, methods were proposed for assigning feature importance scores to measure the contribution of individual features. While in some cases the goal is to understand a specific model, in many cases the goal is to understand the contribution of certain properties (features) to a real-world phenomenon. Thus, a distinction has been made between feature importance scores that explain a model and scores that explain the data. When explaining the data, machine learning models are used as proxies in settings where conducting many real-world experiments is expensive or prohibited. While existing feature importance scores show great success in explaining models, we demonstrate their limitations when explaining the data, especially in the presence of correlations between features. Therefore, we develop a set of axioms to capture properties expected from a feature importance score when explaining data and prove that there exists only one score that satisfies all of them, the Marginal Contribution Feature Importance (MCI). We analyze the theoretical properties of this score function and demonstrate its merits empirically.},
	language = {en},
	urldate = {2021-12-19},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Catav, Amnon and Fu, Boyang and Zoabi, Yazeed and Meilik, Ahuva Libi Weiss and Shomron, Noam and Ernst, Jason and Sankararaman, Sriram and Gilad-Bachrach, Ran},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {1324--1335},
}

@inproceedings{hilgard_learning_2021,
	title = {Learning {Representations} by {Humans}, for {Humans}},
	url = {https://proceedings.mlr.press/v139/hilgard21a.html},
	abstract = {When machine predictors can achieve higher performance than the human decision-makers they support, improving the performance of human decision-makers is often conflated with improving machine accuracy. Here we propose a framework to directly support human decision-making, in which the role of machines is to reframe problems rather than to prescribe actions through prediction. Inspired by the success of representation learning in improving performance of machine predictors, our framework learns human-facing representations optimized for human performance. This “Mind Composed with Machine” framework incorporates a human decision-making model directly into the representation learning paradigm and is trained with a novel human-in-the-loop training procedure. We empirically demonstrate the successful application of the framework to various tasks and representational forms.},
	language = {en},
	urldate = {2021-12-19},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Hilgard, Sophie and Rosenfeld, Nir and Banaji, Mahzarin R. and Cao, Jack and Parkes, David},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {4227--4238},
}

@inproceedings{lu_dance_2021,
	title = {{DANCE}: {Enhancing} saliency maps using decoys},
	shorttitle = {{DANCE}},
	url = {https://proceedings.mlr.press/v139/lu21b.html},
	abstract = {Saliency methods can make deep neural network predictions more interpretable by identifying a set of critical features in an input sample, such as pixels that contribute most strongly to a prediction made by an image classifier. Unfortunately, recent evidence suggests that many saliency methods poorly perform, especially in situations where gradients are saturated, inputs contain adversarial perturbations, or predictions rely upon inter-feature dependence. To address these issues, we propose a framework, DANCE, which improves the robustness of saliency methods by following a two-step procedure. First, we introduce a perturbation mechanism that subtly varies the input sample without changing its intermediate representations. Using this approach, we can gather a corpus of perturbed ("decoy") data samples while ensuring that the perturbed and original input samples follow similar distributions. Second, we compute saliency maps for the decoy samples and propose a new method to aggregate saliency maps. With this design, we offset influence of gradient saturation. From a theoretical perspective, we show that the aggregated saliency map not only captures inter-feature dependence but, more importantly, is robust against previously described adversarial perturbation methods. Our empirical results suggest that, both qualitatively and quantitatively, DANCE outperforms existing methods in a variety of application domains.},
	language = {en},
	urldate = {2021-12-19},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Lu, Yang Young and Guo, Wenbo and Xing, Xinyu and Noble, William Stafford},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {7124--7133},
}

@book{williamson_design_2011,
	address = {Cambridge},
	title = {The {Design} of {Approximation} {Algorithms}},
	isbn = {978-0-511-92173-5},
	url = {http://ebooks.cambridge.org/ref/id/CBO9780511921735},
	language = {en},
	urldate = {2021-12-19},
	publisher = {Cambridge University Press},
	author = {Williamson, David P. and Shmoys, David B.},
	year = {2011},
	doi = {10.1017/CBO9780511921735},
}

@inproceedings{su_learning_2018,
	address = {Salt Lake City, UT},
	title = {Learning {Visual} {Knowledge} {Memory} {Networks} for {Visual} {Question} {Answering}},
	isbn = {978-1-5386-6420-9},
	url = {https://ieeexplore.ieee.org/document/8578905/},
	doi = {10.1109/CVPR.2018.00807},
	abstract = {Visual question answering (VQA) requires joint comprehension of images and natural language questions, where many questions can’t be directly or clearly answered from visual content but require reasoning from structured human knowledge with conﬁrmation from visual content. This paper proposes visual knowledge memory network (VKMN) to address this issue, which seamlessly incorporates structured human knowledge and deep visual features into memory networks in an end-to-end learning framework. Comparing to existing methods for leveraging external knowledge for supporting VQA, this paper stresses more on two missing mechanisms. First is the mechanism for integrating visual contents with knowledge facts. VKMN handles this issue by embedding knowledge triples (subject, relation, target) and deep visual features jointly into the visual knowledge features. Second is the mechanism for handling multiple knowledge facts expanding from question and answer pairs. VKMN stores joint embedding using key-value pair structure in the memory networks so that it is easy to handle multiple facts. Experiments show that the proposed method achieves promising results on both VQA v1.0 and v2.0 benchmarks, while outperforms state-of-the-art methods on the knowledge-reasoning related questions.},
	language = {en},
	urldate = {2021-12-19},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Su, Zhou and Zhu, Chen and Dong, Yinpeng and Cai, Dongqi and Chen, Yurong and Li, Jianguo},
	month = jun,
	year = {2018},
	pages = {7736--7745},
}

@article{denton_image_2020,
	title = {Image {Counterfactual} {Sensitivity} {Analysis} for {Detecting} {Unintended} {Bias}},
	url = {http://arxiv.org/abs/1906.06439},
	abstract = {Facial analysis models are increasingly used in applications that have serious impacts on people’s lives, ranging from authentication to surveillance tracking. It is therefore critical to develop techniques that can reveal unintended biases in facial classiﬁers to help guide the ethical use of facial analysis technology. This work proposes a framework called image counterfactual sensitivity analysis, which we explore as a proof-of-concept in analyzing a smiling attribute classiﬁer trained on faces of celebrities. The framework utilizes counterfactuals to examine how a classiﬁer’s prediction changes if a face characteristic slightly changes. We leverage recent advances in generative adversarial networks to build a realistic generative model of face images that affords controlled manipulation of speciﬁc image characteristics. We then introduce a set of metrics that measure the effect of manipulating a speciﬁc property on the output of the trained classiﬁer. Empirically, we ﬁnd several different factors of variation that affect the predictions of the smiling classiﬁer. This proof-of-concept demonstrates potential ways generative models can be leveraged for ﬁnegrained analysis of bias and fairness.},
	language = {en},
	urldate = {2021-12-19},
	journal = {arXiv:1906.06439 [cs, stat]},
	author = {Denton, Emily and Hutchinson, Ben and Mitchell, Margaret and Gebru, Timnit and Zaldivar, Andrew},
	month = oct,
	year = {2020},
	note = {arXiv: 1906.06439},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{sanh_learning_2020,
	title = {Learning from others' mistakes: {Avoiding} dataset biases without modeling them},
	shorttitle = {Learning from others' mistakes},
	url = {https://openreview.net/forum?id=Hf3qXoiNkR},
	abstract = {State-of-the-art natural language processing (NLP) models often learn to model dataset biases and surface form correlations instead of features that target the intended underlying task. Previous...},
	language = {en},
	urldate = {2021-12-19},
	author = {Sanh, Victor and Wolf, Thomas and Belinkov, Yonatan and Rush, Alexander M.},
	month = sep,
	year = {2020},
}

@inproceedings{ding_prototypical_2020,
	title = {Prototypical {Representation} {Learning} for {Relation} {Extraction}},
	url = {https://openreview.net/forum?id=aCgLmfhIy_f},
	abstract = {Recognizing relations between entities is a pivotal task of relational learning.  
Learning relation representations from distantly-labeled datasets is difficult because of the abundant label noise...},
	language = {en},
	urldate = {2021-12-19},
	author = {Ding, Ning and Wang, Xiaobin and Fu, Yao and Xu, Guangwei and Wang, Rui and Xie, Pengjun and Shen, Ying and Huang, Fei and Zheng, Hai-Tao and Zhang, Rui},
	month = sep,
	year = {2020},
}

@inproceedings{xia_robust_2020,
	title = {Robust early-learning: {Hindering} the memorization of noisy labels},
	shorttitle = {Robust early-learning},
	url = {https://openreview.net/forum?id=Eql5b1_hTE4},
	abstract = {The {\textbackslash}textit\{memorization effects\} of deep networks show that they will first memorize training data with clean labels and then those with noisy labels. The {\textbackslash}textit\{early stopping\} method therefore...},
	language = {en},
	urldate = {2021-12-19},
	author = {Xia, Xiaobo and Liu, Tongliang and Han, Bo and Gong, Chen and Wang, Nannan and Ge, Zongyuan and Chang, Yi},
	month = sep,
	year = {2020},
}

@inproceedings{barrett_implicit_2020,
	title = {Implicit {Gradient} {Regularization}},
	url = {https://openreview.net/forum?id=3q5IqUrkcF},
	abstract = {Gradient descent can be surprisingly good at optimizing deep neural networks without overfitting and without explicit regularization. We find that the discrete steps of gradient descent implicitly...},
	language = {en},
	urldate = {2021-12-19},
	author = {Barrett, David and Dherin, Benoit},
	month = sep,
	year = {2020},
}

@inproceedings{smith_origin_2020,
	title = {On the {Origin} of {Implicit} {Regularization} in {Stochastic} {Gradient} {Descent}},
	url = {https://openreview.net/forum?id=rq_Qr0c1Hyo},
	abstract = {For infinitesimal learning rates, stochastic gradient descent (SGD) follows the path of gradient flow on the full batch loss function. However moderately large learning rates can achieve higher...},
	language = {en},
	urldate = {2021-12-19},
	author = {Smith, Samuel L. and Dherin, Benoit and Barrett, David and De, Soham},
	month = sep,
	year = {2020},
}

@inproceedings{montero_role_2020,
	title = {The role of {Disentanglement} in {Generalisation}},
	url = {https://openreview.net/forum?id=qbH974jKUVy},
	abstract = {Combinatorial generalisation — the ability to understand and produce novel combinations of familiar elements — is a core capacity of human intelligence that current AI systems struggle with....},
	language = {en},
	urldate = {2021-12-19},
	author = {Montero, Milton Llera and Ludwig, Casimir JH and Costa, Rui Ponte and Malhotra, Gaurav and Bowers, Jeffrey},
	month = sep,
	year = {2020},
}

@inproceedings{honke_representation_2020,
	title = {Representation learning for improved interpretability and classification accuracy of clinical factors from {EEG}},
	url = {https://openreview.net/forum?id=TVjLza1t4hI},
	abstract = {Despite extensive standardization, diagnostic interviews for mental health disorders encompass substantial subjective judgment. Previous studies have demonstrated that EEG-based neural measures can...},
	language = {en},
	urldate = {2021-12-19},
	author = {Honke, Garrett and Higgins, Irina and Thigpen, Nina and Miskovic, Vladimir and Link, Katie and Duan, Sunny and Gupta, Pramod and Klawohn, Julia and Hajcak, Greg},
	month = sep,
	year = {2020},
}

@inproceedings{santurkar_breeds_2020,
	title = {{BREEDS}: {Benchmarks} for {Subpopulation} {Shift}},
	shorttitle = {{BREEDS}},
	url = {https://openreview.net/forum?id=mQPBmvyAuk},
	abstract = {We develop a methodology for assessing the robustness of models to subpopulation shift---specifically, their ability to generalize to novel data subpopulations that were not observed during...},
	language = {en},
	urldate = {2021-12-19},
	author = {Santurkar, Shibani and Tsipras, Dimitris and Madry, Aleksander},
	month = sep,
	year = {2020},
}

@inproceedings{liu_certifying_2019,
	title = {On {Certifying} {Non}-{Uniform} {Bounds} against {Adversarial} {Attacks}},
	url = {https://proceedings.mlr.press/v97/liu19h.html},
	abstract = {This work studies the robustness certification problem of neural network models, which aims to find certified adversary-free regions as large as possible around data points. In contrast to the existing approaches that seek regions bounded uniformly along all input features, we consider non-uniform bounds and use it to study the decision boundary of neural network models. We formulate our target as an optimization problem with nonlinear constraints. Then, a framework applicable for general feedforward neural networks is proposed to bound the output logits so that the relaxed problem can be solved by the augmented Lagrangian method. Our experiments show the non-uniform bounds have larger volumes than uniform ones. Compared with normal models, the robust models have even larger non-uniform bounds and better interpretability. Further, the geometric similarity of the non-uniform bounds gives a quantitative, data-agnostic metric of input features’ robustness.},
	language = {en},
	urldate = {2021-12-17},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Liu, Chen and Tomioka, Ryota and Cevher, Volkan},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {4072--4081},
}

@inproceedings{guo_lemna_2018,
	address = {New York, NY, USA},
	series = {{CCS} '18},
	title = {{LEMNA}: {Explaining} {Deep} {Learning} based {Security} {Applications}},
	isbn = {978-1-4503-5693-0},
	shorttitle = {{LEMNA}},
	url = {https://doi.org/10.1145/3243734.3243792},
	doi = {10.1145/3243734.3243792},
	abstract = {While deep learning has shown a great potential in various domains, the lack of transparency has limited its application in security or safety-critical areas. Existing research has attempted to develop explanation techniques to provide interpretable explanations for each classification decision. Unfortunately, current methods are optimized for non-security tasks ( e.g., image analysis). Their key assumptions are often violated in security applications, leading to a poor explanation fidelity. In this paper, we propose LEMNA, a high-fidelity explanation method dedicated for security applications. Given an input data sample, LEMNA generates a small set of interpretable features to explain how the input sample is classified. The core idea is to approximate a local area of the complex deep learning decision boundary using a simple interpretable model. The local interpretable model is specially designed to (1) handle feature dependency to better work with security applications ( e.g., binary code analysis); and (2) handle nonlinear local boundaries to boost explanation fidelity. We evaluate our system using two popular deep learning applications in security (a malware classifier, and a function start detector for binary reverse-engineering). Extensive evaluations show that LEMNA's explanation has a much higher fidelity level compared to existing methods. In addition, we demonstrate practical use cases of LEMNA to help machine learning developers to validate model behavior, troubleshoot classification errors, and automatically patch the errors of the target models.},
	urldate = {2021-12-17},
	booktitle = {Proceedings of the 2018 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {Association for Computing Machinery},
	author = {Guo, Wenbo and Mu, Dongliang and Xu, Jun and Su, Purui and Wang, Gang and Xing, Xinyu},
	month = oct,
	year = {2018},
	keywords = {binary analysis, deep recurrent neural networks, explainable AI},
	pages = {364--379},
}

@inproceedings{pierazzi_intriguing_2020,
	title = {Intriguing {Properties} of {Adversarial} {ML} {Attacks} in the {Problem} {Space}},
	doi = {10.1109/SP40000.2020.00073},
	abstract = {Recent research efforts on adversarial ML have investigated problem-space attacks, focusing on the generation of real evasive objects in domains where, unlike images, there is no clear inverse mapping to the feature space (e.g., software). However, the design, comparison, and real-world implications of problem-space attacks remain underexplored.This paper makes two major contributions. First, we propose a novel formalization for adversarial ML evasion attacks in the problem-space, which includes the definition of a comprehensive set of constraints on available transformations, preserved semantics, robustness to preprocessing, and plausibility. We shed light on the relationship between feature space and problem space, and we introduce the concept of side-effect features as the byproduct of the inverse feature-mapping problem. This enables us to define and prove necessary and sufficient conditions for the existence of problem-space attacks. We further demonstrate the expressive power of our formalization by using it to describe several attacks from related literature across different domains.Second, building on our formalization, we propose a novel problem-space attack on Android malware that overcomes past limitations. Experiments on a dataset with 170K Android apps from 2017 and 2018 show the practical feasibility of evading a state-of-the-art malware classifier along with its hardened version. Our results demonstrate that "adversarial-malware as a service" is a realistic threat, as we automatically generate thousands of realistic and inconspicuous adversarial applications at scale, where on average it takes only a few minutes to generate an adversarial app. Yet, out of the 1600+ papers on adversarial ML published in the past six years, roughly 40 focus on malware [15]-and many remain only in the feature space.Our formalization of problem-space attacks paves the way to more principled research in this domain. We responsibly release the code and dataset of our novel attack to other researchers, to encourage future work on defenses in the problem space.},
	booktitle = {2020 {IEEE} {Symposium} on {Security} and {Privacy} ({SP})},
	author = {Pierazzi, Fabio and Pendlebury, Feargus and Cortellazzi, Jacopo and Cavallaro, Lorenzo},
	month = may,
	year = {2020},
	note = {ISSN: 2375-1207},
	keywords = {Androids, Humanoid robots, Malware, Perturbation methods, Robustness, Semantics, adversarial machine learning, evasion, input space, malware, problem space, program analysis},
	pages = {1332--1349},
}

@article{carratino_mixup_2020,
	title = {On {Mixup} {Regularization}},
	url = {http://arxiv.org/abs/2006.06049},
	abstract = {Mixup is a data augmentation technique that creates new examples as convex combinationsof training points and labels. This simple technique has empirically shown to improvethe accuracy of many state-of-the-art models in different settings and applications, butthe reasons behind this empirical success remain poorly understood. In this paper wetake a substantial step in explaining the theoretical foundations of Mixup, by clarifyingits regularization effects. We show that Mixup can be interpreted as standard empiricalrisk minimization estimator subject to a combination of data transformation and randomperturbation of the transformed data. We gain two core insights from this new interpretation.First, the data transformation suggests that, at test time, a model trained with Mixup shouldalso be applied to transformed data, a one-line change in code that we show empirically toimprove both accuracy and calibration of the prediction. Second, we show how the randomperturbation of the new interpretation of Mixup induces multiple known regularizationschemes, including label smoothing and reduction of the Lipschitz constant of the estimator.These schemes interact synergistically with each other, resulting in a self calibrated andeffective regularization effect that prevents overfitting and overconfident predictions. Wecorroborate our theoretical analysis with experiments that support our conclusions.},
	urldate = {2021-12-17},
	journal = {arXiv:2006.06049 [cs, stat]},
	author = {Carratino, Luigi and Cissé, Moustapha and Jenatton, Rodolphe and Vert, Jean-Philippe},
	month = dec,
	year = {2020},
	note = {arXiv: 2006.06049},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{faramarzi_patchup_2020,
	title = {{PatchUp}: {A} {Regularization} {Technique} for {Convolutional} {Neural} {Networks}},
	shorttitle = {{PatchUp}},
	url = {http://arxiv.org/abs/2006.07794},
	abstract = {Large capacity deep learning models are often prone to a high generalization gap when trained with a limited amount of labeled training data. A recent class of methods to address this problem uses various ways to construct a new training sample by mixing a pair (or more) of training samples. We propose PatchUp, a hidden state block-level regularization technique for Convolutional Neural Networks (CNNs), that is applied on selected contiguous blocks of feature maps from a random pair of samples. Our approach improves the robustness of CNN models against the manifold intrusion problem that may occur in other state-of-the-art mixing approaches like Mixup and CutMix. Moreover, since we are mixing the contiguous block of features in the hidden space, which has more dimensions than the input space, we obtain more diverse samples for training towards different dimensions. Our experiments on CIFAR-10, CIFAR-100, and SVHN datasets with PreactResnet18, PreactResnet34, and WideResnet-28-10 models show that PatchUp improves upon, or equals, the performance of current state-of-the-art regularizers for CNNs. We also show that PatchUp can provide better generalization to affine transformations of samples and is more robust against adversarial attacks.},
	urldate = {2021-12-17},
	journal = {arXiv:2006.07794 [cs, stat]},
	author = {Faramarzi, Mojtaba and Amini, Mohammad and Badrinaaraayanan, Akilesh and Verma, Vikas and Chandar, Sarath},
	month = jun,
	year = {2020},
	note = {arXiv: 2006.07794},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{verma_manifold_2019,
	title = {Manifold {Mixup}: {Better} {Representations} by {Interpolating} {Hidden} {States}},
	shorttitle = {Manifold {Mixup}},
	url = {https://proceedings.mlr.press/v97/verma19a.html},
	abstract = {Deep neural networks excel at learning the training data, but often provide incorrect and confident predictions when evaluated on slightly different test examples. This includes distribution shifts, outliers, and adversarial examples. To address these issues, we propose {\textbackslash}manifoldmixup\{\}, a simple regularizer that encourages neural networks to predict less confidently on interpolations of hidden representations. {\textbackslash}manifoldmixup\{\} leverages semantic interpolations as additional training signal, obtaining neural networks with smoother decision boundaries at multiple levels of representation. As a result, neural networks trained with {\textbackslash}manifoldmixup\{\} learn flatter class-representations, that is, with fewer directions of variance. We prove theory on why this flattening happens under ideal conditions, validate it empirically on practical situations, and connect it to the previous works on information theory and generalization. In spite of incurring no significant computation and being implemented in a few lines of code, {\textbackslash}manifoldmixup\{\} improves strong baselines in supervised learning, robustness to single-step adversarial attacks, and test log-likelihood.},
	language = {en},
	urldate = {2021-12-17},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Verma, Vikas and Lamb, Alex and Beckham, Christopher and Najafi, Amir and Mitliagkas, Ioannis and Lopez-Paz, David and Bengio, Yoshua},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {6438--6447},
}

@inproceedings{kim_puzzle_2020,
	title = {Puzzle {Mix}: {Exploiting} {Saliency} and {Local} {Statistics} for {Optimal} {Mixup}},
	shorttitle = {Puzzle {Mix}},
	url = {https://proceedings.mlr.press/v119/kim20b.html},
	abstract = {While deep neural networks achieve great performance on fitting the training distribution, the learned networks are prone to overfitting and are susceptible to adversarial attacks. In this regard, a number of mixup based augmentation methods have been recently proposed. However, these approaches mainly focus on creating previously unseen virtual examples and can sometimes provide misleading supervisory signal to the network. To this end, we propose Puzzle Mix, a mixup method for explicitly utilizing the saliency information and the underlying statistics of the natural examples. This leads to an interesting optimization problem alternating between the multi-label objective for optimal mixing mask and saliency discounted optimal transport objective. Our experiments show Puzzle Mix achieves the state of the art generalization and the adversarial robustness results compared to other mixup methods on CIFAR-100, Tiny-ImageNet, and ImageNet datasets, and the source code is available at https://github.com/snu-mllab/PuzzleMix.},
	language = {en},
	urldate = {2021-12-17},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Kim, Jang-Hyun and Choo, Wonho and Song, Hyun Oh},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {5275--5285},
}

@inproceedings{zhang_how_2021,
	title = {How {Does} {Mixup} {Help} {With} {Robustness} and {Generalization}?},
	url = {https://openreview.net/forum?id=8yKEo06dKNo},
	abstract = {Mixup is a popular data augmentation technique based on on convex combinations of pairs of examples and their labels. This simple technique has shown to substantially improve both the model's...},
	language = {en},
	urldate = {2021-12-16},
	author = {Zhang, Linjun and Deng, Zhun and Kawaguchi, Kenji and Ghorbani, Amirata and Zou, James},
	year = {2021},
}

@inproceedings{robinson_contrastive_2020,
	title = {Contrastive {Learning} with {Hard} {Negative} {Samples}},
	url = {https://openreview.net/forum?id=CR1XOQ0UTh-},
	abstract = {We consider the question: how can you sample good negative examples for contrastive learning? We argue that, as with metric learning, learning contrastive representations benefits from hard...},
	language = {en},
	urldate = {2021-12-16},
	author = {Robinson, Joshua David and Chuang, Ching-Yao and Sra, Suvrit and Jegelka, Stefanie},
	month = sep,
	year = {2020},
}

@inproceedings{nguyen_wide_2020,
	title = {Do {Wide} and {Deep} {Networks} {Learn} the {Same} {Things}? {Uncovering} {How} {Neural} {Network} {Representations} {Vary} with {Width} and {Depth}},
	shorttitle = {Do {Wide} and {Deep} {Networks} {Learn} the {Same} {Things}?},
	url = {https://openreview.net/forum?id=KJNcAkY8tY4},
	abstract = {A key factor in the success of deep neural networks is the ability to scale models to improve performance by varying the architecture depth and width. This simple property of neural network design...},
	language = {en},
	urldate = {2021-12-16},
	author = {Nguyen, Thao and Raghu, Maithra and Kornblith, Simon},
	month = sep,
	year = {2020},
}

@inproceedings{jeong_training_2020,
	title = {Training {GANs} with {Stronger} {Augmentations} via {Contrastive} {Discriminator}},
	url = {https://openreview.net/forum?id=eo6U4CAwVmg},
	abstract = {Recent works in Generative Adversarial Networks (GANs) are actively revisiting various data augmentation techniques as an effective way to prevent discriminator overfitting. It is still unclear...},
	language = {en},
	urldate = {2021-12-16},
	author = {Jeong, Jongheon and Shin, Jinwoo},
	month = sep,
	year = {2020},
}

@inproceedings{kleinman_usable_2020,
	title = {Usable {Information} and {Evolution} of {Optimal} {Representations} {During} {Training}},
	url = {https://openreview.net/forum?id=p8agn6bmTbr},
	abstract = {We introduce a notion of usable information contained in the representation learned by a deep network, and use it to study how optimal representations for the task emerge during training. We show...},
	language = {en},
	urldate = {2021-12-16},
	author = {Kleinman, Michael and Achille, Alessandro and Idnani, Daksh and Kao, Jonathan},
	month = sep,
	year = {2020},
}

@inproceedings{li_shape-texture_2020,
	title = {Shape-{Texture} {Debiased} {Neural} {Network} {Training}},
	url = {https://openreview.net/forum?id=Db4yerZTYkz},
	abstract = {Shape and texture are two prominent and complementary cues for recognizing objects. Nonetheless, Convolutional Neural Networks are often biased towards either texture or shape, depending on the...},
	language = {en},
	urldate = {2021-12-16},
	author = {Li, Yingwei and Yu, Qihang and Tan, Mingxing and Mei, Jieru and Tang, Peng and Shen, Wei and Yuille, Alan and Xie, Cihang},
	month = sep,
	year = {2020},
}

@inproceedings{xu_robust_2020,
	title = {Robust and {Generalizable} {Visual} {Representation} {Learning} via {Random} {Convolutions}},
	url = {https://openreview.net/forum?id=BVSM0x3EDK6},
	abstract = {While successful for various computer vision tasks, deep neural networks have shown to be vulnerable to texture style shifts and small perturbations to which humans are robust. In this work, we...},
	language = {en},
	urldate = {2021-12-16},
	author = {Xu, Zhenlin and Liu, Deyi and Yang, Junlin and Raffel, Colin and Niethammer, Marc},
	month = sep,
	year = {2020},
}

@inproceedings{hendrycks_aligning_2020,
	title = {Aligning {AI} {With} {Shared} {Human} {Values}},
	url = {https://openreview.net/forum?id=dNy_RKzJacY},
	abstract = {We show how to assess a language model's knowledge of basic concepts of morality. We introduce the ETHICS dataset, a new benchmark that spans concepts in justice, well-being, duties, virtues, and...},
	language = {en},
	urldate = {2021-12-16},
	author = {Hendrycks, Dan and Burns, Collin and Basart, Steven and Critch, Andrew and Li, Jerry and Song, Dawn and Steinhardt, Jacob},
	month = sep,
	year = {2020},
}

@inproceedings{sinha_negative_2020,
	title = {Negative {Data} {Augmentation}},
	url = {https://openreview.net/forum?id=Ovp8dvB8IBH},
	abstract = {Data augmentation is often used to enlarge datasets with synthetic samples generated in accordance with the underlying data distribution. To enable a wider range of augmentations, we explore...},
	language = {en},
	urldate = {2021-12-16},
	author = {Sinha, Abhishek and Ayush, Kumar and Song, Jiaming and Uzkent, Burak and Jin, Hongxia and Ermon, Stefano},
	month = sep,
	year = {2020},
}

@inproceedings{qu_coda_2020,
	title = {{CoDA}: {Contrast}-enhanced and {Diversity}-promoting {Data} {Augmentation} for {Natural} {Language} {Understanding}},
	shorttitle = {{CoDA}},
	url = {https://openreview.net/forum?id=Ozk9MrX1hvA},
	abstract = {Data augmentation has been demonstrated as an effective strategy for improving model generalization and data efficiency.  However, due to the discrete nature of natural language, designing...},
	language = {en},
	urldate = {2021-12-16},
	author = {Qu, Yanru and Shen, Dinghan and Shen, Yelong and Sajeev, Sandra and Chen, Weizhu and Han, Jiawei},
	month = sep,
	year = {2020},
}

@inproceedings{raghu_teaching_2020,
	title = {Teaching with {Commentaries}},
	url = {https://openreview.net/forum?id=4RbdgBh9gE},
	abstract = {Effective training of deep neural networks can be challenging, and there remain many open questions on how to best learn these models. Recently developed methods to improve neural network training...},
	language = {en},
	urldate = {2021-12-16},
	author = {Raghu, Aniruddh and Raghu, Maithra and Kornblith, Simon and Duvenaud, David and Hinton, Geoffrey},
	month = sep,
	year = {2020},
}

@inproceedings{sahoo_scaling_2020,
	title = {Scaling {Symbolic} {Methods} using {Gradients} for {Neural} {Model} {Explanation}},
	url = {https://openreview.net/forum?id=V5j-jdoDDP},
	abstract = {Symbolic techniques based on Satisfiability Modulo Theory (SMT) solvers have been proposed for analyzing and verifying neural network properties, but their usage has been fairly limited owing to...},
	language = {en},
	urldate = {2021-12-16},
	author = {Sahoo, Subham Sekhar and Venugopalan, Subhashini and Li, Li and Singh, Rishabh and Riley, Patrick},
	month = sep,
	year = {2020},
}

@inproceedings{lee_removing_2020,
	title = {Removing {Undesirable} {Feature} {Contributions} {Using} {Out}-of-{Distribution} {Data}},
	url = {https://openreview.net/forum?id=eIHYL6fpbkA},
	abstract = {Several data augmentation methods deploy unlabeled-in-distribution (UID) data to bridge the gap between the training and inference of neural networks. However, these methods have clear limitations...},
	language = {en},
	urldate = {2021-12-16},
	author = {Lee, Saehyung and Park, Changhwa and Lee, Hyungyu and Yi, Jihun and Lee, Jonghyun and Yoon, Sungroh},
	month = sep,
	year = {2020},
}

@inproceedings{chuang_fair_2020,
	title = {Fair {Mixup}: {Fairness} via {Interpolation}},
	shorttitle = {Fair {Mixup}},
	url = {https://openreview.net/forum?id=DNl5s5BXeBn},
	abstract = {Training classifiers under fairness constraints such as group fairness, regularizes the disparities of predictions between the groups. Nevertheless, even though the constraints are satisfied during...},
	language = {en},
	urldate = {2021-12-16},
	author = {Chuang, Ching-Yao and Mroueh, Youssef},
	month = sep,
	year = {2020},
}

@inproceedings{heckel_early_2020,
	title = {Early {Stopping} in {Deep} {Networks}: {Double} {Descent} and {How} to {Eliminate} it},
	shorttitle = {Early {Stopping} in {Deep} {Networks}},
	url = {https://openreview.net/forum?id=tlV90jvZbw},
	abstract = {Over-parameterized models, such as large deep networks, often exhibit a double descent phenomenon, whereas a function of model size, error first decreases, increases, and decreases at last. This...},
	language = {en},
	urldate = {2021-12-16},
	author = {Heckel, Reinhard and Yilmaz, Fatih Furkan},
	month = sep,
	year = {2020},
}

@inproceedings{dittadi_transfer_2020,
	title = {On the {Transfer} of {Disentangled} {Representations} in {Realistic} {Settings}},
	url = {https://openreview.net/forum?id=8VXvj1QNRl1},
	abstract = {Learning meaningful representations that disentangle the underlying structure of the data generating process is considered to be of key importance in machine learning. While disentangled...},
	language = {en},
	urldate = {2021-12-16},
	author = {Dittadi, Andrea and Träuble, Frederik and Locatello, Francesco and Wuthrich, Manuel and Agrawal, Vaibhav and Winther, Ole and Bauer, Stefan and Schölkopf, Bernhard},
	month = sep,
	year = {2020},
}

@inproceedings{parascandolo_learning_2020,
	title = {Learning explanations that are hard to vary},
	url = {https://openreview.net/forum?id=hb1sDDSLbV},
	abstract = {In this paper, we investigate the principle that good explanations are hard to vary in the context of deep learning.
We show that averaging gradients across examples -- akin to a logical OR of...},
	language = {en},
	urldate = {2021-12-16},
	author = {Parascandolo, Giambattista and Neitz, Alexander and Orvieto, Antonio and Gresele, Luigi and Schölkopf, Bernhard},
	month = sep,
	year = {2020},
}

@inproceedings{mendez_lifelong_2020,
	title = {Lifelong {Learning} of {Compositional} {Structures}},
	url = {https://openreview.net/forum?id=ADWd4TJO13G},
	abstract = {A hallmark of human intelligence is the ability to construct self-contained chunks of knowledge and adequately reuse them in novel combinations for solving different yet structurally related...},
	language = {en},
	urldate = {2021-12-16},
	author = {Mendez, Jorge A. and Eaton, Eric},
	month = sep,
	year = {2020},
}

@inproceedings{cao_concept_2020,
	title = {Concept {Learners} for {Few}-{Shot} {Learning}},
	url = {https://openreview.net/forum?id=eJIJF3-LoZO},
	abstract = {Developing algorithms that are able to generalize to a novel task given only a few labeled examples represents a fundamental challenge in closing the gap between machine- and human-level...},
	language = {en},
	urldate = {2021-12-16},
	author = {Cao, Kaidi and Brbic, Maria and Leskovec, Jure},
	month = sep,
	year = {2020},
}

@inproceedings{lurz_generalization_2020,
	title = {Generalization in data-driven models of primary visual cortex},
	url = {https://openreview.net/forum?id=Tp7kI90Htd},
	abstract = {Deep neural networks (DNN) have set new standards at predicting responses of neural populations to visual input.  Most such DNNs consist of a convolutional network (core) shared across all neurons...},
	language = {en},
	urldate = {2021-12-16},
	author = {Lurz, Konstantin-Klemens and Bashiri, Mohammad and Willeke, Konstantin and Jagadish, Akshay and Wang, Eric and Walker, Edgar Y. and Cadena, Santiago A. and Muhammad, Taliah and Cobos, Erick and Tolias, Andreas S. and Ecker, Alexander S. and Sinz, Fabian H.},
	month = sep,
	year = {2020},
}

@inproceedings{pope_intrinsic_2020,
	title = {The {Intrinsic} {Dimension} of {Images} and {Its} {Impact} on {Learning}},
	url = {https://openreview.net/forum?id=XJk19XzGq2J},
	abstract = {It is widely believed that natural image data exhibits low-dimensional structure despite the high dimensionality of conventional pixel representations.  This idea underlies a common intuition for...},
	language = {en},
	urldate = {2021-12-16},
	author = {Pope, Phil and Zhu, Chen and Abdelkader, Ahmed and Goldblum, Micah and Goldstein, Tom},
	month = sep,
	year = {2020},
}

@inproceedings{chen_noise_2021,
	title = {Noise against noise: stochastic label noise helps combat inherent label noise},
	shorttitle = {Noise against noise},
	url = {https://openreview.net/forum?id=80FMcTSZ6J0},
	abstract = {The noise in stochastic gradient descent (SGD) provides a crucial implicit regularization effect, previously studied in optimization by analyzing the dynamics of parameter updates. In this paper...},
	language = {en},
	urldate = {2021-12-16},
	author = {Chen, Pengfei and Chen, Guangyong and Ye, Junjie and Zhao, Jingwei and Heng, Pheng-Ann},
	year = {2021},
}

@inproceedings{xu_understanding_2020,
	title = {Understanding the role of importance weighting for deep learning},
	url = {https://openreview.net/forum?id=_WnwtieRHxM},
	abstract = {The recent paper by Byrd \& Lipton (2019), based on empirical observations, raises a major concern on the impact of importance weighting for the over-parameterized deep learning models. They observe...},
	language = {en},
	urldate = {2021-12-16},
	author = {Xu, Da and Ye, Yuting and Ruan, Chuanwei},
	month = sep,
	year = {2020},
}

@inproceedings{zhang_learning_2021,
	title = {Learning with {Feature}-{Dependent} {Label} {Noise}: {A} {Progressive} {Approach}},
	shorttitle = {Learning with {Feature}-{Dependent} {Label} {Noise}},
	url = {https://openreview.net/forum?id=ZPa2SyGcbwh},
	abstract = {Label noise is frequently observed in real-world large-scale datasets. The noise is introduced due to a variety of reasons; it is heterogeneous and feature-dependent. Most existing approaches to...},
	language = {en},
	urldate = {2021-12-16},
	author = {Zhang, Yikai and Zheng, Songzhu and Wu, Pengxiang and Goswami, Mayank and Chen, Chao},
	year = {2021},
}

@inproceedings{kim_learning_2010,
	title = {Learning motion dynamics to catch a moving object},
	doi = {10.1109/ICHR.2010.5686332},
	abstract = {In this paper, we consider a novel approach to control the timing of motions when these are encoded with autonomous dynamical systems (DS). Accurate timing of motion is crucial if a robot must synchronize its movement with that of a fast moving object. In previous work of ours [1], we developed an approach to encode robot motion into DS. Such a time-independent encoding is advantageous in that it offers robustness against violent perturbation by adapting on the fly the trajectory while ensuring high accuracy at the target. We propose here an extension of the system that allows to control the timing of the motion while still benefitting from all the robustness properties deriving from the time-independent encoding of the DS. We validate the approach in experiments where the iCub robot learns from human demonstrations to catch a ball on the fly.},
	booktitle = {2010 10th {IEEE}-{RAS} {International} {Conference} on {Humanoid} {Robots}},
	author = {Kim, Seungsu and Gribovskaya, Elena and Billard, Aude},
	month = dec,
	year = {2010},
	note = {ISSN: 2164-0580},
	keywords = {Dynamics, Joints, Robot kinematics, Robot sensing systems, Timing, Trajectory},
	pages = {106--111},
}

@article{carneiro_robot_2021,
	title = {Robot {Anticipation} {Learning} {System} for {Ball} {Catching}},
	volume = {10},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/2218-6581/10/4/113},
	doi = {10.3390/robotics10040113},
	abstract = {Catching flying objects is a challenging task in human–robot interaction. Traditional techniques predict the intersection position and time using the information obtained during the free-flying ball motion. A common pain point in these systems is the short ball flight time and uncertainties in the ball’s trajectory estimation. In this paper, we present the Robot Anticipation Learning System (RALS) that accounts for the information obtained from observation of the thrower’s hand motion before the ball is released. RALS takes extra time for the robot to start moving in the direction of the target before the opponent finishes throwing. To the best of our knowledge, this is the first robot control system for ball-catching with anticipation skills. Our results show that the information fused from both throwing and flying motions improves the ball-catching rate by up to 20\% compared to the baseline approach, with the predictions relying only on the information acquired during the flight phase.},
	language = {en},
	number = {4},
	urldate = {2021-12-13},
	journal = {Robotics},
	author = {Carneiro, Diogo and Silva, Filipe and Georgieva, Petia},
	month = dec,
	year = {2021},
	note = {Number: 4
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {anticipation learning, ball catching, human–robot interaction, neural network, trajectory prediction},
	pages = {113},
}

@article{abdelkhalek_trajectory-based_2021,
	title = {Trajectory-based fast ball detection and tracking for an autonomous industrial robot system},
	volume = {20},
	issn = {1740-8865},
	url = {https://www.inderscienceonline.com/doi/abs/10.1504/IJISTA.2021.119029},
	doi = {10.1504/IJISTA.2021.119029},
	abstract = {Autonomising industrial robots is the main goal in this paper; imagine humanoid robots that have several degrees of freedom (DOF) mechanisms as their arms. What if the humanoid's arms could be programmed to be responsive to their surrounding environment, without any hard-coding assigned? This paper presents the idea of an autonomous system, where the system observes the surrounding environment and takes action on its observation. The application here is that of rebuffing an object that is thrown towards a robotic arm's workspace. This application mimics the idea of high dynamic responsiveness of a robot's arm. This paper will present a trajectory generation framework for rebuffing incoming flying objects. The framework bases its assumptions on inputs acquired through image processing and object detection. After extensive testing, it can be said that the proposed framework managed to fulfil the real-time system requirements for this application, with an 80\% successful rebuffing rate.},
	number = {2},
	urldate = {2021-12-13},
	journal = {International Journal of Intelligent Systems Technologies and Applications},
	author = {AbdElKhalek, Youssef M. and Awad, Mohammed Ibrahim and Munim, Hossam E. Abd El and Maged, Shady A.},
	month = jan,
	year = {2021},
	note = {Publisher: Inderscience Publishers},
	keywords = {depth image processing, infrared image processing, object detection, object tracking, ping-pong ball, real-time, serial robot, stereo vision, table tennis, trajectory prediction},
	pages = {126--145},
}

@inproceedings{namiki_robotic_2003,
	title = {Robotic catching using a direct mapping from visual information to motor command},
	volume = {2},
	doi = {10.1109/ROBOT.2003.1241952},
	abstract = {In this paper a robotic catching algorithm based on a nonlinear mapping of visual information to the desired trajectory is proposed. The nonlinear mapping is optimized by learning based on constraints of dynamics and kinematics. As a result a reactive and flexible motion is obtained due to the real-time high-speed visual information. Experimental results on catching a moving object using a high-speed vision system and a manipulation are presented.},
	booktitle = {2003 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({Cat}. {No}.{03CH37422})},
	author = {Namiki, A. and Ishikawa, M.},
	month = sep,
	year = {2003},
	note = {ISSN: 1050-4729},
	keywords = {Constraint optimization, Humans, Kinematics, Machine vision, Manipulator dynamics, Motor drives, Nonlinear dynamical systems, Physics computing, Robots, Trajectory},
	pages = {2400--2405 vol.2},
}

@article{kim_catching_2014,
	title = {Catching {Objects} in {Flight}},
	volume = {30},
	issn = {1941-0468},
	doi = {10.1109/TRO.2014.2316022},
	abstract = {We address the difficult problem of catching in-flight objects with uneven shapes. This requires the solution of three complex problems: accurate prediction of the trajectory of fastmoving objects, predicting the feasible catching configuration, and planning the arm motion, and all within milliseconds. We follow a programming-by-demonstration approach in order to learn, from throwing examples, models of the object dynamics and arm movement. We propose a new methodology to find a feasible catching configuration in a probabilistic manner. We use the dynamical systems approach to encode motion from several demonstrations. This enables a rapid and reactive adaptation of the arm motion in the presence of sensor uncertainty. We validate the approach in simulation with the iCub humanoid robot and in real-world experiments with the KUKA LWR 4+ (7-degree-of-freedom arm robot) to catch a hammer, a tennis racket, an empty bottle, a partially filled bottle, and a cardboard box.},
	number = {5},
	journal = {IEEE Transactions on Robotics},
	author = {Kim, Seungsu and Shukla, Ashwini and Billard, Aude},
	month = oct,
	year = {2014},
	note = {Conference Name: IEEE Transactions on Robotics},
	keywords = {Aerospace electronics, Catching, Dynamics, Gaussian mixture model, Grasping, Robot kinematics, Robot sensing systems, Trajectory, machine learning, robot control, support vector machines},
	pages = {1049--1065},
}

@inproceedings{yang_human_2020,
	title = {Human {Grasp} {Classification} for {Reactive} {Human}-to-{Robot} {Handovers}},
	doi = {10.1109/IROS45743.2020.9341004},
	abstract = {Transfer of objects between humans and robots is a critical capability for collaborative robots. Although there has been a recent surge of interest in human-robot handovers, most prior research focus on robot-to-human handovers. Further, work on the equally critical human-to-robot handovers often assumes humans can place the object in the robot's gripper. In this paper, we propose an approach for human-to-robot handovers in which the robot meets the human halfway, by classifying the human's grasp of the object and quickly planning a trajectory accordingly to take the object from the human's hand according to their intent. To do this, we collect a human grasp dataset which covers typical ways of holding objects with various hand shapes and poses, and learn a deep model on this dataset to classify the hand grasps into one of these categories. We present a planning and execution approach that takes the object from the human hand according to the detected grasp and hand position, and replans as necessary when the handover is interrupted. Through a systematic evaluation, we demonstrate that our system results in more fluent handovers versus two baselines. We also present findings from a user study (N = 9) demonstrating the effectiveness and usability of our approach with naive users in different scenarios. More information can be found at http://wyang.me/handovers.},
	booktitle = {2020 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Yang, Wei and Paxton, Chris and Cakmak, Maya and Fox, Dieter},
	month = oct,
	year = {2020},
	note = {ISSN: 2153-0866},
	keywords = {Collaboration, Handover, Planning, Surges, Systematics, Trajectory, Usability},
	pages = {11123--11130},
}

@article{feix_grasp_2016,
	title = {The {GRASP} {Taxonomy} of {Human} {Grasp} {Types}},
	volume = {46},
	issn = {2168-2305},
	doi = {10.1109/THMS.2015.2470657},
	abstract = {In this paper, we analyze and compare existing human grasp taxonomies and synthesize them into a single new taxonomy (dubbed “The GRASP Taxonomy” after the GRASP project funded by the European Commission). We consider only static and stable grasps performed by one hand. The goal is to extract the largest set of different grasps that were referenced in the literature and arrange them in a systematic way. The taxonomy provides a common terminology to define human hand configurations and is important in many domains such as human-computer interaction and tangible user interfaces where an understanding of the human is basis for a proper interface. Overall, 33 different grasp types are found and arranged into the GRASP taxonomy. Within the taxonomy, grasps are arranged according to 1) opposition type, 2) the virtual finger assignments, 3) type in terms of power, precision, or intermediate grasp, and 4) the position of the thumb. The resulting taxonomy incorporates all grasps found in the reviewed taxonomies that complied with the grasp definition. We also show that due to the nature of the classification, the 33 grasp types might be reduced to a set of 17 more general grasps if only the hand configuration is considered without the object shape/size.},
	number = {1},
	journal = {IEEE Transactions on Human-Machine Systems},
	author = {Feix, Thomas and Romero, Javier and Schmiedmayer, Heinz-Bodo and Dollar, Aaron M. and Kragic, Danica},
	month = feb,
	year = {2016},
	note = {Conference Name: IEEE Transactions on Human-Machine Systems},
	keywords = {Force, Grasping, Hand/wrist posture, Man machine systems, Robots, Shape, Taxonomy, Thumb, human factors, human–robot interaction, robotics, taxonomies},
	pages = {66--77},
}

@incollection{cutkosky_human_1990,
	address = {New York, NY},
	title = {Human {Grasp} {Choice} and {Robotic} {Grasp} {Analysis}},
	isbn = {978-1-4613-8974-3},
	url = {https://doi.org/10.1007/978-1-4613-8974-3_1},
	abstract = {In studying grasping and manipulation we find two very different approaches to the subject: knowledge-based approaches based primarily on empirical studies of human grasping and manipulation, and analytical approaches based primarily on physical models of the manipulation process. This chapter begins with a review of studies of human grasping, in particular our development of a grasp taxonomy and an expert system for predicting human grasp choice. These studies show how object geometry and task requirements (as well as hand capabilities and tactile sensing) combine to dictate grasp choice. We then consider analytic models of grasping and manipulation with robotic hands. To keep the mathematics tractable, these models require numerous simplifications which restrict their generality. Despite their differences, the two approaches can be correlated. This provides insight into why people grasp and manipulate objects as they do, and suggests different approaches for robotic grasp and manipulation planning. The results also bear upon such issues such as object representation and hand design.},
	language = {en},
	urldate = {2021-12-13},
	booktitle = {Dextrous {Robot} {Hands}},
	publisher = {Springer},
	author = {Cutkosky, Mark R. and Howe, Robert D.},
	editor = {Venkataraman, Subramanian T. and Iberall, Thea},
	year = {1990},
	doi = {10.1007/978-1-4613-8974-3_1},
	keywords = {Force Closure, Power Grasp, Robot Hand, Thrust Force, Virtual Finger},
	pages = {5--31},
}

@article{ortenzi_object_2021,
	title = {Object {Handovers}: {A} {Review} for {Robotics}},
	volume = {37},
	issn = {1941-0468},
	shorttitle = {Object {Handovers}},
	doi = {10.1109/TRO.2021.3075365},
	abstract = {This article surveys the literature on human–robot object handovers. A handover is a collaborative joint action, where an agent, the giver, gives an object to another agent, the receiver. The physical exchange starts when the receiver first contacts the object held by the giver and ends when the giver fully releases the object to the receiver. However, important cognitive and physical processes begin before the physical exchange, including initiating implicit agreement with respect to the location and timing of the exchange. From this perspective, we structure our review into the two main phases delimited by the aforementioned events: a prehandover phase and the physical exchange. We focus our analysis on the two actors (giver and receiver) and report the state of the art of robotic givers (robot-to-human handovers) and the robotic receivers (human-to-robot handovers). We report a comprehensive list of qualitative and quantitative metrics commonly used to assess the interaction. While focusing our review on the cognitive level (e.g., prediction, perception, motion planning, and learning) and the physical level (e.g., motion, grasping, and grip release) of the handover, we also discuss safety. We compare the behaviors displayed during human-to-human handovers to the state of the art of robotic assistants and identify the major areas of improvement for robotic assistants to reach performance comparable to human interactions. Finally, we propose a minimal set of metrics that should be used in order to enable a fair comparison among the approaches.},
	number = {6},
	journal = {IEEE Transactions on Robotics},
	author = {Ortenzi, Valerio and Cosgun, Akansel and Pardi, Tommaso and P. Chan, Wesley and Croft, Elizabeth and Kulić, Dana},
	month = dec,
	year = {2021},
	note = {Conference Name: IEEE Transactions on Robotics},
	keywords = {Handover, Human–robot interaction (HRI), Planning, Receivers, Robot kinematics, Robots, Task analysis, Tools, object handover},
	pages = {1855--1873},
}

@article{babin_mechanisms_2021,
	title = {Mechanisms for {Robotic} {Grasping} and {Manipulation}},
	volume = {4},
	url = {https://doi.org/10.1146/annurev-control-061520-010405},
	doi = {10.1146/annurev-control-061520-010405},
	abstract = {This article reviews the literature on the design of robotic mechanical grippers, with a focus on the mechanical aspects, which are believed to be the main bottleneck for effective designs. Our discussion includes gripper architectures and means of actuation, anthropomorphism and grasp planning, and robotic manipulation, emphasizing the complementary concepts of intrinsic and extrinsic dexterity. We also consider interactions of robotic grippers with the environment and with the objects to be grasped and argue that the proper handling of such interactions is key to the development of grasping and manipulation tools and scenarios. Finally, we briefly present examples of recent designs to support the discussion.},
	number = {1},
	urldate = {2021-12-13},
	journal = {Annual Review of Control, Robotics, and Autonomous Systems},
	author = {Babin, Vincent and Gosselin, Clément},
	year = {2021},
	note = {\_eprint: https://doi.org/10.1146/annurev-control-061520-010405},
	keywords = {anthropomorphic grippers, compliant grippers, extrinsic dexterity, grasp quality, intrinsic dexterity, manipulation, physical interactions, robotic hands, soft grippers, underactuation},
	pages = {573--593},
}

@article{liu_systematic_2021,
	title = {A {Systematic} {Analysis} of {Hand} {Movement} {Functionality}: {Qualitative} {Classification} and {Quantitative} {Investigation} of {Hand} {Grasp} {Behavior}},
	volume = {15},
	issn = {1662-5218},
	shorttitle = {A {Systematic} {Analysis} of {Hand} {Movement} {Functionality}},
	url = {https://www.frontiersin.org/article/10.3389/fnbot.2021.658075},
	doi = {10.3389/fnbot.2021.658075},
	abstract = {Understanding human hand movement functionality is fundamental in neuroscience, robotics, prosthetics, and rehabilitation. People are used to investigate movement functionality separately from qualitative or quantitative perspectives. However, it is still limited to providing an integral framework from both perspectives in a logical manner. In this paper, we provide a systematic framework to qualitatively classify hand movement functionality, build prehensile taxonomy to explore the general influence factors of human prehension, and accordingly design a behavioral experiment to quantitatively understand the hand grasp. In qualitative analysis, two facts are explicitly proposed: (1) the arm and wrist make a vital contribution to hand movement functionality; (2) the relative position (relative position in this paper is defined as the distance between the center of the human wrist and the object center of gravity) is a general influence factor significantly impacting human prehension. In quantitative analysis, the significant influence of three factors, object shape, size, and relative position, is quantitatively demonstrated. Simultaneously considering the impact of relative position, object shape, and size, the prehensile taxonomy and behavioral experiment results presented here should be more representative and complete to understand human grasp functionality. The systematic framework presented here is general and applicable to other body parts, such as wrist, arm, etc. Finally, many potential applications and the limitations are clarified.},
	urldate = {2021-12-13},
	journal = {Frontiers in Neurorobotics},
	author = {Liu, Yuan and Jiang, Li and Liu, Hong and Ming, Dong},
	year = {2021},
	pages = {46},
}

@inproceedings{li_learning_2021,
	title = {Learning {Task}-{Oriented} {Dexterous} {Grasping} from {Human} {Knowledge}},
	doi = {10.1109/ICRA48506.2021.9562073},
	abstract = {Industrial automation requires robot dexterity to automate many processes such as product assembling, packaging, and material handling. The existing robotic systems lack the capability to determining proper grasp strategies in the context of object affordances and task designations. In this paper, a framework of task-oriented dexterous grasping is proposed to learn grasp knowledge from human experience and to deploy the grasp strategies while adapting to grasp context. Grasp topology is defined and grasp strategies are learned from an established dataset for task-oriented dexterous manipulation. To adapt to various grasp context, a reinforcement-learning based grasping policy was implemented to deploy different task-oriented strategies. The performance of the system was evaluated in a simulated grasping environment by using an AR10 anthropomorphic hand installed in a Sawyer robotic arm. The proposed framework achieved a hit rate of 100\% for grasp strategies and an overall top-3 match rate of 95.6\%. The success rate of grasping was 85.6\% during 2700 grasping experiments for manipulation tasks given in natural-language instructions.},
	booktitle = {2021 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Li, Hui and Zhang, Yinlong and Li, Yanan and He, Hongsheng},
	month = may,
	year = {2021},
	note = {ISSN: 2577-087X},
	keywords = {Affordances, Automation, Dexterous Grasping, Grasp Topology, Grasping, Network topology, Prediction algorithms, Reinforcement Learning, Reinforcement learning, Service robots, Task-Oriented Grasping},
	pages = {6192--6198},
}

@article{roa_grasp_2015,
	title = {Grasp quality measures: review and performance},
	volume = {38},
	issn = {1573-7527},
	shorttitle = {Grasp quality measures},
	url = {https://doi.org/10.1007/s10514-014-9402-3},
	doi = {10.1007/s10514-014-9402-3},
	abstract = {The correct grasp of objects is a key aspect for the right fulfillment of a given task. Obtaining a good grasp requires algorithms to automatically determine proper contact points on the object as well as proper hand configurations, especially when dexterous manipulation is desired, and the quantification of a good grasp requires the definition of suitable grasp quality measures. This article reviews the quality measures proposed in the literature to evaluate grasp quality. The quality measures are classified into two groups according to the main aspect they evaluate: location of contact points on the object and hand configuration. The approaches that combine different measures from the two previous groups to obtain a global quality measure are also reviewed, as well as some measures related to human hand studies and grasp performance. Several examples are presented to illustrate and compare the performance of the reviewed measures.},
	language = {en},
	number = {1},
	urldate = {2021-12-13},
	journal = {Autonomous Robots},
	author = {Roa, Máximo A. and Suárez, Raúl},
	month = jan,
	year = {2015},
	pages = {65--88},
}

@article{bekey_knowledge-based_1993,
	title = {Knowledge-based control of grasping in robot hands using heuristics from human motor skills},
	volume = {9},
	issn = {2374-958X},
	doi = {10.1109/70.265915},
	abstract = {The development of a grasp planner for multifingered robot hands is described. The planner is knowledge-based, selecting grasp postures by reasoning from symbolic information on target object geometry and the nature of the task. The ability of the planner to utilize task information is based on an attempt to mimic human grasping behavior. Several task attributes and a set of heuristics derived from observation of human motor skills are included in the system. The paper gives several examples of the reasoning of the system in selecting the appropriate grasp mode for spherical and cylindrical objects for different tasks.{\textless}{\textgreater}},
	number = {6},
	journal = {IEEE Transactions on Robotics and Automation},
	author = {Bekey, G.A. and Liu, Huan and Tomovic, R. and Karplus, W.J.},
	month = dec,
	year = {1993},
	note = {Conference Name: IEEE Transactions on Robotics and Automation},
	keywords = {Computational geometry, Computer science, Control systems, Equations, Grasping, Humans, Information geometry, Knowledge based systems, Robots, Shape},
	pages = {709--722},
}

@article{feix_analysis_2014,
	title = {Analysis of {Human} {Grasping} {Behavior}: {Object} {Characteristics} and {Grasp} {Type}},
	volume = {7},
	issn = {2329-4051},
	shorttitle = {Analysis of {Human} {Grasping} {Behavior}},
	doi = {10.1109/TOH.2014.2326871},
	abstract = {This paper is the first of a two-part series analyzing human grasping behavior during a wide range of unstructured tasks. The results help clarify overall characteristics of human hand to inform many domains, such as the design of robotic manipulators, targeting rehabilitation toward important hand functionality, and designing haptic devices for use by the hand. It investigates the properties of objects grasped by two housekeepers and two machinists during the course of almost 10,000 grasp instances and correlates the grasp types used to the properties of the object. We establish an object classification that assigns each object properties from a set of seven classes, including mass, shape and size of the grasp location, grasped dimension, rigidity, and roundness. The results showed that 55 percent of grasped objects had at least one dimension larger than 15 cm, suggesting that more than half of objects cannot physically be grasped using their largest axis. Ninety-two percent of objects had a mass of 500 g or less, implying that a high payload capacity may be unnecessary to accomplish a large subset of human grasping behavior. In terms of grasps, 96 percent of grasp locations were 7 cm or less in width, which can help to define requirements for hand rehabilitation and defines a reasonable grasp aperture size for a robotic hand. Subjects grasped the smallest overall major dimension of the object in 94 percent of the instances. This suggests that grasping the smallest axis of an object could be a reliable default behavior to implement in grasp planners.},
	number = {3},
	journal = {IEEE Transactions on Haptics},
	author = {Feix, Thomas and Bullock, Ian M. and Dollar, Aaron M.},
	month = jul,
	year = {2014},
	note = {Conference Name: IEEE Transactions on Haptics},
	keywords = {Force, Grasping, Human grasping, Joints, Robots, Shape, Thumb, activities of daily living, manipulation, prosthetics, robotic hands},
	pages = {311--323},
}

@article{feix_analysis_2014-1,
	title = {Analysis of {Human} {Grasping} {Behavior}: {Correlating} {Tasks}, {Objects} and {Grasps}},
	volume = {7},
	issn = {2329-4051},
	shorttitle = {Analysis of {Human} {Grasping} {Behavior}},
	doi = {10.1109/TOH.2014.2326867},
	abstract = {This paper is the second in a two-part series analyzing human grasping behavior during a wide range of unstructured tasks. It investigates the tasks performed during the daily work of two housekeepers and two machinists and correlates grasp type and object properties with the attributes of the tasks being performed. The task or activity is classified according to the force required, the degrees of freedom, and the functional task type. We found that 46 percent of tasks are constrained, where the manipulated object is not allowed to move in a full six degrees of freedom. Analyzing the interrelationships between the grasp, object, and task data show that the best predictors of the grasp type are object size, task constraints, and object mass. Using these attributes, the grasp type can be predicted with 47 percent accuracy. Those parameters likely make useful heuristics for grasp planning systems. The results further suggest the common sub-categorization of grasps into power, intermediate, and precision categories may not be appropriate, indicating that grasps are generally more multi-functional than previously thought. We find large and heavy objects are grasped with a power grasp, but small and lightweight objects are not necessarily grasped with precision grasps-even with grasped object size less than 2 cm and mass less than 20 g, precision grasps are only used 61 percent of the time. These results have important implications for robotic hand design and grasp planners, since it appears while power grasps are frequently used for heavy objects, they can still be quite practical for small, lightweight objects.},
	number = {4},
	journal = {IEEE Transactions on Haptics},
	author = {Feix, Thomas and Bullock, Ian M. and Dollar, Aaron M.},
	month = oct,
	year = {2014},
	note = {Conference Name: IEEE Transactions on Haptics},
	keywords = {Grasping, Human grasping, Robots, Shape analysis, Thumb, activities of daily living, manipulation, prosthetics, robotic hands},
	pages = {430--441},
}

@inproceedings{mason_dynamic_1993,
	title = {Dynamic manipulation},
	volume = {1},
	doi = {10.1109/IROS.1993.583093},
	abstract = {Dynamic manipulation is defined, and a brief survey of dynamic operations is given. The design, control, and planning of dynamic manipulation is addressed. An example of dynamic manipulation, club-throwing using dynamic closure, is described.},
	booktitle = {Proceedings of 1993 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS} '93)},
	author = {Mason, M.T. and Lynch, K.M.},
	month = jul,
	year = {1993},
	keywords = {Acceleration, Computer science, Grippers, Kinematics, Manipulator dynamics, Motion planning, Programming profession, Robots, Stability, Taxonomy},
	pages = {152--159 vol.1},
}

@article{su_deep_2021,
	title = {Deep {Neural} {Network} {Approach} in {EMG}-{Based} {Force} {Estimation} for {Human}–{Robot} {Interaction}},
	volume = {2},
	issn = {2691-4581},
	url = {https://ieeexplore.ieee.org/document/9380441/},
	doi = {10.1109/TAI.2021.3066565},
	abstract = {In the human–robot interaction, especially when hand contact appears directly on the robot arm, the dynamics of the human arm presents an essential component in human–robot interaction and object manipulation. Modeling and estimation of the human arm dynamics show great potential for achieving more natural and safer interaction. To enrich the dexterity and guarantee the accuracy of the manipulation, mapping the motor functionality of muscle using biosignals becomes a popular topic. In this article, a novel algorithm was constructed using deep learning to explore the potential model between surface electromyography (sEMG) signals of the human arm and interaction force for human–robot interaction. Its features were extracted by adopting the convolutional neural network from the sEMG signals automatically without using prior knowledge of the biomechanical model. The experiments prove the lower error ({\textless} 0.4 N ) of the designed regression by comparing it with other approaches, such as artiﬁcial neural network and long short-term memory. It should be also mentioned that the antinoise ability is an important index to apply this technique in practical applications. Hence, we also add different Gaussian noises into the dataset to demonstrate the robustness against measurement noises by using the proposed model. Finally, it demonstrates the performance of the proposed algorithm using the Myo controller and KUKA LWR4+ robot.},
	language = {en},
	number = {5},
	urldate = {2021-12-13},
	journal = {IEEE Transactions on Artificial Intelligence},
	author = {Su, Hang and Qi, Wen and Li, Zhijun and Chen, Ziyang and Ferrigno, Giancarlo and De Momi, Elena},
	month = oct,
	year = {2021},
	pages = {404--412},
}

@article{bullock_yale_2015,
	title = {The {Yale} human grasping dataset: {Grasp}, object, and task data in household and machine shop environments},
	volume = {34},
	issn = {0278-3649, 1741-3176},
	shorttitle = {The {Yale} human grasping dataset},
	url = {http://journals.sagepub.com/doi/10.1177/0278364914555720},
	doi = {10.1177/0278364914555720},
	abstract = {This paper presents a dataset of human grasping behavior in unstructured environments. Wide-angle head-mounted camera video was recorded from two housekeepers and two machinists during their regular work activities, and the grasp types, objects, and tasks were analyzed and coded by study staff. The full dataset contains 27.7 hours of tagged video and represents a wide range of manipulative behaviors spanning much of the typical human hand usage. We provide the original videos, a spreadsheet including the tagged grasp type, object, and task parameters, time information for each successive grasp, and video screenshots for each instance. Example code is provided for MATLAB and R, demonstrating how to load in the dataset and produce simple plots.},
	language = {en},
	number = {3},
	urldate = {2021-12-13},
	journal = {The International Journal of Robotics Research},
	author = {Bullock, Ian M. and Feix, Thomas and Dollar, Aaron M.},
	month = mar,
	year = {2015},
	pages = {251--255},
}

@article{calli_benchmarking_2015,
	title = {Benchmarking in {Manipulation} {Research}: {Using} the {Yale}-{CMU}-{Berkeley} {Object} and {Model} {Set}},
	volume = {22},
	issn = {1558-223X},
	shorttitle = {Benchmarking in {Manipulation} {Research}},
	doi = {10.1109/MRA.2015.2448951},
	abstract = {In this article, we present the Yale-Carnegie Mellon University (CMU)-Berkeley (YCB) object and model set, intended to be used to facilitate benchmarking in robotic manipulation research. The objects in the set are designed to cover a wide range of aspects of the manipulation problem. The set includes objects of daily life with different shapes, sizes, textures, weights, and rigidities as well as some widely used manipulation tests. The associated database provides high-resolution red, green, blue, plus depth (RGB-D) scans, physical properties, and geometric models of the objects for easy incorporation into manipulation and planning software platforms. In addition to describing the objects and models in the set along with how they were chosen and derived, we provide a framework and a number of example task protocols, laying out how the set can be used to quantitatively evaluate a range of manipulation approaches, including planning, learning, mechanical design, control, and many others. A comprehensive literature survey on the existing benchmarks and object data sets is also presented, and their scope and limitations are discussed. The YCB set will be freely distributed to research groups worldwide at a series of tutorials at robotics conferences. Subsequent sets will be, otherwise, available to purchase at a reasonable cost. It is our hope that the ready availability of this set along with the ground laid in terms of protocol templates will enable the community of manipulation researchers to more easily compare approaches as well as continually evolve standardized benchmarking tests and metrics as the field matures.},
	number = {3},
	journal = {IEEE Robotics Automation Magazine},
	author = {Calli, Berk and Walsman, Aaron and Singh, Arjun and Srinivasa, Siddhartha and Abbeel, Pieter and Dollar, Aaron M.},
	month = sep,
	year = {2015},
	note = {Conference Name: IEEE Robotics Automation Magazine},
	keywords = {Benchmark testing, Data models, Databases, Object detection, Prosthetics, Robots, Solid modeling},
	pages = {36--52},
}

@inproceedings{kim_co-mixup_2021,
	title = {Co-{Mixup}: {Saliency} {Guided} {Joint} {Mixup} with {Supermodular} {Diversity}},
	shorttitle = {Co-{Mixup}},
	url = {https://openreview.net/forum?id=gvxJzw8kW4b},
	abstract = {While deep neural networks show great performance on fitting to the training distribution, improving the networks' generalization performance to the test distribution and robustness to the...},
	language = {en},
	urldate = {2021-12-07},
	author = {Kim, JangHyun and Choo, Wonho and Jeong, Hosan and Song, Hyun Oh},
	year = {2021},
}

@inproceedings{vedaldi_defense_2020,
	address = {Cham},
	title = {Defense {Against} {Adversarial} {Attacks} via {Controlling} {Gradient} {Leaking} on {Embedded} {Manifolds}},
	volume = {12373},
	isbn = {978-3-030-58603-4 978-3-030-58604-1},
	url = {http://link.springer.com/10.1007/978-3-030-58604-1_45},
	doi = {10.1007/978-3-030-58604-1_45},
	abstract = {Deep neural networks are vulnerable to adversarial attacks. Though various attempts have been made, it is still largely open to fully understand the existence of adversarial samples and thereby develop effective defense strategies. In this paper, we present a new perspective, namely gradient leaking hypothesis, to understand the existence of adversarial examples and to further motivate eﬀective defense strategies. Speciﬁcally, we consider the low dimensional manifold structure of natural images, and empirically verify that the leakage of the gradient (w.r.t input) along the (approximately) perpendicular direction to the tangent space of data manifold is a reason for the vulnerability over adversarial attacks. Based on our investigation, we further present a new robust learning algorithm which encourages a larger gradient component in the tangent space of data manifold, suppressing the gradient leaking phenomenon consequently. Experiments on various tasks demonstrate the eﬀectiveness of our algorithm despite its simplicity.},
	language = {en},
	urldate = {2020-12-22},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Li, Yueru and Cheng, Shuyu and Su, Hang and Zhu, Jun},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {753--769},
}

@inproceedings{shafahi_universal_2020,
	title = {Universal {Adversarial} {Training}},
	volume = {34},
	url = {https://aaai.org/ojs/index.php/AAAI/article/view/6017},
	doi = {10.1609/aaai.v34i04.6017},
	abstract = {Standard adversarial attacks change the predicted class label of a selected image by adding specially tailored small perturbations to its pixels. In contrast, a universal perturbation is an update that can be added to any image in a broad class of images, while still changing the predicted class label. We study the efﬁcient generation of universal adversarial perturbations, and also efﬁcient methods for hardening networks to these attacks. We propose a simple optimization-based universal attack that reduces the top-1 accuracy of various network architectures on ImageNet to less than 20\%, while learning the universal perturbation 13× faster than the standard method.},
	language = {en},
	urldate = {2020-07-23},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Shafahi, Ali and Najibi, Mahyar and Xu, Zheng and Dickerson, John and Davis, Larry S. and Goldstein, Tom},
	month = apr,
	year = {2020},
	pages = {5636--5643},
}

@inproceedings{vedaldi_simple_2020,
	address = {Cham},
	title = {A {Simple} {Way} to {Make} {Neural} {Networks} {Robust} {Against} {Diverse} {Image} {Corruptions}},
	volume = {12348},
	isbn = {978-3-030-58579-2 978-3-030-58580-8},
	url = {https://link.springer.com/10.1007/978-3-030-58580-8_4},
	doi = {10.1007/978-3-030-58580-8_4},
	abstract = {The human visual system is remarkably robust against a wide range of naturally occurring variations and corruptions like rain or snow. In contrast, the performance of modern image recognition models strongly degrades when evaluated on previously unseen corruptions. Here, we demonstrate that a simple but properly tuned training with additive Gaussian and Speckle noise generalizes surprisingly well to unseen corruptions, easily reaching the state of the art on the corruption benchmark ImageNet-C (with ResNet50) and on MNIST-C. We build on top of these strong baseline results and show that an adversarial training of the recognition model against locally correlated worst-case noise distributions leads to an additional increase in performance. This regularization can be combined with previously proposed defense methods for further improvement.},
	language = {en},
	urldate = {2021-12-06},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Rusak, Evgenia and Schott, Lukas and Zimmermann, Roland S. and Bitterwolf, Julian and Bringmann, Oliver and Bethge, Matthias and Brendel, Wieland},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {53--69},
}

@article{gowal_uncovering_2021,
	title = {Uncovering the {Limits} of {Adversarial} {Training} against {Norm}-{Bounded} {Adversarial} {Examples}},
	url = {http://arxiv.org/abs/2010.03593},
	abstract = {Adversarial training and its variants have become de facto standards for learning robust deep neural networks. In this paper, we explore the landscape around adversarial training in a bid to uncover its limits. We systematically study the effect of different training losses, model sizes, activation functions, the addition of unlabeled data (through pseudo-labeling) and other factors on adversarial robustness. We discover that it is possible to train robust models that go well beyond state-of-the-art results by combining larger models, Swish/SiLU activations and model weight averaging. We demonstrate large improvements on CIFAR-10 and CIFAR-100 against \${\textbackslash}ell\_{\textbackslash}infty\$ and \${\textbackslash}ell\_2\$ norm-bounded perturbations of size \$8/255\$ and \$128/255\$, respectively. In the setting with additional unlabeled data, we obtain an accuracy under attack of 65.88\% against \${\textbackslash}ell\_{\textbackslash}infty\$ perturbations of size \$8/255\$ on CIFAR-10 (+6.35\% with respect to prior art). Without additional data, we obtain an accuracy under attack of 57.20\% (+3.46\%). To test the generality of our findings and without any additional modifications, we obtain an accuracy under attack of 80.53\% (+7.62\%) against \${\textbackslash}ell\_2\$ perturbations of size \$128/255\$ on CIFAR-10, and of 36.88\% (+8.46\%) against \${\textbackslash}ell\_{\textbackslash}infty\$ perturbations of size \$8/255\$ on CIFAR-100. All models are available at https://github.com/deepmind/deepmind-research/tree/master/adversarial\_robustness.},
	urldate = {2021-12-04},
	journal = {arXiv:2010.03593 [cs, stat]},
	author = {Gowal, Sven and Qin, Chongli and Uesato, Jonathan and Mann, Timothy and Kohli, Pushmeet},
	month = mar,
	year = {2021},
	note = {arXiv: 2010.03593},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{cubuk_randaugment_2020,
	title = {Randaugment: {Practical} automated data augmentation with a reduced search space},
	language = {en},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Cubuk, Ekin D. and Zoph, Barret and Shlens, Jonathon and Le, Quoc V.},
	year = {2020},
}

@inproceedings{gowal_achieving_2020,
	address = {Seattle, WA, USA},
	title = {Achieving {Robustness} in the {Wild} via {Adversarial} {Mixing} {With} {Disentangled} {Representations}},
	isbn = {978-1-72817-168-5},
	url = {https://ieeexplore.ieee.org/document/9156449/},
	doi = {10.1109/CVPR42600.2020.00129},
	abstract = {Recent research has made the surprising ﬁnding that state-of-the-art deep learning models sometimes fail to generalize to small variations of the input. Adversarial training has been shown to be an effective approach to overcome this problem. However, its application has been limited to enforcing invariance to analytically deﬁned transformations like ℓp-norm bounded perturbations. Such perturbations do not necessarily cover plausible real-world variations that preserve the semantics of the input (such as a change in lighting conditions). In this paper, we propose a novel approach to express and formalize robustness to these kinds of real-world transformations of the input. The two key ideas underlying our formulation are (1) leveraging disentangled representations of the input to deﬁne different factors of variations, and (2) generating new input images by adversarially composing the representations of different images. We use a StyleGAN model to demonstrate the efﬁcacy of this framework. Specifically, we leverage the disentangled latent representations computed by a StyleGAN model to generate perturbations of an image that are similar to real-world variations (like adding make-up, or changing the skin-tone of a person) and train models to be invariant to these perturbations. Extensive experiments show that our method improves generalization and reduces the effect of spurious correlations (reducing the error rate of a “smile” detector by 21\% for example).},
	language = {en},
	urldate = {2021-12-03},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Gowal, Sven and Qin, Chongli and Huang, Po-Sen and Cemgil, Taylan and Dvijotham, Krishnamurthy and Mann, Timothy and Kohli, Pushmeet},
	month = jun,
	year = {2020},
	pages = {1208--1217},
}

@inproceedings{hendrycks_many_2021,
	title = {The {Many} {Faces} of {Robustness}: {A} {Critical} {Analysis} of {Out}-of-{Distribution} {Generalization}},
	abstract = {We introduce four new real-world distribution shift datasets consisting of changes in image style, image blurriness, geographic location, camera operation, and more. With our new datasets, we take stock of previously proposed methods for improving out-of-distribution robustness and put them to the test. We ﬁnd that using larger models and artiﬁcial data augmentations can improve robustness on realworld distribution shifts, contrary to claims in prior work. We ﬁnd improvements in artiﬁcial robustness benchmarks can transfer to real-world distribution shifts, contrary to claims in prior work. Motivated by our observation that data augmentations can help with real-world distribution shifts, we also introduce a new data augmentation method which advances the state-of-the-art and outperforms models pretrained with 1000× more labeled data. Overall we ﬁnd that some methods consistently help with distribution shifts in texture and local image statistics, but these methods do not help with some other distribution shifts like geographic changes. Our results show that future research must study multiple distribution shifts simultaneously, as we demonstrate that no evaluated method consistently improves robustness.},
	language = {en},
	booktitle = {International {Conference} on {Computer} {Vision}},
	author = {Hendrycks, Dan and Basart, Steven and Mu, Norman and Kadavath, Saurav and Wang, Frank and Dorundo, Evan and Desai, Rahul and Zhu, Tyler and Parajuli, Samyak and Guo, Mike and Song, Dawn and Steinhardt, Jacob and Gilmer, Justin},
	year = {2021},
	pages = {10},
}

@inproceedings{hase_out--distribution_2021,
	title = {The {Out}-of-{Distribution} {Problem} in {Explainability} and {Search} {Methods} for {Feature} {Importance} {Explanations}},
	url = {https://openreview.net/forum?id=HCrp4pdk2i},
	abstract = {We present a solution to the out-of-distribution problem which feature importance estimates suffer from and propose a new search-based explanation methods that outperforms strong baselines on...},
	language = {en},
	urldate = {2021-12-02},
	author = {Hase, Peter and Xie, Harry and Bansal, Mohit},
	month = may,
	year = {2021},
}

@inproceedings{lee_towards_2021,
	title = {Towards {Better} {Understanding} of {Training} {Certifiably} {Robust} {Models} against {Adversarial} {Examples}},
	url = {https://openreview.net/forum?id=b18Az57ioHn},
	abstract = {We identify a key factor that influences the performance of certifiable training: smoothness of the loss landscape.},
	language = {en},
	urldate = {2021-12-02},
	author = {Lee, Sungyoon and Lee, Woojin and Park, Jinseong and Lee, Jaewook},
	month = may,
	year = {2021},
}

@inproceedings{lubana_beyond_2021,
	title = {Beyond {BatchNorm}: {Towards} a {Unified} {Understanding} of {Normalization} in {Deep} {Learning}},
	shorttitle = {Beyond {BatchNorm}},
	url = {https://openreview.net/forum?id=DbxKZvfOIhu},
	abstract = {We identify key properties in randomly initialized networks that accurately determine success and failure modes of different normalization layers},
	language = {en},
	urldate = {2021-12-02},
	author = {Lubana, Ekdeep Singh and Dick, Robert and Tanaka, Hidenori},
	month = may,
	year = {2021},
}

@inproceedings{bai_understanding_2021,
	title = {Understanding and {Improving} {Early} {Stopping} for {Learning} with {Noisy} {Labels}},
	url = {https://openreview.net/forum?id=KbV-UZRKb3g},
	abstract = {The memorization effect of deep neural network (DNN) plays a pivotal role in many state-of-the-art label-noise learning methods.  To exploit this property, the early stopping trick, which stops the...},
	language = {en},
	urldate = {2021-12-02},
	author = {Bai, Yingbin and Yang, Erkun and Han, Bo and Yang, Yanhua and Li, Jiatong and Mao, Yinian and Niu, Gang and Liu, Tongliang},
	month = may,
	year = {2021},
}

@inproceedings{zimmermann_how_2021,
	title = {How {Well} do {Feature} {Visualizations} {Support} {Causal} {Understanding} of {CNN} {Activations}?},
	url = {https://openreview.net/forum?id=vLPqnPf9k0},
	abstract = {Using psychophysical experiments, we show that state-of-the-art synthetic feature visualizations do not support causal understanding much better than no visualizations, and only similarly well as...},
	language = {en},
	urldate = {2021-12-02},
	author = {Zimmermann, Roland Simon and Borowski, Judy and Geirhos, Robert and Bethge, Matthias and Wallis, Thomas S. A. and Brendel, Wieland},
	month = may,
	year = {2021},
}

@inproceedings{liu_learning_2021,
	title = {Learning {Causal} {Semantic} {Representation} for {Out}-of-{Distribution} {Prediction}},
	url = {https://openreview.net/forum?id=-msETI57gCH},
	abstract = {A supervised generative model with guarantees on the identifiability of the latent cause of prediction and on the generalizability to out-of-distribution cases.},
	language = {en},
	urldate = {2021-12-02},
	author = {Liu, Chang and Sun, Xinwei and Wang, Jindong and Tang, Haoyue and Li, Tao and Qin, Tao and Chen, Wei and Liu, Tie-Yan},
	month = may,
	year = {2021},
}

@inproceedings{ling_editgan_2021,
	title = {{EditGAN}: {High}-{Precision} {Semantic} {Image} {Editing}},
	shorttitle = {{EditGAN}},
	url = {https://openreview.net/forum?id=jLHWRxwc7_f},
	abstract = {Here, we propose EditGAN, a novel method for high-quality, high-precision semantic image editing, allowing users to edit images by modifying their highly detailed part segmentation masks.},
	language = {en},
	urldate = {2021-12-02},
	author = {Ling, Huan and Kreis, Karsten and Li, Daiqing and Kim, Seung Wook and Torralba, Antonio and Fidler, Sanja},
	month = may,
	year = {2021},
}

@inproceedings{qin_improving_2021,
	title = {Improving {Calibration} through the {Relationship} with {Adversarial} {Robustness}},
	url = {https://openreview.net/forum?id=NJex-5TZIQa},
	abstract = {Neural networks lack adversarial robustness, i.e., they are vulnerable to adversarial examples that through small perturbations to inputs cause incorrect predictions. Further, trust is undermined...},
	language = {en},
	urldate = {2021-12-02},
	author = {Qin, Yao and Wang, Xuezhi and Beutel, Alex and Chi, Ed},
	month = may,
	year = {2021},
}

@inproceedings{wu_wider_2021,
	title = {Do {Wider} {Neural} {Networks} {Really} {Help} {Adversarial} {Robustness}?},
	url = {https://openreview.net/forum?id=wxjtOI_8jO},
	abstract = {We study the relationship between adversarial robustness and the width of deep neural networks.},
	language = {en},
	urldate = {2021-12-02},
	author = {Wu, Boxi and Chen, Jinghui and Cai, Deng and He, Xiaofei and Gu, Quanquan},
	month = may,
	year = {2021},
}

@inproceedings{shu_encoding_2021,
	title = {Encoding {Robustness} to {Image} {Style} via {Adversarial} {Feature} {Perturbations}},
	url = {https://openreview.net/forum?id=A-RON3lv-aR},
	abstract = {This work proposes an adversarial feature perturbation method based on feature normalization statistics, to improve model's generalization to out-of-distribution data.},
	language = {en},
	urldate = {2021-12-02},
	author = {Shu, Manli and Wu, Zuxuan and Goldblum, Micah and Goldstein, Tom},
	month = may,
	year = {2021},
}

@inproceedings{fowl_adversarial_2021,
	title = {Adversarial {Examples} {Make} {Strong} {Poisons}},
	url = {https://openreview.net/forum?id=DE8MOQIgFTK},
	abstract = {We find that adversarial examples make stronger availability poisons than other methods designed specifically for data poisoning.},
	language = {en},
	urldate = {2021-12-02},
	author = {Fowl, Liam H. and Goldblum, Micah and Chiang, Ping-yeh and Geiping, Jonas and Czaja, Wojciech and Goldstein, Tom},
	month = may,
	year = {2021},
}

@inproceedings{xing_algorithmic_2021,
	title = {On the {Algorithmic} {Stability} of {Adversarial} {Training}},
	url = {https://openreview.net/forum?id=xz80iPFIjvG},
	abstract = {Theoretical investigation on the algorithmic stability of adversarial training.},
	language = {en},
	urldate = {2021-12-02},
	author = {Xing, Yue and Song, Qifan and Cheng, Guang},
	month = may,
	year = {2021},
}

@inproceedings{tsai_formalizing_2021,
	title = {Formalizing {Generalization} and {Adversarial} {Robustness} of {Neural} {Networks} to {Weight} {Perturbations}},
	url = {https://openreview.net/forum?id=hOG8swMRmY},
	abstract = {We provide a comprehensive theoretical analysis on the generalization and robustness of neural networks against weight perturbations, and propose a new theory-driven training loss.},
	language = {en},
	urldate = {2021-12-02},
	author = {Tsai, Yu-Lin and Hsu, Chia-Yi and Yu, Chia-Mu and Chen, Pin-Yu},
	month = may,
	year = {2021},
}

@inproceedings{slack_counterfactual_2021,
	title = {Counterfactual {Explanations} {Can} {Be} {Manipulated}},
	url = {https://openreview.net/forum?id=iaO_IH7CnGJ},
	abstract = {Counterfactual explanations that hill climb the counterfactual objective are not robust and vulnerable to manipulation by adversaries.},
	language = {en},
	urldate = {2021-12-02},
	author = {Slack, Dylan Z. and Hilgard, Sophie and Lakkaraju, Himabindu and Singh, Sameer},
	month = may,
	year = {2021},
}

@inproceedings{crabbe_explaining_2021,
	title = {Explaining {Latent} {Representations} with a {Corpus} of {Examples}},
	url = {https://openreview.net/forum?id=PIcuKeiWvj-},
	abstract = {We introduce SimplEx, a method that decomposes latent representations of test examples in terms of the representations of a corpus of examples.},
	language = {en},
	urldate = {2021-12-02},
	author = {Crabbé, Jonathan and Qian, Zhaozhi and Imrie, Fergus and Schaar, Mihaela van der},
	month = may,
	year = {2021},
}

@inproceedings{lu_influence_2021,
	title = {Influence {Patterns} for {Explaining} {Information} {Flow} in {BERT}},
	url = {https://openreview.net/forum?id=FYDE3I9fev0},
	abstract = {This paper introduces a new technique for explaining the information flow in BERT's computational graph using gradient-based method.},
	language = {en},
	urldate = {2021-12-02},
	author = {Lu, Kaiji and Wang, Zifan and Mardziel, Piotr and Datta, Anupam},
	month = may,
	year = {2021},
}

@inproceedings{paleja_utility_2021,
	title = {The {Utility} of {Explainable} {AI} in {Ad} {Hoc} {Human}-{Machine} {Teaming}},
	url = {https://openreview.net/forum?id=w6U6g5Bvug},
	abstract = {We present two human-subject studies quantifying the benefits of deploying Explainable AI techniques within a human-machine teaming scenario, finding that the benefits of xAI are not universal.},
	language = {en},
	urldate = {2021-12-02},
	author = {Paleja, Rohan R. and Ghuy, Muyleng and Arachchige, Nadun Ranawaka and Jensen, Reed and Gombolay, Matthew},
	month = may,
	year = {2021},
}

@inproceedings{zhang_fine-grained_2021,
	title = {Fine-{Grained} {Neural} {Network} {Explanation} by {Identifying} {Input} {Features} with {Predictive} {Information}},
	url = {https://openreview.net/forum?id=HglgPZAYhcG},
	abstract = {We propose a feature attribution method via identifying input features with predictive information.},
	language = {en},
	urldate = {2021-12-02},
	author = {Zhang, Yang and Khakzar, Ashkan and Li, Yawei and Farshad, Azade and Kim, Seong Tae and Navab, Nassir},
	month = may,
	year = {2021},
}

@inproceedings{teso_interactive_2021,
	title = {Interactive {Label} {Cleaning} with {Example}-based {Explanations}},
	url = {https://openreview.net/forum?id=T6m9bNI7C__},
	abstract = {Approach that enables humans to improve data and models by interacting via example-based explanations selected using influence functions},
	language = {en},
	urldate = {2021-12-02},
	author = {Teso, Stefano and Bontempelli, Andrea and Giunchiglia, Fausto and Passerini, Andrea},
	month = may,
	year = {2021},
}

@inproceedings{wickramanayake_explanation-based_2021,
	title = {Explanation-based {Data} {Augmentation} for {Image} {Classification}},
	url = {https://openreview.net/forum?id=Ydlco-tfIG},
	abstract = {An explanation based data augmentation approach is proposed to improve accuracy of image classifiers.},
	language = {en},
	urldate = {2021-12-02},
	author = {Wickramanayake, Sandareka and Hsu, Wynne and Lee, Mong-Li},
	month = may,
	year = {2021},
}

@inproceedings{ghalebikesabi_locality_2021,
	title = {On {Locality} of {Local} {Explanation} {Models}},
	url = {https://openreview.net/forum?id=UKoV0-BamX4},
	abstract = {We consider the formulation of  neighbourhood reference distributions that improve the local interpretability of Shapley values.},
	language = {en},
	urldate = {2021-12-02},
	author = {Ghalebikesabi, Sahra and Ter-Minassian, Lucile and DiazOrdaz, Karla and Holmes, Christopher C.},
	month = may,
	year = {2021},
}

@inproceedings{kumar_shapley_2021,
	title = {Shapley {Residuals}: {Quantifying} the limits of the {Shapley} value for explanations},
	shorttitle = {Shapley {Residuals}},
	url = {https://openreview.net/forum?id=0XJDcC07tQs},
	abstract = {We propose expressing the information lost by the computation of Shapley values with a residual which quantify the extent to which interaction effects are being "lost."},
	language = {en},
	urldate = {2021-12-02},
	author = {Kumar, I. Elizabeth and Scheidegger, Carlos and Venkatasubramanian, Suresh and Friedler, Sorelle},
	month = may,
	year = {2021},
}

@inproceedings{chan_salkg_2021,
	title = {{SalKG}: {Learning} {From} {Knowledge} {Graph} {Explanations} for {Commonsense} {Reasoning}},
	shorttitle = {{SalKG}},
	url = {https://openreview.net/forum?id=FUxXaBop-J_},
	abstract = {Train KG-augmented models using extra supervision from KG saliency explanations.},
	language = {en},
	urldate = {2021-12-02},
	author = {Chan, Aaron and Xu, Jiashu and Long, Boyuan and Sanyal, Soumya and Gupta, Tanishq and Ren, Xiang},
	month = may,
	year = {2021},
}

@inproceedings{slack_reliable_2021,
	title = {Reliable {Post} hoc {Explanations}: {Modeling} {Uncertainty} in {Explainability}},
	shorttitle = {Reliable {Post} hoc {Explanations}},
	url = {https://openreview.net/forum?id=rqfq0CYIekd},
	abstract = {Black box explanations with uncertainty estimates are more consistent, stable, and reliable, while providing guarantees on the uncertainty.},
	language = {en},
	urldate = {2021-12-02},
	author = {Slack, Dylan Z. and Hilgard, Sophie and Singh, Sameer and Lakkaraju, Himabindu},
	month = may,
	year = {2021},
}

@inproceedings{fel_look_2021,
	title = {Look at the {Variance}! {Efficient} {Black}-box {Explanations} with {Sobol}-based {Sensitivity} {Analysis}},
	url = {https://openreview.net/forum?id=hA-PHQGOjqQ},
	abstract = {We explain black-box models by measuring how their predictions vary when we perturb their input at various location using Sobol.},
	language = {en},
	urldate = {2021-12-02},
	author = {Fel, Thomas and Cadene, Remi and Chalvidal, Mathieu and Cord, Matthieu and Vigouroux, David and Serre, Thomas},
	month = may,
	year = {2021},
}

@inproceedings{blanc_provably_2021,
	title = {Provably efficient, succinct, and precise explanations},
	url = {https://openreview.net/forum?id=DV06vy74q92},
	abstract = {We design an efficient algorithm for explaining the predictions of an arbitrary blackbox model, with provable guarantees on the succinctness and precision of the explanations that it returns.},
	language = {en},
	urldate = {2021-12-02},
	author = {Blanc, Guy and Lange, Jane and Tan, Li-Yang},
	month = may,
	year = {2021},
}

@inproceedings{yao_refining_2021,
	title = {Refining {Language} {Models} with {Compositional} {Explanations}},
	url = {https://openreview.net/forum?id=dkw9OQMn1t},
	abstract = {Pre-trained language models have been successful on text classification tasks, but are prone to learning spurious correlations from biased datasets, and are thus vulnerable when making inferences...},
	language = {en},
	urldate = {2021-12-02},
	author = {Yao, Huihan and Chen, Ying and Ye, Qinyuan and Jin, Xisen and Ren, Xiang},
	month = may,
	year = {2021},
}

@inproceedings{shitole_one_2021,
	title = {One {Explanation} is {Not} {Enough}: {Structured} {Attention} {Graphs} for {Image} {Classification}},
	shorttitle = {One {Explanation} is {Not} {Enough}},
	url = {https://openreview.net/forum?id=k5Kbs9uPGP9},
	abstract = {We introduce multiple minimally sufficient explanations for deep image classification with techniques to efficiently search them and succinctly present them.},
	language = {en},
	urldate = {2021-12-02},
	author = {Shitole, Vivswan and Fuxin, Li and Kahng, Minsuk and Tadepalli, Prasad and Fern, Alan},
	month = may,
	year = {2021},
}

@inproceedings{bai_clustering_2021,
	title = {Clustering {Effect} of {Adversarial} {Robust} {Models}},
	url = {https://openreview.net/forum?id=5ga5mfbGsRM},
	abstract = {Clustering Effect of (Linearized) Adversarial Robust Models},
	language = {en},
	urldate = {2021-12-02},
	author = {Bai, Yang and Yan, Xin and Jiang, Yong and Xia, Shu-Tao and Wang, Yisen},
	month = may,
	year = {2021},
}

@inproceedings{singla_shift_2021,
	title = {Shift {Invariance} {Can} {Reduce} {Adversarial} {Robustness}},
	url = {https://openreview.net/forum?id=tqi_45ApQzF},
	abstract = {We provide theoretical and empirical evidence that the property of shift invariance in convolutional neural networks can decrease adversarial robustness.},
	language = {en},
	urldate = {2021-12-02},
	author = {Singla, Vasu and Ge, Songwei and Basri, Ronen and Jacobs, David},
	month = may,
	year = {2021},
}

@inproceedings{kim_distilling_2021,
	title = {Distilling {Robust} and {Non}-{Robust} {Features} in {Adversarial} {Examples} by {Information} {Bottleneck}},
	url = {https://openreview.net/forum?id=90M-91IZ0JC},
	abstract = {We present a way of distilling the robust and non-robust features in adversarial examples, using Information Bottleneck.},
	language = {en},
	urldate = {2021-12-02},
	author = {Kim, Junho and Lee, Byung-Kwan and Ro, Yong Man},
	month = may,
	year = {2021},
}

@inproceedings{yang_class-disentanglement_2021,
	title = {Class-{Disentanglement} and {Applications} in {Adversarial} {Detection} and {Defense}},
	url = {https://openreview.net/forum?id=jFMzBeLyTc0},
	abstract = {We propose a simple VAE+classifier structure to separate the class information from an image by decomposing it into two part.},
	language = {en},
	urldate = {2021-12-02},
	author = {Yang, Kaiwen and Zhou, Tianyi and Zhang, Yonggang and Tian, Xinmei and Tao, Dacheng},
	month = may,
	year = {2021},
}

@inproceedings{ren_towards_2021,
	title = {Towards a {Unified} {Game}-{Theoretic} {View} of {Adversarial} {Perturbations} and {Robustness}},
	url = {https://openreview.net/forum?id=fMaIxda5Y6K},
	abstract = {This paper provides a unified view to explain different adversarial attacks and defense methods, i.e. the view of multi-order interactions between input variables of DNNs.},
	language = {en},
	urldate = {2021-12-02},
	author = {Ren, Jie and Zhang, Die and Wang, Yisen and Chen, Lu and Zhou, Zhanpeng and Chen, Yiting and Cheng, Xu and Wang, Xin and Zhou, Meng and Shi, Jie and Zhang, Quanshi},
	month = may,
	year = {2021},
}

@inproceedings{arenas_foundations_2021,
	title = {Foundations of {Symbolic} {Languages} for {Model} {Interpretability}},
	url = {https://openreview.net/forum?id=Jyxmk4wUoQV},
	abstract = {We propose a systematic study of model interpretability by considering a minimalistic symbolic query language tailored for interpretability, studying its complexity of evaluation and presenting a...},
	language = {en},
	urldate = {2021-12-02},
	author = {Arenas, Marcelo and Báez, Daniel and Barcelo, Pablo and Pérez, Jorge and Subercaseaux, Bernardo},
	month = may,
	year = {2021},
}

@inproceedings{pan_ia-red2_2021,
	title = {{IA}-{RED2}: {Interpretability}-{Aware} {Redundancy} {Reduction} for {Vision} {Transformers}},
	shorttitle = {{IA}-{RED}\${\textasciicircum}2\$},
	url = {https://openreview.net/forum?id=7X_sBjIwtm9},
	abstract = {An input-dependent and interpretable dynamic inference framework for vision transformer, which adaptively decides the patch tokens to compute per input instance.},
	language = {en},
	urldate = {2021-12-02},
	author = {Pan, Bowen and Panda, Rameswar and Jiang, Yifan and Wang, Zhangyang and Feris, Rogerio and Oliva, Aude},
	month = may,
	year = {2021},
}

@inproceedings{ismail_improving_2021,
	title = {Improving {Deep} {Learning} {Interpretability} by {Saliency} {Guided} {Training}},
	url = {https://openreview.net/forum?id=x4zs7eC-BsI},
	abstract = {We introduce a training procedure to improve model interpretability},
	language = {en},
	urldate = {2021-12-02},
	author = {Ismail, Aya Abdelsalam and Bravo, Hector Corrada and Feizi, Soheil},
	month = may,
	year = {2021},
}

@inproceedings{puri_cofrnets_2021,
	title = {{CoFrNets}: {Interpretable} {Neural} {Architecture} {Inspired} by {Continued} {Fractions}},
	shorttitle = {{CoFrNets}},
	url = {https://openreview.net/forum?id=kGXlIEQgvC},
	abstract = {Propose a new interpretable neural architecture inspired by continued fractions},
	language = {en},
	urldate = {2021-12-02},
	author = {Puri, Isha and Dhurandhar, Amit and Pedapati, Tejaswini and Shanmugam, Karthikeyan and Wei, Dennis and Varshney, Kush R.},
	month = may,
	year = {2021},
}

@inproceedings{agarwal_neural_2021,
	title = {Neural {Additive} {Models}: {Interpretable} {Machine} {Learning} with {Neural} {Nets}},
	shorttitle = {Neural {Additive} {Models}},
	url = {https://openreview.net/forum?id=wHkKTW2wrmm},
	abstract = {We propose Neural Additive Models that combine some of the expressivity of DNNs with the inherent intelligibility of generalized additive models.},
	language = {en},
	urldate = {2021-12-02},
	author = {Agarwal, Rishabh and Melnick, Levi and Frosst, Nicholas and Zhang, Xuezhou and Lengerich, Ben and Caruana, Rich and Hinton, Geoffrey},
	month = may,
	year = {2021},
}

@inproceedings{wang_self-interpretable_2021,
	title = {Self-{Interpretable} {Model} with {Transformation} {Equivariant} {Interpretation}},
	url = {https://openreview.net/forum?id=YlM3tey8Z5I},
	abstract = {We propose a self-interpretable model that has transformation-equivariant interpretations, and comparable expressive power as benchmark black-box model.},
	language = {en},
	urldate = {2021-12-02},
	author = {Wang, Yipei and Wang, Xiaoqian},
	month = may,
	year = {2021},
}

@article{wang_hybrid_2021,
	title = {Hybrid {Predictive} {Models}: {When} an {Interpretable} {Model} {Collaborates} with a {Black}-box {Model}},
	volume = {22},
	issn = {1533-7928},
	shorttitle = {Hybrid {Predictive} {Models}},
	url = {http://jmlr.org/papers/v22/19-325.html},
	language = {en},
	number = {137},
	urldate = {2021-12-01},
	journal = {Journal of Machine Learning Research},
	author = {Wang, Tong and Lin, Qihang},
	year = {2021},
	pages = {1--38},
}

@article{xie_smooth_2020,
	title = {Smooth {Adversarial} {Training}},
	url = {https://arxiv.org/abs/2006.14536v2},
	abstract = {It is commonly believed that networks cannot be both accurate and robust, that gaining robustness means losing accuracy. It is also generally believed that, unless making networks larger, network architectural elements would otherwise matter little in improving adversarial robustness. Here we present evidence to challenge these common beliefs by a careful study about adversarial training. Our key observation is that the widely-used ReLU activation function significantly weakens adversarial training due to its non-smooth nature. Hence we propose smooth adversarial training (SAT), in which we replace ReLU with its smooth approximations to strengthen adversarial training. The purpose of smooth activation functions in SAT is to allow it to find harder adversarial examples and compute better gradient updates during adversarial training. Compared to standard adversarial training, SAT improves adversarial robustness for "free", i.e., no drop in accuracy and no increase in computational cost. For example, without introducing additional computations, SAT significantly enhances ResNet-50's robustness from 33.0\% to 42.3\%, while also improving accuracy by 0.9\% on ImageNet. SAT also works well with larger networks: it helps EfficientNet-L1 to achieve 82.2\% accuracy and 58.6\% robustness on ImageNet, outperforming the previous state-of-the-art defense by 9.5\% for accuracy and 11.6\% for robustness. Models are available at https://github.com/cihangxie/SmoothAdversarialTraining.},
	language = {en},
	urldate = {2021-11-17},
	author = {Xie, Cihang and Tan, Mingxing and Gong, Boqing and Yuille, Alan and Le, Quoc V.},
	month = jun,
	year = {2020},
}

@inproceedings{panda_instance-wise_2021,
	address = {Nashville, TN, USA},
	title = {Instance-wise {Causal} {Feature} {Selection} for {Model} {Interpretation}},
	isbn = {978-1-66544-899-4},
	url = {https://ieeexplore.ieee.org/document/9523197/},
	doi = {10.1109/CVPRW53098.2021.00194},
	abstract = {We formulate a causal extension to the recently introduced paradigm of instance-wise feature selection to explain black-box visual classiﬁers. Our method selects a subset of input features that has the greatest causal effect on the model’s output. We quantify the causal inﬂuence of a subset of features by the Relative Entropy Distance measure. Under certain assumptions this is equivalent to the conditional mutual information between the selected subset and the output variable. The resulting causal selections are sparser and cover salient objects in the scene. We show the efﬁcacy of our approach on multiple vision datasets by measuring the post-hoc accuracy and Average Causal Effect of selected features on the model’s output.},
	language = {en},
	urldate = {2021-11-12},
	booktitle = {2021 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops} ({CVPRW})},
	publisher = {IEEE},
	author = {Panda, Pranoy and Kancheti, Sai Srinivas and Balasubramanian, Vineeth N},
	month = jun,
	year = {2021},
	pages = {1756--1759},
}

@inproceedings{miyato_spectral_2018,
	title = {Spectral {Normalization} for {Generative} {Adversarial} {Networks}},
	url = {https://openreview.net/forum?id=B1QRgziT-},
	abstract = {We propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator of GANs.},
	language = {en},
	urldate = {2021-10-29},
	author = {Miyato, Takeru and Kataoka, Toshiki and Koyama, Masanori and Yoshida, Yuichi},
	month = feb,
	year = {2018},
}

@inproceedings{hendrycks_using_2019,
	title = {Using {Pre}-{Training} {Can} {Improve} {Model} {Robustness} and {Uncertainty}},
	url = {https://proceedings.mlr.press/v97/hendrycks19a.html},
	abstract = {He et al. (2018) have called into question the utility of pre-training by showing that training from scratch can often yield similar performance to pre-training. We show that although pre-training may not improve performance on traditional classification metrics, it improves model robustness and uncertainty estimates. Through extensive experiments on label corruption, class imbalance, adversarial examples, out-of-distribution detection, and confidence calibration, we demonstrate large gains from pre-training and complementary effects with task-specific methods. We show approximately a 10\% absolute improvement over the previous state-of-the-art in adversarial robustness. In some cases, using pre-training without task-specific methods also surpasses the state-of-the-art, highlighting the need for pre-training when evaluating future methods on robustness and uncertainty tasks.},
	language = {en},
	urldate = {2021-10-22},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Hendrycks, Dan and Lee, Kimin and Mazeika, Mantas},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {2712--2721},
}

@article{recht_cifar-10_2018,
	title = {Do {CIFAR}-10 {Classifiers} {Generalize} to {CIFAR}-10?},
	url = {http://arxiv.org/abs/1806.00451},
	abstract = {Machine learning is currently dominated by largely experimental work focused on improvements in a few key tasks. However, the impressive accuracy numbers of the best performing models are questionable because the same test sets have been used to select these models for multiple years now. To understand the danger of overﬁtting, we measure the accuracy of CIFAR-10 classiﬁers by creating a new test set of truly unseen images. Although we ensure that the new test set is as close to the original data distribution as possible, we ﬁnd a large drop in accuracy (4\% to 10\%) for a broad range of deep learning models. Yet, more recent models with higher original accuracy show a smaller drop and better overall performance, indicating that this drop is likely not due to overﬁtting based on adaptivity. Instead, we view our results as evidence that current accuracy numbers are brittle and susceptible to even minute natural variations in the data distribution.},
	language = {en},
	urldate = {2021-10-22},
	journal = {arXiv:1806.00451 [cs, stat]},
	author = {Recht, Benjamin and Roelofs, Rebecca and Schmidt, Ludwig and Shankar, Vaishaal},
	month = jun,
	year = {2018},
	note = {arXiv: 1806.00451},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{najafi_robustness_2019,
	title = {Robustness to {Adversarial} {Perturbations} in {Learning} from {Incomplete} {Data}},
	volume = {32},
	url = {https://papers.nips.cc/paper/2019/hash/60ad83801910ec976590f69f638e0d6d-Abstract.html},
	urldate = {2021-10-20},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Najafi, Amir and Maeda, Shin-ichi and Koyama, Masanori and Miyato, Takeru},
	year = {2019},
}

@inproceedings{dombrowski_explanations_2019,
	title = {Explanations can be manipulated and geometry is to blame},
	abstract = {Explanation methods aim to make neural networks more trustworthy and interpretable. In this paper, we demonstrate a property of explanation methods which is disconcerting for both of these purposes. Namely, we show that explanations can be manipulated arbitrarily by applying visually hardly perceptible perturbations to the input that keep the network’s output approximately constant. We establish theoretically that this phenomenon can be related to certain geometrical properties of neural networks. This allows us to derive an upper bound on the susceptibility of explanations to manipulations. Based on this result, we propose effective mechanisms to enhance the robustness of explanations.},
	language = {en},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Dombrowski, Ann-Kathrin and Alber, Maximillian and Anders, Christopher and Ackermann, Marcel and Müller, Klaus-Robert and Kessel, Pan},
	year = {2019},
	pages = {12},
}

@article{dombrowski_towards_2022,
	title = {Towards robust explanations for deep neural networks},
	volume = {121},
	issn = {0031-3203},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320321003769},
	doi = {10.1016/j.patcog.2021.108194},
	abstract = {Explanation methods shed light on the decision process of black-box classifiers such as deep neural networks. But their usefulness can be compromised because they are susceptible to manipulations. With this work, we aim to enhance the resilience of explanations. We develop a unified theoretical framework for deriving bounds on the maximal manipulability of a model. Based on these theoretical insights, we present three different techniques to boost robustness against manipulation: training with weight decay, smoothing activation functions, and minimizing the Hessian of the network. Our experimental results confirm the effectiveness of these approaches.},
	language = {en},
	urldate = {2021-09-29},
	journal = {Pattern Recognition},
	author = {Dombrowski, Ann-Kathrin and Anders, Christopher J. and Müller, Klaus-Robert and Kessel, Pan},
	month = jan,
	year = {2022},
	keywords = {Adversarial attacks, Explanation method, Manipulation, Neural networks,, Saliency map},
	pages = {108194},
}

@inproceedings{guo_calibration_2017,
	title = {On {Calibration} of {Modern} {Neural} {Networks}},
	url = {https://proceedings.mlr.press/v70/guo17a.html},
	abstract = {Confidence calibration – the problem of predicting probability estimates representative of the true correctness likelihood – is important for classification models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various post-processing calibration methods on state-of-the-art architectures with image and document classification datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling – a single-parameter variant of Platt Scaling – is surprisingly effective at calibrating predictions.},
	language = {en},
	urldate = {2021-09-24},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q.},
	month = jul,
	year = {2017},
	note = {ISSN: 2640-3498},
	pages = {1321--1330},
}

@inproceedings{cheng_self-progressing_2021,
	title = {Self-{Progressing} {Robust} {Training}},
	volume = {35},
	copyright = {Copyright (c) 2021 Association for the Advancement of Artificial Intelligence},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/16874},
	abstract = {Enhancing model robustness under new and even adversarial environments is a crucial milestone toward building trustworthy machine learning systems. Current robust training methods such as adversarial training explicitly uses an ``attack'' (e.g., l\_infty-norm bounded perturbation) to generate adversarial examples during model training for improving adversarial robustness. In this paper, we take a different perspective and propose a new framework SPROUT, self-progressing robust training. During model training, SPROUT progressively adjusts training label distribution via our proposed parametrized label smoothing technique, making training free of attack generation and more scalable. We also motivate SPROUT using a general formulation based on vicinity risk minimization, which includes many robust training methods as special cases.
Compared with state-of-the-art adversarial training methods (PGD-l\_infty and TRADES) under l\_infty-norm bounded attacks and various invariance tests, SPROUT consistently attains superior performance and is more scalable to large neural networks. Our results shed new light on scalable, effective and attack-independent robust training methods.},
	language = {en},
	urldate = {2021-09-24},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Cheng, Minhao and Chen, Pin-Yu and Liu, Sijia and Chang, Shiyu and Hsieh, Cho-Jui and Das, Payel},
	month = may,
	year = {2021},
	note = {Number: 8},
	keywords = {(Deep) Neural Network Algorithms},
	pages = {7107--7115},
}

@inproceedings{li_gradient_2020,
	title = {Gradient {Descent} with {Early} {Stopping} is {Provably} {Robust} to {Label} {Noise} for {Overparameterized} {Neural} {Networks}},
	url = {https://proceedings.mlr.press/v108/li20j.html},
	abstract = {Modern neural networks are typically trained in an over-parameterized regime where the parameters of the model far exceed the size of the training data. Such neural networks in principle have the capacity to (over)fit any set of labels including significantly corrupted ones. Despite this (over)fitting capacity in this paper we demonstrate that such overparameterized networks have an intriguing robustness capability: they are surprisingly robust to label noise when first order methods with early stopping is used to train them. This paper also takes a step towards demystifying this phenomena. Under a rich dataset model, we show that gradient descent is provably robust to noise/corruption on a constant fraction of the labels. In particular, we prove that: (i) In the first few iterations where the updates are still in the vicinity of the initialization gradient descent only fits to the correct labels essentially ignoring the noisy labels. (ii) To start to overfit to the noisy labels network must stray rather far from the initialization which can only occur after many more iterations. Together, these results show that gradient descent with early stopping is provably robust to label noise and shed light on the empirical robustness of deep networks as well as commonly adopted heuristics to prevent overfitting.},
	language = {en},
	urldate = {2021-09-22},
	booktitle = {Proceedings of the {Twenty} {Third} {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Li, Mingchen and Soltanolkotabi, Mahdi and Oymak, Samet},
	month = jun,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {4313--4324},
}

@inproceedings{huang_self-adaptive_2020,
	title = {Self-{Adaptive} {Training}: beyond {Empirical} {Risk} {Minimization}},
	abstract = {We propose self-adaptive training—a new training algorithm that dynamically calibrates training process by model predictions without incurring extra computational cost—to improve generalization of deep learning for potentially corrupted training data. This problem is important to robustly learning from data that are corrupted by, e.g., random noise and adversarial examples. The standard empirical risk minimization (ERM) for such data, however, may easily overﬁt noise and thus suffers from sub-optimal performance. In this paper, we observe that model predictions can substantially beneﬁt the training process: self-adaptive training signiﬁcantly mitigates the overﬁtting issue and improves generalization over ERM under both random and adversarial noise. Besides, in sharp contrast to the recently-discovered double-descent phenomenon in ERM, self-adaptive training exhibits a single-descent error-capacity curve, indicating that such a phenomenon might be a result of overﬁtting of noise. Experiments on the CIFAR and ImageNet datasets verify the effectiveness of our approach in two applications: classiﬁcation with label noise and selective classiﬁcation. The code is available at https://github.com/LayneH/self-adaptive-training.},
	language = {en},
	booktitle = {34th {Conference} on {Neural} {Information} {Processing} {Systems}},
	author = {Huang, Lang and Zhang, Chao and Zhang, Hongyang},
	year = {2020},
	pages = {12},
}

@inproceedings{song_improving_2019,
	title = {Improving the {Generalization} of {Adversarial} {Training} with {Domain} {Adaptation}},
	url = {https://openreview.net/forum?id=SyfIfnC5Ym},
	abstract = {We propose a novel adversarial training with domain adaptation method that significantly improves the generalization ability on adversarial examples from different attacks.},
	language = {en},
	urldate = {2021-09-22},
	author = {Song, Chuanbiao and He, Kun and Wang, Liwei and Hopcroft, John E.},
	year = {2019},
}

@inproceedings{gilmer_adversarial_2018,
	title = {Adversarial {Spheres}},
	url = {https://openreview.net/forum?id=SkthlLkPf},
	abstract = {We explore the phenomenon of adversarial examples on a synthetic dataset},
	language = {en},
	urldate = {2021-09-21},
	booktitle = {International {Conference} on {Learning} {Representations} {Workshop}},
	author = {Gilmer, Justin and Metz, Luke and Faghri, Fartash and Schoenholz, Samuel S. and Raghu, Maithra and Wattenberg, Martin and Goodfellow, Ian},
	month = feb,
	year = {2018},
}

@inproceedings{lee_adversarial_2020,
	title = {Adversarial {Vertex} {Mixup}: {Toward} {Better} {Adversarially} {Robust} {Generalization}},
	shorttitle = {Adversarial {Vertex} {Mixup}},
	url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Lee_Adversarial_Vertex_Mixup_Toward_Better_Adversarially_Robust_Generalization_CVPR_2020_paper.html},
	urldate = {2021-09-15},
	author = {Lee, Saehyung and Lee, Hyungyu and Yoon, Sungroh},
	year = {2020},
	pages = {272--281},
}

@inproceedings{farnia_generalizable_2018,
	title = {Generalizable {Adversarial} {Training} via {Spectral} {Normalization}},
	url = {https://openreview.net/forum?id=Hyx4knR9Ym},
	abstract = {Deep neural networks (DNNs) have set benchmarks on a wide array of supervised learning tasks. Trained DNNs, however, often lack robustness to minor adversarial perturbations to the input, which...},
	language = {en},
	urldate = {2021-09-15},
	author = {Farnia, Farzan and Zhang, Jesse and Tse, David},
	month = sep,
	year = {2018},
}

@inproceedings{hacohen_lets_2020,
	title = {Let’s {Agree} to {Agree}: {Neural} {Networks} {Share} {Classification} {Order} on {Real} {Datasets}},
	shorttitle = {Let’s {Agree} to {Agree}},
	url = {https://proceedings.mlr.press/v119/hacohen20a.html},
	language = {en},
	urldate = {2021-09-12},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Hacohen, Guy and Choshen, Leshem and Weinshall, Daphna},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {3950--3960},
}

@inproceedings{toneva_empirical_2018,
	title = {An {Empirical} {Study} of {Example} {Forgetting} during {Deep} {Neural} {Network} {Learning}},
	url = {https://openreview.net/forum?id=BJlxm30cKm},
	abstract = {We show that catastrophic forgetting occurs within what is considered to be a single task and find that examples that are not prone to forgetting can be removed from the training set without loss...},
	language = {en},
	urldate = {2021-09-12},
	author = {Toneva*, Mariya and Sordoni*, Alessandro and Combes*, Remi Tachet des and Trischler, Adam and Bengio, Yoshua and Gordon, Geoffrey J.},
	month = sep,
	year = {2018},
}

@inproceedings{shafahi_are_2018,
	title = {Are adversarial examples inevitable?},
	url = {https://openreview.net/forum?id=r1lWUoA9FQ},
	abstract = {This paper identifies classes of problems for which adversarial examples are inescapable, and derives fundamental bounds on the susceptibility of any classifier to adversarial examples.},
	language = {en},
	urldate = {2021-09-12},
	author = {Shafahi, Ali and Huang, W. Ronny and Studer, Christoph and Feizi, Soheil and Goldstein, Tom},
	month = sep,
	year = {2018},
}

@inproceedings{stutz_confidence-calibrated_2020,
	title = {Confidence-{Calibrated} {Adversarial} {Training}: {Generalizing} to {Unseen} {Attacks}},
	abstract = {Adversarial training yields robust models against a speciﬁc threat model, e.g., L∞ adversarial examples. Typically robustness does not generalize to previously unseen threat models, e.g., other Lp norms, or larger perturbations. Our conﬁdencecalibrated adversarial training (CCAT) tackles this problem by biasing the model towards low conﬁdence predictions on adversarial examples. By allowing to reject examples with low conﬁdence, robustness generalizes beyond the threat model employed during training. CCAT, trained only on L∞ adversarial examples, increases robustness against larger L∞, L2, L1 and L0 attacks, adversarial frames, distal adversarial examples and corrupted examples and yields better clean accuracy compared to adversarial training. For thorough evaluation we developed novel white- and black-box attacks directly attacking CCAT by maximizing conﬁdence. For each threat model, we use 7 attacks with up to 50 restarts and 5000 iterations and report worst-case robust test error, extended to our conﬁdence-thresholded setting, across all attacks.},
	language = {en},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Stutz, David and Hein, Matthias and Schiele, Bernt},
	year = {2020},
	pages = {12},
}

@article{zhu_understanding_2021,
	title = {Understanding the {Interaction} of {Adversarial} {Training} with {Noisy} {Labels}},
	url = {https://arxiv.org/abs/2102.03482v2},
	abstract = {Noisy labels (NL) and adversarial examples both undermine trained models, but interestingly they have hitherto been studied independently. A recent adversarial training (AT) study showed that the number of projected gradient descent (PGD) steps to successfully attack a point (i.e., find an adversarial example in its proximity) is an effective measure of the robustness of this point. Given that natural data are clean, this measure reveals an intrinsic geometric property -- how far a point is from its class boundary. Based on this breakthrough, in this paper, we figure out how AT would interact with NL. Firstly, we find if a point is too close to its noisy-class boundary (e.g., one step is enough to attack it), this point is likely to be mislabeled, which suggests to adopt the number of PGD steps as a new criterion for sample selection for correcting NL. Secondly, we confirm AT with strong smoothing effects suffers less from NL (without NL corrections) than standard training (ST), which suggests AT itself is an NL correction. Hence, AT with NL is helpful for improving even the natural accuracy, which again illustrates the superiority of AT as a general-purpose robust learning criterion.},
	language = {en},
	urldate = {2021-09-10},
	author = {Zhu, Jianing and Zhang, Jingfeng and Han, Bo and Liu, Tongliang and Niu, Gang and Yang, Hongxia and Kankanhalli, Mohan and Sugiyama, Masashi},
	month = feb,
	year = {2021},
}

@article{allen-zhu_feature_2020,
	title = {Feature {Purification}: {How} {Adversarial} {Training} {Performs} {Robust} {Deep} {Learning}},
	shorttitle = {Feature {Purification}},
	url = {https://arxiv.org/abs/2005.10190v3},
	abstract = {Despite the empirical success of using Adversarial Training to defend deep learning models against adversarial perturbations, so far, it still remains rather unclear what the principles are behind the existence of adversarial perturbations, and what adversarial training does to the neural network to remove them. In this paper, we present a principle that we call Feature Purification, where we show one of the causes of the existence of adversarial examples is the accumulation of certain small dense mixtures in the hidden weights during the training process of a neural network; and more importantly, one of the goals of adversarial training is to remove such mixtures to purify hidden weights. We present both experiments on the CIFAR-10 dataset to illustrate this principle, and a theoretical result proving that for certain natural classification tasks, training a two-layer neural network with ReLU activation using randomly initialized gradient descent indeed satisfies this principle. Technically, we give, to the best of our knowledge, the first result proving that the following two can hold simultaneously for training a neural network with ReLU activation. (1) Training over the original data is indeed non-robust to small adversarial perturbations of some radius. (2) Adversarial training, even with an empirical perturbation algorithm such as FGM, can in fact be provably robust against ANY perturbations of the same radius. Finally, we also prove a complexity lower bound, showing that low complexity models such as linear classifiers, low-degree polynomials, or even the neural tangent kernel for this network, CANNOT defend against perturbations of this same radius, no matter what algorithms are used to train them.},
	language = {en},
	urldate = {2021-09-09},
	author = {Allen-Zhu, Zeyuan and Li, Yuanzhi},
	month = may,
	year = {2020},
}

@inproceedings{donhauser_maximizing_2021,
	title = {Maximizing the robust margin provably overfits on noiseless data},
	url = {https://openreview.net/forum?id=ujQKWaxFkrL},
	abstract = {We show that the robust max-margin solution overfits even on noiseless data yielding a worse performance than ridge regularized and early stopped robust logistic regression.},
	language = {en},
	urldate = {2021-09-09},
	author = {Donhauser, Konstantin and Tifrea, Alexandru and Aerni, Michael and Heckel, Reinhard and Yang, Fanny},
	month = jun,
	year = {2021},
}

@inproceedings{xing_generalization_2021,
	title = {On the {Generalization} {Properties} of {Adversarial} {Training}},
	url = {https://proceedings.mlr.press/v130/xing21b.html},
	language = {en},
	urldate = {2021-09-09},
	booktitle = {International {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Xing, Yue and Song, Qifan and Cheng, Guang},
	month = mar,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {505--513},
}

@inproceedings{song_robust_2019,
	title = {Robust {Local} {Features} for {Improving} the {Generalization} of {Adversarial} {Training}},
	url = {https://openreview.net/forum?id=H1lZJpVFvr},
	abstract = {We propose a new stream of adversarial training approach called Robust Local Features for Adversarial Training (RLFAT) that significantly improves both the adversarially robust generalization and...},
	language = {en},
	urldate = {2021-09-09},
	author = {Song, Chuanbiao and He, Kun and Lin, Jiadong and Wang, Liwei and Hopcroft, John E.},
	month = sep,
	year = {2019},
}

@article{zhang_noilin_2021,
	title = {{NoiLIn}: {Do} {Noisy} {Labels} {Always} {Hurt} {Adversarial} {Training}?},
	shorttitle = {{NoiLIn}},
	url = {https://arxiv.org/abs/2105.14676v1},
	abstract = {Adversarial training (AT) based on minimax optimization is a popular learning style that enhances the model's adversarial robustness. Noisy labels (NL) commonly undermine the learning and hurt the model's performance. Interestingly, both research directions hardly crossover and hit sparks. In this paper, we raise an intriguing question -- Does NL always hurt AT? Firstly, we find that NL injection in inner maximization for generating adversarial data augments natural data implicitly, which benefits AT's generalization. Secondly, we find NL injection in outer minimization for the learning serves as regularization that alleviates robust overfitting, which benefits AT's robustness. To enhance AT's adversarial robustness, we propose "NoiLIn" that gradually increases {\textbackslash}underline\{Noi\}sy {\textbackslash}underline\{L\}abels {\textbackslash}underline\{In\}jection over the AT's training process. Empirically, NoiLIn answers the previous question negatively -- the adversarial robustness can be indeed enhanced by NL injection. Philosophically, we provide a new perspective of the learning with NL: NL should not always be deemed detrimental, and even in the absence of NL in the training set, we may consider injecting it deliberately.},
	language = {en},
	urldate = {2021-09-09},
	author = {Zhang, Jingfeng and Xu, Xilie and Han, Bo and Liu, Tongliang and Niu, Gang and Cui, Lizhen and Sugiyama, Masashi},
	month = may,
	year = {2021},
}

@article{stutz_relating_2021,
	title = {Relating {Adversarially} {Robust} {Generalization} to {Flat} {Minima}},
	url = {https://arxiv.org/abs/2104.04448v1},
	abstract = {Adversarial training (AT) has become the de-facto standard to obtain models robust against adversarial examples. However, AT exhibits severe robust overfitting: cross-entropy loss on adversarial examples, so-called robust loss, decreases continuously on training examples, while eventually increasing on test examples. In practice, this leads to poor robust generalization, i.e., adversarial robustness does not generalize well to new examples. In this paper, we study the relationship between robust generalization and flatness of the robust loss landscape in weight space, i.e., whether robust loss changes significantly when perturbing weights. To this end, we propose average- and worst-case metrics to measure flatness in the robust loss landscape and show a correlation between good robust generalization and flatness. For example, throughout training, flatness reduces significantly during overfitting such that early stopping effectively finds flatter minima in the robust loss landscape. Similarly, AT variants achieving higher adversarial robustness also correspond to flatter minima. This holds for many popular choices, e.g., AT-AWP, TRADES, MART, AT with self-supervision or additional unlabeled examples, as well as simple regularization techniques, e.g., AutoAugment, weight decay or label noise. For fair comparison across these approaches, our flatness measures are specifically designed to be scale-invariant and we conduct extensive experiments to validate our findings.},
	language = {en},
	urldate = {2021-09-09},
	author = {Stutz, David and Hein, Matthias and Schiele, Bernt},
	month = apr,
	year = {2021},
}

@article{xu_towards_2021,
	title = {Towards the {Memorization} {Effect} of {Neural} {Networks} in {Adversarial} {Training}},
	url = {http://arxiv.org/abs/2106.04794},
	abstract = {Recent studies suggest that ``memorization'' is one important factor for overparameterized deep neural networks (DNNs) to achieve optimal performance. Specifically, the perfectly fitted DNNs can memorize the labels of many atypical samples, generalize their memorization to correctly classify test atypical samples and enjoy better test performance. While, DNNs which are optimized via adversarial training algorithms can also achieve perfect training performance by memorizing the labels of atypical samples, as well as the adversarially perturbed atypical samples. However, adversarially trained models always suffer from poor generalization, with both relatively low clean accuracy and robustness on the test set. In this work, we study the effect of memorization in adversarial trained DNNs and disclose two important findings: (a) Memorizing atypical samples is only effective to improve DNN's accuracy on clean atypical samples, but hardly improve their adversarial robustness and (b) Memorizing certain atypical samples will even hurt the DNN's performance on typical samples. Based on these two findings, we propose Benign Adversarial Training (BAT) which can facilitate adversarial training to avoid fitting ``harmful'' atypical samples and fit as more ``benign'' atypical samples as possible. In our experiments, we validate the effectiveness of BAT, and show it can achieve better clean accuracy vs. robustness trade-off than baseline methods, in benchmark datasets such as CIFAR100 and Tiny{\textasciitilde}ImageNet.},
	urldate = {2021-09-09},
	journal = {arXiv:2106.04794 [cs]},
	author = {Xu, Han and Liu, Xiaorui and Wang, Wentao and Ding, Wenbiao and Wu, Zhongqin and Liu, Zitao and Jain, Anil and Tang, Jiliang},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.04794},
	keywords = {Computer Science - Machine Learning},
}

@inproceedings{han_co-teaching_2018,
	title = {Co-teaching: {Robust} training of deep neural networks with extremely noisy labels},
	volume = {31},
	shorttitle = {Co-teaching},
	url = {https://papers.nips.cc/paper/2018/hash/a19744e268754fb0148b017647355b7b-Abstract.html},
	urldate = {2021-09-09},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Han, Bo and Yao, Quanming and Yu, Xingrui and Niu, Gang and Xu, Miao and Hu, Weihua and Tsang, Ivor and Sugiyama, Masashi},
	year = {2018},
}

@inproceedings{cao_generalization_2019,
	title = {Generalization {Bounds} of {Stochastic} {Gradient} {Descent} for {Wide} and {Deep} {Neural} {Networks}},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper/2019/hash/cf9dc5e4e194fc21f397b4cac9cc3ae9-Abstract.html},
	urldate = {2021-09-09},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Cao, Yuan and Gu, Quanquan},
	year = {2019},
}

@inproceedings{arora_stronger_2018,
	title = {Stronger {Generalization} {Bounds} for {Deep} {Nets} via a {Compression} {Approach}},
	url = {https://proceedings.mlr.press/v80/arora18b.html},
	language = {en},
	urldate = {2021-09-09},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Arora, Sanjeev and Ge, Rong and Neyshabur, Behnam and Zhang, Yi},
	month = jul,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {254--263},
}

@inproceedings{arpit_closer_2017,
	title = {A {Closer} {Look} at {Memorization} in {Deep} {Networks}},
	url = {https://proceedings.mlr.press/v70/arpit17a.html},
	language = {en},
	urldate = {2021-09-09},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Arpit, Devansh and Jastrzębski, Stanisław and Ballas, Nicolas and Krueger, David and Bengio, Emmanuel and Kanwal, Maxinder S. and Maharaj, Tegan and Fischer, Asja and Courville, Aaron and Bengio, Yoshua and Lacoste-Julien, Simon},
	month = jul,
	year = {2017},
	note = {ISSN: 2640-3498},
	pages = {233--242},
}

@inproceedings{du_gradient_2019,
	title = {Gradient {Descent} {Finds} {Global} {Minima} of {Deep} {Neural} {Networks}},
	url = {https://proceedings.mlr.press/v97/du19c.html},
	language = {en},
	urldate = {2021-09-09},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Du, Simon and Lee, Jason and Li, Haochuan and Wang, Liwei and Zhai, Xiyu},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {1675--1685},
}

@inproceedings{allen-zhu_convergence_2019,
	title = {A {Convergence} {Theory} for {Deep} {Learning} via {Over}-{Parameterization}},
	url = {https://proceedings.mlr.press/v97/allen-zhu19a.html},
	language = {en},
	urldate = {2021-09-09},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Allen-Zhu, Zeyuan and Li, Yuanzhi and Song, Zhao},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {242--252},
}

@inproceedings{jiang_fantastic_2019,
	title = {Fantastic {Generalization} {Measures} and {Where} to {Find} {Them}},
	url = {https://openreview.net/forum?id=SJgIPJBFvH},
	abstract = {We empirically study generalization measures over more than 2000 models, identify common pitfall in existing practice of studying generalization measures and provide some new bounds based on...},
	language = {en},
	urldate = {2021-09-09},
	author = {Jiang*, Yiding and Neyshabur*, Behnam and Mobahi, Hossein and Krishnan, Dilip and Bengio, Samy},
	month = sep,
	year = {2019},
}

@inproceedings{neyshabur_exploring_2017,
	title = {Exploring {Generalization} in {Deep} {Learning}},
	volume = {30},
	url = {https://papers.nips.cc/paper/2017/hash/10ce03a1ed01077e3e289f3e53c72813-Abstract.html},
	urldate = {2021-09-09},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Neyshabur, Behnam and Bhojanapalli, Srinadh and Mcallester, David and Srebro, Nati},
	year = {2017},
}

@inproceedings{novak_sensitivity_2018,
	title = {Sensitivity and {Generalization} in {Neural} {Networks}: an {Empirical} {Study}},
	shorttitle = {Sensitivity and {Generalization} in {Neural} {Networks}},
	url = {https://openreview.net/forum?id=HJC2SzZCW},
	abstract = {We perform massive experimental studies characterizing the relationships between Jacobian norms, linear regions, and generalization.},
	language = {en},
	urldate = {2021-09-09},
	author = {Novak, Roman and Bahri, Yasaman and Abolafia, Daniel A. and Pennington, Jeffrey and Sohl-Dickstein, Jascha},
	month = feb,
	year = {2018},
}

@inproceedings{yin_rademacher_2019,
	title = {Rademacher {Complexity} for {Adversarially} {Robust} {Generalization}},
	url = {https://proceedings.mlr.press/v97/yin19b.html},
	language = {en},
	urldate = {2021-09-08},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Yin, Dong and Kannan, Ramchandran and Bartlett, Peter},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {7085--7094},
}

@inproceedings{feldman_what_2020,
	title = {What {Neural} {Networks} {Memorize} and {Why}: {Discovering} the {Long} {Tail} via {Influence} {Estimation}},
	volume = {33},
	shorttitle = {What {Neural} {Networks} {Memorize} and {Why}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2020/hash/1e14bfe2714193e7af5abc64ecbd6b46-Abstract.html},
	urldate = {2021-09-08},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Feldman, Vitaly and Zhang, Chiyuan},
	year = {2020},
	pages = {2881--2891},
}

@inproceedings{zhang_understanding_2017,
	title = {Understanding {Deep} {Learning} {Requires} {Rethinking} {Generalization}},
	abstract = {Despite their massive size, successful deep artiﬁcial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training.},
	language = {en},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz},
	year = {2017},
	pages = {15},
}

@inproceedings{maennel_what_2020,
	title = {What {Do} {Neural} {Networks} {Learn} {When} {Trained} {With} {Random} {Labels}?},
	abstract = {We study deep neural networks (DNNs) trained on natural image data with entirely random labels. Despite its popularity in the literature, where it is often used to study memorization, generalization, and other phenomena, little is known about what DNNs learn in this setting. In this paper, we show analytically for convolutional and fully connected networks that an alignment between the principal components of network parameters and data takes place when training with random labels. We study this alignment effect by investigating neural networks pre-trained on randomly labelled image data and subsequently ﬁne-tuned on disjoint datasets with random or real labels. We show how this alignment produces a positive transfer: networks pre-trained with random labels train faster downstream compared to training from scratch even after accounting for simple effects, such as weight scaling. We analyze how competing effects, such as specialization at later layers, may hide the positive transfer. These effects are studied in several network architectures, including VGG16 and ResNet18, on CIFAR10 and ImageNet.},
	language = {en},
	booktitle = {Neural {Information} {Processing} {Systems}},
	author = {Maennel, Hartmut and Alabdulmohsin, Ibrahim and Tolstikhin, Ilya and Baldock, Robert J N and Bousquet, Olivier and Gelly, Sylvain and Keysers, Daniel},
	year = {2020},
	pages = {12},
}

@inproceedings{gao_convergence_2019,
	title = {Convergence of {Adversarial} {Training} in {Overparametrized} {Neural} {Networks}},
	abstract = {Neural networks are vulnerable to adversarial examples, i.e. inputs that are imperceptibly perturbed from natural data and yet incorrectly classiﬁed by the network. Adversarial training [31], a heuristic form of robust optimization that alternates between minimization and maximization steps, has proven to be among the most successful methods to train networks to be robust against a pre-deﬁned family of perturbations. This paper provides a partial answer to the success of adversarial training, by showing that it converges to a network where the surrogate loss with respect to the the attack algorithm is within of the optimal robust loss. Then we show that the optimal robust loss is also close to zero, hence adversarial training ﬁnds a robust classiﬁer. The analysis technique leverages recent work on the analysis of neural networks via Neural Tangent Kernel (NTK), combined with motivation from online-learning when the maximization is solved by a heuristic, and the expressiveness of the NTK kernel in the ∞-norm. In addition, we also prove that robust interpolation requires more model capacity, supporting the evidence that adversarial training requires wider networks.},
	language = {en},
	author = {Gao, Ruiqi and Cai, Tianle and Li, Haochuan and Hsieh, Cho-Jui and Wang, Liwei and Lee, Jason D},
	year = {2019},
	pages = {12},
}

@inproceedings{mao_metric_2019,
	title = {Metric {Learning} for {Adversarial} {Robustness}},
	abstract = {Deep networks are well-known to be fragile to adversarial attacks. We conduct an empirical analysis of deep representations under the state-of-the-art attack method called PGD, and ﬁnd that the attack causes the internal representation to shift closer to the “false” class. Motivated by this observation, we propose to regularize the representation space under attack with metric learning to produce more robust classiﬁers. By carefully sampling examples for metric learning, our learned representation not only increases robustness, but also detects previously unseen adversarial samples. Quantitative experiments show improvement of robustness accuracy by up to 4\% and detection efﬁciency by up to 6\% according to Area Under Curve score over prior work. The code of our work is available at https: //github.com/columbia/Metric\_Learning\_Adversarial\_Robustness.},
	language = {en},
	author = {Mao, Chengzhi and Zhong, Ziyuan and Yang, Junfeng and Vondrick, Carl and Ray, Baishakhi},
	year = {2019},
	pages = {12},
}

@article{yu_understanding_2021,
	title = {Understanding {Generalization} in {Adversarial} {Training} via the {Bias}-{Variance} {Decomposition}},
	url = {https://arxiv.org/abs/2103.09947v2},
	abstract = {Adversarially trained models exhibit a large generalization gap: they can interpolate the training set even for large perturbation radii, but at the cost of large test error on clean samples. To investigate this gap, we decompose the test risk into its bias and variance components and study their behavior as a function of adversarial training perturbation radii (\${\textbackslash}varepsilon\$). We find that the bias increases monotonically with \${\textbackslash}varepsilon\$ and is the dominant term in the risk. Meanwhile, the variance is unimodal as a function of \${\textbackslash}varepsilon\$, peaking near the interpolation threshold for the training set. This characteristic behavior occurs robustly across different datasets and also for other robust training procedures such as randomized smoothing. It thus provides a test for proposed explanations of the generalization gap. We find that some existing explanations fail this test--for instance, by predicting a monotonically increasing variance curve. This underscores the power of bias-variance decompositions in modern settings-by providing two measurements instead of one, they can rule out more explanations than test accuracy alone. We also show that bias and variance can provide useful guidance for scalably reducing the generalization gap, highlighting pre-training and unlabeled data as promising routes.},
	language = {en},
	urldate = {2021-09-07},
	author = {Yu, Yaodong and Yang, Zitong and Dobriban, Edgar and Steinhardt, Jacob and Ma, Yi},
	month = mar,
	year = {2021},
}

@article{golgooni_zerograd_2021,
	title = {{ZeroGrad} : {Mitigating} and {Explaining} {Catastrophic} {Overfitting} in {FGSM} {Adversarial} {Training}},
	shorttitle = {{ZeroGrad}},
	url = {http://arxiv.org/abs/2103.15476},
	abstract = {Making deep neural networks robust to small adversarial noises has recently been sought in many applications. Adversarial training through iterative projected gradient descent (PGD) has been established as one of the mainstream ideas to achieve this goal. However, PGD is computationally demanding and often prohibitive in case of large datasets and models. For this reason, single-step PGD, also known as FGSM, has recently gained interest in the field. Unfortunately, FGSM-training leads to a phenomenon called ``catastrophic overfitting," which is a sudden drop in the adversarial accuracy under the PGD attack. In this paper, we support the idea that small input gradients play a key role in this phenomenon, and hence propose to zero the input gradient elements that are small for crafting FGSM attacks. Our proposed idea, while being simple and efficient, achieves competitive adversarial accuracy on various datasets.},
	urldate = {2021-09-07},
	journal = {arXiv:2103.15476 [cs]},
	author = {Golgooni, Zeinab and Saberi, Mehrdad and Eskandar, Masih and Rohban, Mohammad Hossein},
	month = mar,
	year = {2021},
	note = {arXiv: 2103.15476},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{yu_interpreting_2018,
	title = {Interpreting {Adversarial} {Robustness}: {A} {View} from {Decision} {Surface} in {Input} {Space}},
	shorttitle = {Interpreting {Adversarial} {Robustness}},
	url = {http://arxiv.org/abs/1810.00144},
	abstract = {One popular hypothesis of neural network generalization is that the flat local minima of loss surface in parameter space leads to good generalization. However, we demonstrate that loss surface in parameter space has no obvious relationship with generalization, especially under adversarial settings. Through visualizing decision surfaces in both parameter space and input space, we instead show that the geometry property of decision surface in input space correlates well with the adversarial robustness. We then propose an adversarial robustness indicator, which can evaluate a neural network's intrinsic robustness property without testing its accuracy under adversarial attacks. Guided by it, we further propose our robust training method. Without involving adversarial training, our method could enhance network's intrinsic adversarial robustness against various adversarial attacks.},
	urldate = {2021-09-07},
	journal = {arXiv:1810.00144 [cs, stat]},
	author = {Yu, Fuxun and Liu, Chenchen and Wang, Yanzhi and Zhao, Liang and Chen, Xiang},
	month = oct,
	year = {2018},
	note = {arXiv: 1810.00144},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{schmidt_adversarially_2018,
	title = {Adversarially {Robust} {Generalization} {Requires} {More} {Data}},
	volume = {31},
	url = {https://papers.nips.cc/paper/2018/hash/f708f064faaf32a43e4d3c784e6af9ea-Abstract.html},
	urldate = {2021-09-07},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Schmidt, Ludwig and Santurkar, Shibani and Tsipras, Dimitris and Talwar, Kunal and Madry, Aleksander},
	year = {2018},
}

@article{devries_improved_2017,
	title = {Improved {Regularization} of {Convolutional} {Neural} {Networks} with {Cutout}},
	url = {http://arxiv.org/abs/1708.04552},
	abstract = {Convolutional neural networks are capable of learning powerful representational spaces, which are necessary for tackling complex learning tasks. However, due to the model capacity required to capture such representations, they are often susceptible to overﬁtting and therefore require proper regularization in order to generalize well.},
	language = {en},
	urldate = {2021-09-05},
	journal = {arXiv:1708.04552 [cs]},
	author = {DeVries, Terrance and Taylor, Graham W.},
	month = nov,
	year = {2017},
	note = {arXiv: 1708.04552},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{dohmatob_generalized_2019,
	title = {Generalized {No} {Free} {Lunch} {Theorem} for {Adversarial} {Robustness}},
	url = {http://proceedings.mlr.press/v97/dohmatob19a.html},
	language = {en},
	urldate = {2021-08-19},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Dohmatob, Elvis},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {1646--1654},
}

@inproceedings{konam_understanding_2017,
	title = {Understanding {Convolutional} {Networks} with {APPLE} : {Automatic} {Patch} {Pattern} {Labeling} for {Explanation}},
	abstract = {With the success of deep learning, recent efforts have been focused on analyzing how learned networks make their classiﬁcations. We are interested in analyzing the network output based on the network structure and information ﬂow through the network layers. We contribute an algorithm for 1) analyzing a deep network to ﬁnd neurons that are “important" in terms of the network classiﬁcation outcome, and 2) automatically labeling the patches of the input image that activate these important neurons. We propose several measures of importance for neurons and demonstrate that our technique can be used to gain insight into, and explain how a network decomposes an image to make its ﬁnal classiﬁcation.},
	language = {en},
	booktitle = {{AAAI}/{ACM} {Conference} on {AI}, {Ethics}, and {Society}},
	author = {Konam, Sandeep and Quah, Ian and Rosenthal, Stephanie and Veloso, Manuela},
	year = {2017},
	pages = {7},
}

@inproceedings{yu_nisp_2018,
	title = {{NISP}: {Pruning} {Networks} {Using} {Neuron} {Importance} {Score} {Propagation}},
	shorttitle = {{NISP}},
	url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Yu_NISP_Pruning_Networks_CVPR_2018_paper.html},
	urldate = {2021-07-27},
	author = {Yu, Ruichi and Li, Ang and Chen, Chun-Fu and Lai, Jui-Hsin and Morariu, Vlad I. and Han, Xintong and Gao, Mingfei and Lin, Ching-Yung and Davis, Larry S.},
	year = {2018},
	pages = {9194--9203},
}

@inproceedings{gan_semantic_2017,
	title = {Semantic {Compositional} {Networks} for {Visual} {Captioning}},
	url = {https://openaccess.thecvf.com/content_cvpr_2017/html/Gan_Semantic_Compositional_Networks_CVPR_2017_paper.html},
	urldate = {2021-07-17},
	author = {Gan, Zhe and Gan, Chuang and He, Xiaodong and Pu, Yunchen and Tran, Kenneth and Gao, Jianfeng and Carin, Lawrence and Deng, Li},
	year = {2017},
	pages = {5630--5639},
}

@inproceedings{wang_semantic_2015,
	title = {Semantic {Part} {Segmentation} {Using} {Compositional} {Model} {Combining} {Shape} and {Appearance}},
	url = {https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Wang_Semantic_Part_Segmentation_2015_CVPR_paper.html},
	urldate = {2021-07-17},
	author = {Wang, Jianyu and Yuille, Alan L.},
	year = {2015},
	pages = {1788--1797},
}

@article{salakhutdinov_learning_2013,
	title = {Learning with {Hierarchical}-{Deep} {Models}},
	volume = {35},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2012.269},
	abstract = {We introduce HD (or “Hierarchical-Deep”) models, a new compositional learning architecture that integrates deep learning models with structured hierarchical Bayesian (HB) models. Specifically, we show how we can learn a hierarchical Dirichlet process (HDP) prior over the activities of the top-level features in a deep Boltzmann machine (DBM). This compound HDP-DBM model learns to learn novel concepts from very few training example by learning low-level generic features, high-level features that capture correlations among low-level features, and a category hierarchy for sharing priors over the high-level features that are typical of different kinds of concepts. We present efficient learning and inference algorithms for the HDP-DBM model and show that it is able to learn new concepts from very few examples on CIFAR-100 object recognition, handwritten character recognition, and human motion capture datasets.},
	number = {8},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Salakhutdinov, Ruslan and Tenenbaum, Joshua B. and Torralba, Antonio},
	month = aug,
	year = {2013},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Approximation methods, Bayesian methods, Computational modeling, Deep networks, Machine learning, Stochastic processes, Training, Vectors, deep Boltzmann machines, hierarchical Bayesian models, one-shot learning},
	pages = {1958--1971},
}

@inproceedings{wang_learning_2020,
	title = {Learning from {Explanations} with {Neural} {Execution} {Tree}},
	url = {https://openreview.net/forum?id=rJlUt0EYwS},
	abstract = {While deep neural networks have achieved impressive performance on a range of NLP tasks, these data-hungry models heavily rely on labeled data, which restricts their applications in scenarios where...},
	language = {en},
	urldate = {2021-07-09},
	author = {Wang*, Ziqi and Qin*, Yujia and Zhou, Wenxuan and Yan, Jun and Ye, Qinyuan and Neves, Leonardo and Liu, Zhiyuan and Ren, Xiang},
	year = {2020},
}

@inproceedings{sagawa_distributionally_2020,
	title = {Distributionally {Robust} {Neural} {Networks}},
	url = {https://openreview.net/forum?id=ryxGuJrFvS},
	abstract = {Overparameterized neural networks can be distributionally robust, but only when you account for generalization.},
	language = {en},
	urldate = {2021-07-01},
	author = {Sagawa*, Shiori and Koh*, Pang Wei and Hashimoto, Tatsunori B. and Liang, Percy},
	year = {2020},
}

@article{ganin_domain-adversarial_2016,
	title = {Domain-{Adversarial} {Training} of {Neural} {Networks}},
	volume = {17},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v17/15-239.html},
	abstract = {We introduce a new representation learning approach for domain adaptation, in which data at training and test time come from similar but different distributions. Our approach is directly inspired by the theory on domain adaptation suggesting that, for effective domain transfer to be achieved, predictions must be made based on features that cannot discriminate between the training (source) and test (target) domains.

The approach implements this idea in the context of neural network architectures that are trained on labeled data from the source domain and unlabeled data from the target domain (no labeled target-domain data is necessary). As the training progresses, the approach promotes the emergence of features that are (i) discriminative for the main learning task on the source domain and (ii) indiscriminate with respect to the shift between the domains. We show that this adaptation behaviour can be achieved in almost any feed-forward model by augmenting it with few standard layers and a new gradient reversal layer. The resulting augmented architecture can be trained using standard backpropagation and stochastic gradient descent, and can thus be implemented with little effort using any of the deep learning packages.

We demonstrate the success of our approach for two distinct classification problems (document sentiment analysis and image classification), where state-of-the-art domain adaptation performance on standard benchmarks is achieved. We also validate the approach for descriptor learning task in the context of person re-identification application.},
	number = {59},
	urldate = {2021-07-15},
	journal = {Journal of Machine Learning Research},
	author = {Ganin, Yaroslav and Ustinova, Evgeniya and Ajakan, Hana and Germain, Pascal and Larochelle, Hugo and Laviolette, François and March, Mario and Lempitsky, Victor},
	year = {2016},
	pages = {1--35},
}

@article{kubilius_deep_2016,
	title = {Deep {Neural} {Networks} as a {Computational} {Model} for {Human} {Shape} {Sensitivity}},
	volume = {12},
	issn = {1553-7358},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004896},
	doi = {10.1371/journal.pcbi.1004896},
	abstract = {Theories of object recognition agree that shape is of primordial importance, but there is no consensus about how shape might be represented, and so far attempts to implement a model of shape perception that would work with realistic stimuli have largely failed. Recent studies suggest that state-of-the-art convolutional ‘deep’ neural networks (DNNs) capture important aspects of human object perception. We hypothesized that these successes might be partially related to a human-like representation of object shape. Here we demonstrate that sensitivity for shape features, characteristic to human and primate vision, emerges in DNNs when trained for generic object recognition from natural photographs. We show that these models explain human shape judgments for several benchmark behavioral and neural stimulus sets on which earlier models mostly failed. In particular, although never explicitly trained for such stimuli, DNNs develop acute sensitivity to minute variations in shape and to non-accidental properties that have long been implicated to form the basis for object recognition. Even more strikingly, when tested with a challenging stimulus set in which shape and category membership are dissociated, the most complex model architectures capture human shape sensitivity as well as some aspects of the category structure that emerges from human judgments. As a whole, these results indicate that convolutional neural networks not only learn physically correct representations of object categories but also develop perceptually accurate representational spaces of shapes. An even more complete model of human object representations might be in sight by training deep architectures for multiple tasks, which is so characteristic in human development.},
	language = {en},
	number = {4},
	urldate = {2021-07-15},
	journal = {PLOS Computational Biology},
	author = {Kubilius, Jonas and Bracci, Stefania and Beeck, Hans P. Op de},
	month = apr,
	year = {2016},
	note = {Publisher: Public Library of Science},
	keywords = {Computer object recognition, Human performance, Monkeys, Neural networks, Sensory perception, Vision, Visual cortex, Visual object recognition},
	pages = {e1004896},
}

@inproceedings{ritter_cognitive_2017,
	title = {Cognitive {Psychology} for {Deep} {Neural} {Networks}: {A} {Shape} {Bias} {Case} {Study}},
	shorttitle = {Cognitive {Psychology} for {Deep} {Neural} {Networks}},
	url = {http://proceedings.mlr.press/v70/ritter17a.html},
	language = {en},
	urldate = {2021-07-15},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Ritter, Samuel and Barrett, David G. T. and Santoro, Adam and Botvinick, Matt M.},
	month = jul,
	year = {2017},
	note = {ISSN: 2640-3498},
	pages = {2940--2949},
}

@article{bruce_saliency_2009,
	title = {Saliency, attention, and visual search: {An} information theoretic approach},
	volume = {9},
	issn = {1534-7362},
	shorttitle = {Saliency, attention, and visual search},
	url = {https://jov.arvojournals.org/article.aspx?articleid=2193531},
	doi = {10.1167/9.3.5},
	language = {en},
	number = {3},
	urldate = {2021-07-09},
	journal = {Journal of Vision},
	author = {Bruce, Neil D. B. and Tsotsos, John K.},
	month = mar,
	year = {2009},
	note = {Publisher: The Association for Research in Vision and Ophthalmology},
	pages = {5--5},
}

@inproceedings{bruce_saliency_2005,
	title = {Saliency {Based} on {Information} {Maximization}},
	abstract = {A model of bottom-up overt attention is proposed based on the principle of maximizing information sampled from a scene. The proposed operation is based on Shannon's self-information measure and is achieved in a neural circuit, which is demonstrated as having close ties with the circuitry existent in the primate visual cortex. It is further shown that the proposed saliency measure may be extended to address issues that currently elude explanation in the domain of saliency based models. Resu lts on natural images are compared with experimental eye tracking data revealing the efficacy of the model in predicting the deployment of overt attention as compared with existing efforts.},
	language = {en},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Bruce, Neil and Tsotsos, John},
	year = {2005},
	pages = {8},
}

@inproceedings{carlucci_domain_2019,
	title = {Domain {Generalization} by {Solving} {Jigsaw} {Puzzles}},
	url = {https://openaccess.thecvf.com/content_CVPR_2019/html/Carlucci_Domain_Generalization_by_Solving_Jigsaw_Puzzles_CVPR_2019_paper.html},
	urldate = {2021-07-09},
	author = {Carlucci, Fabio M. and D'Innocente, Antonio and Bucci, Silvia and Caputo, Barbara and Tommasi, Tatiana},
	year = {2019},
	pages = {2229--2238},
}

@inproceedings{radenovic_deep_2018,
	title = {Deep {Shape} {Matching}},
	url = {https://openaccess.thecvf.com/content_ECCV_2018/html/Filip_Radenovic_Deep_Shape_Matching_ECCV_2018_paper.html},
	urldate = {2021-07-09},
	author = {Radenovic, Filip and Tolias, Giorgos and Chum, Ondrej},
	year = {2018},
	pages = {751--767},
}

@inproceedings{zhang_rationale-augmented_2016,
	address = {Austin, Texas},
	title = {Rationale-{Augmented} {Convolutional} {Neural} {Networks} for {Text} {Classification}},
	url = {https://www.aclweb.org/anthology/D16-1076},
	doi = {10.18653/v1/D16-1076},
	urldate = {2021-06-25},
	booktitle = {Proceedings of the 2016 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Zhang, Ye and Marshall, Iain and Wallace, Byron C.},
	month = nov,
	year = {2016},
	keywords = {nlp},
	pages = {795--804},
}

@inproceedings{lei_rationalizing_2016,
	address = {Austin, Texas},
	title = {Rationalizing {Neural} {Predictions}},
	url = {https://www.aclweb.org/anthology/D16-1011},
	doi = {10.18653/v1/D16-1011},
	urldate = {2021-01-16},
	booktitle = {Proceedings of the 2016 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Lei, Tao and Barzilay, Regina and Jaakkola, Tommi},
	month = nov,
	year = {2016},
	keywords = {nlp},
	pages = {107--117},
}

@inproceedings{murty_expbert_2020,
	address = {Online},
	title = {{ExpBERT}: {Representation} {Engineering} with {Natural} {Language} {Explanations}},
	shorttitle = {{ExpBERT}},
	url = {https://www.aclweb.org/anthology/2020.acl-main.190},
	doi = {10.18653/v1/2020.acl-main.190},
	abstract = {Suppose we want to specify the inductive bias that married couples typically go on honeymoons for the task of extracting pairs of spouses from text. In this paper, we allow model developers to specify these types of inductive biases as natural language explanations. We use BERT fine-tuned on MultiNLI to “interpret” these explanations with respect to the input sentence, producing explanation-guided representations of the input. Across three relation extraction tasks, our method, ExpBERT, matches a BERT baseline but with 3–20x less labeled data and improves on the baseline by 3–10 F1 points with the same amount of labeled data.},
	urldate = {2021-06-09},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Murty, Shikhar and Koh, Pang Wei and Liang, Percy},
	month = jul,
	year = {2020},
	keywords = {nlp},
	pages = {2106--2113},
}

@inproceedings{camburu_e-snli_2018,
	title = {e-{SNLI}: {Natural} {Language} {Inference} with {Natural} {Language} {Explanations}},
	volume = {31},
	shorttitle = {e-{SNLI}},
	url = {https://proceedings.neurips.cc/paper/2018/hash/4c7a167bb329bd92580a99ce422d6fa6-Abstract.html},
	language = {en},
	urldate = {2021-07-09},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Camburu, Oana-Maria and Rocktäschel, Tim and Lukasiewicz, Thomas and Blunsom, Phil},
	year = {2018},
}

@inproceedings{rajani_explain_2019,
	address = {Florence, Italy},
	title = {Explain {Yourself}! {Leveraging} {Language} {Models} for {Commonsense} {Reasoning}},
	url = {https://aclanthology.org/P19-1487},
	doi = {10.18653/v1/P19-1487},
	abstract = {Deep learning models perform poorly on tasks that require commonsense reasoning, which often necessitates some form of world-knowledge or reasoning over information not immediately present in the input. We collect human explanations for commonsense reasoning in the form of natural language sequences and highlighted annotations in a new dataset called Common Sense Explanations (CoS-E). We use CoS-E to train language models to automatically generate explanations that can be used during training and inference in a novel Commonsense Auto-Generated Explanation (CAGE) framework. CAGE improves the state-of-the-art by 10\% on the challenging CommonsenseQA task. We further study commonsense reasoning in DNNs using both human and auto-generated explanations including transfer to out-of-domain tasks. Empirical results indicate that we can effectively leverage language models for commonsense reasoning.},
	urldate = {2021-07-09},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Rajani, Nazneen Fatema and McCann, Bryan and Xiong, Caiming and Socher, Richard},
	month = jul,
	year = {2019},
	pages = {4932--4942},
}

@inproceedings{hancock_training_2018,
	address = {Melbourne, Australia},
	title = {Training {Classifiers} with {Natural} {Language} {Explanations}},
	url = {https://aclanthology.org/P18-1175},
	doi = {10.18653/v1/P18-1175},
	abstract = {Training accurate classifiers requires many labels, but each label provides only limited information (one bit for binary classification). In this work, we propose BabbleLabble, a framework for training classifiers in which an annotator provides a natural language explanation for each labeling decision. A semantic parser converts these explanations into programmatic labeling functions that generate noisy labels for an arbitrary amount of unlabeled data, which is used to train a classifier. On three relation extraction tasks, we find that users are able to train classifiers with comparable F1 scores from 5-100 faster by providing explanations instead of just labels. Furthermore, given the inherent imperfection of labeling functions, we find that a simple rule-based semantic parser suffices.},
	urldate = {2021-07-09},
	booktitle = {Proceedings of the 56th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Hancock, Braden and Varma, Paroma and Wang, Stephanie and Bringmann, Martin and Liang, Percy and Ré, Christopher},
	month = jul,
	year = {2018},
	pages = {1884--1895},
}

@inproceedings{srivastava_joint_2017,
	address = {Copenhagen, Denmark},
	title = {Joint {Concept} {Learning} and {Semantic} {Parsing} from {Natural} {Language} {Explanations}},
	url = {https://aclanthology.org/D17-1161},
	doi = {10.18653/v1/D17-1161},
	abstract = {Natural language constitutes a predominant medium for much of human learning and pedagogy. We consider the problem of concept learning from natural language explanations, and a small number of labeled examples of the concept. For example, in learning the concept of a phishing email, one might say `this is a phishing email because it asks for your bank account number'. Solving this problem involves both learning to interpret open ended natural language statements, and learning the concept itself. We present a joint model for (1) language interpretation (semantic parsing) and (2) concept learning (classification) that does not require labeling statements with logical forms. Instead, the model prefers discriminative interpretations of statements in context of observable features of the data as a weak signal for parsing. On a dataset of email-related concepts, our approach yields across-the-board improvements in classification performance, with a 30\% relative improvement in F1 score over competitive methods in the low data regime.},
	urldate = {2021-07-09},
	booktitle = {Proceedings of the 2017 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Srivastava, Shashank and Labutov, Igor and Mitchell, Tom},
	month = sep,
	year = {2017},
	pages = {1527--1536},
}

@inproceedings{dong_towards_2019,
	title = {Towards {Interpretable} {Deep} {Neural} {Networks} by {Leveraging} {Adversarial} {Examples}},
	url = {http://arxiv.org/abs/1708.05493},
	abstract = {Deep neural networks (DNNs) have demonstrated impressive performance on a wide array of tasks, but they are usually considered opaque since internal structure and learned parameters are not interpretable. In this paper, we re-examine the internal representations of DNNs using adversarial images, which are generated by an ensembleoptimization algorithm. We ﬁnd that: (1) the neurons in DNNs do not truly detect semantic objects/parts, but respond to objects/parts only as recurrent discriminative patches; (2) deep visual representations are not robust distributed codes of visual concepts because the representations of adversarial images are largely not consistent with those of real images, although they have similar visual appearance, both of which are different from previous ﬁndings. To further improve the interpretability of DNNs, we propose an adversarial training scheme with a consistent loss such that the neurons are endowed with human-interpretable concepts. The induced interpretable representations enable us to trace eventual outcomes back to inﬂuential neurons. Therefore, human users can know how the models make predictions, as well as when and why they make errors.},
	language = {en},
	urldate = {2020-12-21},
	booktitle = {{AAAI}-19 {Workshop} on {Network} {Interpretability} for {Deep} {Learning}},
	author = {Dong, Yinpeng and Su, Hang and Zhu, Jun and Bao, Fan},
	year = {2019},
	note = {arXiv: 1708.05493},
}

@inproceedings{chalasani_concise_2020,
	title = {Concise {Explanations} of {Neural} {Networks} using {Adversarial} {Training}},
	url = {http://proceedings.mlr.press/v119/chalasani20a.html},
	abstract = {We show new connections between adversarial learning and explainability for deep neural networks (DNNs). One form of explanation of the output of a neural network model in terms of its input featur...},
	language = {en},
	urldate = {2021-07-08},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Chalasani, Prasad and Chen, Jiefeng and Chowdhury, Amrita Roy and Wu, Xi and Jha, Somesh},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {1383--1391},
}

@article{franca_fast_2014,
	title = {Fast relational learning using bottom clause propositionalization with artificial neural networks},
	volume = {94},
	issn = {1573-0565},
	url = {https://doi.org/10.1007/s10994-013-5392-1},
	doi = {10.1007/s10994-013-5392-1},
	abstract = {Relational learning can be described as the task of learning first-order logic rules from examples. It has enabled a number of new machine learning applications, e.g. graph mining and link analysis. Inductive Logic Programming (ILP) performs relational learning either directly by manipulating first-order rules or through propositionalization, which translates the relational task into an attribute-value learning task by representing subsets of relations as features. In this paper, we introduce a fast method and system for relational learning based on a novel propositionalization called Bottom Clause Propositionalization (BCP). Bottom clauses are boundaries in the hypothesis search space used by ILP systems Progol and Aleph. Bottom clauses carry semantic meaning and can be mapped directly onto numerical vectors, simplifying the feature extraction process. We have integrated BCP with a well-known neural-symbolic system, C-IL2P, to perform learning from numerical vectors. C-IL2P uses background knowledge in the form of propositional logic programs to build a neural network. The integrated system, which we call CILP++, handles first-order logic knowledge and is available for download from Sourceforge. We have evaluated CILP++ on seven ILP datasets, comparing results with Aleph and a well-known propositionalization method, RSD. The results show that CILP++ can achieve accuracy comparable to Aleph, while being generally faster, BCP achieved statistically significant improvement in accuracy in comparison with RSD when running with a neural network, but BCP and RSD perform similarly when running with C4.5. We have also extended CILP++ to include a statistical feature selection method, mRMR, with preliminary results indicating that a reduction of more than 90 \% of features can be achieved with a small loss of accuracy.},
	language = {en},
	number = {1},
	urldate = {2021-07-08},
	journal = {Machine Learning},
	author = {França, Manoel V. M. and Zaverucha, Gerson and d’Avila Garcez, Artur S.},
	month = jan,
	year = {2014},
	pages = {81--104},
}

@inproceedings{kulkarni_deep_2015,
	title = {Deep {Convolutional} {Inverse} {Graphics} {Network}},
	volume = {28},
	url = {https://proceedings.neurips.cc/paper/2015/hash/ced556cd9f9c0c8315cfbe0744a3baf0-Abstract.html},
	language = {en},
	urldate = {2021-07-08},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Kulkarni, Tejas D. and Whitney, William F. and Kohli, Pushmeet and Tenenbaum, Josh},
	year = {2015},
}

@inproceedings{nguyen_deep_2015,
	title = {Deep {Neural} {Networks} {Are} {Easily} {Fooled}: {High} {Confidence} {Predictions} for {Unrecognizable} {Images}},
	shorttitle = {Deep {Neural} {Networks} {Are} {Easily} {Fooled}},
	url = {https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Nguyen_Deep_Neural_Networks_2015_CVPR_paper.html},
	urldate = {2021-07-08},
	author = {Nguyen, Anh and Yosinski, Jason and Clune, Jeff},
	year = {2015},
	pages = {427--436},
}

@inproceedings{bojarski_visualbackprop_2018,
	title = {{VisualBackProp}: {Efficient} {Visualization} of {CNNs} for {Autonomous} {Driving}},
	shorttitle = {{VisualBackProp}},
	doi = {10.1109/ICRA.2018.8461053},
	abstract = {This paper proposes a new method, that we call VisualBackProp, for visualizing which sets of pixels of the input image contribute most to the predictions made by the convolutional neural network (CNN). The method heavily hinges on exploring the intuition that the feature maps contain less and less irrelevant information to the prediction decision when moving deeper into the network. The technique we propose is dedicated for CNN-based systems for steering self-driving cars and is therefore required to run in real-time. This makes the proposed visualization method a valuable debugging tool which can be easily used during both training and inference. We justify our approach with theoretical arguments and confirm that the proposed method identifies sets of input pixels, rather than individual pixels, that collaboratively contribute to the prediction. We utilize the proposed visualization tool in the NVIDIA neural-network-based end-to-end learning system for autonomous driving, known as PilotNet. We demonstrate that VisualBackProp determines which elements in the road image most influence PilotNet's steering decision and indeed captures relevant objects on the road. The empirical evaluation furthermore shows the plausibility of the proposed approach on public road video data as well as in other applications and reveals that it compares favorably to the layer-wise relevance propagation approach, i.e. it obtains similar visualization results and achieves order of magnitude speed-ups.},
	booktitle = {2018 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Bojarski, Mariusz and Choromanska, Anna and Choromanski, Krzysztof and Firner, Bernhard and Ackel, Larry J and Muller, Urs and Yeres, Phil and Zieba, Karol},
	month = may,
	year = {2018},
	note = {ISSN: 2577-087X},
	keywords = {Biological neural networks, Data visualization, Deconvolution, Neurons, Roads, Tools, Visualization},
	pages = {4701--4708},
}

@inproceedings{ferrari_convnets_2018,
	address = {Cham},
	title = {{ConvNets} and {ImageNet} {Beyond} {Accuracy}: {Understanding} {Mistakes} and {Uncovering} {Biases}},
	volume = {11210},
	isbn = {978-3-030-01230-4 978-3-030-01231-1},
	shorttitle = {{ConvNets} and {ImageNet} {Beyond} {Accuracy}},
	url = {http://link.springer.com/10.1007/978-3-030-01231-1_31},
	doi = {10.1007/978-3-030-01231-1_31},
	abstract = {ConvNets and ImageNet have driven the recent success of deep learning for image classiﬁcation. However, the marked slowdown in performance improvement combined with the lack of robustness of neural networks to adversarial examples and their tendency to exhibit undesirable biases question the reliability of these methods. This work investigates these questions from the perspective of the end-user by using human subject studies and explanations. The contribution of this study is threefold. We ﬁrst experimentally demonstrate that the accuracy and robustness of ConvNets measured on Imagenet are vastly underestimated. Next, we show that explanations can mitigate the impact of misclassiﬁed adversarial examples from the perspective of the end-user. We ﬁnally introduce a novel tool for uncovering the undesirable biases learned by a model. These contributions also show that explanations are a valuable tool both for improving our understanding of ConvNets’ predictions and for designing more reliable models.},
	language = {en},
	urldate = {2021-07-07},
	booktitle = {European {Conference} on {Computer} {Vision}},
	publisher = {Springer International Publishing},
	author = {Stock, Pierre and Cisse, Moustapha},
	editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
	year = {2018},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {504--519},
}

@inproceedings{zhao_men_2017,
	address = {Copenhagen, Denmark},
	title = {Men {Also} {Like} {Shopping}: {Reducing} {Gender} {Bias} {Amplification} using {Corpus}-level {Constraints}},
	shorttitle = {Men {Also} {Like} {Shopping}},
	url = {https://aclanthology.org/D17-1323},
	doi = {10.18653/v1/D17-1323},
	abstract = {Language is increasingly being used to de-fine rich visual recognition problems with supporting image collections sourced from the web. Structured prediction models are used in these tasks to take advantage of correlations between co-occurring labels and visual input but risk inadvertently encoding social biases found in web corpora. In this work, we study data and models associated with multilabel object classification and visual semantic role labeling. We find that (a) datasets for these tasks contain significant gender bias and (b) models trained on these datasets further amplify existing bias. For example, the activity cooking is over 33\% more likely to involve females than males in a training set, and a trained model further amplifies the disparity to 68\% at test time. We propose to inject corpus-level constraints for calibrating existing structured prediction models and design an algorithm based on Lagrangian relaxation for collective inference. Our method results in almost no performance loss for the underlying recognition task but decreases the magnitude of bias amplification by 47.5\% and 40.5\% for multilabel classification and visual semantic role labeling, respectively。},
	urldate = {2021-07-07},
	booktitle = {Proceedings of the 2017 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Zhao, Jieyu and Wang, Tianlu and Yatskar, Mark and Ordonez, Vicente and Chang, Kai-Wei},
	month = sep,
	year = {2017},
	pages = {2979--2989},
}

@inproceedings{wang_learning_2018,
	address = {New York, NY, USA},
	series = {{KDD} '18},
	title = {Learning {Credible} {Models}},
	isbn = {978-1-4503-5552-0},
	url = {https://doi.org/10.1145/3219819.3220070},
	doi = {10.1145/3219819.3220070},
	abstract = {In many settings, it is important that a model be capable of providing reasons for its predictions (ıe, the model must be interpretable). However, the model's reasoning may not conform with well-established knowledge. In such cases, while interpretable, the model lacks credibility. In this work, we formally define credibility in the linear setting and focus on techniques for learning models that are both accurate and credible. In particular, we propose a regularization penalty, expert yielded estimates (EYE), that incorporates expert knowledge about well-known relationships among covariates and the outcome of interest. We give both theoretical and empirical results comparing our proposed method to several other regularization techniques. Across a range of settings, experiments on both synthetic and real data show that models learned using the EYE penalty are significantly more credible than those learned using other penalties. Applied to two large-scale patient risk stratification task, our proposed technique results in a model whose top features overlap significantly with known clinical risk factors, while still achieving good predictive performance.},
	urldate = {2020-12-28},
	booktitle = {Proceedings of the 24th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Wang, Jiaxuan and Oh, Jeeheh and Wang, Haozhu and Wiens, Jenna},
	month = jul,
	year = {2018},
	keywords = {linear, logistic},
	pages = {2417--2426},
}

@inproceedings{bao_deriving_2018,
	address = {Brussels, Belgium},
	title = {Deriving {Machine} {Attention} from {Human} {Rationales}},
	url = {https://www.aclweb.org/anthology/D18-1216},
	doi = {10.18653/v1/D18-1216},
	abstract = {Attention-based models are successful when trained on large amounts of data. In this paper, we demonstrate that even in the low-resource scenario, attention can be learned effectively. To this end, we start with discrete human-annotated rationales and map them into continuous attention. Our central hypothesis is that this mapping is general across domains, and thus can be transferred from resource-rich domains to low-resource ones. Our model jointly learns a domain-invariant representation and induces the desired mapping between rationales and attention. Our empirical results validate this hypothesis and show that our approach delivers significant gains over state-of-the-art baselines, yielding over 15\% average error reduction on benchmark datasets.},
	urldate = {2020-06-24},
	booktitle = {Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Bao, Yujia and Chang, Shiyu and Yu, Mo and Barzilay, Regina},
	month = oct,
	year = {2018},
	keywords = {nlp},
	pages = {1903--1913},
}

@inproceedings{chen_robust_2019,
	title = {Robust {Attribution} {Regularization}},
	volume = {32},
	url = {https://papers.nips.cc/paper/2019/hash/172ef5a94b4dd0aa120c6878fc29f70c-Abstract.html},
	language = {en},
	urldate = {2020-12-28},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Chen, Jiefeng and Wu, Xi and Rastogi, Vaibhav and Liang, Yingyu and Jha, Somesh},
	year = {2019},
	keywords = {image classification},
	pages = {14300--14310},
}

@inproceedings{barrett_sequence_2018,
	address = {Brussels, Belgium},
	title = {Sequence {Classification} with {Human} {Attention}},
	url = {https://www.aclweb.org/anthology/K18-1030},
	doi = {10.18653/v1/K18-1030},
	abstract = {Learning attention functions requires large volumes of data, but many NLP tasks simulate human behavior, and in this paper, we show that human attention really does provide a good inductive bias on many attention functions in NLP. Specifically, we use estimated human attention derived from eye-tracking corpora to regularize attention functions in recurrent neural networks. We show substantial improvements across a range of tasks, including sentiment analysis, grammatical error detection, and detection of abusive language.},
	urldate = {2020-12-28},
	booktitle = {Proceedings of the 22nd {Conference} on {Computational} {Natural} {Language} {Learning}},
	publisher = {Association for Computational Linguistics},
	author = {Barrett, Maria and Bingel, Joachim and Hollenstein, Nora and Rei, Marek and Søgaard, Anders},
	month = oct,
	year = {2018},
	keywords = {nlp},
	pages = {302--312},
}

@inproceedings{choi_why_2019,
	title = {Why {Can}'t {I} {Dance} in the {Mall}? {Learning} to {Mitigate} {Scene} {Bias} in {Action} {Recognition}},
	volume = {32},
	shorttitle = {Why {Can}'t {I} {Dance} in the {Mall}?},
	url = {https://papers.nips.cc/paper/2019/hash/ab817c9349cf9c4f6877e1894a1faa00-Abstract.html},
	language = {en},
	urldate = {2021-01-16},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Choi, Jinwoo and Gao, Chen and Messou, Joseph C. E. and Huang, Jia-Bin},
	year = {2019},
	keywords = {video recognition},
	pages = {853--865},
}

@inproceedings{stone_teaching_2017,
	title = {Teaching {Compositionality} to {CNNs}},
	url = {https://openaccess.thecvf.com/content_cvpr_2017/html/Stone_Teaching_Compositionality_to_CVPR_2017_paper.html},
	urldate = {2021-07-07},
	author = {Stone, Austin and Wang, Huayan and Stark, Michael and Liu, Yi and Scott Phoenix, D. and George, Dileep},
	year = {2017},
	pages = {5058--5067},
}

@inproceedings{liao_learning_2016,
	title = {Learning {Deep} {Parsimonious} {Representations}},
	abstract = {In this paper we aim at facilitating generalization for deep networks while supporting interpretability of the learned representations. Towards this goal, we propose a clustering based regularization that encourages parsimonious representations. Our k-means style objective is easy to optimize and ﬂexible, supporting various forms of clustering, such as sample clustering, spatial clustering, as well as co-clustering. We demonstrate the effectiveness of our approach on the tasks of unsupervised learning, classiﬁcation, ﬁne grained categorization, and zero-shot learning.},
	language = {en},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Liao, Renjie and Schwing, Alex and Zemel, Richard and Urtasun, Raquel},
	year = {2016},
	pages = {9},
}

@inproceedings{dosovitskiy_inverting_2016,
	title = {Inverting {Visual} {Representations} {With} {Convolutional} {Networks}},
	url = {https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Dosovitskiy_Inverting_Visual_Representations_CVPR_2016_paper.html},
	urldate = {2021-07-07},
	author = {Dosovitskiy, Alexey and Brox, Thomas},
	year = {2016},
	pages = {4829--4837},
}

@inproceedings{zhang_examining_2018,
	title = {Examining {CNN} {Representations} {With} {Respect} to {Dataset} {Bias}},
	volume = {32},
	copyright = {Copyright (c)},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/11833},
	language = {en},
	urldate = {2021-07-07},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Zhang, Quanshi and Wang, Wenguan and Zhu, Song-Chun},
	month = apr,
	year = {2018},
	note = {Number: 1},
	keywords = {Knowledge representation},
}

@inproceedings{li_resound_2018,
	title = {{RESOUND}: {Towards} {Action} {Recognition} without {Representation} {Bias}},
	shorttitle = {{RESOUND}},
	url = {https://openaccess.thecvf.com/content_ECCV_2018/html/Yingwei_Li_RESOUND_Towards_Action_ECCV_2018_paper.html},
	urldate = {2021-07-07},
	author = {Li, Yingwei and Li, Yi and Vasconcelos, Nuno},
	year = {2018},
	pages = {513--528},
}

@article{deng_leveraging_2016,
	title = {Leveraging the {Wisdom} of the {Crowd} for {Fine}-{Grained} {Recognition}},
	volume = {38},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2015.2439285},
	abstract = {Fine-grained recognition concerns categorization at sub-ordinate levels, where the distinction between object classes is highly local. Compared to basic level recognition, fine-grained categorization can be more challenging as there are in general less data and fewer discriminative features. This necessitates the use of a stronger prior for feature selection. In this work, we include humans in the loop to help computers select discriminative features. We introduce a novel online game called “Bubbles” that reveals discriminative features humans use. The player's goal is to identify the category of a heavily blurred image. During the game, the player can choose to reveal full details of circular regions (“bubbles”), with a certain penalty. With proper setup the game generates discriminative bubbles with assured quality. We next propose the “BubbleBank” representation that uses the human selected bubbles to improve machine recognition performance. Finally, we demonstrate how to extend BubbleBank to a view-invariant 3D representation. Experiments demonstrate that our approach yields large improvements over the previous state of the art on challenging benchmarks.},
	number = {4},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Deng, Jia and Krause, Jonathan and Stark, Michael and Fei-Fei, Li},
	month = apr,
	year = {2016},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Birds, Crowdsourcing, Detectors, Games, Gamification, Human Computation, Object Recognition, Object recognition, Pattern recognition, Three-dimensional displays, Visualization, human computation},
	pages = {666--676},
}

@inproceedings{linsley_what_2017,
	title = {What are the {Visual} {Features} {Underlying} {Human} {Versus} {Machine} {Vision}?},
	isbn = {978-1-5386-1034-3},
	url = {https://www.computer.org/csdl/proceedings-article/iccvw/2017/1034c706/12OmNCcKQEj},
	doi = {10.1109/ICCVW.2017.331},
	abstract = {Although Deep Convolutional Networks (DCNs) are approaching the accuracy of human observers at object recognition, it is unknown whether they leverage similar visual representations to achieve this performance. To address this, we introduce Clicktionary, a web-based game for identifying visual features used by human observers during object recognition. Importance maps derived from the game are consistent across participants and uncorrelated with image saliency measures. These results suggest that Clicktionary identifies image regions that are meaningful and diagnostic for object recognition but different than those driving eye movements. Surprisingly, Clicktionary importance maps are only weakly correlated with relevance maps derived from DCNs trained for object recognition. Our study demonstrates that the narrowing gap between the object recognition accuracy of human observers and DCNs obscures distinct visual strategies used by each to achieve this performance.},
	language = {English},
	urldate = {2021-07-06},
	publisher = {IEEE Computer Society},
	author = {Linsley, D. and Eberhardt, S. and Sharma, T. and Gupta, P. and Serre, T.},
	month = oct,
	year = {2017},
	note = {ISSN: 2473-9944},
	pages = {2706--2714},
}

@inproceedings{vondrick_learning_2015,
	title = {Learning visual biases from human imagination},
	volume = {28},
	url = {https://papers.nips.cc/paper/2015/hash/8f53295a73878494e9bc8dd6c3c7104f-Abstract.html},
	language = {en},
	urldate = {2021-07-06},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Vondrick, Carl and Pirsiavash, Hamed and Oliva, Aude and Torralba, Antonio},
	year = {2015},
}

@inproceedings{du_learning_2019,
	title = {Learning {Credible} {Deep} {Neural} {Networks} with {Rationale} {Regularization}},
	doi = {10.1109/ICDM.2019.00025},
	abstract = {Recent explainability related studies have shown that state-of-the-art DNNs do not always adopt correct evidences to make decisions. It not only hampers their generalization but also makes them less likely to be trusted by end-users. In pursuit of developing more credible DNNs, in this paper we propose CREX, which encourages DNN models to focus more on evidences that actually matter for the task at hand, and to avoid overfitting to data-dependent bias and artifacts. Specifically, CREX regularizes the training process of DNNs with rationales, i.e., a subset of features highlighted by domain experts as justifications for predictions, to enforce DNNs to generate local explanations that conform with expert rationales. Even when rationales are not available, CREX still could be useful by requiring the generated explanations to be sparse. Experimental results on two text classification datasets demonstrate the increased credibility of DNNs trained with CREX. Comprehensive analysis further shows that while CREX does not always improve prediction accuracy on the held-out test set, it significantly increases DNN accuracy on new and previously unseen data beyond test set, highlighting the advantage of the increased credibility.},
	booktitle = {2019 {IEEE} {International} {Conference} on {Data} {Mining} ({ICDM})},
	author = {Du, M. and Liu, N. and Yang, F. and Hu, X.},
	month = nov,
	year = {2019},
	note = {ISSN: 2374-8486},
	keywords = {nlp},
	pages = {150--159},
}

@inproceedings{simpson_gradmask_2019,
	title = {{GradMask}: {Reduce} {Overfitting} by {Regularizing} {Saliency}},
	shorttitle = {{GradMask}},
	url = {https://openreview.net/forum?id=Syx2z2aMqE},
	abstract = {Regularizing saliency maps to prevent overfitting from incorrect feature attribution},
	language = {en},
	urldate = {2021-06-23},
	author = {Simpson, Becks and Dutil, Francis and Bengio, Yoshua and Cohen, Joseph Paul},
	month = apr,
	year = {2019},
	keywords = {image classification},
}

@inproceedings{toneva_interpreting_2019,
	title = {Interpreting and improving natural-language processing (in machines) with natural language-processing (in the brain)},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper/2019/hash/749a8e6c231831ef7756db230b4359c8-Abstract.html},
	language = {en},
	urldate = {2020-12-21},
	booktitle = {33rd {Conference} on {Neural} {Information} {Processing} {Systems}},
	author = {Toneva, Mariya and Wehbe, Leila},
	year = {2019},
	keywords = {nlp},
	pages = {14954--14964},
}

@inproceedings{selvaraju_taking_2019,
	title = {Taking a {HINT}: {Leveraging} {Explanations} to {Make} {Vision} and {Language} {Models} {More} {Grounded}},
	shorttitle = {Taking a {HINT}},
	url = {https://openaccess.thecvf.com/content_ICCV_2019/html/Selvaraju_Taking_a_HINT_Leveraging_Explanations_to_Make_Vision_and_Language_ICCV_2019_paper.html},
	urldate = {2020-12-28},
	author = {Selvaraju, Ramprasaath R. and Lee, Stefan and Shen, Yilin and Jin, Hongxia and Ghosh, Shalini and Heck, Larry and Batra, Dhruv and Parikh, Devi},
	year = {2019},
	keywords = {visual question answer},
	pages = {2591--2600},
}

@inproceedings{liu_incorporating_2019,
	address = {Florence, Italy},
	title = {Incorporating {Priors} with {Feature} {Attribution} on {Text} {Classification}},
	url = {https://www.aclweb.org/anthology/P19-1631},
	doi = {10.18653/v1/P19-1631},
	abstract = {Feature attribution methods, proposed recently, help users interpret the predictions of complex models. Our approach integrates feature attributions into the objective function to allow machine learning practitioners to incorporate priors in model building. To demonstrate the effectiveness our technique, we apply it to two tasks: (1) mitigating unintended bias in text classiﬁers by neutralizing identity terms; (2) improving classiﬁer performance in a scarce data setting by forcing the model to focus on toxic terms. Our approach adds an L2 distance loss between feature attributions and task-speciﬁc prior values to the objective. Our experiments show that i) a classiﬁer trained with our technique reduces undesired model biases without a tradeoff on the original task; ii) incorporating priors helps model performance in scarce data settings.},
	language = {en},
	urldate = {2020-06-16},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Liu, Frederick and Avci, Besim},
	year = {2019},
	keywords = {nlp},
	pages = {6274--6283},
}

@inproceedings{selvaraju_choose_2018,
	title = {Choose {Your} {Neuron}: {Incorporating} {Domain} {Knowledge} through {Neuron}-{Importance}},
	shorttitle = {Choose {Your} {Neuron}},
	url = {https://openaccess.thecvf.com/content_ECCV_2018/html/Ramprasaath_Ramasamy_Selvaraju_Choose_Your_Neuron_ECCV_2018_paper.html},
	urldate = {2021-07-06},
	author = {Selvaraju, Ramprasaath R. and Chattopadhyay, Prithvijit and Elhoseiny, Mohamed and Sharma, Tilak and Batra, Dhruv and Parikh, Devi and Lee, Stefan},
	year = {2018},
	pages = {526--541},
}

@inproceedings{liu_attention_2017,
	title = {Attention {Correctness} in {Neural} {Image} {Captioning}},
	volume = {31},
	copyright = {Copyright (c)},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/11197},
	language = {en},
	urldate = {2021-07-06},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Liu, Chenxi and Mao, Junhua and Sha, Fei and Yuille, Alan},
	month = feb,
	year = {2017},
	note = {Number: 1},
}

@inproceedings{qiao_exploring_2018,
	title = {Exploring {Human}-{Like} {Attention} {Supervision} in {Visual} {Question} {Answering}},
	copyright = {Authors who publish a paper in this conference agree to the following terms:   Author(s) agree to transfer their copyrights in their article/paper to the Association for the Advancement of Artificial Intelligence (AAAI), in order to deal with future requests for reprints, translations, anthologies, reproductions, excerpts, and other publications. This grant will include, without limitation, the entire copyright in the article/paper in all countries of the world, including all renewals, extensions, and reversions thereof, whether such rights current exist or hereafter come into effect, and also the exclusive right to create electronic versions of the article/paper, to the extent that such right is not subsumed under copyright.  The author(s) warrants that they are the sole author and owner of the copyright in the above article/paper, except for those portions shown to be in quotations; that the article/paper is original throughout; and that the undersigned right to make the grants set forth above is complete and unencumbered.  The author(s) agree that if anyone brings any claim or action alleging facts that, if true, constitute a breach of any of the foregoing warranties, the author(s) will hold harmless and indemnify AAAI, their grantees, their licensees, and their distributors against any liability, whether under judgment, decree, or compromise, and any legal fees and expenses arising out of that claim or actions, and the undersigned will cooperate fully in any defense AAAI may make to such claim or action. Moreover, the undersigned agrees to cooperate in any claim or other action seeking to protect or enforce any right the undersigned has granted to AAAI in the article/paper. If any such claim or action fails because of facts that constitute a breach of any of the foregoing warranties, the undersigned agrees to reimburse whomever brings such claim or action for expenses and attorneys’ fees incurred therein.  Author(s) retain all proprietary rights other than copyright (such as patent rights).  Author(s) may make personal reuse of all or portions of the above article/paper in other works of their own authorship.  Author(s) may reproduce, or have reproduced, their article/paper for the author’s personal use, or for company use provided that AAAI copyright and the source are indicated, and that the copies are not used in a way that implies AAAI endorsement of a product or service of an employer, and that the copies per se are not offered for sale. The foregoing right shall not permit the posting of the article/paper in electronic or digital form on any computer network, except by the author or the author’s employer, and then only on the author’s or the employer’s own web page or ftp site. Such web page or ftp site, in addition to the aforementioned requirements of this Paragraph, must provide an electronic reference or link back to the AAAI electronic server, and shall not post other AAAI copyrighted materials not of the author’s or the employer’s creation (including tables of contents with links to other papers) without AAAI’s written permission.  Author(s) may make limited distribution of all or portions of their article/paper prior to publication.  In the case of work performed under U.S. Government contract, AAAI grants the U.S. Government royalty-free permission to reproduce all or portions of the above article/paper, and to authorize others to do so, for U.S. Government purposes.  In the event the above article/paper is not accepted and published by AAAI, or is withdrawn by the author(s) before acceptance by AAAI, this agreement becomes null and void.},
	url = {https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16485},
	abstract = {Attention mechanisms have been widely applied in the Visual Question Answering (VQA) task, as they help to focus on the area-of-interest of both visual and textual information. To answer the questions correctly, the model needs to selectively target different areas of an image, which suggests that an attention-based model may benefit from an explicit attention supervision. In this work, we aim to address the problem of adding attention supervision to VQA models. Since there is a lack of human attention data, we first propose a Human Attention Network (HAN) to generate human-like attention maps, training on a recently released dataset called Human ATtention Dataset (VQA-HAT). Then, we apply the pre-trained HAN on the VQA v2.0 dataset to automatically produce the human-like attention maps for all image-question pairs. The generated human-like attention map dataset for the VQA v2.0 dataset is named as Human-Like ATtention (HLAT) dataset. Finally, we apply human-like attention supervision to an attention-based VQA model. The experiments show that adding human-like supervision yields a more accurate attention together with a better performance, showing a promising future for human-like attention supervision in VQA.},
	language = {en},
	urldate = {2021-07-06},
	booktitle = {Thirty-{Second} {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Qiao, Tingting and Dong, Jianfeng and Xu, Duanqing},
	month = apr,
	year = {2018},
}

@article{zhang_top-down_2018,
	title = {Top-{Down} {Neural} {Attention} by {Excitation} {Backprop}},
	volume = {126},
	issn = {1573-1405},
	url = {https://doi.org/10.1007/s11263-017-1059-x},
	doi = {10.1007/s11263-017-1059-x},
	abstract = {We aim to model the top-down attention of a convolutional neural network (CNN) classifier for generating task-specific attention maps. Inspired by a top-down human visual attention model, we propose a new backpropagation scheme, called Excitation Backprop, to pass along top-down signals downwards in the network hierarchy via a probabilistic Winner-Take-All process. Furthermore, we introduce the concept of contrastive attention to make the top-down attention maps more discriminative. We show a theoretic connection between the proposed contrastive attention formulation and the Class Activation Map computation. Efficient implementation of Excitation Backprop for common neural network layers is also presented. In experiments, we visualize the evidence of a model’s classification decision by computing the proposed top-down attention maps. For quantitative evaluation, we report the accuracy of our method in weakly supervised localization tasks on the MS COCO, PASCAL VOC07 and ImageNet datasets. The usefulness of our method is further validated in the text-to-region association task. On the Flickr30k Entities dataset, we achieve promising performance in phrase localization by leveraging the top-down attention of a CNN model that has been trained on weakly labeled web images. Finally, we demonstrate applications of our method in model interpretation and data annotation assistance for facial expression analysis and medical imaging tasks.},
	language = {en},
	number = {10},
	urldate = {2021-07-06},
	journal = {International Journal of Computer Vision},
	author = {Zhang, Jianming and Bargal, Sarah Adel and Lin, Zhe and Brandt, Jonathan and Shen, Xiaohui and Sclaroff, Stan},
	month = oct,
	year = {2018},
	pages = {1084--1102},
}

@inproceedings{goyal_making_2017,
	title = {Making the v in {VQA} {Matter}: {Elevating} the {Role} of {Image} {Understanding} in {Visual} {Question} {Answering}},
	shorttitle = {Making the v in {VQA} {Matter}},
	url = {https://openaccess.thecvf.com/content_cvpr_2017/html/Goyal_Making_the_v_CVPR_2017_paper.html},
	urldate = {2021-07-06},
	author = {Goyal, Yash and Khot, Tejas and Summers-Stay, Douglas and Batra, Dhruv and Parikh, Devi},
	year = {2017},
	pages = {6904--6913},
}

@inproceedings{agrawal_dont_2018,
	title = {Don't {Just} {Assume}; {Look} and {Answer}: {Overcoming} {Priors} for {Visual} {Question} {Answering}},
	shorttitle = {Don't {Just} {Assume}; {Look} and {Answer}},
	url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Agrawal_Dont_Just_Assume_CVPR_2018_paper.html},
	urldate = {2021-07-06},
	author = {Agrawal, Aishwarya and Batra, Dhruv and Parikh, Devi and Kembhavi, Aniruddha},
	year = {2018},
	pages = {4971--4980},
}

@article{kortylewski_compositional_2021,
	title = {Compositional {Convolutional} {Neural} {Networks}: {A} {Robust} and {Interpretable} {Model} for {Object} {Recognition} {Under} {Occlusion}},
	volume = {129},
	issn = {1573-1405},
	shorttitle = {Compositional {Convolutional} {Neural} {Networks}},
	url = {https://doi.org/10.1007/s11263-020-01401-3},
	doi = {10.1007/s11263-020-01401-3},
	abstract = {Computer vision systems in real-world applications need to be robust to partial occlusion while also being explainable. In this work, we show that black-box deep convolutional neural networks (DCNNs) have only limited robustness to partial occlusion. We overcome these limitations by unifying DCNNs with part-based models into Compositional Convolutional Neural Networks (CompositionalNets)—an interpretable deep architecture with innate robustness to partial occlusion. Specifically, we propose to replace the fully connected classification head of DCNNs with a differentiable compositional model that can be trained end-to-end. The structure of the compositional model enables CompositionalNets to decompose images into objects and context, as well as to further decompose object representations in terms of individual parts and the objects’ pose. The generative nature of our compositional model enables it to localize occluders and to recognize objects based on their non-occluded parts. We conduct extensive experiments in terms of image classification and object detection on images of artificially occluded objects from the PASCAL3D+ and ImageNet dataset, and real images of partially occluded vehicles from the MS-COCO dataset. Our experiments show that CompositionalNets made from several popular DCNN backbones (VGG-16, ResNet50, ResNext) improve by a large margin over their non-compositional counterparts at classifying and detecting partially occluded objects. Furthermore, they can localize occluders accurately despite being trained with class-level supervision only. Finally, we demonstrate that CompositionalNets provide human interpretable predictions as their individual components can be understood as detecting parts and estimating an objects’ viewpoint.},
	language = {en},
	number = {3},
	urldate = {2021-07-03},
	journal = {International Journal of Computer Vision},
	author = {Kortylewski, Adam and Liu, Qing and Wang, Angtian and Sun, Yihong and Yuille, Alan},
	month = mar,
	year = {2021},
	pages = {736--760},
}

@inproceedings{zhang_mining_2017,
	title = {Mining {Object} {Parts} {From} {CNNs} via {Active} {Question}-{Answering}},
	url = {https://openaccess.thecvf.com/content_cvpr_2017/html/Zhang_Mining_Object_Parts_CVPR_2017_paper.html},
	urldate = {2021-07-03},
	author = {Zhang, Quanshi and Cao, Ruiming and Nian Wu, Ying and Zhu, Song-Chun},
	year = {2017},
	pages = {346--355},
}

@inproceedings{tsang_feature_2020,
	title = {Feature {Interaction} {Interpretability}: {A} {Case} for {Explaining} {Ad}-{Recommendation} {Systems} via {Neural} {Interaction} {Detection}},
	shorttitle = {Feature {Interaction} {Interpretability}},
	url = {https://openreview.net/forum?id=BkgnhTEtDS&fbclid=IwAR2apcGWGmr5dvVPZgLwz-BGXcQ4mQecsUrdSfXG1Rgo8sqoeDmnBZnIPa4},
	abstract = {Proposed methods to extract and leverage interpretations of feature interactions},
	language = {en},
	urldate = {2021-01-16},
	author = {Tsang, Michael and Cheng, Dehua and Liu, Hanpeng and Feng, Xue and Zhou, Eric and Liu, Yan},
	year = {2020},
	keywords = {recommendation},
}

@inproceedings{weinberger_learning_2020,
	title = {Learning {Deep} {Attribution} {Priors} {Based} {On} {Prior} {Knowledge}},
	volume = {33},
	url = {https://proceedings.neurips.cc//paper/2020/hash/a19883fca95d0e5ec7ee6c94c6c32028-Abstract.html},
	language = {en},
	urldate = {2020-12-25},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Weinberger, Ethan and Janizek, Joseph and Lee, Su-In},
	year = {2020},
	keywords = {image classification},
}

@inproceedings{wu_regional_2020,
	title = {Regional {Tree} {Regularization} for {Interpretability} in {Deep} {Neural} {Networks}},
	volume = {34},
	url = {https://aaai.org/ojs/index.php/AAAI/article/view/6112},
	doi = {10.1609/aaai.v34i04.6112},
	abstract = {The lack of interpretability remains a barrier to adopting deep neural networks across many safety-critical domains. Tree regularization was recently proposed to encourage a deep neural network’s decisions to resemble those of a globally compact, axis-aligned decision tree. However, it is often unreasonable to expect a single tree to predict well across all possible inputs. In practice, doing so could lead to neither interpretable nor performant optima. To address this issue, we propose regional tree regularization – a method that encourages a deep model to be well-approximated by several separate decision trees speciﬁc to predeﬁned regions of the input space. Across many datasets, including two healthcare applications, we show our approach delivers simpler explanations than other regularization schemes without compromising accuracy. Speciﬁcally, our regional regularizer ﬁnds many more “desirable” optima compared to global analogues.},
	language = {en},
	urldate = {2020-12-25},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Wu, Mike and Parbhoo, Sonali and Hughes, Michael and Kindle, Ryan and Celi, Leo and Zazzi, Maurizio and Roth, Volker and Doshi-Velez, Finale},
	month = apr,
	year = {2020},
	keywords = {image classification},
	pages = {6413--6421},
}

@article{schramowski_making_2020,
	title = {Making deep neural networks right for the right scientific reasons by interacting with their explanations},
	volume = {2},
	copyright = {2020 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {2522-5839},
	url = {https://www.nature.com/articles/s42256-020-0212-3},
	doi = {10.1038/s42256-020-0212-3},
	abstract = {Deep neural networks have demonstrated excellent performances in many real-world applications. Unfortunately, they may show Clever Hans-like behaviour (making use of confounding factors within datasets) to achieve high performance. In this work we introduce the novel learning setting of explanatory interactive learning and illustrate its benefits on a plant phenotyping research task. Explanatory interactive learning adds the scientist into the training loop, who interactively revises the original model by providing feedback on its explanations. Our experimental results demonstrate that explanatory interactive learning can help to avoid Clever Hans moments in machine learning and encourages (or discourages, if appropriate) trust in the underlying model.},
	language = {en},
	number = {8},
	urldate = {2020-12-28},
	journal = {Nature Machine Intelligence},
	author = {Schramowski, Patrick and Stammer, Wolfgang and Teso, Stefano and Brugger, Anna and Herbert, Franziska and Shao, Xiaoting and Luigs, Hans-Georg and Mahlein, Anne-Katrin and Kersting, Kristian},
	month = aug,
	year = {2020},
	note = {Number: 8
Publisher: Nature Publishing Group},
	keywords = {image classification},
	pages = {476--486},
}

@inproceedings{rieger_interpretations_2020,
	title = {Interpretations are {Useful}: {Penalizing} {Explanations} to {Align} {Neural} {Networks} with {Prior} {Knowledge}},
	shorttitle = {Interpretations are {Useful}},
	url = {http://proceedings.mlr.press/v119/rieger20a.html},
	abstract = {For an explanation of a deep learning model to be effective, it must provide both insight into a model and suggest a corresponding action in order to achieve some objective. Too often, the litany o...},
	language = {en},
	urldate = {2020-12-24},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Rieger, Laura and Singh, Chandan and Murdoch, William and Yu, Bin},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	keywords = {image classification},
	pages = {8116--8126},
}

@inproceedings{sanh_learning_2021,
	title = {Learning from others' mistakes: {Avoiding} dataset biases without modeling them},
	shorttitle = {Learning from others' mistakes},
	url = {https://openreview.net/forum?id=Hf3qXoiNkR},
	abstract = {State-of-the-art natural language processing (NLP) models often learn to model dataset biases and surface form correlations instead of features that target the intended underlying task. Previous...},
	language = {en},
	urldate = {2021-06-23},
	author = {Sanh, Victor and Wolf, Thomas and Belinkov, Yonatan and Rush, Alexander M.},
	year = {2021},
	keywords = {nlp},
}

@inproceedings{nuriel_permuted_2021,
	title = {Permuted {AdaIN}: {Reducing} the {Bias} {Towards} {Global} {Statistics} in {Image} {Classification}},
	shorttitle = {Permuted {AdaIN}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Nuriel_Permuted_AdaIN_Reducing_the_Bias_Towards_Global_Statistics_in_Image_CVPR_2021_paper.html},
	language = {en},
	urldate = {2021-06-16},
	author = {Nuriel, Oren and Benaim, Sagie and Wolf, Lior},
	year = {2021},
	keywords = {image classification},
	pages = {9482--9491},
}

@inproceedings{wang_learning_2019,
	title = {Learning {Robust} {Representations} by {Projecting} {Superficial} {Statistics} {Out}},
	url = {https://openreview.net/forum?id=rJEjjoR9K7},
	abstract = {Building on previous work on domain generalization, we hope to produce a classifier that will generalize to previously unseen domains, even when domain identifiers are not available during training.},
	language = {en},
	urldate = {2021-07-03},
	author = {Wang, Haohan and He, Zexue and Lipton, Zachary C. and Xing, Eric P.},
	year = {2019},
}

@inproceedings{bolukbasi_man_2016,
	title = {Man is to {Computer} {Programmer} as {Woman} is to {Homemaker}? {Debiasing} {Word} {Embeddings}},
	volume = {29},
	shorttitle = {Man is to {Computer} {Programmer} as {Woman} is to {Homemaker}?},
	url = {https://proceedings.neurips.cc/paper/2016/hash/a486cd07e4ac3d270571622f4f316ec5-Abstract.html},
	language = {en},
	urldate = {2021-07-03},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Bolukbasi, Tolga and Chang, Kai-Wei and Zou, James Y. and Saligrama, Venkatesh and Kalai, Adam T.},
	year = {2016},
}

@article{obermeyer_dissecting_2019,
	title = {Dissecting racial bias in an algorithm used to manage the health of populations},
	volume = {366},
	copyright = {Copyright © 2019 The Authors, some rights reserved; exclusive licensee American Association for the Advancement of Science. No claim to original U.S. Government Works. http://www.sciencemag.org/about/science-licenses-journal-article-reuseThis is an article distributed under the terms of the Science Journals Default License.},
	issn = {0036-8075, 1095-9203},
	url = {https://science.sciencemag.org/content/366/6464/447},
	doi = {10.1126/science.aax2342},
	abstract = {{\textless}p{\textgreater}Health systems rely on commercial prediction algorithms to identify and help patients with complex health needs. We show that a widely used algorithm, typical of this industry-wide approach and affecting millions of patients, exhibits significant racial bias: At a given risk score, Black patients are considerably sicker than White patients, as evidenced by signs of uncontrolled illnesses. Remedying this disparity would increase the percentage of Black patients receiving additional help from 17.7 to 46.5\%. The bias arises because the algorithm predicts health care costs rather than illness, but unequal access to care means that we spend less money caring for Black patients than for White patients. Thus, despite health care cost appearing to be an effective proxy for health by some measures of predictive accuracy, large racial biases arise. We suggest that the choice of convenient, seemingly effective proxies for ground truth can be an important source of algorithmic bias in many contexts.{\textless}/p{\textgreater}},
	language = {en},
	number = {6464},
	urldate = {2021-07-03},
	journal = {Science},
	author = {Obermeyer, Ziad and Powers, Brian and Vogeli, Christine and Mullainathan, Sendhil},
	month = oct,
	year = {2019},
	pmid = {31649194},
	note = {Publisher: American Association for the Advancement of Science
Section: Research Article},
	pages = {447--453},
}

@article{winkler_association_2019,
	title = {Association {Between} {Surgical} {Skin} {Markings} in {Dermoscopic} {Images} and {Diagnostic} {Performance} of a {Deep} {Learning} {Convolutional} {Neural} {Network} for {Melanoma} {Recognition}},
	volume = {155},
	issn = {2168-6068},
	url = {https://doi.org/10.1001/jamadermatol.2019.1735},
	doi = {10.1001/jamadermatol.2019.1735},
	abstract = {Deep learning convolutional neural networks (CNNs) have shown a performance at the level of dermatologists in the diagnosis of melanoma. Accordingly, further exploring the potential limitations of CNN technology before broadly applying it is of special interest.To investigate the association between gentian violet surgical skin markings in dermoscopic images and the diagnostic performance of a CNN approved for use as a medical device in the European market.A cross-sectional analysis was conducted from August 1, 2018, to November 30, 2018, using a CNN architecture trained with more than 120 000 dermoscopic images of skin neoplasms and corresponding diagnoses. The association of gentian violet skin markings in dermoscopic images with the performance of the CNN was investigated in 3 image sets of 130 melanocytic lesions each (107 benign nevi, 23 melanomas).The same lesions were sequentially imaged with and without the application of a gentian violet surgical skin marker and then evaluated by the CNN for their probability of being a melanoma. In addition, the markings were removed by manually cropping the dermoscopic images to focus on the melanocytic lesion.Sensitivity, specificity, and area under the curve (AUC) of the receiver operating characteristic (ROC) curve for the CNN’s diagnostic classification in unmarked, marked, and cropped images.In all, 130 melanocytic lesions (107 benign nevi and 23 melanomas) were imaged. In unmarked lesions, the CNN achieved a sensitivity of 95.7\% (95\% CI, 79\%-99.2\%) and a specificity of 84.1\% (95\% CI, 76.0\%-89.8\%). The ROC AUC was 0.969. In marked lesions, an increase in melanoma probability scores was observed that resulted in a sensitivity of 100\% (95\% CI, 85.7\%-100\%) and a significantly reduced specificity of 45.8\% (95\% CI, 36.7\%-55.2\%, P \&lt; .001). The ROC AUC was 0.922. Cropping images led to the highest sensitivity of 100\% (95\% CI, 85.7\%-100\%), specificity of 97.2\% (95\% CI, 92.1\%-99.0\%), and ROC AUC of 0.993. Heat maps created by vanilla gradient descent backpropagation indicated that the blue markings were associated with the increased false-positive rate.This study’s findings suggest that skin markings significantly interfered with the CNN’s correct diagnosis of nevi by increasing the melanoma probability scores and consequently the false-positive rate. A predominance of skin markings in melanoma training images may have induced the CNN’s association of markings with a melanoma diagnosis. Accordingly, these findings suggest that skin markings should be avoided in dermoscopic images intended for analysis by a CNN.German Clinical Trial Register (DRKS) Identifier: DRKS00013570},
	number = {10},
	urldate = {2021-07-03},
	journal = {JAMA Dermatology},
	author = {Winkler, Julia K. and Fink, Christine and Toberer, Ferdinand and Enk, Alexander and Deinlein, Teresa and Hofmann-Wellenhof, Rainer and Thomas, Luc and Lallas, Aimilios and Blum, Andreas and Stolz, Wilhelm and Haenssle, Holger A.},
	month = oct,
	year = {2019},
	pages = {1135--1141},
}

@article{erion_improving_2021,
	title = {Improving performance of deep learning models with axiomatic attribution priors and expected gradients},
	copyright = {2021 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {2522-5839},
	url = {https://www.nature.com/articles/s42256-021-00343-w},
	doi = {10.1038/s42256-021-00343-w},
	abstract = {Recent research has demonstrated that feature attribution methods for deep networks can themselves be incorporated into training; these attribution priors optimize for a model whose attributions have certain desirable properties—most frequently, that particular features are important or unimportant. These attribution priors are often based on attribution methods that are not guaranteed to satisfy desirable interpretability axioms, such as completeness and implementation invariance. Here we introduce attribution priors to optimize for higher-level properties of explanations, such as smoothness and sparsity, enabled by a fast new attribution method formulation called expected gradients that satisfies many important interpretability axioms. This improves model performance on many real-world tasks where previous attribution priors fail. Our experiments show that the gains from combining higher-level attribution priors with expected gradients attributions are consistent across image, gene expression and healthcare datasets. We believe that this work motivates and provides the necessary tools to support the widespread adoption of axiomatic attribution priors in many areas of applied machine learning. The implementations and our results have been made freely available to academic communities.},
	language = {en},
	urldate = {2021-07-02},
	journal = {Nature Machine Intelligence},
	author = {Erion, Gabriel and Janizek, Joseph D. and Sturmfels, Pascal and Lundberg, Scott M. and Lee, Su-In},
	month = may,
	year = {2021},
	note = {Bandiera\_abtest: a
Cg\_type: Nature Research Journals
Primary\_atype: Research
Publisher: Nature Publishing Group
Subject\_term: Computer science;Regulatory networks;Risk factors
Subject\_term\_id: computer-science;regulatory-networks;risk-factors},
	keywords = {image classification},
	pages = {1--12},
}

@inproceedings{ebrahimi_remembering_2021,
	title = {Remembering for the {Right} {Reasons}: {Explanations} {Reduce} {Catastrophic} {Forgetting}},
	shorttitle = {Remembering for the {Right} {Reasons}},
	url = {https://openreview.net/forum?id=tHgJoMfy6nI},
	abstract = {The goal of continual learning (CL) is to learn a sequence of tasks without suffering from the phenomenon of catastrophic forgetting. Previous work has shown that leveraging memory in the form of a...},
	language = {en},
	urldate = {2021-06-23},
	author = {Ebrahimi, Sayna and Petryk, Suzanne and Gokul, Akash and Gan, William and Gonzalez, Joseph E. and Rohrbach, Marcus and Darrell, Trevor},
	year = {2021},
	keywords = {continual learning, image classification},
}

@inproceedings{wang_learning_2019,
	title = {Learning {Robust} {Global} {Representations} by {Penalizing} {Local} {Predictive} {Power}},
	volume = {32},
	url = {https://papers.nips.cc/paper/2019/hash/3eefceb8087e964f89c2d59e8a249915-Abstract.html},
	language = {en},
	urldate = {2021-07-03},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Wang, Haohan and Ge, Songwei and Lipton, Zachary and Xing, Eric P.},
	year = {2019},
}

@inproceedings{shi_informative_2020,
	title = {Informative {Dropout} for {Robust} {Representation} {Learning}: {A} {Shape}-bias {Perspective}},
	shorttitle = {Informative {Dropout} for {Robust} {Representation} {Learning}},
	url = {http://proceedings.mlr.press/v119/shi20e.html},
	abstract = {Convolutional Neural Networks (CNNs) are known to rely more on local texture rather than global shape when making decisions. Recent work also indicates a close relationship between CNN’s texture-bi...},
	language = {en},
	urldate = {2021-07-03},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Shi, Baifeng and Zhang, Dinghuai and Dai, Qi and Zhu, Zhanxing and Mu, Yadong and Wang, Jingdong},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {8828--8839},
}

@inproceedings{hermann_origins_2020,
	title = {The {Origins} and {Prevalence} of {Texture} {Bias} in {Convolutional} {Neural} {Networks}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/db5f9f42a7157abe65bb145000b5871a-Abstract.html},
	language = {en},
	urldate = {2021-07-03},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Hermann, Katherine and Chen, Ting and Kornblith, Simon},
	year = {2020},
	pages = {19000--19015},
}

@inproceedings{gatys_texture_2015,
	title = {Texture {Synthesis} {Using} {Convolutional} {Neural} {Networks}},
	volume = {28},
	url = {https://proceedings.neurips.cc/paper/2015/hash/a5e00132373a7031000fd987a3c9f87b-Abstract.html},
	language = {en},
	urldate = {2021-07-03},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Gatys, Leon and Ecker, Alexander S. and Bethge, Matthias},
	year = {2015},
}

@inproceedings{stammer_right_2021,
	title = {Right for the {Right} {Concept}: {Revising} {Neuro}-{Symbolic} {Concepts} by {Interacting} {With} {Their} {Explanations}},
	shorttitle = {Right for the {Right} {Concept}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Stammer_Right_for_the_Right_Concept_Revising_Neuro-Symbolic_Concepts_by_Interacting_CVPR_2021_paper.html},
	language = {en},
	urldate = {2021-06-16},
	author = {Stammer, Wolfgang and Schramowski, Patrick and Kersting, Kristian},
	year = {2021},
	keywords = {image classification},
	pages = {3619--3629},
}

@inproceedings{chen_neural_2018,
	address = {Melbourne, Australia},
	title = {Neural {Natural} {Language} {Inference} {Models} {Enhanced} with {External} {Knowledge}},
	url = {https://aclanthology.org/P18-1224},
	doi = {10.18653/v1/P18-1224},
	abstract = {Modeling natural language inference is a very challenging task. With the availability of large annotated data, it has recently become feasible to train complex models such as neural-network-based inference models, which have shown to achieve the state-of-the-art performance. Although there exist relatively large annotated data, can machines learn all knowledge needed to perform natural language inference (NLI) from these data? If not, how can neural-network-based NLI models benefit from external knowledge and how to build NLI models to leverage it? In this paper, we enrich the state-of-the-art neural natural language inference models with external knowledge. We demonstrate that the proposed models improve neural NLI models to achieve the state-of-the-art performance on the SNLI and MultiNLI datasets.},
	urldate = {2021-07-02},
	booktitle = {Proceedings of the 56th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Chen, Qian and Zhu, Xiaodan and Ling, Zhen-Hua and Inkpen, Diana and Wei, Si},
	month = jul,
	year = {2018},
	pages = {2406--2417},
}

@inproceedings{madotto_mem2seq_2018,
	address = {Melbourne, Australia},
	title = {{Mem2Seq}: {Effectively} {Incorporating} {Knowledge} {Bases} into {End}-to-{End} {Task}-{Oriented} {Dialog} {Systems}},
	shorttitle = {{Mem2Seq}},
	url = {https://aclanthology.org/P18-1136},
	doi = {10.18653/v1/P18-1136},
	abstract = {End-to-end task-oriented dialog systems usually suffer from the challenge of incorporating knowledge bases. In this paper, we propose a novel yet simple end-to-end differentiable model called memory-to-sequence (Mem2Seq) to address this issue. Mem2Seq is the first neural generative model that combines the multi-hop attention over memories with the idea of pointer network. We empirically show how Mem2Seq controls each generation step, and how its multi-hop attention mechanism helps in learning correlations between memories. In addition, our model is quite general without complicated task-specific designs. As a result, we show that Mem2Seq can be trained faster and attain the state-of-the-art performance on three different task-oriented dialog datasets.},
	urldate = {2021-07-02},
	booktitle = {Proceedings of the 56th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Madotto, Andrea and Wu, Chien-Sheng and Fung, Pascale},
	month = jul,
	year = {2018},
	pages = {1468--1478},
}

@inproceedings{mihaylov_knowledgeable_2018,
	address = {Melbourne, Australia},
	title = {Knowledgeable {Reader}: {Enhancing} {Cloze}-{Style} {Reading} {Comprehension} with {External} {Commonsense} {Knowledge}},
	shorttitle = {Knowledgeable {Reader}},
	url = {https://aclanthology.org/P18-1076},
	doi = {10.18653/v1/P18-1076},
	abstract = {We introduce a neural reading comprehension model that integrates external commonsense knowledge, encoded as a key-value memory, in a cloze-style setting. Instead of relying only on document-to-question interaction or discrete features as in prior work, our model attends to relevant external knowledge and combines this knowledge with the context representation before inferring the answer. This allows the model to attract and imply knowledge from an external knowledge source that is not explicitly stated in the text, but that is relevant for inferring the answer. Our model improves results over a very strong baseline on a hard Common Nouns dataset, making it a strong competitor of much more complex models. By including knowledge explicitly, our model can also provide evidence about the background knowledge used in the RC process.},
	urldate = {2021-07-02},
	booktitle = {Proceedings of the 56th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Mihaylov, Todor and Frank, Anette},
	month = jul,
	year = {2018},
	pages = {821--832},
}

@inproceedings{zhang_ernie_2019,
	address = {Florence, Italy},
	title = {{ERNIE}: {Enhanced} {Language} {Representation} with {Informative} {Entities}},
	shorttitle = {{ERNIE}},
	url = {https://aclanthology.org/P19-1139},
	doi = {10.18653/v1/P19-1139},
	abstract = {Neural language representation models such as BERT pre-trained on large-scale corpora can well capture rich semantic patterns from plain text, and be fine-tuned to consistently improve the performance of various NLP tasks. However, the existing pre-trained language models rarely consider incorporating knowledge graphs (KGs), which can provide rich structured knowledge facts for better language understanding. We argue that informative entities in KGs can enhance language representation with external knowledge. In this paper, we utilize both large-scale textual corpora and KGs to train an enhanced language representation model (ERNIE), which can take full advantage of lexical, syntactic, and knowledge information simultaneously. The experimental results have demonstrated that ERNIE achieves significant improvements on various knowledge-driven tasks, and meanwhile is comparable with the state-of-the-art model BERT on other common NLP tasks. The code and datasets will be available in the future.},
	urldate = {2021-07-02},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Zhang, Zhengyan and Han, Xu and Liu, Zhiyuan and Jiang, Xin and Sun, Maosong and Liu, Qun},
	month = jul,
	year = {2019},
	pages = {1441--1451},
}

@inproceedings{johnson_clevr_2017,
	address = {Honolulu, HI},
	title = {{CLEVR}: {A} {Diagnostic} {Dataset} for {Compositional} {Language} and {Elementary} {Visual} {Reasoning}},
	isbn = {978-1-5386-0457-1},
	shorttitle = {{CLEVR}},
	url = {https://ieeexplore.ieee.org/document/8099698/},
	doi = {10.1109/CVPR.2017.215},
	abstract = {When building artiﬁcial intelligence systems that can reason and answer questions about visual data, we need diagnostic tests to analyze our progress and discover shortcomings. Existing benchmarks for visual question answering can help, but have strong biases that models can exploit to correctly answer questions without reasoning. They also conﬂate multiple sources of error, making it hard to pinpoint model weaknesses. We present a diagnostic dataset that tests a range of visual reasoning abilities. It contains minimal biases and has detailed annotations describing the kind of reasoning each question requires. We use this dataset to analyze a variety of modern visual reasoning systems, providing novel insights into their abilities and limitations.},
	language = {en},
	urldate = {2021-07-02},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Johnson, Justin and Hariharan, Bharath and van der Maaten, Laurens and Fei-Fei, Li and Zitnick, C. Lawrence and Girshick, Ross},
	month = jul,
	year = {2017},
	pages = {1988--1997},
}

@inproceedings{teso_explanatory_2019,
	address = {New York, NY, USA},
	series = {{AIES} '19},
	title = {Explanatory {Interactive} {Machine} {Learning}},
	isbn = {978-1-4503-6324-2},
	url = {https://doi.org/10.1145/3306618.3314293},
	doi = {10.1145/3306618.3314293},
	abstract = {Although interactive learning puts the user into the loop, the learner remains mostly a black box for the user. Understanding the reasons behind predictions and queries is important when assessing how the learner works and, in turn, trust. Consequently, we propose the novel framework of explanatory interactive learning where, in each step, the learner explains its query to the user, and the user interacts by both answering the query and correcting the explanation. We demonstrate that this can boost the predictive and explanatory powers of, and the trust into, the learned model, using text (e.g. SVMs) and image classification (e.g. neural networks) experiments as well as a user study.},
	urldate = {2021-07-01},
	booktitle = {Proceedings of the 2019 {AAAI}/{ACM} {Conference} on {AI}, {Ethics}, and {Society}},
	publisher = {Association for Computing Machinery},
	author = {Teso, Stefano and Kersting, Kristian},
	month = jan,
	year = {2019},
	keywords = {active learning, explainable artificial intelligence, interpretability, machine learning},
	pages = {239--245},
}

@article{lapuschkin_unmasking_2019,
	title = {Unmasking {Clever} {Hans} predictors and assessing what machines really learn},
	volume = {10},
	copyright = {2019 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-019-08987-4},
	doi = {10.1038/s41467-019-08987-4},
	abstract = {Current learning machines have successfully solved hard application problems, reaching high accuracy and displaying seemingly intelligent behavior. Here we apply recent techniques for explaining decisions of state-of-the-art learning machines and analyze various tasks from computer vision and arcade games. This showcases a spectrum of problem-solving behaviors ranging from naive and short-sighted, to well-informed and strategic. We observe that standard performance evaluation metrics can be oblivious to distinguishing these diverse problem solving behaviors. Furthermore, we propose our semi-automated Spectral Relevance Analysis that provides a practically effective way of characterizing and validating the behavior of nonlinear learning machines. This helps to assess whether a learned model indeed delivers reliably for the problem that it was conceived for. Furthermore, our work intends to add a voice of caution to the ongoing excitement about machine intelligence and pledges to evaluate and judge some of these recent successes in a more nuanced manner.},
	language = {en},
	number = {1},
	urldate = {2021-07-01},
	journal = {Nature Communications},
	author = {Lapuschkin, Sebastian and Wäldchen, Stephan and Binder, Alexander and Montavon, Grégoire and Samek, Wojciech and Müller, Klaus-Robert},
	month = mar,
	year = {2019},
	note = {Bandiera\_abtest: a
Cc\_license\_type: cc\_by
Cg\_type: Nature Research Journals
Number: 1
Primary\_atype: Research
Publisher: Nature Publishing Group
Subject\_term: Applied mathematics;Computer science;Machine learning
Subject\_term\_id: applied-mathematics;computer-science;machine-learning},
	pages = {1096},
}

@inproceedings{tartaglione_end_2021,
	title = {{EnD}: {Entangling} and {Disentangling} {Deep} {Representations} for {Bias} {Correction}},
	shorttitle = {{EnD}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Tartaglione_EnD_Entangling_and_Disentangling_Deep_Representations_for_Bias_Correction_CVPR_2021_paper.html},
	language = {en},
	urldate = {2021-06-16},
	author = {Tartaglione, Enzo and Barbano, Carlo Alberto and Grangetto, Marco},
	year = {2021},
	keywords = {image classification},
	pages = {13508--13517},
}

@inproceedings{cadene_rubi_2019,
	title = {{RUBi}: {Reducing} {Unimodal} {Biases} for {Visual} {Question} {Answering}},
	abstract = {Visual Question Answering (VQA) is the task of answering questions about an image. Some VQA models often exploit unimodal biases to provide the correct answer without using the image information. As a result, they suffer from a huge drop in performance when evaluated on data outside their training set distribution. This critical issue makes them unsuitable for real-world settings. We propose RUBi, a new learning strategy to reduce biases in any VQA model. It reduces the importance of the most biased examples, i.e. examples that can be correctly classiﬁed without looking at the image. It implicitly forces the VQA model to use the two input modalities instead of relying on statistical regularities between the question and the answer. We leverage a question-only model that captures the language biases by identifying when these unwanted regularities are used. It prevents the base VQA model from learning them by inﬂuencing its predictions. This leads to dynamically adjusting the loss in order to compensate for biases. We validate our contributions by surpassing the current state-of-the-art results on VQA-CP v2. This dataset is speciﬁcally designed to assess the robustness of VQA models when exposed to different question biases at test time than what was seen during training.},
	language = {en},
	author = {Cadene, Remi and Dancette, Corentin},
	year = {2019},
	pages = {12},
}

@inproceedings{viviano_saliency_2021,
	title = {Saliency is a {Possible} {Red} {Herring} {When} {Diagnosing} {Poor} {Generalization}},
	url = {https://openreview.net/forum?id=c9-WeM-ceB},
	abstract = {Poor generalization is one symptom of models that learn to predict target variables using spuriously-correlated image features present only in the training distribution instead of the true image...},
	language = {en},
	urldate = {2021-02-24},
	author = {Viviano, Joseph D. and Simpson, Becks and Dutil, Francis and Bengio, Yoshua and Cohen, Joseph Paul},
	year = {2021},
	keywords = {image classification},
}

@article{jo_measuring_2017,
	title = {Measuring the tendency of {CNNs} to {Learn} {Surface} {Statistical} {Regularities}},
	url = {http://arxiv.org/abs/1711.11561},
	abstract = {Deep CNNs are known to exhibit the following peculiarity: on the one hand they generalize extremely well to a test set, while on the other hand they are extremely sensitive to so-called adversarial perturbations. The extreme sensitivity of high performance CNNs to adversarial examples casts serious doubt that these networks are learning high level abstractions in the dataset. We are concerned with the following question: How can a deep CNN that does not learn any high level semantics of the dataset manage to generalize so well? The goal of this article is to measure the tendency of CNNs to learn surface statistical regularities of the dataset. To this end, we use Fourier ﬁltering to construct datasets which share the exact same high level abstractions but exhibit qualitatively different surface statistical regularities. For the SVHN and CIFAR-10 datasets, we present two Fourier ﬁltered variants: a low frequency variant and a randomly ﬁltered variant. Each of the Fourier ﬁltering schemes is tuned to preserve the recognizability of the objects. Our main ﬁnding is that CNNs exhibit a tendency to latch onto the Fourier image statistics of the training dataset, sometimes exhibiting up to a 28\% generalization gap across the various test sets. Moreover, we observe that signiﬁcantly increasing the depth of a network has a very marginal impact on closing the aforementioned generalization gap. Thus we provide quantitative evidence supporting the hypothesis that deep CNNs tend to learn surface statistical regularities in the dataset rather than higher-level abstract concepts.},
	language = {en},
	urldate = {2021-07-01},
	journal = {arXiv:1711.11561 [cs, stat]},
	author = {Jo, Jason and Bengio, Yoshua},
	month = nov,
	year = {2017},
	note = {arXiv: 1711.11561},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{kim_why_2019,
	title = {Why are {Saliency} {Maps} {Noisy}? {Cause} of and {Solution} to {Noisy} {Saliency} {Maps}},
	shorttitle = {Why are {Saliency} {Maps} {Noisy}?},
	doi = {10.1109/ICCVW.2019.00510},
	abstract = {Saliency Map, the gradient of the score function with respect to the input, is the most basic technique for interpreting deep neural network decisions. However, saliency maps are often visually noisy. Although several hypotheses were proposed to account for this phenomenon, there are few works that provide rigorous analyses of noisy saliency maps. In this paper, we first propose a new hypothesis that noise may occur in saliency maps when irrelevant features pass through ReLU activation functions. Then, we propose Rectified Gradient, a method that alleviates this problem through layer-wise thresholding during backpropagation. Experiments with neural networks trained on CIFAR-10 and ImageNet showed effectiveness of our method and its superiority to other attribution methods.},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} {Workshop} ({ICCVW})},
	author = {Kim, Beomsu and Seo, Junghoon and Jeon, Seunghyeon and Koo, Jamyoung and Choe, Jeongyeol and Jeon, Taegyun},
	month = oct,
	year = {2019},
	note = {ISSN: 2473-9944},
	keywords = {Attribution-Map, Attribution-Method, Backpropagation, Deconvolution, Heating systems, Interpretability, Neural networks, Noise measurement, Training, Visualization},
	pages = {4149--4157},
}

@inproceedings{zhuang_care_2019,
	title = {{CARE}: {Class} {Attention} to {Regions} of {Lesion} for {Classification} on {Imbalanced} {Data}},
	shorttitle = {{CARE}},
	url = {http://proceedings.mlr.press/v102/zhuang19a.html},
	abstract = {To date, it is still an open and challenging problem for intelligent diagnosis systems to effectively learn from imbalanced data, especially with large samples of common diseases and much smaller s...},
	language = {en},
	urldate = {2021-07-01},
	booktitle = {International {Conference} on {Medical} {Imaging} with {Deep} {Learning}},
	publisher = {PMLR},
	author = {Zhuang, Jiaxin and Cai, Jiabin and Wang, Ruixuan and Zhang, Jianguo and Zheng, Weishi},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {588--597},
}

@inproceedings{wang_removing_2021,
	title = {Removing the {Background} by {Adding} the {Background}: {Towards} {Background} {Robust} {Self}-{Supervised} {Video} {Representation} {Learning}},
	shorttitle = {Removing the {Background} by {Adding} the {Background}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Wang_Removing_the_Background_by_Adding_the_Background_Towards_Background_Robust_CVPR_2021_paper.html},
	language = {en},
	urldate = {2021-06-17},
	author = {Wang, Jinpeng and Gao, Yuting and Li, Ke and Lin, Yiqi and Ma, Andy J. and Cheng, Hao and Peng, Pai and Huang, Feiyue and Ji, Rongrong and Sun, Xing},
	year = {2021},
	keywords = {video recognition},
	pages = {11804--11813},
}

@inproceedings{zhang_explicit_2021,
	title = {Explicit {Knowledge} {Incorporation} for {Visual} {Reasoning}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Zhang_Explicit_Knowledge_Incorporation_for_Visual_Reasoning_CVPR_2021_paper.html},
	language = {en},
	urldate = {2021-06-16},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Zhang, Yifeng and Jiang, Ming and Zhao, Qi},
	year = {2021},
	keywords = {visual reasoning},
	pages = {1356--1365},
}

@inproceedings{zunino_explainable_2021,
	title = {Explainable {Deep} {Classification} {Models} for {Domain} {Generalization}},
	url = {https://openaccess.thecvf.com/content/CVPR2021W/TCV/html/Zunino_Explainable_Deep_Classification_Models_for_Domain_Generalization_CVPRW_2021_paper.html},
	language = {en},
	urldate = {2021-06-17},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshop}},
	author = {Zunino, Andrea and Bargal, Sarah Adel and Volpi, Riccardo and Sameki, Mehrnoosh and Zhang, Jianming and Sclaroff, Stan and Murino, Vittorio and Saenko, Kate},
	year = {2021},
	keywords = {image classification},
	pages = {3233--3242},
}

@inproceedings{geirhos_imagenet-trained_2019,
	title = {{ImageNet}-trained {CNNs} are biased towards texture; increasing shape bias improves accuracy and robustness},
	url = {http://arxiv.org/abs/1811.12231},
	abstract = {Convolutional Neural Networks (CNNs) are commonly thought to recognise objects by learning increasingly complex representations of object shapes. Some recent studies suggest a more important role of image textures. We here put these conﬂicting hypotheses to a quantitative test by evaluating CNNs and human observers on images with a texture-shape cue conﬂict. We show that ImageNettrained CNNs are strongly biased towards recognising textures rather than shapes, which is in stark contrast to human behavioural evidence and reveals fundamentally different classiﬁcation strategies. We then demonstrate that the same standard architecture (ResNet-50) that learns a texture-based representation on ImageNet is able to learn a shape-based representation instead when trained on ‘StylizedImageNet’, a stylized version of ImageNet. This provides a much better ﬁt for human behavioural performance in our well-controlled psychophysical lab setting (nine experiments totalling 48,560 psychophysical trials across 97 observers) and comes with a number of unexpected emergent beneﬁts such as improved object detection performance and previously unseen robustness towards a wide range of image distortions, highlighting advantages of a shape-based representation.},
	language = {en},
	urldate = {2020-06-15},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Geirhos, Robert and Rubisch, Patricia and Michaelis, Claudio and Bethge, Matthias and Wichmann, Felix A. and Brendel, Wieland},
	year = {2019},
}

@inproceedings{li_deep_2018,
	title = {Deep {Learning} for {Case}-{Based} {Reasoning} {Through} {Prototypes}: {A} {Neural} {Network} {That} {Explains} {Its} {Predictions}},
	volume = {32},
	copyright = {Copyright (c)},
	shorttitle = {Deep {Learning} for {Case}-{Based} {Reasoning} {Through} {Prototypes}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/11771},
	language = {en},
	urldate = {2021-01-16},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Li, Oscar and Liu, Hao and Chen, Chaofan and Rudin, Cynthia},
	month = apr,
	year = {2018},
	note = {Number: 1},
}

@inproceedings{marcos_semantically_2019,
	title = {Semantically {Interpretable} {Activation} {Maps}: what-where-how explanations within {CNNs}},
	shorttitle = {Semantically {Interpretable} {Activation} {Maps}},
	doi = {10.1109/ICCVW.2019.00518},
	abstract = {A main issue preventing the use of Convolutional Neural Networks (CNN) in end user applications is the low level of transparency in the decision process. Previous work on CNN interpretability has mostly focused either on localizing the regions of the image that contribute to the result or on building an external model that generates plausible explanations. However, the former does not provide any semantic information and the latter does not guarantee the faithfulness of the explanation. We propose an intermediate representation composed of multiple Semantically Interpretable Activation Maps (SIAM) indicating the presence of predefined attributes at different locations of the image. These attribute maps are then linearly combined to produce the final output. This gives the user insight into what the model has seen, where, and a final output directly linked to this information in a comprehensive and interpretable way. We test the method on the task of landscape scenicness (aesthetic value) estimation, using an intermediate representation of 33 attributes from the SUN Attributes database. The results confirm that SIAM makes it possible to understand what attributes in the image are contributing to the final score and where they are located. Since it is based on learning from multiple tasks and datasets, SIAM improve the explanability of the prediction without additional annotation efforts or computational overhead at inference time, while keeping good performances on both the final and intermediate tasks.},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} {Workshop} ({ICCVW})},
	author = {Marcos, D. and Lobry, S. and Tuia, D.},
	month = oct,
	year = {2019},
	note = {ISSN: 2473-9944},
	pages = {4207--4215},
}

@inproceedings{wu_beyond_2017,
	title = {Beyond {Sparsity}: {Tree} {Regularization} of {Deep} {Models} for {Interpretability}},
	shorttitle = {Beyond {Sparsity}},
	url = {http://arxiv.org/abs/1711.06178},
	abstract = {The lack of interpretability remains a key barrier to the adoption of deep models in many applications. In this work, we explicitly regularize deep models so human users might step through the process behind their predictions in little time. Specifically, we train deep time-series models so their class-probability predictions have high accuracy while being closely modeled by decision trees with few nodes. Using intuitive toy examples as well as medical tasks for treating sepsis and HIV, we demonstrate that this new tree regularization yields models that are easier for humans to simulate than simpler L1 or L2 penalties without sacrificing predictive power.},
	urldate = {2020-12-25},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Wu, Mike and Hughes, Michael C. and Parbhoo, Sonali and Zazzi, Maurizio and Roth, Volker and Doshi-Velez, Finale},
	month = nov,
	year = {2017},
	note = {arXiv: 1711.06178},
}

@article{mitsuhara_embedding_2019,
	title = {Embedding {Human} {Knowledge} into {Deep} {Neural} {Network} via {Attention} {Map}},
	url = {http://arxiv.org/abs/1905.03540},
	abstract = {In this work, we aim to realize a method for embedding human knowledge into deep neural networks. While the conventional method to embed human knowledge has been applied for non-deep machine learning, it is challenging to apply it for deep learning models due to the enormous number of model parameters. To tackle this problem, we focus on the attention mechanism of an attention branch network (ABN). In this paper, we propose a ﬁne-tuning method that utilizes a single-channel attention map which is manually edited by a human expert. Our ﬁne-tuning method can train a network so that the output attention map corresponds to the edited ones. As a result, the ﬁne-tuned network can output an attention map that takes into account human knowledge. Experimental results with ImageNet, CUB-200-2010, and IDRiD demonstrate that it is possible to obtain a clear attention map for a visual explanation and improve the classiﬁcation performance. Our ﬁndings can be a novel framework for optimizing networks through human intuitive editing via a visual interface and suggest new possibilities for human-machine cooperation in addition to the improvement of visual explanations.},
	language = {en},
	urldate = {2020-06-16},
	journal = {arXiv:1905.03540 [cs]},
	author = {Mitsuhara, Masahiro and Fukui, Hiroshi and Sakashita, Yusuke and Ogata, Takanori and Hirakawa, Tsubasa and Yamashita, Takayoshi and Fujiyoshi, Hironobu},
	month = dec,
	year = {2019},
	note = {arXiv: 1905.03540},
}

@inproceedings{jiang_learning_2017,
	title = {Learning {Discriminative} {Features} via {Label} {Consistent} {Neural} {Network}},
	doi = {10.1109/WACV.2017.30},
	abstract = {Deep Convolutional Neural Networks (CNN) enforce supervised information only at the output layer, and hidden layers are trained by back propagating the prediction error from the output layer without explicit supervision. We propose a supervised feature learning approach, Label Consistent Neural Network, which enforces direct supervision in late hidden layers in a novel way. We associate each neuron in a hidden layer with a particular class label and encourage it to be activated for input signals from the same class. More specifically, we introduce a label consistency regularization called "discriminative representation error" loss for late hidden layers and combine it with classification error loss to build our overall objective function. This label consistency constraint alleviates the common problem of gradient vanishing and tends to faster convergence, it also makes the features derived from late hidden layers discriminative enough for classification even using a simple k-NN classifier. Experimental results demonstrate that our approach achieves state-of-the-art performances on several public datasets for action and object category recognition.},
	booktitle = {2017 {IEEE} {Winter} {Conference} on {Applications} of {Computer} {Vision} ({WACV})},
	author = {Jiang, Z. and Wang, Y. and Davis, L. and Andrews, W. and Rozgic, V.},
	month = mar,
	year = {2017},
	pages = {207--216},
}

@article{zhang_interpretable_2020,
	title = {Interpretable {CNNs} for {Object} {Classification}},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2020.2982882},
	abstract = {This paper proposes a generic method to learn interpretable convolutional filters in a deep convolutional neural network (CNN) for object classification, where each interpretable filter encodes features of a specific object part. Our method does not require additional annotations of object parts or textures for supervision. Instead, we use the same training data as traditional CNNs. Our method automatically assigns each interpretable filter in a high conv-layer with an object part of a certain category during the learning process. Such explicit knowledge representations in conv-layers of CNN help people clarify the logic encoded in the CNN, i.e., answering what patterns the CNN extracts from an input image and uses for prediction. We have tested our method using different benchmark CNNs with various structures to demonstrate the broad applicability of our method. Experiments have shown that our interpretable filters are much more semantically meaningful than traditional filters.},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Zhang, Q. and Wang, X. and Wu, Y. N. and Zhou, H. and Zhu, S.-C.},
	year = {2020},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	pages = {1--1},
}

@inproceedings{zeiler_adaptive_2011,
	title = {Adaptive deconvolutional networks for mid and high level feature learning},
	doi = {10.1109/ICCV.2011.6126474},
	abstract = {We present a hierarchical model that learns image decompositions via alternating layers of convolutional sparse coding and max pooling. When trained on natural images, the layers of our model capture image information in a variety of forms: low-level edges, mid-level edge junctions, high-level object parts and complete objects. To build our model we rely on a novel inference scheme that ensures each layer reconstructs the input, rather than just the output of the layer directly beneath, as is common with existing hierarchical approaches. This makes it possible to learn multiple layers of representation and we show models with 4 layers, trained on images from the Caltech-101 and 256 datasets. When combined with a standard classifier, features extracted from these models outperform SIFT, as well as representations from other feature learning methods.},
	booktitle = {2011 {International} {Conference} on {Computer} {Vision}},
	author = {Zeiler, Matthew D. and Taylor, Graham W. and Fergus, Rob},
	month = nov,
	year = {2011},
	note = {ISSN: 2380-7504},
	pages = {2018--2025},
}

@inproceedings{abbasi-asl_interpreting_2017,
	title = {Interpreting {Convolutional} {Neural} {Networks} {Through} {Compression}},
	url = {http://arxiv.org/abs/1711.02329},
	abstract = {Convolutional neural networks (CNNs) achieve state-of-the-art performance in a wide variety of tasks in computer vision. However, interpreting CNNs still remains a challenge. This is mainly due to the large number of parameters in these networks. Here, we investigate the role of compression and particularly pruning filters in the interpretation of CNNs. We exploit our recently-proposed greedy structural compression scheme that prunes filters in a trained CNN. In our compression, the filter importance index is defined as the classification accuracy reduction (CAR) of the network after pruning that filter. The filters are then iteratively pruned based on the CAR index. We demonstrate the interpretability of CAR-compressed CNNs by showing that our algorithm prunes filters with visually redundant pattern selectivity. Specifically, we show the importance of shape-selective filters for object recognition, as opposed to color-selective filters. Out of top 20 CAR-pruned filters in AlexNet, 17 of them in the first layer and 14 of them in the second layer are color-selective filters. Finally, we introduce a variant of our CAR importance index that quantifies the importance of each image class to each CNN filter. We show that the most and the least important class labels present a meaningful interpretation of each filter that is consistent with the visualized pattern selectivity of that filter.},
	urldate = {2021-01-16},
	booktitle = {{NeurIPS} 2017 {Symposium} on {Interpretable} {Machine} {Learning}},
	author = {Abbasi-Asl, Reza and Yu, Bin},
	month = nov,
	year = {2017},
	note = {arXiv: 1711.02329},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{linsley_learning_2019,
	title = {Learning what and where to attend},
	url = {https://openreview.net/forum?id=BJgLg3R9KQ},
	abstract = {A large-scale dataset for training attention models for object recognition leads to more accurate, interpretable, and human-like object recognition.},
	language = {en},
	urldate = {2021-01-16},
	author = {Linsley, Drew and Shiebler, Dan and Eberhardt, Sven and Serre, Thomas},
	year = {2019},
}

@inproceedings{singh_hierarchical_2019,
	title = {Hierarchical interpretations for neural network predictions},
	url = {https://openreview.net/forum?id=SkEqro0ctQ},
	abstract = {We introduce and validate hierarchical local interpretations, the first technique to automatically search for and display important interactions for individual predictions made by LSTMs and CNNs.},
	language = {en},
	urldate = {2021-01-16},
	author = {Singh, Chandan and Murdoch, W. James and Yu, Bin},
	year = {2019},
}

@inproceedings{gu_understanding_2019,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Understanding {Individual} {Decisions} of {CNNs} via {Contrastive} {Backpropagation}},
	isbn = {978-3-030-20893-6},
	doi = {10.1007/978-3-030-20893-6_8},
	abstract = {A number of backpropagation-based approaches such as DeConvNets, vanilla Gradient Visualization and Guided Backpropagation have been proposed to better understand individual decisions of deep convolutional neural networks. The saliency maps produced by them are proven to be non-discriminative. Recently, the Layer-wise Relevance Propagation (LRP) approach was proposed to explain the classification decisions of rectifier neural networks. In this work, we evaluate the discriminativeness of the generated explanations and analyze the theoretical foundation of LRP, i.e. Deep Taylor Decomposition. The experiments and analysis conclude that the explanations generated by LRP are not class-discriminative. Based on LRP, we propose Contrastive Layer-wise Relevance Propagation (CLRP), which is capable of producing instance-specific, class-discriminative, pixel-wise explanations. In the experiments, we use the CLRP to explain the decisions and understand the difference between neurons in individual classification decisions. We also evaluate the explanations quantitatively with a Pointing Game and an ablation study. Both qualitative and quantitative evaluations show that the CLRP generates better explanations than the LRP.},
	language = {en},
	booktitle = {Asian {Conference} on {Computer} {Vision}},
	publisher = {Springer International Publishing},
	author = {Gu, Jindong and Yang, Yinchong and Tresp, Volker},
	editor = {Jawahar, C. V. and Li, Hongdong and Mori, Greg and Schindler, Konrad},
	year = {2019},
	keywords = {Discriminative saliency maps, Explainable deep learning, LRP},
	pages = {119--134},
}

@inproceedings{ross_evaluating_2021,
	address = {New York, NY, USA},
	title = {Evaluating the {Interpretability} of {Generative} {Models} by {Interactive} {Reconstruction}},
	isbn = {978-1-4503-8096-6},
	url = {https://doi.org/10.1145/3411764.3445296},
	abstract = {For machine learning models to be most useful in numerous sociotechnical systems, many have argued that they must be human-interpretable. However, despite increasing interest in interpretability, there remains no firm consensus on how to measure it. This is especially true in representation learning, where interpretability research has focused on “disentanglement” measures only applicable to synthetic datasets and not grounded in human factors. We introduce a task to quantify the human-interpretability of generative model representations, where users interactively modify representations to reconstruct target instances. On synthetic datasets, we find performance on this task much more reliably differentiates entangled and disentangled models than baseline approaches. On a real dataset, we find it differentiates between representation learning methods widely believed but never shown to produce more or less interpretable models. In both cases, we ran small-scale think-aloud studies and large-scale experiments on Amazon Mechanical Turk to confirm that our qualitative and quantitative results agreed.},
	urldate = {2021-06-25},
	booktitle = {Proceedings of the 2021 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Ross, Andrew and Chen, Nina and Hang, Elisa Zhao and Glassman, Elena L. and Doshi-Velez, Finale},
	month = may,
	year = {2021},
	keywords = {evaluation methods, interpretability, representation learning},
	pages = {1--15},
}

@inproceedings{ibrahim_global_2019,
	address = {New York, NY, USA},
	series = {{AIES} '19},
	title = {Global {Explanations} of {Neural} {Networks}: {Mapping} the {Landscape} of {Predictions}},
	isbn = {978-1-4503-6324-2},
	shorttitle = {Global {Explanations} of {Neural} {Networks}},
	url = {https://doi.org/10.1145/3306618.3314230},
	doi = {10.1145/3306618.3314230},
	abstract = {A barrier to the wider adoption of neural networks is their lack of interpretability. While local explanation methods exist for one prediction, most global attributions still reduce neural network decisions to a single set of features. In response, we present an approach for generating global attributions called GAM, which explains the landscape of neural network predictions across subpopulations. GAM augments global explanations with the proportion of samples that each attribution best explains and specifies which samples are described by each attribution. Global explanations also have tunable granularity to detect more or fewer subpopulations. We demonstrate that GAM's global explanations 1) yield the known feature importances of simulated data, 2) match feature weights of interpretable statistical models on real data, and 3) are intuitive to practitioners through user studies. With more transparent predictions, GAM can help ensure neural network decisions are generated for the right reasons.},
	urldate = {2021-06-25},
	booktitle = {Proceedings of the 2019 {AAAI}/{ACM} {Conference} on {AI}, {Ethics}, and {Society}},
	publisher = {Association for Computing Machinery},
	author = {Ibrahim, Mark and Louie, Melissa and Modarres, Ceena and Paisley, John},
	month = jan,
	year = {2019},
	keywords = {explainable deep learning, global interpretability, neural networks},
	pages = {279--287},
}

@article{moraffah_causal_2020,
	title = {Causal {Interpretability} for {Machine} {Learning} - {Problems}, {Methods} and {Evaluation}},
	volume = {22},
	issn = {1931-0145},
	url = {https://doi.org/10.1145/3400051.3400058},
	doi = {10.1145/3400051.3400058},
	abstract = {Machine learning models have had discernible achievements in a myriad of applications. However, most of these models are black-boxes, and it is obscure how the decisions are made by them. This makes the models unreliable and untrustworthy. To provide insights into the decision making processes of these models, a variety of traditional interpretable models have been proposed. Moreover, to generate more humanfriendly explanations, recent work on interpretability tries to answer questions related to causality such as "Why does this model makes such decisions?" or "Was it a specific feature that caused the decision made by the model?". In this work, models that aim to answer causal questions are referred to as causal interpretable models. The existing surveys have covered concepts and methodologies of traditional interpretability. In this work, we present a comprehensive survey on causal interpretable models from the aspects of the problems and methods. In addition, this survey provides in-depth insights into the existing evaluation metrics for measuring interpretability, which can help practitioners understand for what scenarios each evaluation metric is suitable.},
	number = {1},
	urldate = {2020-12-21},
	journal = {ACM SIGKDD Explorations Newsletter},
	author = {Moraffah, Raha and Karami, Mansooreh and Guo, Ruocheng and Raglin, Adrienne and Liu, Huan},
	month = may,
	year = {2020},
	keywords = {causal inference, counterfactuals, explainability, interpratablity, machine learning},
	pages = {18--33},
}

@article{patterson_sun_2014,
	title = {The {SUN} {Attribute} {Database}: {Beyond} {Categories} for {Deeper} {Scene} {Understanding}},
	volume = {108},
	issn = {1573-1405},
	shorttitle = {The {SUN} {Attribute} {Database}},
	url = {https://doi.org/10.1007/s11263-013-0695-z},
	doi = {10.1007/s11263-013-0695-z},
	abstract = {In this paper we present the first large-scale scene attribute database. First, we perform crowdsourced human studies to find a taxonomy of 102 discriminative attributes. We discover attributes related to materials, surface properties, lighting, affordances, and spatial layout. Next, we build the “SUN attribute database” on top of the diverse SUN categorical database. We use crowdsourcing to annotate attributes for 14,340 images from 707 scene categories. We perform numerous experiments to study the interplay between scene attributes and scene categories. We train and evaluate attribute classifiers and then study the feasibility of attributes as an intermediate scene representation for scene classification, zero shot learning, automatic image captioning, semantic image search, and parsing natural images. We show that when used as features for these tasks, low dimensional scene attributes can compete with or improve on the state of the art performance. The experiments suggest that scene attributes are an effective low-dimensional feature for capturing high-level context and semantics in scenes.},
	language = {en},
	number = {1},
	urldate = {2021-06-25},
	journal = {International Journal of Computer Vision},
	author = {Patterson, Genevieve and Xu, Chen and Su, Hang and Hays, James},
	month = may,
	year = {2014},
	pages = {59--81},
}

@inproceedings{marcos_contextual_2020,
	title = {Contextual {Semantic} {Interpretability}},
	url = {https://openaccess.thecvf.com/content/ACCV2020/html/Marcos_Contextual_Semantic_Interpretability_ACCV_2020_paper.html},
	language = {en},
	urldate = {2021-06-25},
	author = {Marcos, Diego and Fong, Ruth and Lobry, Sylvain and Flamary, Remi and Courty, Nicolas and Tuia, Devis},
	year = {2020},
}

@inproceedings{jin_towards_2020,
	title = {Towards {Hierarchical} {Importance} {Attribution}: {Explaining} {Compositional} {Semantics} for {Neural} {Sequence} {Models}},
	shorttitle = {Towards {Hierarchical} {Importance} {Attribution}},
	url = {https://openreview.net/forum?id=BkxRRkSKwr},
	abstract = {We propose measurement of phrase importance and algorithms for hierarchical explanation of neural sequence model predictions},
	language = {en},
	urldate = {2021-01-16},
	author = {Jin, Xisen and Wei, Zhongyu and Du, Junyi and Xue, Xiangyang and Ren, Xiang},
	year = {2020},
}

@inproceedings{yang_learn_2020,
	title = {Learn to {Explain} {Efficiently} via {Neural} {Logic} {Inductive} {Learning}},
	url = {https://openreview.net/forum?id=SJlh8CEYDB},
	abstract = {An efficient differentiable ILP model that learns first-order logic rules that can explain the data.},
	language = {en},
	urldate = {2021-01-16},
	author = {Yang, Yuan and Song, Le},
	year = {2020},
}

@inproceedings{chen_explaining_2019,
	title = {Explaining {Neural} {Networks} {Semantically} and {Quantitatively}},
	url = {https://openaccess.thecvf.com/content_ICCV_2019/html/Chen_Explaining_Neural_Networks_Semantically_and_Quantitatively_ICCV_2019_paper.html},
	urldate = {2021-06-25},
	author = {Chen, Runjin and Chen, Hao and Ren, Jie and Huang, Ge and Zhang, Quanshi},
	year = {2019},
	pages = {9187--9196},
}

@inproceedings{barcelo_model_2020,
	title = {Model {Interpretability} through the lens of {Computational} {Complexity}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/b1adda14824f50ef24ff1c05bb66faf3-Abstract.html},
	language = {en},
	urldate = {2020-12-25},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Barceló, Pablo and Monet, Mikaël and Pérez, Jorge and Subercaseaux, Bernardo},
	year = {2020},
}

@inproceedings{bass_icam_2020,
	title = {{ICAM}: {Interpretable} {Classification} via {Disentangled} {Representations} and {Feature} {Attribution} {Mapping}},
	volume = {33},
	shorttitle = {{ICAM}},
	url = {https://proceedings.neurips.cc/paper/2020/hash/56f9f88906aebf4ad985aaec7fa01313-Abstract.html},
	language = {en},
	urldate = {2020-12-25},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Bass, Cher and da Silva, Mariana and Sudre, Carole and Tudosiu, Petru-Daniel and Smith, Stephen and Robinson, Emma},
	year = {2020},
}

@inproceedings{chen_understanding_2020,
	title = {Understanding {Deep} {Architecture} with {Reasoning} {Layer}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/0d82627e10660af39ea7eb69c3568955-Abstract.html},
	language = {en},
	urldate = {2020-12-26},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Chen, Xinshi and Zhang, Yufei and Reisinger, Christoph and Song, Le},
	year = {2020},
}

@inproceedings{covert_understanding_2020,
	title = {Understanding {Global} {Feature} {Contributions} {With} {Additive} {Importance} {Measures}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/c7bf0b7c1a86d5eb3be2c722cf2cf746-Abstract.html},
	language = {en},
	urldate = {2020-12-26},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Covert, Ian and Lundberg, Scott M. and Lee, Su-In},
	year = {2020},
}

@inproceedings{crabbe_learning_2020,
	title = {Learning outside the {Black}-{Box}: {The} pursuit of interpretable models},
	volume = {33},
	shorttitle = {Learning outside the {Black}-{Box}},
	url = {https://proceedings.neurips.cc/paper/2020/hash/ce758408f6ef98d7c7a7b786eca7b3a8-Abstract.html},
	language = {en},
	urldate = {2020-12-25},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Crabbe, Jonathan and Zhang, Yao and Zame, William and van der Schaar, Mihaela},
	year = {2020},
}

@inproceedings{jeyakumar_how_2020,
	title = {How {Can} {I} {Explain} {This} to {You}? {An} {Empirical} {Study} of {Deep} {Neural} {Network} {Explanation} {Methods}},
	volume = {33},
	shorttitle = {How {Can} {I} {Explain} {This} to {You}?},
	url = {https://proceedings.neurips.cc/paper/2020/hash/2c29d89cc56cdb191c60db2f0bae796b-Abstract.html},
	language = {en},
	urldate = {2020-12-26},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Jeyakumar, Jeya Vikranth and Noor, Joseph and Cheng, Yu-Hsi and Garcia, Luis and Srivastava, Mani},
	year = {2020},
}

@inproceedings{laina_quantifying_2020,
	title = {Quantifying {Learnability} and {Describability} of {Visual} {Concepts} {Emerging} in {Representation} {Learning}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/98dce83da57b0395e163467c9dae521b-Abstract.html},
	language = {en},
	urldate = {2020-12-26},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Laina, Iro and Fong, Ruth and Vedaldi, Andrea},
	year = {2020},
}

@inproceedings{lakshminarayanan_neural_2020,
	title = {Neural {Path} {Features} and {Neural} {Path} {Kernel} : {Understanding} the role of gates in deep learning},
	volume = {33},
	shorttitle = {Neural {Path} {Features} and {Neural} {Path} {Kernel}},
	url = {https://proceedings.neurips.cc/paper/2020/hash/37f76c6fe3ab45e0cd7ecb176b5a046d-Abstract.html},
	language = {en},
	urldate = {2020-12-26},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Lakshminarayanan, Chandrashekar and Vikram Singh, Amit},
	year = {2020},
}

@inproceedings{natesan_ramamurthy_model_2020,
	title = {Model {Agnostic} {Multilevel} {Explanations}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/426f990b332ef8193a61cc90516c1245-Abstract.html},
	language = {en},
	urldate = {2020-12-26},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Natesan Ramamurthy, Karthikeyan and Vinzamuri, Bhanukiran and Zhang, Yunfeng and Dhurandhar, Amit},
	year = {2020},
}

@inproceedings{oshaughnessy_generative_2020,
	title = {Generative causal explanations of black-box classifiers},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/3a93a609b97ec0ab0ff5539eb79ef33a-Abstract.html},
	language = {en},
	urldate = {2020-12-26},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {O'Shaughnessy, Matthew and Canal, Gregory and Connor, Marissa and Rozell, Christopher and Davenport, Mark},
	year = {2020},
}

@inproceedings{pedapati_learning_2020,
	title = {Learning {Global} {Transparent} {Models} consistent with {Local} {Contrastive} {Explanations}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/24aef8cb3281a2422a59b51659f1ad2e-Abstract.html},
	language = {en},
	urldate = {2020-12-26},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Pedapati, Tejaswini and Balakrishnan, Avinash and Shanmugam, Karthikeyan and Dhurandhar, Amit},
	year = {2020},
}

@inproceedings{tsang_how_2020,
	title = {How does {This} {Interaction} {Affect} {Me}? {Interpretable} {Attribution} for {Feature} {Interactions}},
	volume = {33},
	shorttitle = {How does {This} {Interaction} {Affect} {Me}?},
	url = {https://proceedings.neurips.cc/paper/2020/hash/443dec3062d0286986e21dc0631734c9-Abstract.html},
	language = {en},
	urldate = {2020-12-25},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Tsang, Michael and Rambhatla, Sirisha and Liu, Yan},
	year = {2020},
}

@inproceedings{wang_smoothed_2020,
	title = {Smoothed {Geometry} for {Robust} {Attribution}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/9d94c8981a48d12adfeecfe1ae6e0ec1-Abstract.html},
	language = {en},
	urldate = {2020-12-26},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Wang, Zifan and Wang, Haofan and Ramkumar, Shakul and Mardziel, Piotr and Fredrikson, Matt and Datta, Anupam},
	year = {2020},
}

@inproceedings{zhou_learning_2020,
	title = {Learning identifiable and interpretable latent models of high-dimensional neural activity using pi-{VAE}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/510f2318f324cf07fce24c3a4b89c771-Abstract.html},
	language = {en},
	urldate = {2020-12-25},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Zhou, Ding and Wei, Xue-Xin},
	year = {2020},
}

@inproceedings{zhou_towards_2020,
	title = {Towards {Interpretable} {Natural} {Language} {Understanding} with {Explanations} as {Latent} {Variables}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/4be2c8f27b8a420492f2d44463933eb6-Abstract.html},
	language = {en},
	urldate = {2020-12-25},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Zhou, Wangchunshu and Hu, Jinyi and Zhang, Hanlin and Liang, Xiaodan and Sun, Maosong and Xiong, Chenyan and Tang, Jian},
	year = {2020},
}

@inproceedings{patir_interpretability_2020,
	address = {Cham},
	series = {Communications in {Computer} and {Information} {Science}},
	title = {Interpretability of {Black} {Box} {Models} {Through} {Data}-{View} {Extraction} and {Shadow} {Model} {Creation}},
	isbn = {978-3-030-63823-8},
	doi = {10.1007/978-3-030-63823-8_44},
	abstract = {Deep learning models trained using massive amounts of data, tend to capture one view of the data and its associated mapping. Different deep learning models built on the same training data may capture different views of the data based on the underlying techniques used. For explaining the decisions arrived by Black box deep learning models, we argue that it is essential to reproduce that model’s view of the training data faithfully. This faithful reproduction can then be used for explanation generation. We investigate two methods for data-view extraction: Hill Climbing approach and a GAN-driven approach. We then use this synthesized data for explanation generation by using methods such as Decision-Trees and Permutation Importance. We evaluate these approaches on a Black box model trained on public datasets and show its usefulness in explanation generation.},
	language = {en},
	booktitle = {International {Conference} on {Neural} {Information} {Processing}},
	publisher = {Springer International Publishing},
	author = {Patir, Rupam and Singhal, Shubham and Anantaram, C. and Goyal, Vikram},
	editor = {Yang, Haiqin and Pasupa, Kitsuchart and Leung, Andrew Chi-Sing and Kwok, James T. and Chan, Jonathan H. and King, Irwin},
	year = {2020},
	keywords = {Data synthesis, Data-view extraction, Interpretability},
	pages = {378--385},
}

@inproceedings{yeh_completeness-aware_2020,
	title = {On {Completeness}-aware {Concept}-{Based} {Explanations} in {Deep} {Neural} {Networks}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/ecb287ff763c169694f682af52c1f309-Abstract.html},
	language = {en},
	urldate = {2021-06-23},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Yeh, Chih-Kuan and Kim, Been and Arik, Sercan and Li, Chun-Liang and Pfister, Tomas and Ravikumar, Pradeep},
	year = {2020},
	pages = {20554--20565},
}

@inproceedings{ignatiev_towards_2020,
	title = {Towards {Trustable} {Explainable} {AI}},
	volume = {5},
	url = {https://www.ijcai.org/proceedings/2020/726},
	doi = {10.24963/ijcai.2020/726},
	abstract = {Electronic proceedings of IJCAI 2020},
	language = {en},
	urldate = {2021-01-16},
	booktitle = {Proceedings of the {Twenty}-{Ninth} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	author = {Ignatiev, Alexey},
	month = jul,
	year = {2020},
	note = {ISSN: 1045-0823},
	pages = {5154--5158},
}

@inproceedings{alsallakh_mind_2021,
	title = {Mind the {Pad} -- {CNNs} {Can} {Develop} {Blind} {Spots}},
	url = {https://openreview.net/forum?id=m1CD7tPubNy},
	abstract = {We show how feature maps in convolutional networks are susceptible to spatial bias. Due to a combination of architectural choices, the activation at certain locations is systematically elevated or...},
	language = {en},
	urldate = {2021-06-23},
	author = {Alsallakh, Bilal and Kokhlikyan, Narine and Miglani, Vivek and Yuan, Jun and Reblitz-Richardson, Orion},
	year = {2021},
}

@inproceedings{antoran_getting_2021,
	title = {Getting a {CLUE}: {A} {Method} for {Explaining} {Uncertainty} {Estimates}},
	shorttitle = {Getting a {CLUE}},
	url = {https://openreview.net/forum?id=XSLF1XFq5h},
	abstract = {Both uncertainty estimation and interpretability are important factors for trustworthy machine learning systems. However, there is little work at the intersection of these two areas. We address...},
	language = {en},
	urldate = {2021-06-23},
	author = {Antoran, Javier and Bhatt, Umang and Adel, Tameem and Weller, Adrian and Hernández-Lobato, José Miguel},
	year = {2021},
}

@inproceedings{basu_influence_2021,
	title = {Influence {Functions} in {Deep} {Learning} {Are} {Fragile}},
	url = {https://openreview.net/forum?id=xHKVVHGDOEk},
	abstract = {Influence functions approximate the effect of training samples in test-time predictions and have a wide variety of applications in machine learning interpretability and uncertainty estimation. A...},
	language = {en},
	urldate = {2021-06-23},
	author = {Basu, Samyadeep and Pope, Phil and Feizi, Soheil},
	year = {2021},
}

@inproceedings{frye_shapley_2021,
	title = {Shapley explainability on the data manifold},
	url = {https://openreview.net/forum?id=OPyWRrcjVQw},
	abstract = {Explainability in AI is crucial for model development, compliance with regulation, and providing operational nuance to predictions. The Shapley framework for explainability attributes a model’s...},
	language = {en},
	urldate = {2021-02-23},
	author = {Frye, Christopher and Mijolla, Damien de and Begley, Tom and Cowton, Laurence and Stanley, Megan and Feige, Ilya},
	year = {2021},
}

@inproceedings{islam_shape_2021,
	title = {Shape or {Texture}: {Understanding} {Discriminative} {Features} in {CNNs}},
	shorttitle = {Shape or {Texture}},
	url = {https://openreview.net/forum?id=NcFEZOi-rLa},
	abstract = {Contrasting the previous evidence that neurons in the later layers of a Convolutional Neural Network (CNN) respond to complex object shapes, recent studies have shown that CNNs actually exhibit a...},
	language = {en},
	urldate = {2021-06-23},
	author = {Islam, Md Amirul and Kowal, Matthew and Esser, Patrick and Jia, Sen and Ommer, Björn and Derpanis, Konstantinos G. and Bruce, Neil},
	year = {2021},
}

@inproceedings{leavitt_selectivity_2021,
	title = {Selectivity considered harmful: evaluating the causal impact of class selectivity in {DNNs}},
	shorttitle = {Selectivity considered harmful},
	url = {https://openreview.net/forum?id=8nl0k08uMi},
	abstract = {The properties of individual neurons are often analyzed in order to understand the biological and artificial neural networks in which they're embedded. Class selectivity—typically defined as how...},
	language = {en},
	urldate = {2021-06-23},
	author = {Leavitt, Matthew L. and Morcos, Ari S.},
	year = {2021},
}

@inproceedings{sahoo_scaling_2021,
	title = {Scaling {Symbolic} {Methods} using {Gradients} for {Neural} {Model} {Explanation}},
	url = {https://openreview.net/forum?id=V5j-jdoDDP},
	abstract = {Symbolic techniques based on Satisfiability Modulo Theory (SMT) solvers have been proposed for analyzing and verifying neural network properties, but their usage has been fairly limited owing to...},
	language = {en},
	urldate = {2021-06-23},
	author = {Sahoo, Subham Sekhar and Venugopalan, Subhashini and Li, Li and Singh, Rishabh and Riley, Patrick},
	year = {2021},
}

@inproceedings{sanyal_how_2020,
	title = {How {Benign} is {Benign} {Overfitting} ?},
	url = {https://openreview.net/forum?id=g-wu9TMPODo},
	abstract = {We investigate two causes for adversarial vulnerability in deep neural networks: bad data and (poorly) trained models. When trained with SGD, deep neural networks essentially achieve zero training...},
	language = {en},
	urldate = {2021-06-23},
	author = {Sanyal, Amartya and Dokania, Puneet K. and Kanade, Varun and Torr, Philip},
	month = sep,
	year = {2020},
}

@inproceedings{uddin_saliencymix_2020,
	title = {{SaliencyMix}: {A} {Saliency} {Guided} {Data} {Augmentation} {Strategy} for {Better} {Regularization}},
	shorttitle = {{SaliencyMix}},
	url = {https://openreview.net/forum?id=-M0QkvBGTTq},
	abstract = {Advanced data augmentation strategies have widely been studied to improve the generalization ability of deep learning models. Regional dropout is one of the popular solutions that guides the model...},
	language = {en},
	urldate = {2021-06-23},
	author = {Uddin, A. F. M. Shahab and Monira, Mst Sirazam and Shin, Wheemyung and Chung, TaeChoong and Bae, Sung-Ho},
	month = sep,
	year = {2020},
}

@inproceedings{wang_shapley_2021,
	title = {Shapley {Explanation} {Networks}},
	url = {https://openreview.net/forum?id=vsU0efpivw},
	abstract = {Shapley values have become one of the most popular feature attribution explanation methods. However, most prior work has focused on post-hoc Shapley explanations, which can be computationally...},
	language = {en},
	urldate = {2021-02-23},
	author = {Wang, Rui and Wang, Xiaoqian and Inouye, David I.},
	year = {2021},
}

@inproceedings{srinivas_rethinking_2021,
	title = {Rethinking the {Role} of {Gradient}-based {Attribution} {Methods} for {Model} {Interpretability}},
	url = {https://openreview.net/forum?id=dYeAHXnpWJ4},
	abstract = {Current methods for the interpretability of discriminative deep neural networks commonly rely on the model's input-gradients, i.e., the gradients of the output logits w.r.t. the inputs. The common...},
	language = {en},
	urldate = {2021-06-22},
	author = {Srinivas, Suraj and Fleuret, Francois},
	year = {2021},
}

@article{li_scouter_2020,
	title = {{SCOUTER}: {Slot} {Attention}-based {Classifier} for {Explainable} {Image} {Recognition}},
	shorttitle = {{SCOUTER}},
	url = {https://arxiv.org/abs/2009.06138v3},
	abstract = {Explainable artificial intelligence has been gaining attention in the past few years. However, most existing methods are based on gradients or intermediate features, which are not directly involved in the decision-making process of the classifier. In this paper, we propose a slot attention-based classifier called SCOUTER for transparent yet accurate classification. Two major differences from other attention-based methods include: (a) SCOUTER's explanation is involved in the final confidence for each category, offering more intuitive interpretation, and (b) all the categories have their corresponding positive or negative explanation, which tells "why the image is of a certain category" or "why the image is not of a certain category." We design a new loss tailored for SCOUTER that controls the model's behavior to switch between positive and negative explanations, as well as the size of explanatory regions. Experimental results show that SCOUTER can give better visual explanations while keeping good accuracy on small and medium-sized datasets.},
	language = {en},
	urldate = {2021-06-22},
	author = {Li, Liangzhi and Wang, Bowen and Verma, Manisha and Nakashima, Yuta and Kawasaki, Ryo and Nagahara, Hajime},
	month = sep,
	year = {2020},
}

@inproceedings{raghu_svcca_2017,
	address = {Red Hook, NY, USA},
	series = {{NIPS}'17},
	title = {{SVCCA}: singular vector canonical correlation analysis for deep learning dynamics and interpretability},
	isbn = {978-1-5108-6096-4},
	shorttitle = {{SVCCA}},
	abstract = {We propose a new technique, Singular Vector Canonical Correlation Analysis (SVCCA), a tool for quickly comparing two representations in a way that is both invariant to affine transform (allowing comparison between different layers and networks) and fast to compute (allowing more comparisons to be calculated than with previous methods). We deploy this tool to measure the intrinsic dimensionality of layers, showing in some cases needless over-parameterization; to probe learning dynamics throughout training, finding that networks converge to final representations from the bottom up; to show where class-specific information in networks is formed; and to suggest new training regimes that simultaneously save computation and overfit less.},
	urldate = {2021-06-22},
	booktitle = {Proceedings of the 31st {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates Inc.},
	author = {Raghu, Maithra and Gilmer, Justin and Yosinski, Jason and Sohl-Dickstein, Jascha},
	month = dec,
	year = {2017},
	pages = {6078--6087},
}

@inproceedings{rombach_making_2020,
	title = {Making {Sense} of {CNNs}: {Interpreting} {Deep} {Representations} \& {Their} {Invariances} with {INNs}},
	abstract = {To tackle increasingly complex tasks, it has become an essential ability of neural networks to learn abstract representations. These task-speciﬁc representations and, particularly, the invariances they capture turn neural networks into black box models that lack interpretability. To open such a black box, it is, therefore, crucial to uncover the diﬀerent semantic concepts a model has learned as well as those that it has learned to be invariant to. We present an approach based on INNs that (i) recovers the task-speciﬁc, learned invariances by disentangling the remaining factor of variation in the data and that (ii) invertibly transforms these recovered invariances combined with the model representation into an equally expressive one with accessible semantic concepts. As a consequence, neural network representations become understandable by providing the means to (i) expose their semantic meaning, (ii) semantically modify a representation, and (iii) visualize individual learned semantic concepts and invariances. Our invertible approach signiﬁcantly extends the abilities to understand black box models by enabling posthoc interpretations of state-of-the-art networks without compromising their performance.},
	language = {en},
	booktitle = {European {Conference} on {Computer} {Vision}},
	author = {Rombach, Robin and Esser, Patrick and Ommer, Bjorn},
	year = {2020},
	pages = {18},
}

@inproceedings{srinivas_full-gradient_2019,
	title = {Full-{Gradient} {Representation} for {Neural} {Network} {Visualization}},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper/2019/hash/80537a945c7aaa788ccfcdf1b99b5d8f-Abstract.html},
	language = {en},
	urldate = {2021-06-22},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Srinivas, Suraj and Fleuret, François},
	year = {2019},
}

@inproceedings{zhang_interpreting_2018,
	title = {Interpreting {CNN} {Knowledge} via an {Explanatory} {Graph}},
	volume = {32},
	copyright = {Copyright (c)},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/11819},
	language = {en},
	urldate = {2021-06-22},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Zhang, Quanshi and Cao, Ruiming and Shi, Feng and Wu, Ying Nian and Zhu, Song-Chun},
	month = apr,
	year = {2018},
	note = {Number: 1},
	keywords = {Interpretable Model},
}

@inproceedings{hase_evaluating_2020,
	address = {Online},
	title = {Evaluating {Explainable} {AI}: {Which} {Algorithmic} {Explanations} {Help} {Users} {Predict} {Model} {Behavior}?},
	shorttitle = {Evaluating {Explainable} {AI}},
	url = {https://www.aclweb.org/anthology/2020.acl-main.491},
	doi = {10.18653/v1/2020.acl-main.491},
	abstract = {Algorithmic approaches to interpreting machine learning models have proliferated in recent years. We carry out human subject tests that are the first of their kind to isolate the effect of algorithmic explanations on a key aspect of model interpretability, simulatability, while avoiding important confounding experimental factors. A model is simulatable when a person can predict its behavior on new inputs. Through two kinds of simulation tests involving text and tabular data, we evaluate five explanations methods: (1) LIME, (2) Anchor, (3) Decision Boundary, (4) a Prototype model, and (5) a Composite approach that combines explanations from each method. Clear evidence of method effectiveness is found in very few cases: LIME improves simulatability in tabular classification, and our Prototype method is effective in counterfactual simulation tests. We also collect subjective ratings of explanations, but we do not find that ratings are predictive of how helpful explanations are. Our results provide the first reliable and comprehensive estimates of how explanations influence simulatability across a variety of explanation methods and data domains. We show that (1) we need to be careful about the metrics we use to evaluate explanation methods, and (2) there is significant room for improvement in current methods.},
	urldate = {2021-06-22},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Hase, Peter and Bansal, Mohit},
	month = jul,
	year = {2020},
	pages = {5540--5552},
}

@incollection{zhou_comparing_2019,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Comparing the {Interpretability} of {Deep} {Networks} via {Network} {Dissection}},
	isbn = {978-3-030-28954-6},
	url = {https://doi.org/10.1007/978-3-030-28954-6_12},
	abstract = {In this chapter, we introduce Network Dissection (The complete paper and code are available at http://netdissect.csail.mit.edu), a general framework to quantify the interpretability of the units inside a deep convolutional neural networks (CNNs). We compare the different vocabularies of interpretable units as concept detectors emerged from the networks trained to solve different supervised learning tasks such as object recognition on ImageNet and scene classification on Places. The network dissection is further applied to analyze how the units acting as semantic detectors grow and evolve over the training iterations both in the scenario of the train-from-scratch and in the stage of the fine-tuning between data sources. Our results highlight that interpretability is an important property of deep neural networks that provides new insights into their hierarchical structure.},
	language = {en},
	urldate = {2021-06-22},
	booktitle = {Explainable {AI}: {Interpreting}, {Explaining} and {Visualizing} {Deep} {Learning}},
	publisher = {Springer International Publishing},
	author = {Zhou, Bolei and Bau, David and Oliva, Aude and Torralba, Antonio},
	editor = {Samek, Wojciech and Montavon, Grégoire and Vedaldi, Andrea and Hansen, Lars Kai and Müller, Klaus-Robert},
	year = {2019},
	doi = {10.1007/978-3-030-28954-6_12},
	keywords = {Deep neural networks, Interpretable machine learning, Model visualization},
	pages = {243--252},
}

@incollection{hu_unsupervised_2019,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Unsupervised {Discrete} {Representation} {Learning}},
	isbn = {978-3-030-28954-6},
	url = {https://doi.org/10.1007/978-3-030-28954-6_6},
	abstract = {Learning discrete representations of data is a central machine learning task because of the compactness of the representations and ease of interpretation. The task includes clustering and hash learning as special cases. Deep neural networks are promising to be used because they can model the non-linearity of data and scale to large datasets. However, their model complexity is huge, and therefore, we need to carefully regularize the networks in order to learn useful and interpretable representations that exhibit intended invariance for applications of interest. To this end, we propose a method called Information Maximizing Self-Augmented Training (IMSAT). In IMSAT, we use data augmentation to impose the invariance on discrete representations. More specifically, we encourage the predicted representations of augmented data points to be close to those of the original data points in an end-to-end fashion. At the same time, we maximize the information-theoretic dependency between data and their predicted discrete representations. Our IMSAT is able to discover interpretable representations that exhibit intended invariance. Extensive experiments on benchmark datasets show that IMSAT produces state-of-the-art results for both clustering and unsupervised hash learning.},
	language = {en},
	urldate = {2021-06-22},
	booktitle = {Explainable {AI}: {Interpreting}, {Explaining} and {Visualizing} {Deep} {Learning}},
	publisher = {Springer International Publishing},
	author = {Hu, Weihua and Miyato, Takeru and Tokui, Seiya and Matsumoto, Eiichi and Sugiyama, Masashi},
	editor = {Samek, Wojciech and Montavon, Grégoire and Vedaldi, Andrea and Hansen, Lars Kai and Müller, Klaus-Robert},
	year = {2019},
	doi = {10.1007/978-3-030-28954-6_6},
	keywords = {Clustering, Discrete representation learning, Hash learning},
	pages = {97--119},
}

@incollection{montavon_gradient-based_2019,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Gradient-{Based} {Vs}. {Propagation}-{Based} {Explanations}: {An} {Axiomatic} {Comparison}},
	isbn = {978-3-030-28954-6},
	shorttitle = {Gradient-{Based} {Vs}. {Propagation}-{Based} {Explanations}},
	url = {https://doi.org/10.1007/978-3-030-28954-6_13},
	abstract = {Deep neural networks, once considered to be inscrutable black-boxes, are now supplemented with techniques that can explain how these models decide. This raises the question whether the produced explanations are reliable. In this chapter, we consider two popular explanation techniques, one based on gradient computation and one based on a propagation mechanism. We evaluate them using three “axiomatic” properties: conservation, continuity, and implementation invariance. These properties are tested on the overall explanation, but also at intermediate layers, where our analysis brings further insights on how the explanation is being formed.},
	language = {en},
	urldate = {2021-06-22},
	booktitle = {Explainable {AI}: {Interpreting}, {Explaining} and {Visualizing} {Deep} {Learning}},
	publisher = {Springer International Publishing},
	author = {Montavon, Grégoire},
	editor = {Samek, Wojciech and Montavon, Grégoire and Vedaldi, Andrea and Hansen, Lars Kai and Müller, Klaus-Robert},
	year = {2019},
	doi = {10.1007/978-3-030-28954-6_13},
	keywords = {Axioms, Deep neural networks, Explanations},
	pages = {253--265},
}

@incollection{oh_towards_2019,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Towards {Reverse}-{Engineering} {Black}-{Box} {Neural} {Networks}},
	isbn = {978-3-030-28954-6},
	url = {https://doi.org/10.1007/978-3-030-28954-6_7},
	abstract = {Much progress in interpretable AI is built around scenarios where the user, one who interprets the model, has a full ownership of the model to be diagnosed. The user either owns the training data and computing resources to train an interpretable model herself or owns a full access to an already trained model to be interpreted post-hoc. In this chapter, we consider a less investigated scenario of diagnosing black-box neural networks, where the user can only send queries and read off outputs. Black-box access is a common deployment mode for many public and commercial models, since internal details, such as architecture, optimisation procedure, and training data, can be proprietary and aggravate their vulnerability to attacks like adversarial examples. We propose a method for exposing internals of black-box models and show that the method is surprisingly effective at inferring a diverse set of internal information. We further show how the exposed internals can be exploited to strengthen adversarial examples against the model. Our work starts an important discussion on the security implications of diagnosing deployed models with limited accessibility. The code is available at goo.gl/MbYfsv.},
	language = {en},
	urldate = {2021-06-22},
	booktitle = {Explainable {AI}: {Interpreting}, {Explaining} and {Visualizing} {Deep} {Learning}},
	publisher = {Springer International Publishing},
	author = {Oh, Seong Joon and Schiele, Bernt and Fritz, Mario},
	editor = {Samek, Wojciech and Montavon, Grégoire and Vedaldi, Andrea and Hansen, Lars Kai and Müller, Klaus-Robert},
	year = {2019},
	doi = {10.1007/978-3-030-28954-6_7},
	keywords = {Black box, Explainability, Machine Learning, Security},
	pages = {121--144},
}

@incollection{hong_interpretable_2019,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Interpretable {Text}-to-{Image} {Synthesis} with {Hierarchical} {Semantic} {Layout} {Generation}},
	isbn = {978-3-030-28954-6},
	url = {https://doi.org/10.1007/978-3-030-28954-6_5},
	abstract = {Generating images from natural language description has drawn a lot of attention in the research community for its practical usefulness and for understanding the method in which the model relates text with visual concepts by synthesizing them. Deep generative models have been successfully employed to address this task, which formulates the problem as a translation task from text to image. However, learning a direct mapping from text to image is challenging due to the complexity of the mapping and makes it difficult to understand the underlying generation process. To address these issues, we propose a novel hierarchical approach for text-to-image synthesis by inferring a semantic layout. Our algorithm decomposes the generation process into multiple steps. First, it constructs a semantic layout from the text using the layout generator and then converts the layout to an image with the image generator. The proposed layout generator progressively constructs a semantic layout in a coarse-to-fine manner by generating object bounding boxes and refining each box by estimating the object shapes inside the box. The image generator synthesizes an image conditioned on the inferred semantic layout, which provides a useful semantic structure of an image matching the text description. Conditioning the generation with the inferred semantic layout allows our model to generate semantically more meaningful images and provides interpretable representations to allow users to interactively control the generation process by modifying the layout. We demonstrate the capability of the proposed model on the challenging MS-COCO dataset and show that the model can substantially improve the image quality and interpretability of the output and semantic alignment to input text over existing approaches.},
	language = {en},
	urldate = {2021-06-22},
	booktitle = {Explainable {AI}: {Interpreting}, {Explaining} and {Visualizing} {Deep} {Learning}},
	publisher = {Springer International Publishing},
	author = {Hong, Seunghoon and Yang, Dingdong and Choi, Jongwook and Lee, Honglak},
	editor = {Samek, Wojciech and Montavon, Grégoire and Vedaldi, Andrea and Hansen, Lars Kai and Müller, Klaus-Robert},
	year = {2019},
	doi = {10.1007/978-3-030-28954-6_5},
	pages = {77--95},
}

@incollection{fong_explanations_2019,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Explanations for {Attributing} {Deep} {Neural} {Network} {Predictions}},
	isbn = {978-3-030-28954-6},
	url = {https://doi.org/10.1007/978-3-030-28954-6_8},
	abstract = {Given the recent success of deep neural networks and their applications to more high impact and high risk applications, like autonomous driving and healthcare decision-making, there is a great need for faithful and interpretable explanations of “why” an algorithm is making a certain prediction. In this chapter, we introduce 1. Meta-Predictors as Explanations, a principled framework for learning explanations for any black box algorithm, and 2. Meaningful Perturbations, an instantiation of our paradigm applied to the problem of attribution, which is concerned with attributing what features of an input (i.e., regions of an input image) are responsible for a model’s output (i.e., a CNN classifier’s object class prediction). We first introduced these contributions in [8]. We also briefly survey existing visual attribution methods and highlight how they faith to be both faithful and interpretable.},
	language = {en},
	urldate = {2021-06-22},
	booktitle = {Explainable {AI}: {Interpreting}, {Explaining} and {Visualizing} {Deep} {Learning}},
	publisher = {Springer International Publishing},
	author = {Fong, Ruth and Vedaldi, Andrea},
	editor = {Samek, Wojciech and Montavon, Grégoire and Vedaldi, Andrea and Hansen, Lars Kai and Müller, Klaus-Robert},
	year = {2019},
	doi = {10.1007/978-3-030-28954-6_8},
	keywords = {Computer vision, Explainable artificial intelligence, Machine learning},
	pages = {149--167},
}

@incollection{kindermans_reliability_2019,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {The ({Un})reliability of {Saliency} {Methods}},
	isbn = {978-3-030-28954-6},
	url = {https://doi.org/10.1007/978-3-030-28954-6_14},
	abstract = {Saliency methods aim to explain the predictions of deep neural networks. These methods lack reliability when the explanation is sensitive to factors that do not contribute to the model prediction. We use a simple and common pre-processing step which can be compensated for easily—adding a constant shift to the input data—to show that a transformation with no effect on how the model makes the decision can cause numerous methods to attribute incorrectly. In order to guarantee reliability, we believe that the explanation should not change when we can guarantee that two networks process the images in identical manners. We show, through several examples, that saliency methods that do not satisfy this requirement result in misleading attribution. The approach can be seen as a type of unit test; we construct a narrow ground truth to measure one stated desirable property. As such, we hope the community will embrace the development of additional tests.},
	language = {en},
	urldate = {2021-06-22},
	booktitle = {Explainable {AI}: {Interpreting}, {Explaining} and {Visualizing} {Deep} {Learning}},
	publisher = {Springer International Publishing},
	author = {Kindermans, Pieter-Jan and Hooker, Sara and Adebayo, Julius and Alber, Maximilian and Schütt, Kristof T. and Dähne, Sven and Erhan, Dumitru and Kim, Been},
	editor = {Samek, Wojciech and Montavon, Grégoire and Vedaldi, Andrea and Hansen, Lars Kai and Müller, Klaus-Robert},
	year = {2019},
	doi = {10.1007/978-3-030-28954-6_14},
	pages = {267--280},
}

@incollection{montavon_layer-wise_2019,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Layer-{Wise} {Relevance} {Propagation}: {An} {Overview}},
	isbn = {978-3-030-28954-6},
	shorttitle = {Layer-{Wise} {Relevance} {Propagation}},
	url = {https://doi.org/10.1007/978-3-030-28954-6_10},
	abstract = {For a machine learning model to generalize well, one needs to ensure that its decisions are supported by meaningful patterns in the input data. A prerequisite is however for the model to be able to explain itself, e.g. by highlighting which input features it uses to support its prediction. Layer-wise Relevance Propagation (LRP) is a technique that brings such explainability and scales to potentially highly complex deep neural networks. It operates by propagating the prediction backward in the neural network, using a set of purposely designed propagation rules. In this chapter, we give a concise introduction to LRP with a discussion of (1) how to implement propagation rules easily and efficiently, (2) how the propagation procedure can be theoretically justified as a ‘deep Taylor decomposition’, (3) how to choose the propagation rules at each layer to deliver high explanation quality, and (4) how LRP can be extended to handle a variety of machine learning scenarios beyond deep neural networks.},
	language = {en},
	urldate = {2021-06-22},
	booktitle = {Explainable {AI}: {Interpreting}, {Explaining} and {Visualizing} {Deep} {Learning}},
	publisher = {Springer International Publishing},
	author = {Montavon, Grégoire and Binder, Alexander and Lapuschkin, Sebastian and Samek, Wojciech and Müller, Klaus-Robert},
	editor = {Samek, Wojciech and Montavon, Grégoire and Vedaldi, Andrea and Hansen, Lars Kai and Müller, Klaus-Robert},
	year = {2019},
	doi = {10.1007/978-3-030-28954-6_10},
	keywords = {Deep Neural Networks, Deep Taylor Decomposition, Explanations, Layer-wise Relevance Propagation},
	pages = {193--209},
}

@incollection{weller_transparency_2019,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Transparency: {Motivations} and {Challenges}},
	isbn = {978-3-030-28954-6},
	shorttitle = {Transparency},
	url = {https://doi.org/10.1007/978-3-030-28954-6_2},
	abstract = {Transparency is often deemed critical to enable effective real-world deployment of intelligent systems. Yet the motivations for and benefits of different types of transparency can vary significantly depending on context, and objective measurement criteria are difficult to identify. We provide a brief survey, suggesting challenges and related concerns, particularly when agents have misaligned interests. We highlight and review settings where transparency may cause harm, discussing connections across privacy, multi-agent game theory, economics, fairness and trust.},
	language = {en},
	urldate = {2021-06-22},
	booktitle = {Explainable {AI}: {Interpreting}, {Explaining} and {Visualizing} {Deep} {Learning}},
	publisher = {Springer International Publishing},
	author = {Weller, Adrian},
	editor = {Samek, Wojciech and Montavon, Grégoire and Vedaldi, Andrea and Hansen, Lars Kai and Müller, Klaus-Robert},
	year = {2019},
	doi = {10.1007/978-3-030-28954-6_2},
	keywords = {Explainable, Interpretability, Social good, Transparency},
	pages = {23--40},
}

@incollection{samek_towards_2019,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Towards {Explainable} {Artificial} {Intelligence}},
	isbn = {978-3-030-28954-6},
	url = {https://doi.org/10.1007/978-3-030-28954-6_1},
	abstract = {In recent years, machine learning (ML) has become a key enabling technology for the sciences and industry. Especially through improvements in methodology, the availability of large databases and increased computational power, today’s ML algorithms are able to achieve excellent performance (at times even exceeding the human level) on an increasing number of complex tasks. Deep learning models are at the forefront of this development. However, due to their nested non-linear structure, these powerful models have been generally considered “black boxes”, not providing any information about what exactly makes them arrive at their predictions. Since in many applications, e.g., in the medical domain, such lack of transparency may be not acceptable, the development of methods for visualizing, explaining and interpreting deep learning models has recently attracted increasing attention. This introductory paper presents recent developments and applications in this field and makes a plea for a wider use of explainable learning algorithms in practice.},
	language = {en},
	urldate = {2021-06-22},
	booktitle = {Explainable {AI}: {Interpreting}, {Explaining} and {Visualizing} {Deep} {Learning}},
	publisher = {Springer International Publishing},
	author = {Samek, Wojciech and Müller, Klaus-Robert},
	editor = {Samek, Wojciech and Montavon, Grégoire and Vedaldi, Andrea and Hansen, Lars Kai and Müller, Klaus-Robert},
	year = {2019},
	doi = {10.1007/978-3-030-28954-6_1},
	keywords = {Deep learning, Explainable artificial intelligence, Interpretability, Model transparency, Neural networks},
	pages = {5--22},
}

@incollection{arras_explaining_2019,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Explaining and {Interpreting} {LSTMs}},
	isbn = {978-3-030-28954-6},
	url = {https://doi.org/10.1007/978-3-030-28954-6_11},
	abstract = {While neural networks have acted as a strong unifying force in the design of modern AI systems, the neural network architectures themselves remain highly heterogeneous due to the variety of tasks to be solved. In this chapter, we explore how to adapt the Layer-wise Relevance Propagation (LRP) technique used for explaining the predictions of feed-forward networks to the LSTM architecture used for sequential data modeling and forecasting. The special accumulators and gated interactions present in the LSTM require both a new propagation scheme and an extension of the underlying theoretical framework to deliver faithful explanations.},
	language = {en},
	urldate = {2021-06-22},
	booktitle = {Explainable {AI}: {Interpreting}, {Explaining} and {Visualizing} {Deep} {Learning}},
	publisher = {Springer International Publishing},
	author = {Arras, Leila and Arjona-Medina, José and Widrich, Michael and Montavon, Grégoire and Gillhofer, Michael and Müller, Klaus-Robert and Hochreiter, Sepp and Samek, Wojciech},
	editor = {Samek, Wojciech and Montavon, Grégoire and Vedaldi, Andrea and Hansen, Lars Kai and Müller, Klaus-Robert},
	year = {2019},
	doi = {10.1007/978-3-030-28954-6_11},
	keywords = {Explainable artificial intelligence, Interpretability, LSTM, Model transparency, Recurrent neural networks},
	pages = {211--238},
}

@incollection{ancona_gradient-based_2019,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Gradient-{Based} {Attribution} {Methods}},
	isbn = {978-3-030-28954-6},
	url = {https://doi.org/10.1007/978-3-030-28954-6_9},
	abstract = {The problem of explaining complex machine learning models, including Deep Neural Networks, has gained increasing attention over the last few years. While several methods have been proposed to explain network predictions, the definition itself of explanation is still debated. Moreover, only a few attempts to compare explanation methods from a theoretical perspective has been done. In this chapter, we discuss the theoretical properties of several attribution methods and show how they share the same idea of using the gradient information as a descriptive factor for the functioning of a model. Finally, we discuss the strengths and limitations of these methods and compare them with available alternatives.},
	language = {en},
	urldate = {2021-06-22},
	booktitle = {Explainable {AI}: {Interpreting}, {Explaining} and {Visualizing} {Deep} {Learning}},
	publisher = {Springer International Publishing},
	author = {Ancona, Marco and Ceolini, Enea and Öztireli, Cengiz and Gross, Markus},
	editor = {Samek, Wojciech and Montavon, Grégoire and Vedaldi, Andrea and Hansen, Lars Kai and Müller, Klaus-Robert},
	year = {2019},
	doi = {10.1007/978-3-030-28954-6_9},
	keywords = {Attribution methods, Deep Neural Networks, Explainable artificial intelligence},
	pages = {169--191},
}

@incollection{nguyen_understanding_2019,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Understanding {Neural} {Networks} via {Feature} {Visualization}: {A} {Survey}},
	isbn = {978-3-030-28954-6},
	shorttitle = {Understanding {Neural} {Networks} via {Feature} {Visualization}},
	url = {https://doi.org/10.1007/978-3-030-28954-6_4},
	abstract = {A neuroscience method to understanding the brain is to find and study the preferred stimuli that highly activate an individual cell or groups of cells. Recent advances in machine learning enable a family of methods to synthesize preferred stimuli that cause a neuron in an artificial or biological brain to fire strongly. Those methods are known as Activation Maximization (AM) [10] or Feature Visualization via Optimization. In this chapter, we (1) review existing AM techniques in the literature; (2) discuss a probabilistic interpretation for AM; and (3) review the applications of AM in debugging and explaining networks.},
	language = {en},
	urldate = {2021-06-22},
	booktitle = {Explainable {AI}: {Interpreting}, {Explaining} and {Visualizing} {Deep} {Learning}},
	publisher = {Springer International Publishing},
	author = {Nguyen, Anh and Yosinski, Jason and Clune, Jeff},
	editor = {Samek, Wojciech and Montavon, Grégoire and Vedaldi, Andrea and Hansen, Lars Kai and Müller, Klaus-Robert},
	year = {2019},
	doi = {10.1007/978-3-030-28954-6_4},
	keywords = {Activation Maximization, Feature visualization, Generative models, Generator network, Neural networks, Optimization},
	pages = {55--76},
}

@incollection{hansen_interpretability_2019,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Interpretability in {Intelligent} {Systems} – {A} {New} {Concept}?},
	isbn = {978-3-030-28954-6},
	url = {https://doi.org/10.1007/978-3-030-28954-6_3},
	abstract = {The very active community for interpretable machine learning can learn from the rich 50+ year history of explainable AI. We here give two specific examples from this legacy that could enrich current interpretability work: First, Explanation desiderata were we point to the rich set of ideas developed in the ‘explainable expert systems’ field and, second, tools for quantification of uncertainty of high-dimensional feature importance maps which have been developed in the field of computational neuroimaging.},
	language = {en},
	urldate = {2021-06-22},
	booktitle = {Explainable {AI}: {Interpreting}, {Explaining} and {Visualizing} {Deep} {Learning}},
	publisher = {Springer International Publishing},
	author = {Hansen, Lars Kai and Rieger, Laura},
	editor = {Samek, Wojciech and Montavon, Grégoire and Vedaldi, Andrea and Hansen, Lars Kai and Müller, Klaus-Robert},
	year = {2019},
	doi = {10.1007/978-3-030-28954-6_3},
	keywords = {Interpretable AI, Machine learning, Uncertainty quantification},
	pages = {41--49},
}

@inproceedings{mothilal_explaining_2020,
	address = {New York, NY, USA},
	series = {{FAT}* '20},
	title = {Explaining machine learning classifiers through diverse counterfactual explanations},
	isbn = {978-1-4503-6936-7},
	url = {https://doi.org/10.1145/3351095.3372850},
	doi = {10.1145/3351095.3372850},
	abstract = {Post-hoc explanations of machine learning models are crucial for people to understand and act on algorithmic predictions. An intriguing class of explanations is through counterfactuals, hypothetical examples that show people how to obtain a different prediction. We posit that effective counterfactual explanations should satisfy two properties: feasibility of the counterfactual actions given user context and constraints, and diversity among the counterfactuals presented. To this end, we propose a framework for generating and evaluating a diverse set of counterfactual explanations based on determinantal point processes. To evaluate the actionability of counterfactuals, we provide metrics that enable comparison of counterfactual-based methods to other local explanation methods. We further address necessary tradeoffs and point to causal implications in optimizing for counterfactuals. Our experiments on four real-world datasets show that our framework can generate a set of counterfactuals that are diverse and well approximate local decision boundaries, outperforming prior approaches to generating diverse counterfactuals. We provide an implementation of the framework at https://github.com/microsoft/DiCE.},
	urldate = {2021-06-22},
	booktitle = {Proceedings of the 2020 {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {Association for Computing Machinery},
	author = {Mothilal, Ramaravind K. and Sharma, Amit and Tan, Chenhao},
	month = jan,
	year = {2020},
	pages = {607--617},
}

@inproceedings{borowski_exemplary_2021,
	title = {Exemplary {Natural} {Images} {Explain} {CNN} {Activations} {Better} than {State}-of-the-{Art} {Feature} {Visualization}},
	url = {https://openreview.net/forum?id=QO9-y8also-},
	abstract = {Feature visualizations such as synthetic maximally activating images are a widely used explanation method to better understand the information processing of convolutional neural networks (CNNs). At...},
	language = {en},
	urldate = {2021-06-22},
	author = {Borowski, Judy and Zimmermann, Roland Simon and Schepers, Judith and Geirhos, Robert and Wallis, Thomas S. A. and Bethge, Matthias and Brendel, Wieland},
	year = {2021},
}

@inproceedings{hsieh_evaluations_2021,
	title = {Evaluations and {Methods} for {Explanation} through {Robustness} {Analysis}},
	url = {https://openreview.net/forum?id=4dXmpCDGNp7},
	abstract = {Feature based explanations, that provide importance of each feature towards the model prediction, is arguably one of the most intuitive ways to explain a model. In this paper, we establish a novel...},
	language = {en},
	urldate = {2021-02-24},
	author = {Hsieh, Cheng-Yu and Yeh, Chih-Kuan and Liu, Xuanqing and Ravikumar, Pradeep Kumar and Kim, Seungyeon and Kumar, Sanjiv and Hsieh, Cho-Jui},
	year = {2021},
}

@inproceedings{yu_interpreting_2019,
	title = {Interpreting and {Evaluating} {Neural} {Network} {Robustness}},
	url = {https://www.ijcai.org/proceedings/2019/583},
	abstract = {Electronic proceedings of IJCAI 2019},
	urldate = {2021-06-22},
	booktitle = {Proceedings of the {Twenty}-{Eighth} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	author = {Yu, Fuxun and Qin, Zhuwei and Liu, Chenchen and Zhao, Liang and Wang, Yanzhi and Chen, Xiang},
	year = {2019},
	pages = {4199--4205},
}

@inproceedings{zhang_interpretable_2020-1,
	title = {Interpretable {Deep} {Learning} under {Fire}},
	isbn = {978-1-939133-17-5},
	url = {https://www.usenix.org/conference/usenixsecurity20/presentation/zhang-xinyang},
	language = {en},
	urldate = {2021-06-22},
	author = {Zhang, Xinyang and Wang, Ningfei and Shen, Hua and Ji, Shouling and Luo, Xiapu and Wang, Ting},
	year = {2020},
	pages = {1659--1676},
}

@inproceedings{hanawa_evaluation_2021,
	title = {Evaluation of {Similarity}-based {Explanations}},
	url = {https://openreview.net/forum?id=9uvhpyQwzM_},
	abstract = {Explaining the predictions made by complex machine learning models helps users to understand and accept the predicted outputs with confidence. One promising way is to use similarity-based...},
	language = {en},
	urldate = {2021-06-22},
	author = {Hanawa, Kazuaki and Yokoi, Sho and Hara, Satoshi and Inui, Kentaro},
	year = {2021},
}

@inproceedings{mummadi_does_2020,
	title = {Does enhanced shape bias improve neural network robustness to common corruptions?},
	url = {https://openreview.net/forum?id=yUxUNaj2Sl},
	abstract = {Convolutional neural networks (CNNs) learn to extract representations of complex features, such as object shapes and textures to solve image recognition tasks. Recent work indicates that CNNs...},
	language = {en},
	urldate = {2021-02-24},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Mummadi, Chaithanya Kumar and Subramaniam, Ranjitha and Hutmacher, Robin and Vitay, Julien and Fischer, Volker and Metzen, Jan Hendrik},
	month = sep,
	year = {2020},
}

@inproceedings{tsirtsis_decisions_2020,
	title = {Decisions, {Counterfactual} {Explanations} and {Strategic} {Behavior}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/c2ba1bc54b239208cb37b901c0d3b363-Abstract.html},
	language = {en},
	urldate = {2020-12-26},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Tsirtsis, Stratis and Gomez Rodriguez, Manuel},
	year = {2020},
}

@inproceedings{adebayo_debugging_2020,
	title = {Debugging {Tests} for {Model} {Explanations}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/075b051ec3d22dac7b33f788da631fd4-Abstract.html},
	language = {en},
	urldate = {2020-12-26},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Adebayo, Julius and Muelly, Michael and Liccardi, Ilaria and Kim, Been},
	year = {2020},
}

@inproceedings{schwab_cxplain_2019,
	title = {{CXPlain}: {Causal} {Explanations} for {Model} {Interpretation} under {Uncertainty}},
	abstract = {Feature importance estimates that inform users about the degree to which given inputs inﬂuence the output of a predictive model are crucial for understanding, validating, and interpreting machine-learning models. However, providing fast and accurate estimates of feature importance for high-dimensional data, and quantifying the uncertainty of such estimates remain open challenges. Here, we frame the task of providing explanations for the decisions of machine-learning models as a causal learning task, and train causal explanation (CXPlain) models that learn to estimate to what degree certain inputs cause outputs in another machine-learning model. CXPlain can, once trained, be used to explain the target model in little time, and enables the quantiﬁcation of the uncertainty associated with its feature importance estimates via bootstrap ensembling. We present experiments that demonstrate that CXPlain is signiﬁcantly more accurate and faster than existing model-agnostic methods for estimating feature importance. In addition, we conﬁrm that the uncertainty estimates provided by CXPlain ensembles are strongly correlated with their ability to accurately estimate feature importance on held-out data.},
	language = {en},
	booktitle = {33rd {Conference} on {Neural} {Information} {Processing} {Systems}},
	author = {Schwab, Patrick and Karlen, Walter},
	year = {2019},
	pages = {11},
}

@inproceedings{mu_compositional_2020,
	title = {Compositional {Explanations} of {Neurons}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/c74956ffb38ba48ed6ce977af6727275-Abstract.html},
	language = {en},
	urldate = {2020-12-26},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Mu, Jesse and Andreas, Jacob},
	year = {2020},
}

@inproceedings{higgins_beta-vae_2017,
	title = {beta-{VAE}: {Learning} {Basic} {Visual} {Concepts} with a {Constrained} {Variational} {Framework}},
	shorttitle = {beta-{VAE}},
	url = {https://openreview.net/forum?id=Sy2fzU9gl},
	abstract = {We introduce beta-VAE, a new state-of-the-art framework for automated discovery of interpretable factorised latent representations from raw image data in a completely unsupervised manner.},
	language = {en},
	urldate = {2021-06-09},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Higgins, Irina and Matthey, Loic and Pal, Arka and Burgess, Christopher and Glorot, Xavier and Botvinick, Matthew and Mohamed, Shakir and Lerchner, Alexander},
	year = {2017},
}

@inproceedings{heskes_causal_2020,
	title = {Causal {Shapley} {Values}: {Exploiting} {Causal} {Knowledge} to {Explain} {Individual} {Predictions} of {Complex} {Models}},
	volume = {33},
	shorttitle = {Causal {Shapley} {Values}},
	url = {https://proceedings.neurips.cc/paper/2020/hash/32e54441e6382a7fbacbbbaf3c450059-Abstract.html},
	language = {en},
	urldate = {2020-12-26},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Heskes, Tom and Sijben, Evi and Bucur, Ioan Gabriel and Claassen, Tom},
	year = {2020},
}

@inproceedings{choksi_brain-inspired_2020,
	title = {Brain-inspired predictive coding dynamics improve the robustness of deep neural networks},
	abstract = {Deep neural networks excel at image classiﬁcation, but their performance is far less robust to input perturbations than human perception. In this work we address this shortcoming by incorporating brain-inspired recurrent dynamics in deep convolutional networks. We augment a pretrained feedforward classiﬁcation model (VGG16 trained on ImageNet) with a “predictive coding” strategy: a framework popular in neuroscience for characterizing cortical function. At each layer of the hierarchical model, generative feedback “predicts” (i.e., reconstructs) the pattern of activity in the previous layer. The reconstruction errors are used to iteratively update the network’s representations across timesteps, and to optimize the network’s feedback weights over the natural image dataset–a form of unsupervised training. We demonstrate that this results in a network with improved robustness compared to the corresponding feedforward baseline, not only against various types of noise but also against a suite of adversarial attacks. We propose that most feedforward models could be equipped with these brain-inspired feedback dynamics, thus improving their robustness to input perturbations.},
	language = {en},
	booktitle = {2nd {Workshop} on {Shared} {Visual} {Representations} in {Human} and {Machine} {Intelligence} ({SVRHM}), {NeurIPS}},
	author = {Choksi, Bhavin and Mozafari, Milad and O’May, Callum Biggs and Ador, Benjamin and Alamia, Andrea and VanRullen, Ruﬁn},
	year = {2020},
	pages = {13},
}

@inproceedings{nie_bongard-logo_2020,
	title = {Bongard-{LOGO}: {A} {New} {Benchmark} for {Human}-{Level} {Concept} {Learning} and {Reasoning}},
	volume = {33},
	shorttitle = {Bongard-{LOGO}},
	url = {https://proceedings.neurips.cc/paper/2020/hash/bf15e9bbff22c7719020f9df4badc20a-Abstract.html},
	language = {en},
	urldate = {2020-12-26},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Nie, Weili and Yu, Zhiding and Mao, Lei and Patel, Ankit B. and Zhu, Yuke and Anandkumar, Anima},
	year = {2020},
}

@article{adler_auditing_2018,
	title = {Auditing black-box models for indirect influence},
	volume = {54},
	issn = {0219-3116},
	url = {https://doi.org/10.1007/s10115-017-1116-3},
	doi = {10.1007/s10115-017-1116-3},
	abstract = {Data-trained predictive models see widespread use, but for the most part they are used as black boxes which output a prediction or score. It is therefore hard to acquire a deeper understanding of model behavior and in particular how different features influence the model prediction. This is important when interpreting the behavior of complex models or asserting that certain problematic attributes (such as race or gender) are not unduly influencing decisions. In this paper, we present a technique for auditing black-box models, which lets us study the extent to which existing models take advantage of particular features in the data set, without knowing how the models work. Our work focuses on the problem of indirect influence: how some features might indirectly influence outcomes via other, related features. As a result, we can find attribute influences even in cases where, upon further direct examination of the model, the attribute is not referred to by the model at all. Our approach does not require the black-box model to be retrained. This is important if, for example, the model is only accessible via an API, and contrasts our work with other methods that investigate feature influence such as feature selection. We present experimental evidence for the effectiveness of our procedure using a variety of publicly available data sets and models. We also validate our procedure using techniques from interpretable learning and feature selection, as well as against other black-box auditing procedures. To further demonstrate the effectiveness of this technique, we use it to audit a black-box recidivism prediction algorithm.},
	language = {en},
	number = {1},
	urldate = {2021-06-19},
	journal = {Knowledge and Information Systems},
	author = {Adler, Philip and Falk, Casey and Friedler, Sorelle A. and Nix, Tionney and Rybeck, Gabriel and Scheidegger, Carlos and Smith, Brandon and Venkatasubramanian, Suresh},
	month = jan,
	year = {2018},
	pages = {95--122},
}

@inproceedings{frye_asymmetric_2020,
	title = {Asymmetric {Shapley} values: incorporating causal knowledge into model-agnostic explainability},
	volume = {33},
	shorttitle = {Asymmetric {Shapley} values},
	url = {https://proceedings.neurips.cc/paper/2020/hash/0d770c496aa3da6d2c3f2bd19e7b9d6b-Abstract.html},
	language = {en},
	urldate = {2020-12-26},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Frye, Christopher and Rowat, Colin and Feige, Ilya},
	year = {2020},
}

@inproceedings{zhang_are_2019,
	title = {Are {All} {Layers} {Created} {Equal}?},
	abstract = {Understanding learning with deep architectures has been a major research objective in the recent years with notable theoretical progress. A main focal point of those studies stems from the success of excessively large networks. We study empirically the layer-wise functional structure of overparameterized deep models. We provide evidence for the heterogeneous characteristic of layers. To do so, we introduce the notion of (post training) re-initialization and re-randomization robustness. We show that layers can be categorized into either “robust” or “critical”. In contrast to critical layers, resetting the robust layers to their initial value has no negative consequence, and in many cases they barely change throughout training. Our study provides evidence atness or robustness analysis of the model parameters needs to respect the network architectures.},
	language = {en},
	booktitle = {{ICML} {Workshop} on {Deep} {Phenomena}},
	author = {Zhang, Chiyuan and Bengio, Samy and Singer, Yoram},
	year = {2019},
	pages = {18},
}

@article{yoon_rl-lim_2019,
	title = {{RL}-{LIM}: {Reinforcement} {Learning}-based {Locally} {Interpretable} {Modeling}},
	shorttitle = {{RL}-{LIM}},
	url = {https://arxiv.org/abs/1909.12367v1},
	abstract = {Understanding black-box machine learning models is important towards their widespread adoption. However, developing globally interpretable models that explain the behavior of the entire model is challenging. An alternative approach is to explain black-box models through explaining individual prediction using a locally interpretable model. In this paper, we propose a novel method for locally interpretable modeling - Reinforcement Learning-based Locally Interpretable Modeling (RL-LIM). RL-LIM employs reinforcement learning to select a small number of samples and distill the black-box model prediction into a low-capacity locally interpretable model. Training is guided with a reward that is obtained directly by measuring agreement of the predictions from the locally interpretable model with the black-box model. RL-LIM near-matches the overall prediction performance of black-box models while yielding human-like interpretability, and significantly outperforms state of the art locally interpretable models in terms of overall prediction performance and fidelity.},
	language = {en},
	urldate = {2021-06-18},
	author = {Yoon, Jinsung and Arik, Sercan O. and Pfister, Tomas},
	month = sep,
	year = {2019},
}

@inproceedings{plumb_model_2018,
	title = {Model {Agnostic} {Supervised} {Local} {Explanations}},
	abstract = {Model interpretability is an increasingly important component of practical machine learning. Some of the most common forms of interpretability systems are example-based, local, and global explanations. One of the main challenges in interpretability is designing explanation systems that can capture aspects of each of these explanation types, in order to develop a more thorough understanding of the model. We address this challenge in a novel model called MAPLE that uses local linear modeling techniques along with a dual interpretation of random forests (both as a supervised neighborhood approach and as a feature selection method). MAPLE has two fundamental advantages over existing interpretability systems. First, while it is effective as a black-box explanation system, MAPLE itself is a highly accurate predictive model that provides faithful self explanations, and thus sidesteps the typical accuracy-interpretability trade-off. Speciﬁcally, we demonstrate, on several UCI datasets, that MAPLE is at least as accurate as random forests and that it produces more faithful local explanations than LIME, a popular interpretability system. Second, MAPLE provides both example-based and local explanations and can detect global patterns, which allows it to diagnose limitations in its local explanations.},
	language = {en},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Plumb, Gregory and Molitor, Denali and Talwalkar, Ameet S},
	year = {2018},
	pages = {10},
}

@inproceedings{li_learning_2021,
	title = {A {Learning} {Theoretic} {Perspective} on {Local} {Explainability}},
	url = {https://openreview.net/forum?id=7aL-OtQrBWD},
	abstract = {In this paper, we explore connections between interpretable machine learning and learning theory through the lens of local approximation explanations. First, we tackle the traditional problem of...},
	language = {en},
	urldate = {2021-06-18},
	author = {Li, Jeffrey and Nagarajan, Vaishnavh and Plumb, Gregory and Talwalkar, Ameet},
	year = {2021},
}

@inproceedings{wang_mtunet_2021,
	title = {{MTUNet}: {Few}-{Shot} {Image} {Classification} {With} {Visual} {Explanations}},
	shorttitle = {{MTUNet}},
	url = {https://openaccess.thecvf.com/content/CVPR2021W/RCV/html/Wang_MTUNet_Few-Shot_Image_Classification_With_Visual_Explanations_CVPRW_2021_paper.html},
	language = {en},
	urldate = {2021-06-17},
	author = {Wang, Bowen and Li, Liangzhi and Verma, Manisha and Nakashima, Yuta and Kawasaki, Ryo and Nagahara, Hajime},
	year = {2021},
	pages = {2294--2298},
}

@inproceedings{poppi_revisiting_2021,
	title = {Revisiting the {Evaluation} of {Class} {Activation} {Mapping} for {Explainability}: {A} {Novel} {Metric} and {Experimental} {Analysis}},
	shorttitle = {Revisiting the {Evaluation} of {Class} {Activation} {Mapping} for {Explainability}},
	url = {https://openaccess.thecvf.com/content/CVPR2021W/RCV/html/Poppi_Revisiting_the_Evaluation_of_Class_Activation_Mapping_for_Explainability_A_CVPRW_2021_paper.html},
	language = {en},
	urldate = {2021-06-17},
	author = {Poppi, Samuele and Cornia, Marcella and Baraldi, Lorenzo and Cucchiara, Rita},
	year = {2021},
	pages = {2299--2304},
}

@inproceedings{rahnama_adversarial_2021,
	title = {An {Adversarial} {Approach} for {Explaining} the {Predictions} of {Deep} {Neural} {Networks}},
	url = {https://openaccess.thecvf.com/content/CVPR2021W/TCV/html/Rahnama_An_Adversarial_Approach_for_Explaining_the_Predictions_of_Deep_Neural_CVPRW_2021_paper.html},
	language = {en},
	urldate = {2021-06-17},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshop}},
	author = {Rahnama, Arash and Tseng, Andrew},
	year = {2021},
	pages = {3253--3262},
}

@inproceedings{gu_interpreting_2021,
	title = {Interpreting {Super}-{Resolution} {Networks} {With} {Local} {Attribution} {Maps}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Gu_Interpreting_Super-Resolution_Networks_With_Local_Attribution_Maps_CVPR_2021_paper.html},
	language = {en},
	urldate = {2021-06-17},
	author = {Gu, Jinjin and Dong, Chao},
	year = {2021},
	pages = {9199--9208},
}

@inproceedings{singla_understanding_2021,
	title = {Understanding {Failures} of {Deep} {Networks} via {Robust} {Feature} {Extraction}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Singla_Understanding_Failures_of_Deep_Networks_via_Robust_Feature_Extraction_CVPR_2021_paper.html},
	language = {en},
	urldate = {2021-06-17},
	author = {Singla, Sahil and Nushi, Besmira and Shah, Shital and Kamar, Ece and Horvitz, Eric},
	year = {2021},
	pages = {12853--12862},
}

@inproceedings{nauta_neural_2021,
	title = {Neural {Prototype} {Trees} for {Interpretable} {Fine}-{Grained} {Image} {Recognition}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Nauta_Neural_Prototype_Trees_for_Interpretable_Fine-Grained_Image_Recognition_CVPR_2021_paper.html},
	language = {en},
	urldate = {2021-06-17},
	author = {Nauta, Meike and van Bree, Ron and Seifert, Christin},
	year = {2021},
	pages = {14933--14943},
}

@inproceedings{mao_generative_2021,
	title = {Generative {Interventions} for {Causal} {Learning}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Mao_Generative_Interventions_for_Causal_Learning_CVPR_2021_paper.html},
	language = {en},
	urldate = {2021-06-17},
	author = {Mao, Chengzhi and Cha, Augustine and Gupta, Amogh and Wang, Hao and Yang, Junfeng and Vondrick, Carl},
	year = {2021},
	pages = {3947--3956},
}

@inproceedings{bohle_convolutional_2021,
	title = {Convolutional {Dynamic} {Alignment} {Networks} for {Interpretable} {Classifications}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Bohle_Convolutional_Dynamic_Alignment_Networks_for_Interpretable_Classifications_CVPR_2021_paper.html},
	language = {en},
	urldate = {2021-06-17},
	author = {Bohle, Moritz and Fritz, Mario and Schiele, Bernt},
	year = {2021},
	pages = {10029--10038},
}

@inproceedings{elliott_explaining_2021,
	title = {Explaining {Classifiers} {Using} {Adversarial} {Perturbations} on the {Perceptual} {Ball}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Elliott_Explaining_Classifiers_Using_Adversarial_Perturbations_on_the_Perceptual_Ball_CVPR_2021_paper.html},
	language = {en},
	urldate = {2021-06-17},
	author = {Elliott, Andrew and Law, Stephen and Russell, Chris},
	year = {2021},
	pages = {10693--10702},
}

@inproceedings{jaume_quantifying_2021,
	title = {Quantifying {Explainers} of {Graph} {Neural} {Networks} in {Computational} {Pathology}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Jaume_Quantifying_Explainers_of_Graph_Neural_Networks_in_Computational_Pathology_CVPR_2021_paper.html},
	language = {en},
	urldate = {2021-06-16},
	author = {Jaume, Guillaume and Pati, Pushpak and Bozorgtabar, Behzad and Foncubierta, Antonio and Anniciello, Anna Maria and Feroce, Florinda and Rau, Tilman and Thiran, Jean-Philippe and Gabrani, Maria and Goksel, Orcun},
	year = {2021},
	pages = {8106--8116},
}

@inproceedings{shen_closed-form_2021,
	title = {Closed-{Form} {Factorization} of {Latent} {Semantics} in {GANs}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Shen_Closed-Form_Factorization_of_Latent_Semantics_in_GANs_CVPR_2021_paper.html},
	language = {en},
	urldate = {2021-06-16},
	author = {Shen, Yujun and Zhou, Bolei},
	year = {2021},
	pages = {1532--1540},
}

@inproceedings{ramaswamy_fair_2021,
	title = {Fair {Attribute} {Classification} {Through} {Latent} {Space} {De}-{Biasing}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Ramaswamy_Fair_Attribute_Classification_Through_Latent_Space_De-Biasing_CVPR_2021_paper.html},
	language = {en},
	urldate = {2021-06-16},
	author = {Ramaswamy, Vikram V. and Kim, Sunnie S. Y. and Russakovsky, Olga},
	year = {2021},
	pages = {9301--9310},
}

@inproceedings{zhu_where_2021,
	title = {Where and {What}? {Examining} {Interpretable} {Disentangled} {Representations}},
	shorttitle = {Where and {What}?},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Zhu_Where_and_What_Examining_Interpretable_Disentangled_Representations_CVPR_2021_paper.html},
	language = {en},
	urldate = {2021-06-16},
	author = {Zhu, Xinqi and Xu, Chang and Tao, Dacheng},
	year = {2021},
	pages = {5861--5870},
}

@inproceedings{xu_linear_2021,
	title = {Linear {Semantics} in {Generative} {Adversarial} {Networks}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Xu_Linear_Semantics_in_Generative_Adversarial_Networks_CVPR_2021_paper.html},
	language = {en},
	urldate = {2021-06-16},
	author = {Xu, Jianjin and Zheng, Changxi},
	year = {2021},
	pages = {9351--9360},
}

@inproceedings{pham_learning_2021,
	title = {Learning {To} {Predict} {Visual} {Attributes} in the {Wild}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Pham_Learning_To_Predict_Visual_Attributes_in_the_Wild_CVPR_2021_paper.html},
	language = {en},
	urldate = {2021-06-16},
	author = {Pham, Khoi and Kafle, Kushal and Lin, Zhe and Ding, Zhihong and Cohen, Scott and Tran, Quan and Shrivastava, Abhinav},
	year = {2021},
	pages = {13018--13028},
}

@inproceedings{chefer_transformer_2021,
	title = {Transformer {Interpretability} {Beyond} {Attention} {Visualization}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Chefer_Transformer_Interpretability_Beyond_Attention_Visualization_CVPR_2021_paper.html},
	language = {en},
	urldate = {2021-06-16},
	author = {Chefer, Hila and Gur, Shir and Wolf, Lior},
	year = {2021},
	pages = {782--791},
}

@inproceedings{ge_peek_2021,
	title = {A {Peek} {Into} the {Reasoning} of {Neural} {Networks}: {Interpreting} {With} {Structural} {Visual} {Concepts}},
	shorttitle = {A {Peek} {Into} the {Reasoning} of {Neural} {Networks}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Ge_A_Peek_Into_the_Reasoning_of_Neural_Networks_Interpreting_With_CVPR_2021_paper.html},
	language = {en},
	urldate = {2021-06-16},
	author = {Ge, Yunhao and Xiao, Yao and Xu, Zhi and Zheng, Meng and Karanam, Srikrishna and Chen, Terrence and Itti, Laurent and Wu, Ziyan},
	year = {2021},
	pages = {2195--2204},
}

@inproceedings{khakzar_neural_2021,
	title = {Neural {Response} {Interpretation} {Through} the {Lens} of {Critical} {Pathways}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Khakzar_Neural_Response_Interpretation_Through_the_Lens_of_Critical_Pathways_CVPR_2021_paper.html},
	language = {en},
	urldate = {2021-06-16},
	author = {Khakzar, Ashkan and Baselizadeh, Soroosh and Khanduja, Saurabh and Rupprecht, Christian and Kim, Seong Tae and Navab, Nassir},
	year = {2021},
	pages = {13528--13538},
}

@inproceedings{zhao_graph-based_2021,
	title = {Graph-{Based} {High}-{Order} {Relation} {Discovery} for {Fine}-{Grained} {Recognition}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Zhao_Graph-Based_High-Order_Relation_Discovery_for_Fine-Grained_Recognition_CVPR_2021_paper.html},
	language = {en},
	urldate = {2021-06-16},
	author = {Zhao, Yifan and Yan, Ke and Huang, Feiyue and Li, Jia},
	year = {2021},
	pages = {15079--15088},
}

@inproceedings{lim_building_2021,
	title = {Building {Reliable} {Explanations} of {Unreliable} {Neural} {Networks}: {Locally} {Smoothing} {Perspective} of {Model} {Interpretation}},
	shorttitle = {Building {Reliable} {Explanations} of {Unreliable} {Neural} {Networks}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Lim_Building_Reliable_Explanations_of_Unreliable_Neural_Networks_Locally_Smoothing_Perspective_CVPR_2021_paper.html},
	language = {en},
	urldate = {2021-06-16},
	author = {Lim, Dohun and Lee, Hyeonseok and Kim, Sungchan},
	year = {2021},
	pages = {6468--6477},
}

@inproceedings{lee_relevance-cam_2021,
	title = {Relevance-{CAM}: {Your} {Model} {Already} {Knows} {Where} {To} {Look}},
	shorttitle = {Relevance-{CAM}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Lee_Relevance-CAM_Your_Model_Already_Knows_Where_To_Look_CVPR_2021_paper.html},
	language = {en},
	urldate = {2021-06-16},
	author = {Lee, Jeong Ryong and Kim, Sewon and Park, Inyong and Eo, Taejoon and Hwang, Dosik},
	year = {2021},
	pages = {14944--14953},
}

@inproceedings{yang_causalvae_2021,
	title = {{CausalVAE}: {Disentangled} {Representation} {Learning} via {Neural} {Structural} {Causal} {Models}},
	shorttitle = {{CausalVAE}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Yang_CausalVAE_Disentangled_Representation_Learning_via_Neural_Structural_Causal_Models_CVPR_2021_paper.html},
	language = {en},
	urldate = {2021-06-16},
	author = {Yang, Mengyue and Liu, Furui and Chen, Zhitang and Shen, Xinwei and Hao, Jianye and Wang, Jun},
	year = {2021},
	pages = {9593--9602},
}

@inproceedings{mackowiak_generative_2021,
	title = {Generative {Classifiers} as a {Basis} for {Trustworthy} {Image} {Classification}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Mackowiak_Generative_Classifiers_as_a_Basis_for_Trustworthy_Image_Classification_CVPR_2021_paper.html},
	language = {en},
	urldate = {2021-06-16},
	author = {Mackowiak, Radek and Ardizzone, Lynton and Kothe, Ullrich and Rother, Carsten},
	year = {2021},
	pages = {2971--2981},
}

@article{chan_redunet_2021,
	title = {{ReduNet}: {A} {White}-box {Deep} {Network} from the {Principle} of {Maximizing} {Rate} {Reduction}},
	shorttitle = {{ReduNet}},
	url = {https://arxiv.org/abs/2105.10446v1},
	abstract = {This work attempts to provide a plausible theoretical framework that aims to interpret modern deep (convolutional) networks from the principles of data compression and discriminative representation. We show that for high-dimensional multi-class data, the optimal linear discriminative representation maximizes the coding rate difference between the whole dataset and the average of all the subsets. We show that the basic iterative gradient ascent scheme for optimizing the rate reduction objective naturally leads to a multi-layer deep network, named ReduNet, that shares common characteristics of modern deep networks. The deep layered architectures, linear and nonlinear operators, and even parameters of the network are all explicitly constructed layer-by-layer via forward propagation, instead of learned via back propagation. All components of so-obtained "white-box" network have precise optimization, statistical, and geometric interpretation. Moreover, all linear operators of the so-derived network naturally become multi-channel convolutions when we enforce classification to be rigorously shift-invariant. The derivation also indicates that such a deep convolution network is significantly more efficient to construct and learn in the spectral domain. Our preliminary simulations and experiments clearly verify the effectiveness of both the rate reduction objective and the associated ReduNet. All code and data are available at https://github.com/Ma-Lab-Berkeley.},
	language = {en},
	urldate = {2021-06-10},
	author = {Chan, Kwan Ho Ryan and Yu, Yaodong and You, Chong and Qi, Haozhi and Wright, John and Ma, Yi},
	month = may,
	year = {2021},
}

@inproceedings{cellier_quantifying_2020,
	address = {Cham},
	title = {Quantifying {Model} {Complexity} via {Functional} {Decomposition} for {Better} {Post}-hoc {Interpretability}},
	volume = {1167},
	isbn = {978-3-030-43822-7 978-3-030-43823-4},
	url = {http://link.springer.com/10.1007/978-3-030-43823-4_17},
	doi = {10.1007/978-3-030-43823-4_17},
	abstract = {Post-hoc model-agnostic interpretation methods such as partial dependence plots can be employed to interpret complex machine learning models. While these interpretation methods can be applied regardless of model complexity, they can produce misleading and verbose results if the model is too complex, especially w.r.t. feature interactions. To quantify the complexity of arbitrary machine learning models, we propose model-agnostic complexity measures based on functional decomposition: number of features used, interaction strength and main eﬀect complexity. We show that post-hoc interpretation of models that minimize the three measures is more reliable and compact. Furthermore, we demonstrate the application of these measures in a multi-objective optimization approach which simultaneously minimizes loss and complexity.},
	language = {en},
	urldate = {2021-06-10},
	booktitle = {Machine {Learning} and {Knowledge} {Discovery} in {Databases}},
	publisher = {Springer International Publishing},
	author = {Molnar, Christoph and Casalicchio, Giuseppe and Bischl, Bernd},
	editor = {Cellier, Peggy and Driessens, Kurt},
	year = {2020},
	note = {Series Title: Communications in Computer and Information Science},
	pages = {193--204},
}

@article{hinton_how_2021,
	title = {How to represent part-whole hierarchies in a neural network},
	url = {http://arxiv.org/abs/2102.12627},
	abstract = {This paper does not describe a working system. Instead, it presents a single idea about representation which allows advances made by several diﬀerent groups to be combined into an imaginary system called GLOM1. The advances include transformers, neural ﬁelds, contrastive representation learning, distillation and capsules. GLOM answers the question: How can a neural network with a ﬁxed architecture parse an image into a partwhole hierarchy which has a diﬀerent structure for each image? The idea is simply to use islands of identical vectors to represent the nodes in the parse tree. If GLOM can be made to work, it should signiﬁcantly improve the interpretability of the representations produced by transformer-like systems when applied to vision or language.},
	language = {en},
	urldate = {2021-06-10},
	journal = {arXiv:2102.12627 [cs]},
	author = {Hinton, Geoffrey},
	month = feb,
	year = {2021},
	note = {arXiv: 2102.12627},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, I.2.6, I.4.8},
}

@article{alfonseca_superintelligence_2021,
	title = {Superintelligence {Cannot} be {Contained}: {Lessons} from {Computability} {Theory}},
	volume = {70},
	copyright = {Copyright (c)},
	issn = {1076-9757},
	shorttitle = {Superintelligence {Cannot} be {Contained}},
	url = {https://www.jair.org/index.php/jair/article/view/12202},
	doi = {10.1613/jair.1.12202},
	language = {en},
	urldate = {2021-06-10},
	journal = {Journal of Artificial Intelligence Research},
	author = {Alfonseca, Manuel and Cebrian, Manuel and Anta, Antonio Fernandez and Coviello, Lorenzo and Abeliuk, Andrés and Rahwan, Iyad},
	month = jan,
	year = {2021},
	keywords = {computational social systems, mathematical foundations, philosophical foundations},
	pages = {65--76},
}

@inproceedings{singla_second-order_2020,
	title = {Second-{Order} {Provable} {Defenses} against {Adversarial} {Attacks}},
	url = {http://proceedings.mlr.press/v119/singla20a.html},
	abstract = {A robustness certificate against adversarial examples is the minimum distance of a given input to the decision boundary of the classifier (or its lower bound). For {\textbackslash}emph\{any\} perturbation of the in...},
	language = {en},
	urldate = {2021-06-10},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Singla, Sahil and Feizi, Soheil},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {8981--8991},
}

@inproceedings{yao_pyhessian_2020,
	title = {{PyHessian}: {Neural} {Networks} {Through} the {Lens} of the {Hessian}},
	shorttitle = {{PyHessian}},
	doi = {10.1109/BigData50022.2020.9378171},
	abstract = {We present PYHESSIAN, a new scalable framework that enables fast computation of Hessian (i.e., second-order derivative) information for deep neural networks. PYHESSIAN enables fast computations of the top Hessian eigenvalues, the Hessian trace, and the full Hessian eigenvalue/spectral density; it supports distributed-memory execution on cloud/supercomputer systems; and it is available as open source [1]. This general framework can be used to analyze neural network models, including the topology of the loss landscape (i.e., curvature information) to gain insight into the behavior of different models/optimizers. As an example, we analyze the effect of residual connections and Batch Normalization layers on the trainability of neural networks. One recent claim, based on simpler first-order analysis, is that residual connections and Batch Normalization make the loss landscape "smoother," thus making it easier for Stochastic Gradient Descent to converge to a good solution. Our second-order analysis, easily enabled by PYHESSIAN, shows new finer-scale insights, demonstrating that while conventional wisdom is sometimes validated, in other cases it is simply incorrect. In particular, we find that Batch Normalization does not necessarily make the loss landscape smoother, especially for shallow networks.},
	booktitle = {2020 {IEEE} {International} {Conference} on {Big} {Data} ({Big} {Data})},
	author = {Yao, Zhewei and Gholami, Amir and Keutzer, Kurt and Mahoney, Michael W.},
	month = dec,
	year = {2020},
	keywords = {Analytical models, Artificial neural networks, Big Data, Eigenvalues and eigenfunctions, Electrostatic discharges, Lenses, Training},
	pages = {581--590},
}

@article{lee_first-order_2019,
	title = {First-order methods almost always avoid strict saddle points},
	volume = {176},
	issn = {1436-4646},
	url = {https://doi.org/10.1007/s10107-019-01374-3},
	doi = {10.1007/s10107-019-01374-3},
	abstract = {We establish that first-order methods avoid strict saddle points for almost all initializations. Our results apply to a wide variety of first-order methods, including (manifold) gradient descent, block coordinate descent, mirror descent and variants thereof. The connecting thread is that such algorithms can be studied from a dynamical systems perspective in which appropriate instantiations of the Stable Manifold Theorem allow for a global stability analysis. Thus, neither access to second-order derivative information nor randomness beyond initialization is necessary to provably avoid strict saddle points.},
	language = {en},
	number = {1},
	urldate = {2021-06-10},
	journal = {Mathematical Programming},
	author = {Lee, Jason D. and Panageas, Ioannis and Piliouras, Georgios and Simchowitz, Max and Jordan, Michael I. and Recht, Benjamin},
	month = jul,
	year = {2019},
	pages = {311--337},
}

@inproceedings{guo_explaining_2018,
	title = {Explaining {Deep} {Learning} {Models} -- {A} {Bayesian} {Non}-parametric {Approach}},
	abstract = {Understanding and interpreting how machine learning (ML) models make decisions have been a big challenge. While recent research has proposed various technical approaches to provide some clues as to how an ML model makes individual predictions, they cannot provide users with an ability to inspect a model as a complete entity. In this work, we propose a novel technical approach that augments a Bayesian non-parametric regression mixture model with multiple elastic nets. Using the enhanced mixture model, we can extract generalizable insights for a target model through a global approximation. To demonstrate the utility of our approach, we evaluate it on different ML models in the context of image recognition. The empirical results indicate that our proposed approach not only outperforms the state-of-the-art techniques in explaining individual decisions but also provides users with an ability to discover the vulnerabilities of the target ML models.},
	language = {en},
	booktitle = {32nd {Conference} on {Neural} {Information} {Processing} {Systems}},
	author = {Guo, Wenbo and Huang, Sui and Tao, Yunzhe and Xing, Xinyu and Lin, Lin},
	year = {2018},
	pages = {11},
}

@article{goyal_explaining_2020,
	title = {Explaining {Classifiers} with {Causal} {Concept} {Effect} ({CaCE})},
	url = {http://arxiv.org/abs/1907.07165},
	abstract = {How can we understand classiﬁcation decisions made by deep neural networks? Many existing explainability methods rely solely on correlations and fail to account for confounding, which may result in potentially misleading explanations. To overcome this problem, we deﬁne the Causal Concept Effect (CaCE) as the causal effect of (the presence or absence of) a human-interpretable concept on a deep neural net’s predictions. We show that the CaCE measure can avoid errors stemming from confounding. Estimating CaCE is difﬁcult in situations where we cannot easily simulate the do-operator. To mitigate this problem, we use a generative model, speciﬁcally a Variational AutoEncoder (VAE), to measure VAE-CaCE. In an extensive experimental analysis, we show that the VAE-CaCE is able to estimate the true concept causal effect, compared to baselines for a number of datasets including high dimensional images.},
	language = {en},
	urldate = {2021-06-09},
	journal = {arXiv:1907.07165 [cs, stat]},
	author = {Goyal, Yash and Feder, Amir and Shalit, Uri and Kim, Been},
	month = feb,
	year = {2020},
	note = {arXiv: 1907.07165},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{huang_part-stacked_2016,
	title = {Part-{Stacked} {CNN} for {Fine}-{Grained} {Visual} {Categorization}},
	url = {https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Huang_Part-Stacked_CNN_for_CVPR_2016_paper.html},
	urldate = {2021-06-09},
	author = {Huang, Shaoli and Xu, Zhe and Tao, Dacheng and Zhang, Ya},
	year = {2016},
	pages = {1173--1182},
}

@article{zhu_robustness_2019,
	title = {Robustness of {Object} {Recognition} under {Extreme} {Occlusion} in {Humans} and {Computational} {Models}},
	url = {http://arxiv.org/abs/1905.04598},
	abstract = {Most objects in the visual world are partially occluded, but humans can recognize them without difficulty. However, it remains unknown whether object recognition models like convolutional neural networks (CNNs) can handle real-world occlusion. It is also a question whether efforts to make these models robust to constant mask occlusion are effective for real-world occlusion. We test both humans and the above-mentioned computational models in a challenging task of object recognition under extreme occlusion, where target objects are heavily occluded by irrelevant real objects in real backgrounds. Our results show that human vision is very robust to extreme occlusion while CNNs are not, even with modifications to handle constant mask occlusion. This implies that the ability to handle constant mask occlusion does not entail robustness to real-world occlusion. As a comparison, we propose another computational model that utilizes object parts/subparts in a compositional manner to build robustness to occlusion. This performs significantly better than CNN-based models on our task with error patterns similar to humans. These findings suggest that testing under extreme occlusion can better reveal the robustness of visual recognition, and that the principle of composition can encourage such robustness.},
	urldate = {2021-06-09},
	journal = {arXiv:1905.04598 [cs]},
	author = {Zhu, Hongru and Tang, Peng and Park, Jeongho and Park, Soojin and Yuille, Alan},
	month = jun,
	year = {2019},
	note = {arXiv: 1905.04598},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
}

@article{chen_concept_2020,
	title = {Concept whitening for interpretable image recognition},
	volume = {2},
	copyright = {2020 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {2522-5839},
	url = {https://www.nature.com/articles/s42256-020-00265-z},
	doi = {10.1038/s42256-020-00265-z},
	abstract = {What does a neural network encode about a concept as we traverse through the layers? Interpretability in machine learning is undoubtedly important, but the calculations of neural networks are very challenging to understand. Attempts to see inside their hidden layers can be misleading, unusable or rely on the latent space to possess properties that it may not have. Here, rather than attempting to analyse a neural network post hoc, we introduce a mechanism, called concept whitening (CW), to alter a given layer of the network to allow us to better understand the computation leading up to that layer. When a concept whitening module is added to a convolutional neural network, the latent space is whitened (that is, decorrelated and normalized) and the axes of the latent space are aligned with known concepts of interest. By experiment, we show that CW can provide us with a much clearer understanding of how the network gradually learns concepts over layers. CW is an alternative to a batch normalization layer in that it normalizes, and also decorrelates (whitens), the latent space. CW can be used in any layer of the network without hurting predictive performance.},
	language = {en},
	number = {12},
	urldate = {2021-06-09},
	journal = {Nature Machine Intelligence},
	author = {Chen, Zhi and Bei, Yijie and Rudin, Cynthia},
	month = dec,
	year = {2020},
	note = {Number: 12
Publisher: Nature Publishing Group},
	pages = {772--782},
}

@inproceedings{jacquot_can_2020,
	title = {Can {Deep} {Learning} {Recognize} {Subtle} {Human} {Activities}?},
	url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Jacquot_Can_Deep_Learning_Recognize_Subtle_Human_Activities_CVPR_2020_paper.html},
	urldate = {2021-06-09},
	author = {Jacquot, Vincent and Ying, Zhuofan and Kreiman, Gabriel},
	year = {2020},
	pages = {14244--14253},
}

@inproceedings{kortylewski_compositional_2020,
	title = {Compositional {Convolutional} {Neural} {Networks}: {A} {Deep} {Architecture} {With} {Innate} {Robustness} to {Partial} {Occlusion}},
	shorttitle = {Compositional {Convolutional} {Neural} {Networks}},
	url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Kortylewski_Compositional_Convolutional_Neural_Networks_A_Deep_Architecture_With_Innate_Robustness_CVPR_2020_paper.html},
	urldate = {2021-06-09},
	author = {Kortylewski, Adam and He, Ju and Liu, Qing and Yuille, Alan L.},
	year = {2020},
	pages = {8940--8949},
}

@article{yuille_deep_2021,
	title = {Deep {Nets}: {What} have {They} {Ever} {Done} for {Vision}?},
	volume = {129},
	issn = {1573-1405},
	shorttitle = {Deep {Nets}},
	url = {https://doi.org/10.1007/s11263-020-01405-z},
	doi = {10.1007/s11263-020-01405-z},
	abstract = {This is an opinion paper about the strengths and weaknesses of Deep Nets for vision. They are at the heart of the enormous recent progress in artificial intelligence and are of growing importance in cognitive science and neuroscience. They have had many successes but also have several limitations and there is limited understanding of their inner workings. At present Deep Nets perform very well on specific visual tasks with benchmark datasets but they are much less general purpose, flexible, and adaptive than the human visual system. We argue that Deep Nets in their current form are unlikely to be able to overcome the fundamental problem of computer vision, namely how to deal with the combinatorial explosion, caused by the enormous complexity of natural images, and obtain the rich understanding of visual scenes that the human visual achieves. We argue that this combinatorial explosion takes us into a regime where “big data is not enough” and where we need to rethink our methods for benchmarking performance and evaluating vision algorithms. We stress that, as vision algorithms are increasingly used in real world applications, that performance evaluation is not merely an academic exercise but has important consequences in the real world. It is impractical to review the entire Deep Net literature so we restrict ourselves to a limited range of topics and references which are intended as entry points into the literature. The views expressed in this paper are our own and do not necessarily represent those of anybody else in the computer vision community.},
	language = {en},
	number = {3},
	urldate = {2021-06-09},
	journal = {International Journal of Computer Vision},
	author = {Yuille, Alan L. and Liu, Chenxi},
	month = mar,
	year = {2021},
	pages = {781--802},
}

@inproceedings{koh_concept_2020,
	title = {Concept {Bottleneck} {Models}},
	url = {http://proceedings.mlr.press/v119/koh20a.html},
	abstract = {We seek to learn models that we can interact with using high-level concepts: if the model did not think there was a bone spur in the x-ray, would it still predict severe arthritis? State-of-the-art...},
	language = {en},
	urldate = {2021-06-02},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Koh, Pang Wei and Nguyen, Thao and Tang, Yew Siang and Mussmann, Stephen and Pierson, Emma and Kim, Been and Liang, Percy},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {5338--5348},
}

@article{nicolae_adversarial_2019,
	title = {Adversarial {Robustness} {Toolbox} v1.0.0},
	url = {http://arxiv.org/abs/1807.01069},
	abstract = {Adversarial Robustness Toolbox (ART) is a Python library supporting developers and researchers in defending Machine Learning models (Deep Neural Networks, Gradient Boosted Decision Trees, Support Vector Machines, Random Forests, Logistic Regression, Gaussian Processes, Decision Trees, Scikit-learn Pipelines, etc.) against adversarial threats and helps making AI systems more secure and trustworthy. Machine Learning models are vulnerable to adversarial examples, which are inputs (images, texts, tabular data, etc.) deliberately modified to produce a desired response by the Machine Learning model. ART provides the tools to build and deploy defences and test them with adversarial attacks. Defending Machine Learning models involves certifying and verifying model robustness and model hardening with approaches such as pre-processing inputs, augmenting training data with adversarial samples, and leveraging runtime detection methods to flag any inputs that might have been modified by an adversary. The attacks implemented in ART allow creating adversarial attacks against Machine Learning models which is required to test defenses with state-of-the-art threat models. Supported Machine Learning Libraries include TensorFlow (v1 and v2), Keras, PyTorch, MXNet, Scikit-learn, XGBoost, LightGBM, CatBoost, and GPy. The source code of ART is released with MIT license at https://github.com/IBM/adversarial-robustness-toolbox. The release includes code examples, notebooks with tutorials and documentation (http://adversarial-robustness-toolbox.readthedocs.io).},
	urldate = {2021-05-28},
	journal = {arXiv:1807.01069 [cs, stat]},
	author = {Nicolae, Maria-Irina and Sinn, Mathieu and Tran, Minh Ngoc and Buesser, Beat and Rawat, Ambrish and Wistuba, Martin and Zantedeschi, Valentina and Baracaldo, Nathalie and Chen, Bryant and Ludwig, Heiko and Molloy, Ian M. and Edwards, Ben},
	month = nov,
	year = {2019},
	note = {arXiv: 1807.01069},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{montufar_number_2014,
	title = {On the {Number} of {Linear} {Regions} of {Deep} {Neural} {Networks}},
	volume = {27},
	url = {https://papers.nips.cc/paper/2014/hash/109d2dd3608f669ca17920c511c2a41e-Abstract.html},
	language = {en},
	urldate = {2021-05-22},
	journal = {Advances in Neural Information Processing Systems},
	author = {Montufar, Guido F. and Pascanu, Razvan and Cho, Kyunghyun and Bengio, Yoshua},
	year = {2014},
}

@inproceedings{kurakin_placeholder_2017,
	title = {Placeholder {Adversarial} {Examples} in {The} {Physical} {World}},
	abstract = {Most existing machine learning classiﬁers are highly vulnerable to adversarial examples. An adversarial example is a sample of input data which has been modiﬁed very slightly in a way that is intended to cause a machine learning classiﬁer to misclassify it. In many cases, these modiﬁcations can be so subtle that a human observer does not even notice the modiﬁcation at all, yet the classiﬁer still makes a mistake. Adversarial examples pose security concerns because they could be used to perform an attack on machine learning systems, even if the adversary has no access to the underlying model. Up to now, all previous work has assumed a threat model in which the adversary can feed data directly into the machine learning classiﬁer. This is not always the case for systems operating in the physical world, for example those which are using signals from cameras and other sensors as input. This paper shows that even in such physical world scenarios, machine learning systems are vulnerable to adversarial examples. We demonstrate this by feeding adversarial images obtained from a cell-phone camera to an ImageNet Inception classiﬁer and measuring the classiﬁcation accuracy of the system. We ﬁnd that a large fraction of adversarial examples are classiﬁed incorrectly even when perceived through the camera.},
	language = {en},
	booktitle = {International {Conference} on {Learning} {Representations} {Workshop}},
	author = {Kurakin, Alexey and Goodfellow, Ian J and Bengio, Samy},
	year = {2017},
	pages = {14},
}

@inproceedings{kim_understanding_2020,
	title = {Understanding {Catastrophic} {Overfitting} in {Single}-step {Adversarial} {Training}},
	url = {http://arxiv.org/abs/2010.01799},
	abstract = {Although fast adversarial training has demonstrated both robustness and efﬁciency, the problem of “catastrophic overﬁtting” has been observed. This is a phenomenon in which, during single-step adversarial training, robust accuracy against projected gradient descent (PGD) suddenly decreases to 0\% after a few epochs, whereas robust accuracy against fast gradient sign method (FGSM) increases to 100\%. In this paper, we demonstrate that catastrophic overﬁtting is very closely related to the characteristic of single-step adversarial training which uses only adversarial examples with the maximum perturbation, and not all adversarial examples in the adversarial direction, which leads to decision boundary distortion and a highly curved loss surface. Based on this observation, we propose a simple method that not only prevents catastrophic overﬁtting, but also overrides the belief that it is difﬁcult to prevent multi-step adversarial attacks with single-step adversarial training.},
	language = {en},
	urldate = {2021-02-19},
	booktitle = {{AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Kim, Hoki and Lee, Woojin and Lee, Jaewook},
	month = dec,
	year = {2020},
	note = {arXiv: 2010.01799},
	keywords = {Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing, Statistics - Machine Learning},
}

@inproceedings{chan_what_2020,
	title = {What {It} {Thinks} {Is} {Important} {Is} {Important}: {Robustness} {Transfers} {Through} {Input} {Gradients}},
	shorttitle = {What {It} {Thinks} {Is} {Important} {Is} {Important}},
	url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Chan_What_It_Thinks_Is_Important_Is_Important_Robustness_Transfers_Through_CVPR_2020_paper.html},
	urldate = {2021-04-29},
	author = {Chan, Alvin and Tay, Yi and Ong, Yew-Soon},
	year = {2020},
	pages = {332--341},
}

@inproceedings{croce_reliable_2020,
	title = {Reliable {Evaluation} of {Adversarial} {Robustness} with an {Ensemble} of {Diverse} {Parameter}-free {Attacks}},
	abstract = {The feld of defense strategies against adversarial attacks has signifcantly grown over the last years, but progress is hampered as the evaluation of adversarial defenses is often insuffcient and thus gives a wrong impression of robustness. Many promising defenses could be broken later on, making it diffcult to identify the state-of-the-art. Frequent pitfalls in the evaluation are improper tuning of hyperparameters of the attacks, gradient obfuscation or masking. In this paper we frst propose two extensions of the PGD-attack overcoming failures due to suboptimal step size and problems of the objective function. We then combine our novel attacks with two complementary existing ones to form a parameter-free, computationally affordable and user-independent ensemble of attacks to test adversarial robustness. We apply our ensemble to over 50 models from papers published at recent top machine learning and computer vision venues. In all except one of the cases we achieve lower robust test accuracy than reported in these papers, often by more than 10\%, identifying several broken defenses.},
	language = {en},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	author = {Croce, Francesco and Hein, Matthias},
	year = {2020},
	pages = {11},
}

@article{mosbach_logit_2019,
	title = {Logit {Pairing} {Methods} {Can} {Fool} {Gradient}-{Based} {Attacks}},
	url = {http://arxiv.org/abs/1810.12042},
	abstract = {Recently, Kannan et al. [2018] proposed several logit regularization methods to improve the adversarial robustness of classifiers. We show that the computationally fast methods they propose - Clean Logit Pairing (CLP) and Logit Squeezing (LSQ) - just make the gradient-based optimization problem of crafting adversarial examples harder without providing actual robustness. We find that Adversarial Logit Pairing (ALP) may indeed provide robustness against adversarial examples, especially when combined with adversarial training, and we examine it in a variety of settings. However, the increase in adversarial accuracy is much smaller than previously claimed. Finally, our results suggest that the evaluation against an iterative PGD attack relies heavily on the parameters used and may result in false conclusions regarding robustness of a model.},
	urldate = {2021-04-15},
	journal = {arXiv:1810.12042 [cs, stat]},
	author = {Mosbach, Marius and Andriushchenko, Maksym and Trost, Thomas and Hein, Matthias and Klakow, Dietrich},
	month = mar,
	year = {2019},
	note = {arXiv: 1810.12042},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{qin_adversarial_2019,
	title = {Adversarial {Robustness} through {Local} {Linearization}},
	abstract = {Adversarial training is an effective methodology to train deep neural networks which are robust against adversarial, norm-bounded perturbations. However, the computational cost of adversarial training grows prohibitively as the size of the model and number of input dimensions increase. Further, training against less expensive and therefore weaker adversaries produces models that are robust against weak attacks but break down under attacks that are stronger. This is often attributed to the phenomenon of gradient obfuscation; such models have a highly non-linear loss surface in the vicinity of training examples, making it hard for gradient-based attacks to succeed even though adversarial examples still exist. In this work, we introduce a novel regularizer that encourages the loss to behave linearly in the vicinity of the training data, thereby penalizing gradient obfuscation while encouraging robustness. We show via extensive experiments on CIFAR-10 and ImageNet, that models trained with our regularizer avoid gradient obfuscation and can be trained signiﬁcantly faster than adversarial training. Using this regularizer, we exceed current state of the art and achieve 47\% adversarial accuracy for ImageNet with ∞ adversarial perturbations of radius 4/255 under an untargeted, strong, white-box attack. Additionally, we match state of the art results for CIFAR-10 at 8/255.},
	language = {en},
	booktitle = {33rd {Conference} on {Neural} {Information} {Processing} {Systems} ({NeurIPS} 2019)},
	author = {Qin, Chongli and Martens, James and Gowal, Sven and Krishnan, Dilip and Dvijotham, Krishnamurthy and Fawzi, Alhussein and De, Soham and Stanforth, Robert and Kohli, Pushmeet},
	year = {2019},
	pages = {10},
}

@inproceedings{jetley_friends_2018,
	title = {With {Friends} {Like} {These}, {Who} {Needs} {Adversaries}?},
	volume = {31},
	url = {https://proceedings.neurips.cc/paper/2018/hash/803a82dee7e3fbb3438a149508484250-Abstract.html},
	language = {en},
	urldate = {2021-03-22},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Jetley, Saumya and Lord, Nicholas and Torr, Philip},
	year = {2018},
}

@inproceedings{ghorbani_investigation_2019,
	title = {An {Investigation} into {Neural} {Net} {Optimization} via {Hessian} {Eigenvalue} {Density}},
	url = {http://proceedings.mlr.press/v97/ghorbani19b.html},
	abstract = {To understand the dynamics of training in deep neural networks, we study the evolution of the Hessian eigenvalue density throughout the optimization process. In non-batch normalized networks, we ob...},
	language = {en},
	urldate = {2021-03-10},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Ghorbani, Behrooz and Krishnan, Shankar and Xiao, Ying},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {2232--2241},
}

@inproceedings{wu_adversarial_2020,
	title = {Adversarial {Weight} {Perturbation} {Helps} {Robust} {Generalization}},
	volume = {33},
	url = {https://papers.nips.cc/paper/2020/hash/1ef91c212e30e14bf125e9374262401f-Abstract.html},
	language = {en},
	urldate = {2021-03-09},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Wu, Dongxian and Xia, Shu-Tao and Wang, Yisen},
	year = {2020},
	pages = {2958--2969},
}

@inproceedings{yang_closer_2020,
	title = {A {Closer} {Look} at {Accuracy} vs. {Robustness}},
	language = {en},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 33 ({NeurIPS} 2020)},
	author = {Yang, Yao-Yuan and Rashtchian, Cyrus and Zhang, Hongyang and Salakhutdinov, Ruslan and Chaudhuri, Kamalika},
	year = {2020},
	pages = {14},
}

@inproceedings{moosavi-dezfooli_robustness_2019,
	title = {Robustness via {Curvature} {Regularization}, and {Vice} {Versa}},
	url = {https://openaccess.thecvf.com/content_CVPR_2019/html/Moosavi-Dezfooli_Robustness_via_Curvature_Regularization_and_Vice_Versa_CVPR_2019_paper},
	urldate = {2021-02-19},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Moosavi-Dezfooli, Seyed-Mohsen and Fawzi, Alhussein and Uesato, Jonathan and Frossard, Pascal},
	year = {2019},
	pages = {9078--9086},
}

@inproceedings{liang_can_2020,
	title = {Can a {Fruit} {Fly} {Learn} {Word} {Embeddings}?},
	url = {https://openreview.net/forum?id=xfmSoxdxFCG},
	abstract = {The mushroom body of the fruit fly brain is one of the best studied systems in neuroscience. At its core it consists of a population of Kenyon cells, which receive inputs from multiple sensory...},
	language = {en},
	urldate = {2021-02-24},
	author = {Liang, Yuchen and Ryali, Chaitanya and Hoover, Benjamin and Navlakha, Saket and Grinberg, Leopold and Zaki, Mohammed J. and Krotov, Dmitry},
	month = sep,
	year = {2020},
}

@inproceedings{frankle_training_2020,
	title = {Training {BatchNorm} and {Only} {BatchNorm}: {On} the {Expressive} {Power} of {Random} {Features} in {CNNs}},
	shorttitle = {Training {BatchNorm} and {Only} {BatchNorm}},
	url = {https://openreview.net/forum?id=vYeQQ29Tbvx},
	abstract = {A wide variety of deep learning techniques from style transfer to multitask learning rely on training affine transformations of features. Most prominent among these is the popular feature...},
	language = {en},
	urldate = {2021-02-24},
	author = {Frankle, Jonathan and Schwab, David J. and Morcos, Ari S.},
	month = sep,
	year = {2020},
}

@inproceedings{wang_towards_2020,
	title = {Towards {A} {Unified} {Understanding} and {Improving} of {Adversarial} {Transferability}},
	url = {https://openreview.net/forum?id=X76iqnUbBjz},
	abstract = {In this paper, we use the interaction inside adversarial perturbations to explain and boost the adversarial transferability. We discover and prove the negative correlation between the adversarial...},
	language = {en},
	urldate = {2021-02-23},
	author = {Wang, Xin and Ren, Jie and Lin, Shuyun and Zhu, Xiangming and Wang, Yisen and Zhang, Quanshi},
	month = sep,
	year = {2020},
}

@inproceedings{utrera_adversarially-trained_2020,
	title = {Adversarially-{Trained} {Deep} {Nets} {Transfer} {Better}},
	url = {https://openreview.net/forum?id=ijJZbomCJIm},
	abstract = {Transfer learning has emerged as a powerful methodology for adapting pre-trained deep neural networks on image recognition tasks to new domains. This process consists of taking a neural network...},
	language = {en},
	urldate = {2021-02-23},
	author = {Utrera, Francisco and Kravitz, Evan and Erichson, N. Benjamin and Khanna, Rajiv and Mahoney, Michael W.},
	month = sep,
	year = {2020},
}

@inproceedings{nguyen_wide_2020,
	title = {Do {Wide} and {Deep} {Networks} {Learn} the {Same} {Things}? {Uncovering} {How} {Neural} {Network} {Representations} {Vary} with {Width} and {Depth}},
	shorttitle = {Do {Wide} and {Deep} {Networks} {Learn} the {Same} {Things}?},
	url = {https://openreview.net/forum?id=KJNcAkY8tY4},
	abstract = {A key factor in the success of deep neural networks is the ability to scale models to improve performance by varying the architecture depth and width. This simple property of neural network design...},
	language = {en},
	urldate = {2021-02-23},
	author = {Nguyen, Thao and Raghu, Maithra and Kornblith, Simon},
	month = sep,
	year = {2020},
}

@inproceedings{bahadori_debiasing_2020,
	title = {Debiasing {Concept}-based {Explanations} with {Causal} {Analysis}},
	url = {https://openreview.net/forum?id=6puUoArESGp},
	abstract = {Studying the concept-based explanation techniques, we provided evidences for potential existence of spurious association between the features and concepts due to  unobserved latent variables or...},
	language = {en},
	urldate = {2021-02-23},
	author = {Bahadori, Mohammad Taha and Heckerman, David},
	month = sep,
	year = {2020},
}

@inproceedings{pope_intrinsic_2020,
	title = {The {Intrinsic} {Dimension} of {Images} and {Its} {Impact} on {Learning}},
	url = {https://openreview.net/forum?id=XJk19XzGq2J},
	abstract = {It is widely believed that natural image data exhibits low-dimensional structure despite being embedded in a high-dimensional pixel space. This idea underlies a common intuition for the success of...},
	language = {en},
	urldate = {2021-02-23},
	author = {Pope, Phil and Zhu, Chen and Abdelkader, Ahmed and Goldblum, Micah and Goldstein, Tom},
	month = sep,
	year = {2020},
}

@inproceedings{zhang_how_2020,
	title = {How {Does} {Mixup} {Help} {With} {Robustness} and {Generalization}?},
	url = {https://openreview.net/forum?id=8yKEo06dKNo},
	abstract = {Mixup is a popular data augmentation technique based on on convex combinations of pairs of examples and their labels. This simple technique has shown to substantially improve both the model's...},
	language = {en},
	urldate = {2021-02-23},
	author = {Zhang, Linjun and Deng, Zhun and Kawaguchi, Kenji and Ghorbani, Amirata and Zou, James},
	month = sep,
	year = {2020},
}

@inproceedings{bai_improving_2020,
	title = {Improving {Adversarial} {Robustness} via {Channel}-wise {Activation} {Suppressing}},
	url = {https://openreview.net/forum?id=zQTezqCCtNx},
	abstract = {The study of adversarial examples and their activations have attracted significant attention for secure and robust learning with deep neural networks (DNNs).  Different from existing works, in this...},
	language = {en},
	urldate = {2021-02-23},
	author = {Bai, Yang and Zeng, Yuyuan and Jiang, Yong and Xia, Shu-Tao and Ma, Xingjun and Wang, Yisen},
	month = sep,
	year = {2020},
}

@inproceedings{glorot_deep_2011,
	title = {Deep {Sparse} {Rectifier} {Neural} {Networks}},
	url = {http://proceedings.mlr.press/v15/glorot11a.html},
	abstract = {While logistic sigmoid neurons are more biologically plausible than hyperbolic tangent neurons, the latter work better for training multi-layer neural networks. This paper shows that rectifying neu...},
	language = {en},
	urldate = {2021-01-29},
	booktitle = {Proceedings of the {Fourteenth} {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {JMLR Workshop and Conference Proceedings},
	author = {Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
	month = jun,
	year = {2011},
	note = {ISSN: 1938-7228},
	pages = {315--323},
}

@inproceedings{yao_hessian-based_2018,
	title = {Hessian-based {Analysis} of {Large} {Batch} {Training} and {Robustness} to {Adversaries}},
	volume = {31},
	url = {https://papers.nips.cc/paper/2018/hash/102f0bb6efb3a6128a3c750dd16729be-Abstract.html},
	language = {en},
	urldate = {2021-01-24},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Yao, Zhewei and Gholami, Amir and Lei, Qi and Keutzer, Kurt and Mahoney, Michael W.},
	year = {2018},
	pages = {4949--4959},
}

@inproceedings{liu_loss_2020,
	title = {On the {Loss} {Landscape} of {Adversarial} {Training}: {Identifying} {Challenges} and {How} to {Overcome} {Them}},
	volume = {33},
	shorttitle = {On the {Loss} {Landscape} of {Adversarial} {Training}},
	url = {https://proceedings.neurips.cc/paper/2020/hash/f56d8183992b6c54c92c16a8519a6e2b-Abstract.html},
	language = {en},
	urldate = {2021-01-21},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Liu, Chen and Salzmann, Mathieu and Lin, Tao and Tomioka, Ryota and Süsstrunk, Sabine},
	year = {2020},
}

@article{pearlmutter_fast_1994,
	title = {Fast {Exact} {Multiplication} by the {Hessian}},
	volume = {6},
	issn = {0899-7667},
	url = {https://doi.org/10.1162/neco.1994.6.1.147},
	doi = {10.1162/neco.1994.6.1.147},
	abstract = {Just storing the Hessian H (the matrix of second derivatives δ2E/δwiδwj of the error E with respect to each pair of weights) of a large neural network is difficult. Since a common use of a large matrix like H is to compute its product with various vectors, we derive a technique that directly calculates Hv, where v is an arbitrary vector. To calculate Hv, we first define a differential operator Rv\{f(w)\} = (δ/δr)f(w + rv){\textbar}r=0, note that Rv\{▽w\} = Hv and Rv\{w\} = v, and then apply Rv\{·\} to the equations used to compute ▽w. The result is an exact and numerically stable procedure for computing Hv, which takes about as much computation, and is about as local, as a gradient evaluation. We then apply the technique to a one pass gradient calculation algorithm (backpropagation), a relaxation gradient calculation algorithm (recurrent backpropagation), and two stochastic gradient calculation algorithms (Boltzmann machines and weight perturbation). Finally, we show that this technique can be used at the heart of many iterative techniques for computing various properties of H, obviating any need to calculate the full Hessian.},
	number = {1},
	urldate = {2021-01-20},
	journal = {Neural Computation},
	author = {Pearlmutter, Barak A.},
	month = jan,
	year = {1994},
	note = {Publisher: MIT Press},
	pages = {147--160},
}

@inproceedings{simon-gabriel_first-order_2019,
	title = {First-{Order} {Adversarial} {Vulnerability} of {Neural} {Networks} and {Input} {Dimension}},
	url = {http://proceedings.mlr.press/v97/simon-gabriel19a.html},
	abstract = {Over the past few years, neural networks were proven vulnerable to adversarial images: targeted but imperceptible image perturbations lead to drastically different predictions. We show that adversa...},
	language = {en},
	urldate = {2021-01-19},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Simon-Gabriel, Carl-Johann and Ollivier, Yann and Bottou, Leon and Schölkopf, Bernhard and Lopez-Paz, David},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {5809--5817},
}

@article{lee_gradient_2021,
	title = {Gradient {Masking} of {Label} {Smoothing} in {Adversarial} {Robustness}},
	volume = {9},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2020.3048120},
	abstract = {Deep neural networks (DNNs) have achieved impressive results in several image classification tasks. However, these architectures are unstable for adversarial examples (AEs) such as inputs crafted by a hardly perceptible perturbation with the intent of causing neural networks to make errors. AEs must be considered to prevent accidents in areas such as unmanned car driving using visual object detection in Internet of Things (IoT) networks. Gaussian noise with label smoothing or logit squeezing can be used to increase the robustness against AEs in the training of DNNs. However, from a model interpretability aspect, Gaussian noise with label smoothing does not increase the adversarial robustness of the model. To resolve this problem, we tackle the AE instead of measuring the accuracy of the model against AEs. Considering that a robust model shows a small curvature of the loss surface, we propose a metric to measure the strength of the AEs and the robustness of the model. Furthermore, we introduce a method to verify the existence of the obfuscated gradients of the model based on the black-box attack sanity check method. The proposed method enables us to identify a gradient masking problem wherein the model does not provide useful gradients and exploits false defenses. We evaluate our technique against representative adversarially trained models using the CIFAR10, CIFAR100, SVHN, and Restricted ImageNet datasets. Our results show that the performance of some false defense models decreases by up to 32\% compared to the previous evaluation metrics. Moreover, our metric reveals that traditional metrics used to measure the robustness of the model may produce false results.},
	journal = {IEEE Access},
	author = {Lee, H. and Bae, H. and Yoon, S.},
	year = {2021},
	note = {Conference Name: IEEE Access},
	keywords = {Adversarial learning, Data models, Gaussian noise, IoT, IoT security, Measurement, Perturbation methods, Robustness, Smoothing methods, Training, deep learning, evasion attack, gradient masking, interpretability, label smoothing},
	pages = {6453--6464},
}

@article{su_one_2019,
	title = {One {Pixel} {Attack} for {Fooling} {Deep} {Neural} {Networks}},
	volume = {23},
	issn = {1941-0026},
	doi = {10.1109/TEVC.2019.2890858},
	abstract = {Recent research has revealed that the output of deep neural networks (DNNs) can be easily altered by adding relatively small perturbations to the input vector. In this paper, we analyze an attack in an extremely limited scenario where only one pixel can be modified. For that we propose a novel method for generating one-pixel adversarial perturbations based on differential evolution (DE). It requires less adversarial information (a black-box attack) and can fool more types of networks due to the inherent features of DE. The results show that 67.97\% of the natural images in Kaggle CIFAR-10 test dataset and 16.04\% of the ImageNet (ILSVRC 2012) test images can be perturbed to at least one target class by modifying just one pixel with 74.03\% and 22.91\% confidence on average. We also show the same vulnerability on the original CIFAR-10 dataset. Thus, the proposed attack explores a different take on adversarial machine learning in an extreme limited scenario, showing that current DNNs are also vulnerable to such low dimension attacks. Besides, we also illustrate an important application of DE (or broadly speaking, evolutionary computation) in the domain of adversarial machine learning: creating tools that can effectively generate low-cost adversarial attacks against neural networks for evaluating robustness.},
	number = {5},
	journal = {IEEE Transactions on Evolutionary Computation},
	author = {Su, J. and Vargas, D. V. and Sakurai, K.},
	month = oct,
	year = {2019},
	note = {Conference Name: IEEE Transactions on Evolutionary Computation},
	keywords = {Additives, CIFAR-10 dataset, Convolutional neural network, DNN, ILSVRC 2012, Image color analysis, Image recognition, ImageNet test images, Neural networks, Perturbation methods, Robustness, adversarial information, adversarial machine learning, deep neural networks, differential evolution, differential evolution (DE), evolutionary computation, extreme limited scenario, feature extraction, image classification, image recognition, information security, input vector, learning (artificial intelligence), low dimension attacks, low-cost adversarial attacks, natural images, neural nets, object recognition, one-pixel adversarial perturbations, pixel attack},
	pages = {828--841},
}

@inproceedings{scholbeck_sampling_2020,
	address = {Cham},
	series = {Communications in {Computer} and {Information} {Science}},
	title = {Sampling, {Intervention}, {Prediction}, {Aggregation}: {A} {Generalized} {Framework} for {Model}-{Agnostic} {Interpretations}},
	isbn = {978-3-030-43823-4},
	shorttitle = {Sampling, {Intervention}, {Prediction}, {Aggregation}},
	doi = {10.1007/978-3-030-43823-4_18},
	abstract = {Model-agnostic interpretation techniques allow us to explain the behavior of any predictive model. Due to different notations and terminology, it is difficult to see how they are related. A unified view on these methods has been missing. We present the generalized SIPA (sampling, intervention, prediction, aggregation) framework of work stages for model-agnostic interpretations and demonstrate how several prominent methods for feature effects can be embedded into the proposed framework. Furthermore, we extend the framework to feature importance computations by pointing out how variance-based and performance-based importance measures are based on the same work stages. The SIPA framework reduces the diverse set of model-agnostic techniques to a single methodology and establishes a common terminology to discuss them in future work.},
	language = {en},
	booktitle = {Machine {Learning} and {Knowledge} {Discovery} in {Databases}},
	publisher = {Springer International Publishing},
	author = {Scholbeck, Christian A. and Molnar, Christoph and Heumann, Christian and Bischl, Bernd and Casalicchio, Giuseppe},
	editor = {Cellier, Peggy and Driessens, Kurt},
	year = {2020},
	keywords = {Explainable AI, Feature Effect, Feature Importance, Interpretable Machine Learning, Model-Agnostic, Partial Dependence},
	pages = {205--216},
}

@inproceedings{nam_relative_2020,
	title = {Relative {Attributing} {Propagation}: {Interpreting} the {Comparative} {Contributions} of {Individual} {Units} in {Deep} {Neural} {Networks}},
	volume = {34},
	copyright = {Copyright (c) 2020 Association for the Advancement of Artificial Intelligence},
	shorttitle = {Relative {Attributing} {Propagation}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/5632},
	doi = {10.1609/aaai.v34i03.5632},
	language = {en},
	urldate = {2021-01-17},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Nam, Woo-Jeoung and Gur, Shir and Choi, Jaesik and Wolf, Lior and Lee, Seong-Whan},
	month = apr,
	year = {2020},
	note = {Number: 03},
	pages = {2501--2508},
}

@inproceedings{ancona_towards_2018,
	title = {Towards better understanding of gradient-based attribution methods for {Deep} {Neural} {Networks}},
	url = {https://openreview.net/forum?id=Sy21R9JAW&noteId=Sy21R9JAW},
	abstract = {Four existing backpropagation-based attribution methods are fundamentally similar. How to assess it?},
	language = {en},
	urldate = {2021-01-17},
	author = {Ancona, Marco and Ceolini, Enea and Öztireli, Cengiz and Gross, Markus},
	month = feb,
	year = {2018},
}

@article{lundberg_local_2020,
	title = {From local explanations to global understanding with explainable {AI} for trees},
	volume = {2},
	copyright = {2020 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {2522-5839},
	url = {https://www.nature.com/articles/s42256-019-0138-9},
	doi = {10.1038/s42256-019-0138-9},
	abstract = {Tree-based machine learning models such as random forests, decision trees and gradient boosted trees are popular nonlinear predictive models, yet comparatively little attention has been paid to explaining their predictions. Here we improve the interpretability of tree-based models through three main contributions. (1) A polynomial time algorithm to compute optimal explanations based on game theory. (2) A new type of explanation that directly measures local feature interaction effects. (3) A new set of tools for understanding global model structure based on combining many local explanations of each prediction. We apply these tools to three medical machine learning problems and show how combining many high-quality local explanations allows us to represent global structure while retaining local faithfulness to the original model. These tools enable us to (1) identify high-magnitude but low-frequency nonlinear mortality risk factors in the US population, (2) highlight distinct population subgroups with shared risk characteristics, (3) identify nonlinear interaction effects among risk factors for chronic kidney disease and (4) monitor a machine learning model deployed in a hospital by identifying which features are degrading the model’s performance over time. Given the popularity of tree-based machine learning models, these improvements to their interpretability have implications across a broad set of domains.},
	language = {en},
	number = {1},
	urldate = {2021-01-17},
	journal = {Nature Machine Intelligence},
	author = {Lundberg, Scott M. and Erion, Gabriel and Chen, Hugh and DeGrave, Alex and Prutkin, Jordan M. and Nair, Bala and Katz, Ronit and Himmelfarb, Jonathan and Bansal, Nisha and Lee, Su-In},
	month = jan,
	year = {2020},
	note = {Number: 1
Publisher: Nature Publishing Group},
	pages = {56--67},
}

@article{merrick_randomized_2019,
	title = {Randomized {Ablation} {Feature} {Importance}},
	url = {http://arxiv.org/abs/1910.00174},
	abstract = {Given a model \$f\$ that predicts a target \$y\$ from a vector of input features \${\textbackslash}pmb\{x\} = x\_1, x\_2, {\textbackslash}ldots, x\_M\$, we seek to measure the importance of each feature with respect to the model's ability to make a good prediction. To this end, we consider how (on average) some measure of goodness or badness of prediction (which we term "loss" \${\textbackslash}ell\$), changes when we hide or ablate each feature from the model. To ablate a feature, we replace its value with another possible value randomly. By averaging over many points and many possible replacements, we measure the importance of a feature on the model's ability to make good predictions. Furthermore, we present statistical measures of uncertainty that quantify how confident we are that the feature importance we measure from our finite dataset and finite number of ablations is close to the theoretical true importance value.},
	urldate = {2021-01-17},
	journal = {arXiv:1910.00174 [cs, stat]},
	author = {Merrick, Luke},
	month = oct,
	year = {2019},
	note = {arXiv: 1910.00174},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{miller_explanation_2019,
	title = {Explanation in artificial intelligence: {Insights} from the social sciences},
	volume = {267},
	issn = {0004-3702},
	shorttitle = {Explanation in artificial intelligence},
	url = {http://www.sciencedirect.com/science/article/pii/S0004370218305988},
	doi = {10.1016/j.artint.2018.07.007},
	abstract = {There has been a recent resurgence in the area of explainable artificial intelligence as researchers and practitioners seek to provide more transparency to their algorithms. Much of this research is focused on explicitly explaining decisions or actions to a human observer, and it should not be controversial to say that looking at how humans explain to each other can serve as a useful starting point for explanation in artificial intelligence. However, it is fair to say that most work in explainable artificial intelligence uses only the researchers' intuition of what constitutes a ‘good’ explanation. There exist vast and valuable bodies of research in philosophy, psychology, and cognitive science of how people define, generate, select, evaluate, and present explanations, which argues that people employ certain cognitive biases and social expectations to the explanation process. This paper argues that the field of explainable artificial intelligence can build on this existing research, and reviews relevant papers from philosophy, cognitive psychology/science, and social psychology, which study these topics. It draws out some important findings, and discusses ways that these can be infused with work on explainable artificial intelligence.},
	language = {en},
	urldate = {2021-01-17},
	journal = {Artificial Intelligence},
	author = {Miller, Tim},
	month = feb,
	year = {2019},
	keywords = {Explainability, Explainable AI, Explanation, Interpretability, Transparency},
	pages = {1--38},
}

@inproceedings{garreau_explaining_2020,
	title = {Explaining the {Explainer}: {A} {First} {Theoretical} {Analysis} of {LIME}},
	shorttitle = {Explaining the {Explainer}},
	url = {http://proceedings.mlr.press/v108/garreau20a.html},
	abstract = {Machine learning is used more and more often for sensitive applications, sometimes replacing humans in critical decision-making processes. As such, interpretability of these algorithms is a pressin...},
	language = {en},
	urldate = {2021-01-17},
	booktitle = {International {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Garreau, Damien and Luxburg, Ulrike},
	month = jun,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {1287--1296},
}

@article{sokol_one_2020,
	title = {One {Explanation} {Does} {Not} {Fit} {All}},
	volume = {34},
	issn = {1610-1987},
	url = {https://doi.org/10.1007/s13218-020-00637-y},
	doi = {10.1007/s13218-020-00637-y},
	abstract = {The need for transparency of predictive systems based on Machine Learning algorithms arises as a consequence of their ever-increasing proliferation in the industry. Whenever black-box algorithmic predictions influence human affairs, the inner workings of these algorithms should be scrutinised and their decisions explained to the relevant stakeholders, including the system engineers, the system’s operators and the individuals whose case is being decided. While a variety of interpretability and explainability methods is available, none of them is a panacea that can satisfy all diverse expectations and competing objectives that might be required by the parties involved. We address this challenge in this paper by discussing the promises of Interactive Machine Learning for improved transparency of black-box systems using the example of contrastive explanations—a state-of-the-art approach to Interpretable Machine Learning. Specifically, we show how to personalise counterfactual explanations by interactively adjusting their conditional statements and extract additional explanations by asking follow-up “What if?” questions. Our experience in building, deploying and presenting this type of system allowed us to list desired properties as well as potential limitations, which can be used to guide the development of interactive explainers. While customising the medium of interaction, i.e., the user interface comprising of various communication channels, may give an impression of personalisation, we argue that adjusting the explanation itself and its content is more important. To this end, properties such as breadth, scope, context, purpose and target of the explanation have to be considered, in addition to explicitly informing the explainee about its limitations and caveats. Furthermore, we discuss the challenges of mirroring the explainee’s mental model, which is the main building block of intelligible human–machine interactions. We also deliberate on the risks of allowing the explainee to freely manipulate the explanations and thereby extracting information about the underlying predictive model, which might be leveraged by malicious actors to steal or game the model. Finally, building an end-to-end interactive explainability system is a challenging engineering task; unless the main goal is its deployment, we recommend “Wizard of Oz” studies as a proxy for testing and evaluating standalone interactive explainability algorithms.},
	language = {en},
	number = {2},
	urldate = {2021-01-17},
	journal = {KI - Künstliche Intelligenz},
	author = {Sokol, Kacper and Flach, Peter},
	month = jun,
	year = {2020},
	pages = {235--250},
}

@inproceedings{sokol_explainability_2020,
	address = {New York, NY, USA},
	series = {{FAT}* '20},
	title = {Explainability fact sheets: a framework for systematic assessment of explainable approaches},
	isbn = {978-1-4503-6936-7},
	shorttitle = {Explainability fact sheets},
	url = {https://doi.org/10.1145/3351095.3372870},
	doi = {10.1145/3351095.3372870},
	abstract = {Explanations in Machine Learning come in many forms, but a consensus regarding their desired properties is yet to emerge. In this paper we introduce a taxonomy and a set of descriptors that can be used to characterise and systematically assess explainable systems along five key dimensions: functional, operational, usability, safety and validation. In order to design a comprehensive and representative taxonomy and associated descriptors we surveyed the eXplainable Artificial Intelligence literature, extracting the criteria and desiderata that other authors have proposed or implicitly used in their research. The survey includes papers introducing new explainability algorithms to see what criteria are used to guide their development and how these algorithms are evaluated, as well as papers proposing such criteria from both computer science and social science perspectives. This novel framework allows to systematically compare and contrast explainability approaches, not just to better understand their capabilities but also to identify discrepancies between their theoretical qualities and properties of their implementations. We developed an operationalisation of the framework in the form of Explainability Fact Sheets, which enable researchers and practitioners alike to quickly grasp capabilities and limitations of a particular explainable method. When used as a Work Sheet, our taxonomy can guide the development of new explainability approaches by aiding in their critical evaluation along the five proposed dimensions.},
	urldate = {2021-01-16},
	booktitle = {Proceedings of the 2020 {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {Association for Computing Machinery},
	author = {Sokol, Kacper and Flach, Peter},
	month = jan,
	year = {2020},
	keywords = {AI, ML, desiderata, explainability, fact sheet, interpretability, taxonomy, transparency, work sheet},
	pages = {56--67},
}

@inproceedings{poyiadzi_face_2020,
	address = {New York, NY, USA},
	series = {{AIES} '20},
	title = {{FACE}: {Feasible} and {Actionable} {Counterfactual} {Explanations}},
	isbn = {978-1-4503-7110-0},
	shorttitle = {{FACE}},
	url = {https://doi.org/10.1145/3375627.3375850},
	doi = {10.1145/3375627.3375850},
	abstract = {Work in Counterfactual Explanations tends to focus on the principle of "the closest possible world" that identifies small changes leading to the desired outcome. In this paper we argue that while this approach might initially seem intuitively appealing it exhibits shortcomings not addressed in the current literature. First, a counterfactual example generated by the state-of-the-art systems is not necessarily representative of the underlying data distribution, and may therefore prescribe unachievable goals (e.g., an unsuccessful life insurance applicant with severe disability may be advised to do more sports). Secondly, the counterfactuals may not be based on a "feasible path" between the current state of the subject and the suggested one, making actionable recourse infeasible (e.g., low-skilled unsuccessful mortgage applicants may be told to double their salary, which may be hard without first increasing their skill level). These two shortcomings may render counterfactual explanations impractical and sometimes outright offensive. To address these two major flaws, first of all, we propose a new line of Counterfactual Explanations research aimed at providing actionable and feasible paths to transform a selected instance into one that meets a certain goal. Secondly, we propose FACE: an algorithmically sound way of uncovering these "feasible paths" based on the shortest path distances defined via density-weighted metrics. Our approach generates counterfactuals that are coherent with the underlying data distribution and supported by the "feasible paths" of change, which are achievable and can be tailored to the problem at hand.},
	urldate = {2021-01-16},
	booktitle = {Proceedings of the {AAAI}/{ACM} {Conference} on {AI}, {Ethics}, and {Society}},
	publisher = {Association for Computing Machinery},
	author = {Poyiadzi, Rafael and Sokol, Kacper and Santos-Rodriguez, Raul and De Bie, Tijl and Flach, Peter},
	month = feb,
	year = {2020},
	keywords = {black-box models, counterfactuals, explainability, interpretability},
	pages = {344--350},
}

@inproceedings{le_grace_2020,
	address = {New York, NY, USA},
	series = {{KDD} '20},
	title = {{GRACE}: {Generating} {Concise} and {Informative} {Contrastive} {Sample} to {Explain} {Neural} {Network} {Model}'s {Prediction}},
	isbn = {978-1-4503-7998-4},
	shorttitle = {{GRACE}},
	url = {https://doi.org/10.1145/3394486.3403066},
	doi = {10.1145/3394486.3403066},
	abstract = {Despite the recent development in the topic of explainable AI/ML for image and text data, the majority of current solutions are not suitable to explain the prediction of neural network models when the datasets are tabular and their features are in high-dimensional vectorized formats. To mitigate this limitation, therefore, we borrow two notable ideas (i.e., "explanation by intervention" from causality and "explanation are contrastive" from philosophy) and propose a novel solution, named as GRACE, that better explains neural network models' predictions for tabular datasets. In particular, given a model's prediction as label X, GRACE intervenes and generates a minimally-modified contrastive sample to be classified as Y, with an intuitive textual explanation, answering the question of "Why X rather than Y?" We carry out comprehensive experiments using eleven public datasets of different scales and domains (e.g., \# of features ranges from 5 to 216) and compare GRACE with competing baselines on different measures: fidelity, conciseness, info-gain, and influence. The user-studies show that our generated explanation is not only more intuitive and easy-to-understand but also facilitates end-users to make as much as 60\% more accurate post-explanation decisions than that of Lime.},
	urldate = {2021-01-16},
	booktitle = {Proceedings of the 26th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Le, Thai and Wang, Suhang and Lee, Dongwon},
	month = aug,
	year = {2020},
	keywords = {contrastive samples, counterfactual samples, data generation, deep learning, explainability, interpretability, neural networks},
	pages = {238--248},
}

@inproceedings{rasouli_explan_2020,
	title = {{EXPLAN}: {Explaining} {Black}-box {Classifiers} using {Adaptive} {Neighborhood} {Generation}},
	shorttitle = {{EXPLAN}},
	doi = {10.1109/IJCNN48605.2020.9206710},
	abstract = {Defining a representative locality is an urgent challenge in perturbation-based explanation methods, which influences the fidelity and soundness of explanations. We address this issue by proposing a robust and intuitive approach for EXPLaining black-box classifiers using Adaptive Neighborhood generation (EXPLAN). EXPLAN is a module-based algorithm consisted of dense data generation, representative data selection, data balancing, and rule-based interpretable model. It takes into account the adjacency information derived from the black-box decision function and the structure of the data for creating a representative neighborhood for the instance being explained. As a local model-agnostic explanation method, EXPLAN generates explanations in the form of logical rules that are highly interpretable and well-suited for qualitative analysis of the model's behavior. We discuss fidelity-interpretability trade-offs and demonstrate the performance of the proposed algorithm by a comprehensive comparison with state-of-the-art explanation methods LIME, LORE, and Anchor. The conducted experiments on real-world data sets show our method achieves solid empirical results in terms of fidelity, precision, and stability of explanations.},
	booktitle = {2020 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	author = {Rasouli, P. and Yu, I. C.},
	month = jul,
	year = {2020},
	note = {ISSN: 2161-4407},
	keywords = {Adaptive Neighborhood generation, Analytical models, Data Sampling, Data models, Decision trees, EXPLAN, Interpretable Machine Learning, Machine learning, Neural networks, Perturbation-based Explanation Methods, Predictive models, Training data, XAI, black-box decision function, data balancing, data structure, data structures, dense data generation, explaining black-box classifiers, explanation, feature selection, knowledge based systems, learning (artificial intelligence), local model-agnostic explanation, pattern classification, perturbation-based explanation, representative data selection, representative neighborhood, rule-based interpretable model},
	pages = {1--9},
}

@book{samek_explainable_2019,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Explainable {AI}: {Interpreting}, {Explaining} and {Visualizing} {Deep} {Learning}},
	volume = {11700},
	isbn = {978-3-030-28953-9 978-3-030-28954-6},
	shorttitle = {Explainable {AI}},
	url = {http://link.springer.com/10.1007/978-3-030-28954-6},
	language = {en},
	urldate = {2021-01-16},
	publisher = {Springer International Publishing},
	editor = {Samek, Wojciech and Montavon, Grégoire and Vedaldi, Andrea and Hansen, Lars Kai and Müller, Klaus-Robert},
	year = {2019},
	doi = {10.1007/978-3-030-28954-6},
}

@inproceedings{kenny_twin-systems_2019,
	address = {Macao, China},
	title = {Twin-{Systems} to {Explain} {Artificial} {Neural} {Networks} using {Case}-{Based} {Reasoning}: {Comparative} {Tests} of {Feature}-{Weighting} {Methods} in {ANN}-{CBR} {Twins} for {XAI}},
	isbn = {978-0-9992411-4-1},
	shorttitle = {Twin-{Systems} to {Explain} {Artificial} {Neural} {Networks} using {Case}-{Based} {Reasoning}},
	url = {https://www.ijcai.org/proceedings/2019/376},
	doi = {10.24963/ijcai.2019/376},
	abstract = {In this paper, twin-systems are described to address the eXplainable artiﬁcial intelligence (XAI) problem, where a black box model is mapped to a white box “twin” that is more interpretable, with both systems using the same dataset. The framework is instantiated by twinning an artiﬁcial neural network (ANN; black box) with a case-based reasoning system (CBR; white box), and mapping the feature weights from the former to the latter to ﬁnd cases that explain the ANN’s outputs. Using a novel evaluation method, the effectiveness of this twin-system approach is demonstrated by showing that nearest neighbor cases can be found to match the ANN predictions for benchmark datasets. Several feature-weighting methods are competitively tested in two experiments, including our novel, contributions-based method (called COLE) that is found to perform best. The tests consider the ”twinning” of traditional multilayer perceptron (MLP) networks and convolutional neural networks (CNN) with CBR systems. For the CNNs trained on image data, qualitative evidence shows that cases provide plausible explanations for the CNN’s classiﬁcations.},
	language = {en},
	urldate = {2021-01-16},
	booktitle = {Proceedings of the {Twenty}-{Eighth} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Kenny, Eoin M. and Keane, Mark T.},
	month = aug,
	year = {2019},
	pages = {2708--2715},
}

@inproceedings{papenmeier_how_2019,
	title = {How model accuracy and explanation fidelity influence user trust in {AI}},
	url = {https://research.utwente.nl/en/publications/how-model-accuracy-and-explanation-fidelity-influence-user-trust-},
	language = {English},
	urldate = {2021-01-16},
	author = {Papenmeier, Andrea and Englebienne, Gwenn and Seifert, Christin},
	year = {2019},
}

@inproceedings{gilpin_explaining_2018,
	title = {Explaining {Explanations} to {Society}},
	abstract = {There is a disconnect between explanatory artiﬁcial intelligence (XAI) methods and the types of explanations that are useful for and demanded by society (policy makers, government ofﬁcials, etc.) Questions that experts in artiﬁcial intelligence (AI) ask opaque systems provide inside explanations, focused on debugging, reliability, and validation. These are different from those that society will ask of these systems to build trust and conﬁdence in their decisions. Although explanatory AI systems can answer many questions that experts desire, they often don’t explain why they made decisions in a way that is precise (true to the model) and understandable to humans. These outside explanations can be used to build trust, comply with regulatory and policy changes, and act as external validation. In this paper, we focus on XAI methods for deep neural networks (DNNs) because of DNNs’ use in decision-making and inherent opacity. We explore the types of questions that explanatory DNN systems can answer and discuss challenges in building explanatory systems that provide outside explanations for societal requirements and beneﬁt.},
	language = {en},
	booktitle = {32nd {Conference} on {Neural} {Information} {Processing} {Systems}},
	author = {Gilpin, Leilani H and Testart, Cecilia and Fruchter, Nathaniel and Adebayo, Julius},
	year = {2018},
	pages = {6},
}

@article{roscher_explainable_2020,
	title = {Explainable {Machine} {Learning} for {Scientific} {Insights} and {Discoveries}},
	volume = {8},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2020.2976199},
	abstract = {Machine learning methods have been remarkably successful for a wide range of application areas in the extraction of essential information from data. An exciting and relatively recent development is the uptake of machine learning in the natural sciences, where the major goal is to obtain novel scientific insights and discoveries from observational or simulated data. A prerequisite for obtaining a scientific outcome is domain knowledge, which is needed to gain explainability, but also to enhance scientific consistency. In this article, we review explainable machine learning in view of applications in the natural sciences and discuss three core elements that we identified as relevant in this context: transparency, interpretability, and explainability. With respect to these core elements, we provide a survey of recent scientific works that incorporate machine learning and the way that explainable machine learning is used in combination with domain knowledge from the application areas.},
	journal = {IEEE Access},
	author = {Roscher, R. and Bohn, B. and Duarte, M. F. and Garcke, J.},
	year = {2020},
	note = {Conference Name: IEEE Access},
	keywords = {Approximation algorithms, Biological system modeling, Data mining, Data models, Explainable machine learning, Kernel, Machine learning, Mathematical model, explainability, informed machine learning, interpretability, learning (artificial intelligence), machine learning methods, natural sciences, natural sciences computing, scientific consistency, scientific discoveries, scientific insights, scientific outcome, transparency},
	pages = {42200--42216},
}

@inproceedings{mittelstadt_explaining_2019,
	address = {New York, NY, USA},
	series = {{FAT}* '19},
	title = {Explaining {Explanations} in {AI}},
	isbn = {978-1-4503-6125-5},
	url = {https://doi.org/10.1145/3287560.3287574},
	doi = {10.1145/3287560.3287574},
	abstract = {Recent work on interpretability in machine learning and AI has focused on the building of simplified models that approximate the true criteria used to make decisions. These models are a useful pedagogical device for teaching trained professionals how to predict what decisions will be made by the complex system, and most importantly how the system might break. However, when considering any such model it's important to remember Box's maxim that "All models are wrong but some are useful." We focus on the distinction between these models and explanations in philosophy and sociology. These models can be understood as a "do it yourself kit" for explanations, allowing a practitioner to directly answer "what if questions" or generate contrastive explanations without external assistance. Although a valuable ability, giving these models as explanations appears more difficult than necessary, and other forms of explanation may not have the same trade-offs. We contrast the different schools of thought on what makes an explanation, and suggest that machine learning might benefit from viewing the problem more broadly.},
	urldate = {2021-01-15},
	booktitle = {Proceedings of the {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {Association for Computing Machinery},
	author = {Mittelstadt, Brent and Russell, Chris and Wachter, Sandra},
	month = jan,
	year = {2019},
	keywords = {Accountability, Explanations, Interpretability, Philosophy of Science},
	pages = {279--288},
}

@inproceedings{dhurandhar_explanations_2018,
	title = {Explanations based on the {Missing}: {Towards} {Contrastive} {Explanations} with {Pertinent} {Negatives}},
	volume = {31},
	shorttitle = {Explanations based on the {Missing}},
	url = {https://papers.nips.cc/paper/2018/hash/c5ff2543b53f4cc0ad3819a36752467b-Abstract.html},
	language = {en},
	urldate = {2021-01-16},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Dhurandhar, Amit and Chen, Pin-Yu and Luss, Ronny and Tu, Chun-Chen and Ting, Paishun and Shanmugam, Karthikeyan and Das, Payel},
	year = {2018},
	pages = {592--603},
}

@inproceedings{lucic_why_2020,
	address = {New York, NY, USA},
	series = {{FAT}* '20},
	title = {Why does my model fail? contrastive local explanations for retail forecasting},
	isbn = {978-1-4503-6936-7},
	shorttitle = {Why does my model fail?},
	url = {https://doi.org/10.1145/3351095.3372824},
	doi = {10.1145/3351095.3372824},
	abstract = {In various business settings, there is an interest in using more complex machine learning techniques for sales forecasting. It is difficult to convince analysts, along with their superiors, to adopt these techniques since the models are considered to be "black boxes," even if they perform better than current models in use. We examine the impact of contrastive explanations about large errors on users' attitudes towards a "black-box" model. We propose an algorithm, Monte Carlo Bounds for Reasonable Predictions. Given a large error, MC-BRP determines (1) feature values that would result in a reasonable prediction, and (2) general trends between each feature and the target, both based on Monte Carlo simulations. We evaluate on a real dataset with real users by conducting a user study with 75 participants to determine if explanations generated by MC-BRP help users understand why a prediction results in a large error, and if this promotes trust in an automatically-learned model. Our study shows that users are able to answer objective questions about the model's predictions with overall 81.1\% accuracy when provided with these contrastive explanations. We show that users who saw MC-BRP explanations understand why the model makes large errors in predictions significantly more than users in the control group. We also conduct an in-depth analysis of the difference in attitudes between Practitioners and Researchers, and confirm that our results hold when conditioning on the users' background.},
	urldate = {2021-01-15},
	booktitle = {Proceedings of the 2020 {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {Association for Computing Machinery},
	author = {Lucic, Ana and Haned, Hinda and de Rijke, Maarten},
	month = jan,
	year = {2020},
	keywords = {erroneous predictions, explainability, interpretability},
	pages = {90--98},
}

@article{guidotti_survey_2018,
	title = {A {Survey} of {Methods} for {Explaining} {Black} {Box} {Models}},
	volume = {51},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/3236009},
	doi = {10.1145/3236009},
	abstract = {In recent years, many accurate decision support systems have been constructed as black boxes, that is as systems that hide their internal logic to the user. This lack of explanation constitutes both a practical and an ethical issue. The literature reports many approaches aimed at overcoming this crucial weakness, sometimes at the cost of sacrificing accuracy for interpretability. The applications in which black box decision systems can be used are various, and each approach is typically developed to provide a solution for a specific problem and, as a consequence, it explicitly or implicitly delineates its own definition of interpretability and explanation. The aim of this article is to provide a classification of the main problems addressed in the literature with respect to the notion of explanation and the type of black box system. Given a problem definition, a black box type, and a desired explanation, this survey should help the researcher to find the proposals more useful for his own work. The proposed classification of approaches to open black box models should also be useful for putting the many research open questions in perspective.},
	number = {5},
	urldate = {2021-01-16},
	journal = {ACM Computing Surveys},
	author = {Guidotti, Riccardo and Monreale, Anna and Ruggieri, Salvatore and Turini, Franco and Giannotti, Fosca and Pedreschi, Dino},
	month = aug,
	year = {2018},
	keywords = {Open the black box, explanations, interpretability, transparent models},
	pages = {93:1--93:42},
}

@inproceedings{wang_reinforcement_2018,
	title = {A {Reinforcement} {Learning} {Framework} for {Explainable} {Recommendation}},
	doi = {10.1109/ICDM.2018.00074},
	abstract = {Explainable recommendation, which provides explanations about why an item is recommended, has attracted increasing attention due to its ability in helping users make better decisions and increasing users' trust in the system. Existing explainable recommendation methods either ignore the working mechanism of the recommendation model or are designed for a specific recommendation model. Moreover, it is difficult for existing methods to ensure the presentation quality of the explanations (e.g., consistency). To solve these problems, we design a reinforcement learning framework for explainable recommendation. Our framework can explain any recommendation model (model-agnostic) and can flexibly control the explanation quality based on the application scenario. To demonstrate the effectiveness of our framework, we show how it can be used for generating sentence-level explanations. Specifically, we instantiate the explanation generator in the framework with a personalized-attention-based neural network. Offline experiments demonstrate that our method can well explain both collaborative filtering methods and deep-learning-based models. Evaluation with human subjects shows that the explanations generated by our method are significantly more useful than the explanations generated by the baselines.},
	booktitle = {2018 {IEEE} {International} {Conference} on {Data} {Mining} ({ICDM})},
	author = {Wang, X. and Chen, Y. and Yang, J. and Wu, L. and Wu, Z. and Xie, X.},
	month = nov,
	year = {2018},
	note = {ISSN: 2374-8486},
	keywords = {Collaboration, Explainable recommendation, reinforcement learning, personalized explanation, attention networks, Neural networks, Predictive models, Quality control, Recommender systems, Reinforcement learning, Transforms, collaborative filtering methods, deep-learning-based models, explainable recommendation methods, explanation generator, explanation quality, learning (artificial intelligence), neural nets, personalized-attention-based neural network, recommendation model, recommender systems, reinforcement learning framework, sentence-level explanations},
	pages = {587--596},
}

@article{battaglia_relational_2018,
	title = {Relational inductive biases, deep learning, and graph networks},
	url = {http://arxiv.org/abs/1806.01261},
	abstract = {Artificial intelligence (AI) has undergone a renaissance recently, making major progress in key domains such as vision, language, control, and decision-making. This has been due, in part, to cheap data and cheap compute resources, which have fit the natural strengths of deep learning. However, many defining characteristics of human intelligence, which developed under much different pressures, remain out of reach for current approaches. In particular, generalizing beyond one's experiences--a hallmark of human intelligence from infancy--remains a formidable challenge for modern AI. The following is part position paper, part review, and part unification. We argue that combinatorial generalization must be a top priority for AI to achieve human-like abilities, and that structured representations and computations are key to realizing this objective. Just as biology uses nature and nurture cooperatively, we reject the false choice between "hand-engineering" and "end-to-end" learning, and instead advocate for an approach which benefits from their complementary strengths. We explore how using relational inductive biases within deep learning architectures can facilitate learning about entities, relations, and rules for composing them. We present a new building block for the AI toolkit with a strong relational inductive bias--the graph network--which generalizes and extends various approaches for neural networks that operate on graphs, and provides a straightforward interface for manipulating structured knowledge and producing structured behaviors. We discuss how graph networks can support relational reasoning and combinatorial generalization, laying the foundation for more sophisticated, interpretable, and flexible patterns of reasoning. As a companion to this paper, we have released an open-source software library for building graph networks, with demonstrations of how to use them in practice.},
	urldate = {2021-01-16},
	journal = {arXiv:1806.01261 [cs, stat]},
	author = {Battaglia, Peter W. and Hamrick, Jessica B. and Bapst, Victor and Sanchez-Gonzalez, Alvaro and Zambaldi, Vinicius and Malinowski, Mateusz and Tacchetti, Andrea and Raposo, David and Santoro, Adam and Faulkner, Ryan and Gulcehre, Caglar and Song, Francis and Ballard, Andrew and Gilmer, Justin and Dahl, George and Vaswani, Ashish and Allen, Kelsey and Nash, Charles and Langston, Victoria and Dyer, Chris and Heess, Nicolas and Wierstra, Daan and Kohli, Pushmeet and Botvinick, Matt and Vinyals, Oriol and Li, Yujia and Pascanu, Razvan},
	month = oct,
	year = {2018},
	note = {arXiv: 1806.01261},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{barrett_measuring_2018,
	title = {Measuring abstract reasoning in neural networks},
	url = {http://proceedings.mlr.press/v80/barrett18a.html},
	abstract = {Whether neural networks can learn abstract reasoning or whether they merely rely on superficial statistics is a topic of recent debate. Here, we propose a dataset and challenge designed to probe ab...},
	language = {en},
	urldate = {2021-01-16},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Barrett, David and Hill, Felix and Santoro, Adam and Morcos, Ari and Lillicrap, Timothy},
	month = jul,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {511--520},
}

@inproceedings{paszke_automatic_2017,
	title = {Automatic differentiation in {PyTorch}},
	url = {https://openreview.net/forum?id=BJJsrmfCZ},
	abstract = {A summary of automatic differentiation techniques employed in PyTorch library, including novelties like support for in-place modification in presence of objects aliasing the same data, performance...},
	language = {en},
	urldate = {2021-01-16},
	booktitle = {31st {Conference} on {Neural} {Information} {Processing} {Systems} {Workshop}},
	author = {Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
	month = oct,
	year = {2017},
}

@article{baydin_automatic_2017,
	title = {Automatic differentiation in machine learning: a survey},
	volume = {18},
	issn = {1532-4435},
	shorttitle = {Automatic differentiation in machine learning},
	abstract = {Derivatives, mostly in the form of gradients and Hessians, are ubiquitous in machine learning. Automatic differentiation (AD), also called algorithmic differentiation or simply "auto-diff", is a family of techniques similar to but more general than backpropagation for efficiently and accurately evaluating derivatives of numeric functions expressed as computer programs. AD is a small but established field with applications in areas including computational uid dynamics, atmospheric sciences, and engineering design optimization. Until very recently, the fields of machine learning and AD have largely been unaware of each other and, in some cases, have independently discovered each other's results. Despite its relevance, general-purpose AD has been missing from the machine learning toolbox, a situation slowly changing with its ongoing adoption under the names "dynamic computational graphs" and "differentiable programming". We survey the intersection of AD and machine learning, cover applications where AD has direct relevance, and address the main implementation techniques. By precisely defining the main differentiation techniques and their interrelationships, we aim to bring clarity to the usage of the terms "autodiff", "automatic differentiation", and "symbolic differentiation" as these are encountered more and more in machine learning settings.},
	number = {1},
	journal = {The Journal of Machine Learning Research},
	author = {Baydin, Atılım Günes and Pearlmutter, Barak A. and Radul, Alexey Andreyevich and Siskind, Jeffrey Mark},
	month = jan,
	year = {2017},
	keywords = {backpropagation, differentiable programming},
	pages = {5595--5637},
}

@inproceedings{deng_imagenet_2009,
	title = {{ImageNet}: {A} large-scale hierarchical image database},
	shorttitle = {{ImageNet}},
	doi = {10.1109/CVPR.2009.5206848},
	abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
	booktitle = {2009 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Deng, J. and Dong, W. and Socher, R. and Li, L. and {Kai Li} and {Li Fei-Fei}},
	month = jun,
	year = {2009},
	note = {ISSN: 1063-6919},
	keywords = {Explosions, Image databases, Image retrieval, ImageNet database, Information retrieval, Internet, Large-scale systems, Multimedia databases, Ontologies, Robustness, Spine, computer vision, image resolution, image retrieval, large-scale hierarchical image database, large-scale ontology, multimedia computing, multimedia data, ontologies (artificial intelligence), subtree, trees (mathematics), very large databases, visual databases, wordNet structure},
	pages = {248--255},
}

@inproceedings{paszke_pytorch_2019,
	title = {{PyTorch}: {An} {Imperative} {Style}, {High}-{Performance} {Deep} {Learning} {Library}},
	volume = {32},
	shorttitle = {{PyTorch}},
	url = {https://papers.nips.cc/paper/2019/hash/bdbca288fee7f92f2bfa9f7012727740-Abstract.html},
	language = {en},
	urldate = {2021-01-16},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	year = {2019},
	pages = {8026--8037},
}

@inproceedings{abadi_tensorflow_2016,
	title = {{TensorFlow}: {A} {System} for {Large}-{Scale} {Machine} {Learning}},
	isbn = {978-1-931971-33-1},
	shorttitle = {{TensorFlow}},
	url = {https://www.usenix.org/conference/osdi16/technical-sessions/presentation/abadi},
	language = {en},
	urldate = {2021-01-16},
	author = {Abadi, Martin and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and Kudlur, Manjunath and Levenberg, Josh and Monga, Rajat and Moore, Sherry and Murray, Derek G. and Steiner, Benoit and Tucker, Paul and Vasudevan, Vijay and Warden, Pete and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
	year = {2016},
	pages = {265--283},
}

@inproceedings{tomsett_why_2018,
	title = {Why the {Failure}? {How} {Adversarial} {Examples} {Can} {Provide} {Insights} for {Interpretable} {Machine} {Learning}},
	shorttitle = {Why the {Failure}?},
	doi = {10.23919/ICIF.2018.8455710},
	abstract = {Recent advances in Machine Learning (ML) have profoundly changed many detection, classification, recognition and inference tasks. Given the complexity of the battlespace, ML has the potential to revolutionise how Coalition Situation Understanding is synthesised and revised. However, many issues must be overcome before its widespread adoption. In this paper we consider two - interpretability and adversarial attacks. Interpretability is needed because military decision-makers must be able to justify their decisions. Adversarial attacks arise because many ML algorithms are very sensitive to certain kinds of input perturbations. In this paper, we argue that these two issues are conceptually linked, and insights in one can provide insights in the other. We illustrate these ideas with relevant examples from the literature and our own experiments.},
	booktitle = {2018 21st {International} {Conference} on {Information} {Fusion} ({FUSION})},
	author = {Tomsett, R. and Widdicombe, A. and Xing, T. and Chakraborty, S. and Julier, S. and Gurram, P. and Rao, R. and Srivastava, M.},
	month = jul,
	year = {2018},
	keywords = {AI alignment, Coalition Situation Understanding, Data models, Internet, ML algorithms, Machine learning, Measurement, Sensors, Task analysis, Taxonomy, adversarial attacks, adversarial examples, adversarial machine learning, decision making, deep learning, explainable AI, inference mechanisms, inference tasks, internet of battlefield things, interpretability, interpretable machine learning, learning (artificial intelligence), military computing, military decision-makers},
	pages = {838--845},
}

@article{sangkloy_sketchy_2016,
	title = {The sketchy database: learning to retrieve badly drawn bunnies},
	volume = {35},
	issn = {0730-0301},
	shorttitle = {The sketchy database},
	url = {https://doi.org/10.1145/2897824.2925954},
	doi = {10.1145/2897824.2925954},
	abstract = {We present the Sketchy database, the first large-scale collection of sketch-photo pairs. We ask crowd workers to sketch particular photographic objects sampled from 125 categories and acquire 75,471 sketches of 12,500 objects. The Sketchy database gives us fine-grained associations between particular photos and sketches, and we use this to train cross-domain convolutional networks which embed sketches and photographs in a common feature space. We use our database as a benchmark for fine-grained retrieval and show that our learned representation significantly outperforms both hand-crafted features as well as deep features trained for sketch or photo classification. Beyond image retrieval, we believe the Sketchy database opens up new opportunities for sketch and image understanding and synthesis.},
	number = {4},
	urldate = {2021-01-16},
	journal = {ACM Transactions on Graphics},
	author = {Sangkloy, Patsorn and Burnell, Nathan and Ham, Cusuh and Hays, James},
	month = jul,
	year = {2016},
	keywords = {deep learning, image synthesis, siamese network, sketch-based image retrieval, triplet network},
	pages = {119:1--119:12},
}

@article{mytkowicz_producing_2009,
	title = {Producing wrong data without doing anything obviously wrong!},
	volume = {44},
	issn = {0362-1340},
	url = {https://doi.org/10.1145/1508284.1508275},
	doi = {10.1145/1508284.1508275},
	abstract = {This paper presents a surprising result: changing a seemingly innocuous aspect of an experimental setup can cause a systems researcher to draw wrong conclusions from an experiment. What appears to be an innocuous aspect in the experimental setup may in fact introduce a significant bias in an evaluation. This phenomenon is called measurement bias in the natural and social sciences. Our results demonstrate that measurement bias is significant and commonplace in computer system evaluation. By significant we mean that measurement bias can lead to a performance analysis that either over-states an effect or even yields an incorrect conclusion. By commonplace we mean that measurement bias occurs in all architectures that we tried (Pentium 4, Core 2, and m5 O3CPU), both compilers that we tried (gcc and Intel's C compiler), and most of the SPEC CPU2006 C programs. Thus, we cannot ignore measurement bias. Nevertheless, in a literature survey of 133 recent papers from ASPLOS, PACT, PLDI, and CGO, we determined that none of the papers with experimental results adequately consider measurement bias. Inspired by similar problems and their solutions in other sciences, we describe and demonstrate two methods, one for detecting (causal analysis) and one for avoiding (setup randomization) measurement bias.},
	number = {3},
	urldate = {2021-01-16},
	journal = {ACM SIGPLAN Notices},
	author = {Mytkowicz, Todd and Diwan, Amer and Hauswirth, Matthias and Sweeney, Peter F.},
	month = mar,
	year = {2009},
	keywords = {bias, measurement, performance},
	pages = {265--276},
}

@article{fleming_how_1986,
	title = {How not to lie with statistics: the correct way to summarize benchmark results},
	volume = {29},
	issn = {0001-0782},
	shorttitle = {How not to lie with statistics},
	url = {https://doi.org/10.1145/5666.5673},
	doi = {10.1145/5666.5673},
	abstract = {Using the arithmetic mean to summarize normalized benchmark results leads to mistaken conclusions that can be avoided by using the preferred method: the geometric mean.},
	number = {3},
	urldate = {2021-01-16},
	journal = {Communications of the ACM},
	author = {Fleming, Philip J. and Wallace, John J.},
	month = mar,
	year = {1986},
	pages = {218--221},
}

@inproceedings{hosseini_dropping_2019,
	title = {Dropping {Pixels} for {Adversarial} {Robustness}},
	url = {https://openaccess.thecvf.com/content_CVPRW_2019/html/CV-COPS/Hosseini_Dropping_Pixels_for_Adversarial_Robustness_CVPRW_2019_paper.html},
	urldate = {2021-01-16},
	author = {Hosseini, Hossein and Kannan, Sreeram and Poovendran, Radha},
	year = {2019},
	pages = {0--0},
}

@inproceedings{murdoch_beyond_2018,
	title = {Beyond {Word} {Importance}: {Contextual} {Decomposition} to {Extract} {Interactions} from {LSTMs}},
	shorttitle = {Beyond {Word} {Importance}},
	url = {https://openreview.net/forum?id=rkRwGg-0Z&utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=NLP%20News},
	abstract = {We introduce contextual decompositions, an interpretation algorithm for LSTMs capable of extracting word, phrase and interaction-level importance score},
	language = {en},
	urldate = {2021-01-16},
	author = {Murdoch, W. James and Liu, Peter J. and Yu, Bin},
	month = feb,
	year = {2018},
}

@inproceedings{elsayed_adversarial_2018,
	title = {Adversarial {Examples} that {Fool} both {Computer} {Vision} and {Time}-{Limited} {Humans}},
	volume = {31},
	url = {https://papers.nips.cc/paper/2018/hash/8562ae5e286544710b2e7ebe9858833b-Abstract.html},
	language = {en},
	urldate = {2021-01-16},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Elsayed, Gamaleldin and Shankar, Shreya and Cheung, Brian and Papernot, Nicolas and Kurakin, Alexey and Goodfellow, Ian and Sohl-Dickstein, Jascha},
	year = {2018},
	pages = {3910--3920},
}

@article{mohseni_multidisciplinary_2020,
	title = {A {Multidisciplinary} {Survey} and {Framework} for {Design} and {Evaluation} of {Explainable} {AI} {Systems}},
	url = {http://arxiv.org/abs/1811.11839},
	abstract = {The need for interpretable and accountable intelligent systems grows along with the prevalence of artificial intelligence applications used in everyday life. Explainable intelligent systems are designed to self-explain the reasoning behind system decisions and predictions, and researchers from different disciplines work together to define, design, and evaluate interpretable systems. However, scholars from different disciplines focus on different objectives and fairly independent topics of interpretable machine learning research, which poses challenges for identifying appropriate design and evaluation methodology and consolidating knowledge across efforts. To this end, this paper presents a survey and framework intended to share knowledge and experiences of XAI design and evaluation methods across multiple disciplines. Aiming to support diverse design goals and evaluation methods in XAI research, after a thorough review of XAI related papers in the fields of machine learning, visualization, and human-computer interaction, we present a categorization of interpretable machine learning design goals and evaluation methods to show a mapping between design goals for different XAI user groups and their evaluation methods. From our findings, we develop a framework with step-by-step design guidelines paired with evaluation methods to close the iterative design and evaluation cycles in multidisciplinary XAI teams. Further, we provide summarized ready-to-use tables of evaluation methods and recommendations for different goals in XAI research.},
	urldate = {2021-01-16},
	journal = {arXiv:1811.11839 [cs]},
	author = {Mohseni, Sina and Zarei, Niloofar and Ragan, Eric D.},
	month = aug,
	year = {2020},
	note = {arXiv: 1811.11839},
	keywords = {Computer Science - Human-Computer Interaction},
}

@article{zhang_visual_2018,
	title = {Visual interpretability for deep learning: a survey},
	volume = {19},
	issn = {2095-9230},
	shorttitle = {Visual interpretability for deep learning},
	url = {https://doi.org/10.1631/FITEE.1700808},
	doi = {10.1631/FITEE.1700808},
	abstract = {This paper reviews recent studies in understanding neural-network representations and learning neural networks with interpretable/disentangled middle-layer representations. Although deep neural networks have exhibited superior performance in various tasks, interpretability is always Achilles’ heel of deep neural networks. At present, deep neural networks obtain high discrimination power at the cost of a low interpretability of their black-box representations. We believe that high model interpretability may help people break several bottlenecks of deep learning, e.g., learning from a few annotations, learning via human–computer communications at the semantic level, and semantically debugging network representations. We focus on convolutional neural networks (CNNs), and revisit the visualization of CNN representations, methods of diagnosing representations of pre-trained CNNs, approaches for disentangling pre-trained CNN representations, learning of CNNs with disentangled representations, and middle-to-end learning based on model interpretability. Finally, we discuss prospective trends in explainable artificial intelligence.},
	language = {en},
	number = {1},
	urldate = {2021-01-16},
	journal = {Frontiers of Information Technology \& Electronic Engineering},
	author = {Zhang, Quan-shi and Zhu, Song-chun},
	month = jan,
	year = {2018},
	pages = {27--39},
}

@inproceedings{wang_residual_2017,
	title = {Residual {Attention} {Network} for {Image} {Classification}},
	url = {https://openaccess.thecvf.com/content_cvpr_2017/html/Wang_Residual_Attention_Network_CVPR_2017_paper.html},
	urldate = {2021-01-16},
	author = {Wang, Fei and Jiang, Mengqing and Qian, Chen and Yang, Shuo and Li, Cheng and Zhang, Honggang and Wang, Xiaogang and Tang, Xiaoou},
	year = {2017},
	pages = {3156--3164},
}

@inproceedings{dabkowski_real_2017,
	title = {Real {Time} {Image} {Saliency} for {Black} {Box} {Classifiers}},
	volume = {30},
	url = {https://papers.nips.cc/paper/2017/hash/0060ef47b12160b9198302ebdb144dcf-Abstract.html},
	language = {en},
	urldate = {2021-01-03},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Dabkowski, Piotr and Gal, Yarin},
	year = {2017},
	pages = {6967--6976},
}

@inproceedings{sutskever_importance_2013,
	title = {On the importance of initialization and momentum in deep learning},
	url = {http://proceedings.mlr.press/v28/sutskever13.html},
	abstract = {Deep and recurrent neural networks (DNNs and RNNs respectively) are powerful models that were considered to be almost impossible to train using stochastic gradient descent with momentum. In this pa...},
	language = {en},
	urldate = {2021-01-16},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
	month = may,
	year = {2013},
	note = {ISSN: 1938-7228},
	pages = {1139--1147},
}

@inproceedings{keskar_large-batch_2016,
	title = {On {Large}-{Batch} {Training} for {Deep} {Learning}: {Generalization} {Gap} and {Sharp} {Minima}},
	shorttitle = {On {Large}-{Batch} {Training} for {Deep} {Learning}},
	url = {https://openreview.net/forum?id=H1oyRlYgg},
	abstract = {We present numerical evidence for the argument that if deep networks are trained using large (mini-)batches, they converge to sharp minimizers, and these minimizers have poor generalization...},
	language = {en},
	urldate = {2021-01-16},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
	month = nov,
	year = {2016},
}

@article{zhang_interpreting_2021,
	title = {Interpreting and {Improving} {Adversarial} {Robustness} of {Deep} {Neural} {Networks} {With} {Neuron} {Sensitivity}},
	volume = {30},
	issn = {1941-0042},
	doi = {10.1109/TIP.2020.3042083},
	abstract = {Deep neural networks (DNNs) are vulnerable to adversarial examples where inputs with imperceptible perturbations mislead DNNs to incorrect results. Despite the potential risk they bring, adversarial examples are also valuable for providing insights into the weakness and blind-spots of DNNs. Thus, the interpretability of a DNN in the adversarial setting aims to explain the rationale behind its decision-making process and makes deeper understanding which results in better practical applications. To address this issue, we try to explain adversarial robustness for deep models from a new perspective of neuron sensitivity which is measured by neuron behavior variation intensity against benign and adversarial examples. In this paper, we first draw the close connection between adversarial robustness and neuron sensitivities, as sensitive neurons make the most non-trivial contributions to model predictions in the adversarial setting. Based on that, we further propose to improve adversarial robustness by stabilizing the behaviors of sensitive neurons. Moreover, we demonstrate that state-of-the-art adversarial training methods improve model robustness by reducing neuron sensitivities, which in turn confirms the strong connections between adversarial robustness and neuron sensitivity. Extensive experiments on various datasets demonstrate that our algorithm effectively achieves excellent results. To the best of our knowledge, we are the first to study adversarial robustness using neuron sensitivities.},
	journal = {IEEE Transactions on Image Processing},
	author = {Zhang, C. and Liu, A. and Liu, X. and Xu, Y. and Yu, H. and Ma, Y. and Li, T.},
	year = {2021},
	note = {Conference Name: IEEE Transactions on Image Processing},
	keywords = {Analytical models, Computational modeling, Deep learning, Model interpretation, Neurons, Robustness, Sensitivity, Training, adversarial examples, neuron sensitivity},
	pages = {1291--1304},
}

@book{molnar_interpretable_2020,
	title = {Interpretable {Machine} {Learning}},
	isbn = {978-0-244-76852-2},
	abstract = {This book is about making machine learning models and their decisions interpretable. After exploring the concepts of interpretability, you will learn about simple, interpretable models such as decision trees, decision rules and linear regression. Later chapters focus on general model-agnostic methods for interpreting black box models like feature importance and accumulated local effects and explaining individual predictions with Shapley values and LIME. All interpretation methods are explained in depth and discussed critically. How do they work under the hood? What are their strengths and weaknesses? How can their outputs be interpreted? This book will enable you to select and correctly apply the interpretation method that is most suitable for your machine learning project.},
	language = {en},
	publisher = {Lulu.com},
	author = {Molnar, Christoph},
	month = feb,
	year = {2020},
	note = {Google-Books-ID: jBm3DwAAQBAJ},
}

@inproceedings{kim_interpretable_2017,
	title = {Interpretable {Learning} for {Self}-{Driving} {Cars} by {Visualizing} {Causal} {Attention}},
	url = {https://openaccess.thecvf.com/content_iccv_2017/html/Kim_Interpretable_Learning_for_ICCV_2017_paper.html},
	urldate = {2021-01-16},
	author = {Kim, Jinkyu and Canny, John},
	year = {2017},
	pages = {2942--2950},
}

@article{britton_vine_2019,
	title = {{VINE}: {Visualizing} {Statistical} {Interactions} in {Black} {Box} {Models}},
	shorttitle = {{VINE}},
	url = {http://arxiv.org/abs/1904.00561},
	abstract = {As machine learning becomes more pervasive, there is an urgent need for interpretable explanations of predictive models. Prior work has developed effective methods for visualizing global model behavior, as well as generating local (instance-specific) explanations. However, relatively little work has addressed regional explanations - how groups of similar instances behave in a complex model, and the related issue of visualizing statistical feature interactions. The lack of utilities available for these analytical needs hinders the development of models that are mission-critical, transparent, and align with social goals. We present VINE (Visual INteraction Effects), a novel algorithm to extract and visualize statistical interaction effects in black box models. We also present a novel evaluation metric for visualizations in the interpretable ML space.},
	urldate = {2021-01-16},
	journal = {arXiv:1904.00561 [cs, stat]},
	author = {Britton, Matthew},
	month = apr,
	year = {2019},
	note = {arXiv: 1904.00561},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{chen_hopskipjumpattack_2020,
	title = {{HopSkipJumpAttack}: {A} {Query}-{Efficient} {Decision}-{Based} {Attack}},
	shorttitle = {{HopSkipJumpAttack}},
	doi = {10.1109/SP40000.2020.00045},
	abstract = {The goal of a decision-based adversarial attack on a trained model is to generate adversarial examples based solely on observing output labels returned by the targeted model. We develop HopSkipJumpAttack, a family of algorithms based on a novel estimate of the gradient direction using binary information at the decision boundary. The proposed family includes both untargeted and targeted attacks optimized for ℓ and ℓ∞ similarity metrics respectively. Theoretical analysis is provided for the proposed algorithms and the gradient direction estimate. Experiments show HopSkipJumpAttack requires significantly fewer model queries than several state-of-the-art decision-based adversarial attacks. It also achieves competitive performance in attacking several widely-used defense mechanisms.},
	booktitle = {2020 {IEEE} {Symposium} on {Security} and {Privacy} ({SP})},
	author = {Chen, J. and Jordan, M. I. and Wainwright, M. J.},
	month = may,
	year = {2020},
	note = {ISSN: 2375-1207},
	keywords = {Estimation, HopSkipJumpAttack, Iterative methods, Measurement, Neural networks, Optimization, Perturbation methods, Predictive models, adversarial examples, binary information, decision boundary, decision-based adversarial attacks, gradient direction estimate, inference mechanisms, learning (artificial intelligence), model queries, optimisation, output labels, query processing, query-efficient decision-based attack, security of data, target tracking, targeted model, trained model, untargeted attacks, ℓ similarity metrics, ℓ∞ similarity metrics},
	pages = {1277--1294},
}

@inproceedings{benz_revisiting_2021,
	title = {Revisiting {Batch} {Normalization} for {Improving} {Corruption} {Robustness}},
	url = {https://openaccess.thecvf.com/content/WACV2021/html/Benz_Revisiting_Batch_Normalization_for_Improving_Corruption_Robustness_WACV_2021_paper.html},
	language = {en},
	urldate = {2021-01-15},
	author = {Benz, Philipp and Zhang, Chaoning and Karjauv, Adil and Kweon, In So},
	year = {2021},
	pages = {494--503},
}

@article{geirhos_shortcut_2020,
	title = {Shortcut learning in deep neural networks},
	volume = {2},
	copyright = {2020 Springer Nature Limited},
	issn = {2522-5839},
	url = {https://www.nature.com/articles/s42256-020-00257-z},
	doi = {10.1038/s42256-020-00257-z},
	abstract = {Deep learning has triggered the current rise of artificial intelligence and is the workhorse of today’s machine intelligence. Numerous success stories have rapidly spread all over science, industry and society, but its limitations have only recently come into focus. In this Perspective we seek to distil how many of deep learning’s failures can be seen as different symptoms of the same underlying problem: shortcut learning. Shortcuts are decision rules that perform well on standard benchmarks but fail to transfer to more challenging testing conditions, such as real-world scenarios. Related issues are known in comparative psychology, education and linguistics, suggesting that shortcut learning may be a common characteristic of learning systems, biological and artificial alike. Based on these observations, we develop a set of recommendations for model interpretation and benchmarking, highlighting recent advances in machine learning to improve robustness and transferability from the lab to real-world applications.},
	language = {en},
	number = {11},
	urldate = {2021-01-14},
	journal = {Nature Machine Intelligence},
	author = {Geirhos, Robert and Jacobsen, Jörn-Henrik and Michaelis, Claudio and Zemel, Richard and Brendel, Wieland and Bethge, Matthias and Wichmann, Felix A.},
	month = nov,
	year = {2020},
	note = {Number: 11
Publisher: Nature Publishing Group},
	pages = {665--673},
}

@inproceedings{brendel_accurate_2019,
	title = {Accurate, reliable and fast robustness evaluation},
	abstract = {Throughout the past ﬁve years, the susceptibility of neural networks to minimal adversarial perturbations has moved from a peculiar phenomenon to a core issue in Deep Learning. Despite much attention, however, progress towards more robust models is signiﬁcantly impaired by the difﬁculty of evaluating the robustness of neural network models. Today’s methods are either fast but brittle (gradient-based attacks), or they are fairly reliable but slow (score- and decision-based attacks). We here develop a new set of gradient-based adversarial attacks which (a) are more reliable in the face of gradient-masking than other gradient-based attacks, (b) perform better and are more query efﬁcient than current state-of-the-art gradientbased attacks, (c) can be ﬂexibly adapted to a wide range of adversarial criteria and (d) require virtually no hyperparameter tuning. These ﬁndings are carefully validated across a diverse set of six different models and hold for L0, L1, L2 and L∞ in both targeted as well as untargeted scenarios. Implementations will soon be available in all major toolboxes (Foolbox, CleverHans and ART). We hope that this class of attacks will make robustness evaluations easier and more reliable, thus contributing to more signal in the search for more robust machine learning models.},
	language = {en},
	booktitle = {33rd {Conference} on {Neural} {Information} {Processing} {Systems}},
	author = {Brendel, Wieland and Rauber, Jonas and Kümmerer, Matthias and Ustyuzhaninov, Ivan and Bethge, Matthias},
	year = {2019},
	pages = {11},
}

@techreport{krizhevsky_learning_2009,
	title = {Learning multiple layers of features from tiny images},
	abstract = {Groups at MIT and NYU have collected a dataset of millions of tiny colour images from the web. It is, in principle, an excellent dataset for unsupervised training of deep generative models, but previous researchers who have tried this have found it difficult to learn a good set of filters from the images. We show how to train a multi-layer generative model that learns to extract meaningful features which resemble those found in the human visual cortex. Using a novel parallelization algorithm to distribute the work among multiple machines connected on a network, we show how training such a model can be done in reasonable time. A second problematic aspect of the tiny images dataset is that there are no reliable class labels which makes it hard to use for object recognition experiments. We created two sets of reliable labels. The CIFAR-10 set has 6000 examples of each of 10 classes and the CIFAR-100 set has 600 examples of each of 100 non-overlapping classes. Using these labels, we show that object recognition is significantly},
	author = {Krizhevsky, Alex},
	year = {2009},
}

@inproceedings{netzer_reading_2011,
	title = {Reading {Digits} in {Natural} {Images} with {Unsupervised} {Feature} {Learning}},
	url = {http://ufldl.stanford.edu/housenumbers/nips2011_housenumbers.pdf},
	urldate = {2021-01-04},
	booktitle = {{NIPS} {Workshop} on {Deep} {Learning} and {Unsupervised} {Feature} {Learning} 2011},
	author = {Netzer, Yuval and Wang, Tao and Coates, Adam and Bissacco, Alessandro and Wu, Bo and Ng, Andrew Y.},
	year = {2011},
}

@article{lecun_gradient-based_1998,
	title = {Gradient-based learning applied to document recognition},
	volume = {86},
	issn = {1558-2256},
	doi = {10.1109/5.726791},
	abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.},
	number = {11},
	journal = {Proceedings of the IEEE},
	author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
	month = nov,
	year = {1998},
	note = {Conference Name: Proceedings of the IEEE},
	keywords = {2D shape variability, Character recognition, Feature extraction, GTN, Hidden Markov models, Machine learning, Multi-layer neural network, Neural networks, Optical character recognition software, Optical computing, Pattern recognition, Principal component analysis, back-propagation, backpropagation, cheque reading, complex decision surface synthesis, convolution, convolutional neural network character recognizers, document recognition, document recognition systems, field extraction, gradient based learning technique, gradient-based learning, graph transformer networks, handwritten character recognition, handwritten digit recognition task, high-dimensional patterns, language modeling, multilayer neural networks, multilayer perceptrons, multimodule systems, optical character recognition, performance measure minimization, segmentation recognition},
	pages = {2278--2324},
}

@inproceedings{boopathy_proper_2020,
	title = {Proper {Network} {Interpretability} {Helps} {Adversarial} {Robustness} in {Classiﬁcation}},
	abstract = {Recent works have empirically shown that there exist adversarial examples that can be hidden from neural network interpretability (namely, making network interpretation maps visually similar), or interpretability is itself susceptible to adversarial attacks. In this paper, we theoretically show that with a proper measurement of interpretation, it is actually difﬁcult to prevent prediction-evasion adversarial attacks from causing interpretation discrepancy, as conﬁrmed by experiments on MNIST, CIFAR-10 and Restricted ImageNet. Spurred by that, we develop an interpretability-aware defensive scheme built only on promoting robust interpretation (without the need for resorting to adversarial loss minimization). We show that our defense achieves both robust classiﬁcation and robust interpretation, outperforming state-of-theart adversarial training methods against attacks of large perturbation in particular.},
	language = {en},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	author = {Boopathy, Akhilan and Liu, Sijia and Zhang, Gaoyuan and Liu, Cynthia and Chen, Pin-Yu and Chang, Shiyu and Daniel, Luca},
	year = {2020},
	pages = {10},
}

@article{samek_evaluating_2017,
	title = {Evaluating the {Visualization} of {What} a {Deep} {Neural} {Network} {Has} {Learned}},
	volume = {28},
	issn = {2162-2388},
	doi = {10.1109/TNNLS.2016.2599820},
	abstract = {Deep neural networks (DNNs) have demonstrated impressive performance in complex machine learning tasks such as image classification or speech recognition. However, due to their multilayer nonlinear structure, they are not transparent, i.e., it is hard to grasp what makes them arrive at a particular classification or recognition decision, given a new unseen data sample. Recently, several approaches have been proposed enabling one to understand and interpret the reasoning embodied in a DNN for a single test image. These methods quantify the “importance” of individual pixels with respect to the classification decision and allow a visualization in terms of a heatmap in pixel/input space. While the usefulness of heatmaps can be judged subjectively by a human, an objective quality measure is missing. In this paper, we present a general methodology based on region perturbation for evaluating ordered collections of pixels such as heatmaps. We compare heatmaps computed by three different methods on the SUN397, ILSVRC2012, and MIT Places data sets. Our main result is that the recently proposed layer-wise relevance propagation algorithm qualitatively and quantitatively provides a better explanation of what made a DNN arrive at a particular classification decision than the sensitivity-based approach or the deconvolution method. We provide theoretical arguments to explain this result and discuss its practical implications. Finally, we investigate the use of heatmaps for unsupervised assessment of the neural network performance.},
	number = {11},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Samek, W. and Binder, A. and Montavon, G. and Lapuschkin, S. and Müller, K.},
	month = nov,
	year = {2017},
	note = {Conference Name: IEEE Transactions on Neural Networks and Learning Systems},
	keywords = {Algorithm design and analysis, Biological neural networks, Convolutional neural networks, DNN, Deconvolution, Heating, ILSVRC2012, Learning systems, MIT Places data sets, Neurons, SUN397, Sensitivity, complex machine learning tasks, data visualisation, data visualization, deconvolution method, deep neural network, explaining classification, heatmap, image classification, interpretable machine learning, learning (artificial intelligence), multilayer nonlinear structure, neural nets, relevance models, relevance propagation algorithm, sensitivity-based approach},
	pages = {2660--2673},
}

@inproceedings{hanson_minkowski-r_1988,
	title = {Minkowski-r {Back}-{Propagation}: {Learning} in {Connectionist} {Models} with {Non}-{Euclidian} {Error} {Signals}},
	shorttitle = {Minkowski-r {Back}-{Propagation}},
	url = {https://proceedings.neurips.cc/paper/1987/file/fc490ca45c00b1249bbe3554a4fdf6fb-Paper.pdf},
	urldate = {2021-01-01},
	booktitle = {Neural {Information} {Processing} {Systems}},
	publisher = {American Institute of Physics},
	author = {Hanson, Stephen and Burr, David},
	editor = {Anderson, D.},
	year = {1988},
	pages = {348--357},
}

@article{wachter_counterfactual_2017,
	title = {Counterfactual {Explanations} without {Opening} the {Black} {Box}: {Automated} {Decisions} and the {GDPR}},
	volume = {31},
	shorttitle = {Counterfactual {Explanations} without {Opening} the {Black} {Box}},
	url = {https://heinonline.org/HOL/P?h=hein.journals/hjlt31&i=860},
	language = {eng},
	number = {2},
	urldate = {2020-12-31},
	journal = {Harvard Journal of Law \& Technology (Harvard JOLT)},
	author = {Wachter, Sandra and Mittelstadt, Brent and Russell, Chris},
	year = {2017},
	pages = {841--888},
}

@inproceedings{lin_microsoft_2014,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Microsoft {COCO}: {Common} {Objects} in {Context}},
	isbn = {978-3-319-10602-1},
	shorttitle = {Microsoft {COCO}},
	doi = {10.1007/978-3-319-10602-1_48},
	abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2014},
	publisher = {Springer International Publishing},
	author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Dollár, Piotr and Zitnick, C. Lawrence},
	editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
	year = {2014},
	keywords = {Common Object, Object Category, Object Detection, Object Instance, Scene Understanding},
	pages = {740--755},
}

@inproceedings{tao_attacks_2018,
	title = {Attacks {Meet} {Interpretability}: {Attribute}-steered {Detection} of {Adversarial} {Samples}},
	volume = {31},
	shorttitle = {Attacks {Meet} {Interpretability}},
	url = {https://papers.nips.cc/paper/2018/hash/b994697479c5716eda77e8e9713e5f0f-Abstract.html},
	language = {en},
	urldate = {2020-12-21},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Tao, Guanhong and Ma, Shiqing and Liu, Yingqi and Zhang, Xiangyu},
	year = {2018},
	pages = {7717--7728},
}

@inproceedings{chen_detect_2014,
	title = {Detect {What} {You} {Can}: {Detecting} and {Representing} {Objects} using {Holistic} {Models} and {Body} {Parts}},
	shorttitle = {Detect {What} {You} {Can}},
	url = {https://openaccess.thecvf.com/content_cvpr_2014/html/Chen_Detect_What_You_2014_CVPR_paper.html},
	urldate = {2020-12-29},
	author = {Chen, Xianjie and Mottaghi, Roozbeh and Liu, Xiaobai and Fidler, Sanja and Urtasun, Raquel and Yuille, Alan},
	year = {2014},
	pages = {1971--1978},
}

@inproceedings{ghorbani_interpretation_2019,
	title = {Interpretation of {Neural} {Networks} {Is} {Fragile}},
	volume = {33},
	copyright = {Copyright (c) 2019 Association for the Advancement of Artificial Intelligence},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/4252},
	doi = {10.1609/aaai.v33i01.33013681},
	language = {en},
	urldate = {2020-12-29},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Ghorbani, Amirata and Abid, Abubakar and Zou, James},
	month = jul,
	year = {2019},
	note = {Number: 01},
	pages = {3681--3688},
}

@article{antunes_structuring_2008,
	title = {Structuring dimensions for collaborative systems evaluation},
	volume = {44},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/2089125.2089128},
	doi = {10.1145/2089125.2089128},
	abstract = {Collaborative systems evaluation is always necessary to determine the impact a solution will have on the individuals, groups, and the organization. Several methods of evaluation have been proposed. These methods comprise a variety of approaches with various goals. Thus, the need for a strategy to select the most appropriate method for a specific case is clear. This research work presents a detailed framework to evaluate collaborative systems according to given variables and performance levels. The proposal assumes that evaluation is an evolving process during the system lifecycle. Therefore, the framework, illustrated with two examples, is complemented with a collection of guidelines to evaluate collaborative systems according to product development status.},
	number = {2},
	urldate = {2020-12-29},
	journal = {ACM Computing Surveys},
	author = {Antunes, Pedro and Herskovic, Valeria and Ochoa, Sergio F. and Pino, Jose A.},
	month = mar,
	year = {2008},
	keywords = {Collaborative systems evaluation, evaluation dimensions, evaluation guidelines, human-computer interaction, interaction assessment},
	pages = {8:1--8:28},
}

@inproceedings{hu_harnessing_2016,
	address = {Berlin, Germany},
	title = {Harnessing {Deep} {Neural} {Networks} with {Logic} {Rules}},
	url = {https://www.aclweb.org/anthology/P16-1228},
	doi = {10.18653/v1/P16-1228},
	urldate = {2020-12-28},
	booktitle = {Proceedings of the 54th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Hu, Zhiting and Ma, Xuezhe and Liu, Zhengzhong and Hovy, Eduard and Xing, Eric},
	month = aug,
	year = {2016},
	pages = {2410--2420},
}

@inproceedings{jin_collaborative_2016,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Collaborative {Layer}-{Wise} {Discriminative} {Learning} in {Deep} {Neural} {Networks}},
	isbn = {978-3-319-46478-7},
	doi = {10.1007/978-3-319-46478-7_45},
	abstract = {Intermediate features at different layers of a deep neural network are known to be discriminative for visual patterns of different complexities. However, most existing works ignore such cross-layer heterogeneities when classifying samples of different complexities. For example, if a training sample has already been correctly classified at a specific layer with high confidence, we argue that it is unnecessary to enforce rest layers to classify this sample correctly and a better strategy is to encourage those layers to focus on other samples.In this paper, we propose a layer-wise discriminative learning method to enhance the discriminative capability of a deep network by allowing its layers to work collaboratively for classification. Towards this target, we introduce multiple classifiers on top of multiple layers. Each classifier not only tries to correctly classify the features from its input layer, but also coordinates with other classifiers to jointly maximize the final classification performance. Guided by the other companion classifiers, each classifier learns to concentrate on certain training examples and boosts the overall performance. Allowing for end-to-end training, our method can be conveniently embedded into state-of-the-art deep networks. Experiments with multiple popular deep networks, including Network in Network, GoogLeNet and VGGNet, on scale-various object classification benchmarks, including CIFAR100, MNIST and ImageNet, and scene classification benchmarks, including MIT67, SUN397 and Places205, demonstrate the effectiveness of our method. In addition, we also analyze the relationship between the proposed method and classical conditional random fields models.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2016},
	publisher = {Springer International Publishing},
	author = {Jin, Xiaojie and Chen, Yunpeng and Dong, Jian and Feng, Jiashi and Yan, Shuicheng},
	editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
	year = {2016},
	keywords = {Companion Classifier, Conditional Random Field, Deep Neural Network, Hide Layer, Prediction Score},
	pages = {733--749},
}

@inproceedings{agrawal_analyzing_2014,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Analyzing the {Performance} of {Multilayer} {Neural} {Networks} for {Object} {Recognition}},
	isbn = {978-3-319-10584-0},
	doi = {10.1007/978-3-319-10584-0_22},
	abstract = {In the last two years, convolutional neural networks (CNNs) have achieved an impressive suite of results on standard recognition datasets and tasks. CNN-based features seem poised to quickly replace engineered representations, such as SIFT and HOG. However, compared to SIFT and HOG, we understand much less about the nature of the features learned by large CNNs. In this paper, we experimentally probe several aspects of CNN feature learning in an attempt to help practitioners gain useful, evidence-backed intuitions about how to apply CNNs to computer vision problems.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2014},
	publisher = {Springer International Publishing},
	author = {Agrawal, Pulkit and Girshick, Ross and Malik, Jitendra},
	editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
	year = {2014},
	keywords = {convolutional neural networks, empirical analysis, object recognition},
	pages = {329--344},
}

@article{hurley_comparing_2009,
	title = {Comparing {Measures} of {Sparsity}},
	volume = {55},
	issn = {1557-9654},
	doi = {10.1109/TIT.2009.2027527},
	abstract = {Sparsity of representations of signals has been shown to be a key concept of fundamental importance in fields such as blind source separation, compression, sampling and signal analysis. The aim of this paper is to compare several commonly-used sparsity measures based on intuitive attributes. Intuitively, a sparse representation is one in which a small number of coefficients contain a large proportion of the energy. In this paper, six properties are discussed: (Robin Hood, Scaling, Rising Tide, Cloning, Bill Gates, and Babies), each of which a sparsity measure should have. The main contributions of this paper are the proofs and the associated summary table which classify commonly-used sparsity measures based on whether or not they satisfy these six propositions. Only two of these measures satisfy all six: the pq-mean with p les 1, q {\textgreater} 1 and the Gini index.},
	number = {10},
	journal = {IEEE Transactions on Information Theory},
	author = {Hurley, N. and Rickard, S.},
	month = oct,
	year = {2009},
	note = {Conference Name: IEEE Transactions on Information Theory},
	keywords = {Adaptive signal processing, Blind source separation, Cloning, Gini index, Image coding, Machine learning, Measures of sparsity, Sampling methods, Sea measurements, Signal analysis, Source separation, Tides, blind source separation, compression analysis, information theory, measuring sparsity, sampling analysis, signal analysis, sparse distribution, sparse representation, sparsity, sparsity measures},
	pages = {4723--4741},
}

@misc{noauthor_inceptionism_nodate,
	title = {Inceptionism: {Going} {Deeper} into {Neural} {Networks}},
	shorttitle = {Inceptionism},
	url = {http://ai.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html},
	abstract = {Posted by Alexander Mordvintsev, Software Engineer, Christopher Olah, Software Engineering Intern and Mike Tyka, Software Engineer Update - ...},
	language = {en},
	urldate = {2020-12-28},
	journal = {Google AI Blog},
}

@article{lavrac_selected_1999,
	series = {Data {Mining} {Techniques} and {Applications} in {Medicine}},
	title = {Selected techniques for data mining in medicine},
	volume = {16},
	issn = {0933-3657},
	url = {http://www.sciencedirect.com/science/article/pii/S0933365798000621},
	doi = {10.1016/S0933-3657(98)00062-1},
	abstract = {Widespread use of medical information systems and explosive growth of medical databases require traditional manual data analysis to be coupled with methods for efficient computer-assisted analysis. This paper presents selected data mining techniques that can be applied in medicine, and in particular some machine learning techniques including the mechanisms that make them better suited for the analysis of medical databases (derivation of symbolic rules, use of background knowledge, sensitivity and specificity of induced descriptions). The importance of the interpretability of results of data analysis is discussed and illustrated on selected medical applications.},
	language = {en},
	number = {1},
	urldate = {2020-12-28},
	journal = {Artificial Intelligence in Medicine},
	author = {Lavrač, Nada},
	month = may,
	year = {1999},
	keywords = {Data mining, Machine learning, Medical applications},
	pages = {3--23},
}

@inproceedings{altendorf_learning_2005,
	address = {Arlington, Virginia, USA},
	series = {{UAI}'05},
	title = {Learning from sparse data by exploiting monotonicity constraints},
	isbn = {978-0-9749039-1-0},
	abstract = {When training data is sparse, more domain knowledge must be incorporated into the learning algorithm in order to reduce the effective size of the hypothesis space. This paper builds on previous work in which knowledge about qualitative monotonicities was formally represented and incorporated into learning algorithms (e.g., Clark \& Matwin's work with the CN2 rule learning algorithm). We show how to interpret knowledge of qualitative influences, and in particular of monotonicities, as constraints on probability distributions, and to incorporate this knowledge into Bayesian network learning algorithms. We show that this yields improved accuracy, particularly with very small training sets (e.g. less than 10 examples).},
	urldate = {2020-12-27},
	booktitle = {Proceedings of the {Twenty}-{First} {Conference} on {Uncertainty} in {Artificial} {Intelligence}},
	publisher = {AUAI Press},
	author = {Altendorf, Eric E. and Restificar, Angelo C. and Dietterich, Thomas G.},
	month = jul,
	year = {2005},
	pages = {18--26},
}

@inproceedings{wang_interpret_2018,
	title = {Interpret {Neural} {Networks} by {Identifying} {Critical} {Data} {Routing} {Paths}},
	url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Interpret_Neural_Networks_CVPR_2018_paper.html},
	urldate = {2020-12-27},
	author = {Wang, Yulong and Su, Hang and Zhang, Bo and Hu, Xiaolin},
	year = {2018},
	pages = {8906--8914},
}

@inproceedings{wang_learning_2018,
	title = {Learning a {Discriminative} {Filter} {Bank} {Within} a {CNN} for {Fine}-{Grained} {Recognition}},
	url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Learning_a_Discriminative_CVPR_2018_paper.html},
	urldate = {2020-12-27},
	author = {Wang, Yaming and Morariu, Vlad I. and Davis, Larry S.},
	year = {2018},
	pages = {4148--4157},
}

@article{freitas_comprehensible_2014,
	title = {Comprehensible classification models: a position paper},
	volume = {15},
	issn = {1931-0145},
	shorttitle = {Comprehensible classification models},
	url = {https://doi.org/10.1145/2594473.2594475},
	doi = {10.1145/2594473.2594475},
	abstract = {The vast majority of the literature evaluates the performance of classification models using only the criterion of predictive accuracy. This paper reviews the case for considering also the comprehensibility (interpretability) of classification models, and discusses the interpretability of five types of classification models, namely decision trees, classification rules, decision tables, nearest neighbors and Bayesian network classifiers. We discuss both interpretability issues which are specific to each of those model types and more generic interpretability issues, namely the drawbacks of using model size as the only criterion to evaluate the comprehensibility of a model, and the use of monotonicity constraints to improve the comprehensibility and acceptance of classification models by users.},
	number = {1},
	urldate = {2020-12-27},
	journal = {ACM SIGKDD Explorations Newsletter},
	author = {Freitas, Alex A.},
	month = mar,
	year = {2014},
	keywords = {Bayesian network classifiers, decision table, decision tree, monotonicity constraint, nearest neighbors, rule induction},
	pages = {1--10},
}

@inproceedings{ross_neural_2017,
	title = {The {Neural} {LASSO}: {Local} {Linear} {Sparsity} for {Interpretable} {Explanations}},
	abstract = {Neural networks often perform better on prediction problems than simpler classes of models, but their behavior is difﬁcult to explain. This makes it challenging to trust their predictions in safety critical domains. Recent work has focused on explaining their predictions using local linear approximations [1, 10], but these explanations can be complex when they depend on many features and it is unclear if they can be used to understand global trends in model behavior. In this work, we train neural networks to have sparse local explanations by applying L1 penalties to their input gradients. We show explanations of these networks depend on fewer inputs while their performance remains comparable across datasets and architectures. We illustrate how our approach encourages a different kind of sparsity than L1 weight decay. In a case study with ICU data, we observe that gradients vary smoothly over the input space, which suggests they can be used to gain insight into the global behavior of the model.},
	language = {en},
	booktitle = {31st {Conference} on {Neural} {Information} {Processing} {Systems}},
	author = {Ross, Andrew Slavin and Lage, Isaac},
	year = {2017},
	pages = {5},
}

@inproceedings{lage_human---loop_2020,
	title = {Human-in-the-{Loop} {Learning} of {Interpretable} and {Intuitive} {Representations}},
	volume = {1},
	booktitle = {{ICML} {Workshop} on {Human} {Interpretability} in {Machine} {Learning},},
	author = {Lage, I. and Doshi-Velez, F.},
	year = {2020},
	pages = {1--10},
}

@inproceedings{lund_labeled_2018,
	address = {Brussels, Belgium},
	title = {Labeled {Anchors} and a {Scalable}, {Transparent}, and {Interactive} {Classifier}},
	url = {https://www.aclweb.org/anthology/D18-1095},
	doi = {10.18653/v1/D18-1095},
	abstract = {We propose Labeled Anchors, an interactive and supervised topic model based on the anchor words algorithm (Arora et al., 2013). Labeled Anchors is similar to Supervised Anchors (Nguyen et al., 2014) in that it extends the vector-space representation of words to include document labels. However, our formulation also admits a classifier which requires no training beyond inferring topics, which means our approach is also fast enough to be interactive. We run a small user study that demonstrates that untrained users can interactively update topics in order to improve classification accuracy.},
	urldate = {2020-12-27},
	booktitle = {Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Lund, Jeffrey and Cowley, Stephen and Fearn, Wilson and Hales, Emily and Seppi, Kevin},
	month = oct,
	year = {2018},
	pages = {824--829},
}

@article{poursabzi-sangdeh_manipulating_2019,
	title = {Manipulating and {Measuring} {Model} {Interpretability}},
	url = {http://arxiv.org/abs/1802.07810},
	abstract = {With the increased use of machine learning in decision-making scenarios, there has been a growing interest in creating human-interpretable machine learning models. While many such models have been proposed, there have been relatively few experimental studies of whether these models achieve their intended effects, such as encouraging people to follow the model's predictions when the model is correct and to deviate when it makes a mistake. We present a series of randomized, pre-registered experiments comprising 3,800 participants in which people were shown functionally identical models that varied only in two factors thought to influence interpretability: the number of input features and the model transparency (clear or black-box). Predictably, participants who were shown a clear model with a small number of features were better able to simulate the model's predictions. However, contrary to what one might expect when manipulating interpretability, we found no improvements in the degree to which participants followed the model's predictions when it was beneficial to do so. Even more surprisingly, increased transparency hampered people's ability to detect when the model makes a sizable mistake and correct for it, seemingly due to information overload. These counterintuitive results suggest that decision scientists creating interpretable models should harbor a healthy skepticism of their intuitions and empirically verify that interpretable models achieve their intended effects.},
	urldate = {2020-12-27},
	journal = {arXiv:1802.07810 [cs]},
	author = {Poursabzi-Sangdeh, Forough and Goldstein, Daniel G. and Hofman, Jake M. and Vaughan, Jennifer Wortman and Wallach, Hanna},
	month = nov,
	year = {2019},
	note = {arXiv: 1802.07810},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computers and Society},
}

@inproceedings{lage_evaluation_2018,
	title = {An {Evaluation} of the {Human}-{Interpretability} of {Explanation}},
	abstract = {The evaluation of interpretable machine learning systems is challenging, as explanation is almost always a means toward some downstream task. In this work, we carefully control a number of properties of logic-based explanations (overall length, number of repeated terms, etc.) to determine their effect on human ability to perform three basic tasks: simulating the system’s response, veriﬁcation of a suggested response, and counterfactual reasoning. Our ﬁndings about how each of these properties affect the ability of humans to perform each task provide insights on how we might construct regularizers to optimize for task performance.},
	language = {en},
	booktitle = {32nd {Conference} on {Neural} {Information} {Processing} {Systems}},
	author = {Lage, Isaac and Chen, Emily and He, Jeffrey and Narayanan, Menaka and Kim, Been and Gershman, Samuel J and Doshi-Velez, Finale},
	year = {2018},
	pages = {7},
}

@inproceedings{lage_human_2019,
	title = {Human {Evaluation} of {Models} {Built} for {Interpretability}},
	volume = {7},
	copyright = {Copyright (c) 2019 Association for the Advancement of Artificial Intelligence},
	url = {https://ojs.aaai.org/index.php/HCOMP/article/view/5280},
	language = {en},
	urldate = {2020-12-27},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Human} {Computation} and {Crowdsourcing}},
	author = {Lage, Isaac and Chen, Emily and He, Jeffrey and Narayanan, Menaka and Kim, Been and Gershman, Samuel J. and Doshi-Velez, Finale},
	month = oct,
	year = {2019},
	note = {Number: 1},
	pages = {59--67},
}

@article{ross_learning_2018,
	title = {Learning {Qualitatively} {Diverse} and {Interpretable} {Rules} for {Classification}},
	url = {http://arxiv.org/abs/1806.08716},
	abstract = {There has been growing interest in developing accurate models that can also be explained to humans. Unfortunately, if there exist multiple distinct but accurate models for some dataset, current machine learning methods are unlikely to find them: standard techniques will likely recover a complex model that combines them. In this work, we introduce a way to identify a maximal set of distinct but accurate models for a dataset. We demonstrate empirically that, in situations where the data supports multiple accurate classifiers, we tend to recover simpler, more interpretable classifiers rather than more complex ones.},
	language = {en},
	urldate = {2020-12-27},
	journal = {arXiv:1806.08716 [cs, stat]},
	author = {Ross, Andrew Slavin and Pan, Weiwei and Doshi-Velez, Finale},
	month = jul,
	year = {2018},
	note = {arXiv: 1806.08716},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{achille_emergence_2018,
	title = {Emergence of invariance and disentanglement in deep representations},
	volume = {19},
	issn = {1532-4435},
	abstract = {Using established principles from Statistics and Information Theory, we show that invariance to nuisance factors in a deep neural network is equivalent to information minimality of the learned representation, and that stacking layers and injecting noise during training naturally bias the network towards learning invariant representations. We then decompose the cross-entropy loss used during training and highlight the presence of an inherent overfitting term. We propose regularizing the loss by bounding such a term in two equivalent ways: One with a Kullbach-Leibler term, which relates to a PAC-Bayes perspective; the other using the information in the weights as a measure of complexity of a learned model, yielding a novel Information Bottleneck for the weights. Finally, we show that invariance and independence of the components of the representation learned by the network are bounded above and below by the information in the weights, and therefore are implicitly optimized during training. The theory enables us to quantify and predict sharp phase transitions between underfitting and overfitting of random labels when using our regularized loss, which we verify in experiments, and sheds light on the relation between the geometry of the loss function, invariance properties of the learned representation, and generalization error.},
	number = {1},
	journal = {The Journal of Machine Learning Research},
	author = {Achille, Alessandro and Soatto, Stefano},
	month = jan,
	year = {2018},
	keywords = {PAC-bayes, flat minima, generalization, independence, information bottleneck, invariance, representation learning},
	pages = {1947--1980},
}

@inproceedings{fong_understanding_2019,
	title = {Understanding {Deep} {Networks} via {Extremal} {Perturbations} and {Smooth} {Masks}},
	url = {https://openaccess.thecvf.com/content_ICCV_2019/html/Fong_Understanding_Deep_Networks_via_Extremal_Perturbations_and_Smooth_Masks_ICCV_2019_paper.html},
	urldate = {2020-12-26},
	author = {Fong, Ruth and Patrick, Mandela and Vedaldi, Andrea},
	year = {2019},
	pages = {2950--2958},
}

@article{zhang_causal_2020,
	title = {A {Causal} {View} on {Robustness} of {Neural} {Networks}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/02ed812220b0705fabb868ddbf17ea20-Abstract.html},
	language = {en},
	urldate = {2020-12-26},
	journal = {Advances in Neural Information Processing Systems},
	author = {Zhang, Cheng and Zhang, Kun and Li, Yingzhen},
	year = {2020},
}

@article{chen_towards_2020,
	title = {Towards {Understanding} {Hierarchical} {Learning}: {Benefits} of {Neural} {Representations}},
	volume = {33},
	shorttitle = {Towards {Understanding} {Hierarchical} {Learning}},
	url = {https://proceedings.neurips.cc/paper/2020/hash/fb647ca6672b0930e9d00dc384d8b16f-Abstract.html},
	language = {en},
	urldate = {2020-12-26},
	journal = {Advances in Neural Information Processing Systems},
	author = {Chen, Minshuo and Bai, Yu and Lee, Jason D. and Zhao, Tuo and Wang, Huan and Xiong, Caiming and Socher, Richard},
	year = {2020},
}

@inproceedings{park_attribution_2020,
	title = {Attribution {Preservation} in {Network} {Compression} for {Reliable} {Network} {Interpretation}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/35adf1ae7eb5734122c84b7a9ea5cc13-Abstract.html},
	language = {en},
	urldate = {2020-12-25},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Park, Geondo and Yang, June Yong and Hwang, Sung Ju and Yang, Eunho},
	year = {2020},
}

@inproceedings{lage_human---loop_2018,
	title = {Human-in-the-{Loop} {Interpretability} {Prior}},
	abstract = {We often desire our models to be interpretable as well as accurate. Prior work on optimizing models for interpretability has relied on easy-to-quantify proxies for interpretability, such as sparsity or the number of operations required. In this work, we optimize for interpretability by directly including humans in the optimization loop. We develop an algorithm that minimizes the number of user studies to ﬁnd models that are both predictive and interpretable and demonstrate our approach on several data sets. Our human subjects results show trends towards different proxy notions of interpretability on different datasets, which suggests that different proxies are preferred on different tasks.},
	language = {en},
	booktitle = {32nd {Conference} on {Neural} {Information} {Processing} {Systems}},
	author = {Lage, Isaac and Ross, Andrew and Gershman, Samuel J and Kim, Been and Doshi-Velez, Finale},
	year = {2018},
	pages = {10},
}

@inproceedings{zintgraf_visualizing_2017,
	title = {Visualizing {Deep} {Neural} {Network} {Decisions}: {Prediction} {Difference} {Analysis}},
	abstract = {This article presents the prediction difference analysis method for visualizing the response of a deep neural network to a speciﬁc input. When classifying images, the method highlights areas in a given input image that provide evidence for or against a certain class. It overcomes several shortcoming of previous methods and provides great additional insight into the decision making process of classiﬁers. Making neural network decisions interpretable through visualization is important both to improve models and to accelerate the adoption of black-box classiﬁers in application areas such as medicine. We illustrate the method in experiments on natural images (ImageNet data), as well as medical images (MRI brain scans).},
	language = {en},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Zintgraf, Luisa M and Cohen, Taco S and Adel, Tameem and Welling, Max},
	year = {2017},
	pages = {12},
}

@inproceedings{chen_this_2019,
	title = {This {Looks} {Like} {That}: {Deep} {Learning} for {Interpretable} {Image} {Recognition}},
	abstract = {When we are faced with challenging image classiﬁcation tasks, we often explain our reasoning by dissecting the image, and pointing out prototypical aspects of one class or another. The mounting evidence for each of the classes helps us make our ﬁnal decision. In this work, we introduce a deep network architecture –prototypical part network (ProtoPNet), that reasons in a similar way: the network dissects the image by ﬁnding prototypical parts, and combines evidence from the prototypes to make a ﬁnal classiﬁcation. The model thus reasons in a way that is qualitatively similar to the way ornithologists, physicians, and others would explain to people on how to solve challenging image classiﬁcation tasks. The network uses only image-level labels for training without any annotations for parts of images. We demonstrate our method on the CUB-200-2011 dataset and the Stanford Cars dataset. Our experiments show that ProtoPNet can achieve comparable accuracy with its analogous non-interpretable counterpart, and when several ProtoPNets are combined into a larger network, it can achieve an accuracy that is on par with some of the best-performing deep models. Moreover, ProtoPNet provides a level of interpretability that is absent in other interpretable deep models.},
	language = {en},
	booktitle = {33rd {Conference} on {Neural} {Information} {Processing} {Systems}},
	author = {Chen, Chaofan and Li, Oscar and Tao, Daniel and Barnett, Alina and Rudin, Cynthia and Su, Jonathan K},
	year = {2019},
	pages = {12},
}

@inproceedings{xie_relating_2017,
	title = {Relating {Input} {Concepts} to {Convolutional} {Neural} {Network} {Decisions}},
	abstract = {Many current methods to interpret convolutional neural networks (CNNs) use visualization techniques and words to highlight concepts of the input seemingly relevant to a CNN’s decision. The methods hypothesize that the recognition of these concepts are instrumental in the decision a CNN reaches, but the nature of this relationship has not been well explored. To address this gap, this paper examines the quality of a concept’s recognition by a CNN and the degree to which the recognitions are associated with CNN decisions. The study considers a CNN trained for scene recognition over the ADE20k dataset. It uses a novel approach to ﬁnd and score the strength of minimally distributed representations of input concepts (deﬁned by objects in scene images) across late stage feature maps. Subsequent analysis ﬁnds evidence that concept recognition impacts decision making. Strong recognition of concepts frequently-occurring in few scenes are indicative of correct decisions, but recognizing concepts common to many scenes may mislead the network.},
	language = {en},
	booktitle = {31st {Conference} on {Neural} {Information} {Processing} {Systems}},
	author = {Xie, Ning and Sarker, Kamruzzaman and Doran, Derek and Hitzler, Pascal and Raymer, Michael},
	year = {2017},
	pages = {10},
}

@inproceedings{lee_deeply-supervised_2015,
	title = {Deeply-{Supervised} {Nets}},
	url = {http://proceedings.mlr.press/v38/lee15a.html},
	abstract = {We propose deeply-supervised nets (DSN), a method that simultaneously minimizes classification error and improves the directness and transparency of the hidden layer learning process. We focus our ...},
	language = {en},
	urldate = {2020-12-25},
	booktitle = {Artificial {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Lee, Chen-Yu and Xie, Saining and Gallagher, Patrick and Zhang, Zhengyou and Tu, Zhuowen},
	month = feb,
	year = {2015},
	note = {ISSN: 1938-7228},
	pages = {562--570},
}

@inproceedings{ravanelli_interpretable_2018,
	title = {Interpretable {Convolutional} {Filters} with {SincNet}},
	abstract = {Deep learning is currently playing a crucial role toward higher levels of artiﬁcial intelligence. This paradigm allows neural networks to learn complex and abstract representations, that are progressively obtained by combining simpler ones. Nevertheless, the internal "black-box" representations automatically discovered by current neural architectures often suffer from a lack of interpretability, making of primary interest the study of explainable machine learning techniques.},
	language = {en},
	booktitle = {32nd {Conference} on {Neural} {Information} {Processing} {Systems} {IRASL} workshop},
	author = {Ravanelli, Mirco and Bengio, Yoshua},
	year = {2018},
	pages = {13},
}

@inproceedings{borowski_natural_2020,
	title = {Natural {Images} are {More} {Informative} for {Interpreting} {CNN} {Activations} than {State}-of-the-{Art} {Synthetic} {Feature} {Visualizations}},
	abstract = {Feature visualizations such as synthetic maximally activating images are a widely used explanation method to better understand the information processing of convolutional neural networks (CNNs). At the same time, there are concerns that these visualizations might not accurately represent CNNs’ inner workings. Here, we measure how much extremely activating images help humans in predicting CNN activations. Using a well-controlled psychophysical paradigm, we compare the informativeness of synthetic images by Olah et al. [45] with a simple baseline visualization, namely natural images that also strongly activate a speciﬁc feature map. Given either synthetic or natural reference images, human participants choose which of two query images leads to strong positive activation. The experiment is designed to maximize participants’ performance, and is the ﬁrst to probe intermediate instead of ﬁnal layer representations. We ﬁnd that synthetic images indeed provide helpful information about feature map activations (82 ± 4\% accuracy; chance would be 50\%). However, natural images—originally intended to be a baseline—outperform these synthetic images by a wide margin (92 ± 2\% accuracy). The superiority of natural images holds across the investigated network and various conditions. Therefore, we argue that visualization methods should improve over this simple baseline.},
	language = {en},
	booktitle = {2nd {Workshop} on {Shared} {Visual} {Representations} in {Human} and {Machine} {Intelligence} ({SVRHM}), {NeurIPS}},
	author = {Borowski, Judy and Zimmermann, Roland S and Schepers, Judith and Geirhos, Robert and Wallis, Thomas S A and Bethge, Matthias and Brendel, Wieland},
	year = {2020},
	pages = {19},
}

@inproceedings{kumarl_ibrahim_ben_daya_beyond_2019,
	title = {Beyond {Explainability}: {Leveraging} {Interpretability} for {Improved} {Adversarial} {Learning}},
	shorttitle = {Beyond {Explainability}},
	url = {https://openaccess.thecvf.com/content_CVPRW_2019/html/Explainable_AI/Daya_Beyond_Explainability_Leveraging_Interpretability_for_Improved_Adversarial_Learning_CVPRW_2019_paper.html},
	urldate = {2020-12-25},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops}},
	author = {Kumarl Ibrahim Ben Daya, Devinder and Vats, Kanav and Feng, Jeffery and Taylor, Graham and Wong, Alexander},
	year = {2019},
	pages = {16--19},
}

@inproceedings{chen_infogan_2016,
	title = {{InfoGAN}: {Interpretable} {Representation} {Learning} by {Information} {Maximizing} {Generative} {Adversarial} {Nets}},
	volume = {29},
	shorttitle = {{InfoGAN}},
	url = {https://papers.nips.cc/paper/2016/hash/7c9d0b1f96aebd7b5eca8c3edaa19ebb-Abstract.html},
	language = {en},
	urldate = {2020-12-25},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Chen, Xi and Duan, Yan and Houthooft, Rein and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
	year = {2016},
	pages = {2172--2180},
}

@inproceedings{mascharka_transparency_2018,
	title = {Transparency by {Design}: {Closing} the {Gap} {Between} {Performance} and {Interpretability} in {Visual} {Reasoning}},
	shorttitle = {Transparency by {Design}},
	url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Mascharka_Transparency_by_Design_CVPR_2018_paper.html},
	urldate = {2020-12-16},
	booktitle = {Proceedings of the {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Mascharka, David and Tran, Philip and Soklaski, Ryan and Majumdar, Arjun},
	year = {2018},
	pages = {4942--4950},
}

@inproceedings{liang_training_2020,
	address = {Cham},
	title = {Training {Interpretable} {Convolutional} {Neural} {Networks} by {Differentiating} {Class}-{Specific} {Filters}},
	volume = {12347},
	isbn = {978-3-030-58535-8 978-3-030-58536-5},
	url = {http://link.springer.com/10.1007/978-3-030-58536-5_37},
	doi = {10.1007/978-3-030-58536-5_37},
	abstract = {Convolutional neural networks (CNNs) have been successfully used in a range of tasks. However, CNNs are often viewed as “blackbox” and lack of interpretability. One main reason is due to the ﬁlter-class entanglement – an intricate many-to-many correspondence between ﬁlters and classes. Most existing works attempt post-hoc interpretation on a pre-trained model, while neglecting to reduce the entanglement underlying the model. In contrast, we focus on alleviating ﬁlter-class entanglement during training. Inspired by cellular diﬀerentiation, we propose a novel strategy to train interpretable CNNs by encouraging class-speciﬁc ﬁlters, among which each ﬁlter responds to only one (or few) class. Concretely, we design a learnable sparse Class-Speciﬁc Gate (CSG) structure to assign each ﬁlter with one (or few) class in a ﬂexible way. The gate allows a ﬁlter’s activation to pass only when the input samples come from the speciﬁc class. Extensive experiments demonstrate the fabulous performance of our method in generating a sparse and highly classrelated representation of the input, which leads to stronger interpretability. Moreover, comparing with the standard training strategy, our model displays beneﬁts in applications like object localization and adversarial sample detection. Code link: https://github.com/hyliang96/CSGCNN.},
	language = {en},
	urldate = {2020-12-22},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Liang, Haoyu and Ouyang, Zhihao and Zeng, Yuyuan and Su, Hang and He, Zihao and Xia, Shu-Tao and Zhu, Jun and Zhang, Bo and He, Zihao and Xia, Shu-Tao and Zhu, Jun and Zhang, Bo},
	year = {2020},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {622--638},
}

@inproceedings{lee_towards_2019,
	title = {Towards {Robust}, {Locally} {Linear} {Deep} {Network}},
	language = {en},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Lee, Guang-He and Alvarez-Melis, David and Jaakkola, Tommi S},
	year = {2019},
	pages = {21},
}

@inproceedings{chang_invariant_2020,
	title = {Invariant {Rationalization}},
	url = {http://proceedings.mlr.press/v119/chang20c.html},
	abstract = {Selective rationalization improves neural network interpretability by identifying a small subset of input features \{—\} the rationale \{—\} that best explains or supports the prediction. A typical rat...},
	language = {en},
	urldate = {2020-12-25},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Chang, Shiyu and Zhang, Yang and Yu, Mo and Jaakkola, Tommi},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {1448--1458},
}

@inproceedings{melis_towards_2018,
	title = {Towards {Robust} {Interpretability} with {Self}-{Explaining} {Neural} {Networks}},
	abstract = {Most recent work on interpretability of complex machine learning models has focused on estimating a posteriori explanations for previously trained models around speciﬁc predictions. Self-explaining models where interpretability plays a key role already during learning have received much less attention. We propose three desiderata for explanations in general – explicitness, faithfulness, and stability – and show that existing methods do not satisfy them. In response, we design self-explaining models in stages, progressively generalizing linear classiﬁers to complex yet architecturally explicit models. Faithfulness and stability are enforced via regularization speciﬁcally tailored to such models. Experimental results across various benchmark datasets show that our framework offers a promising direction for reconciling model complexity and interpretability.},
	language = {en},
	booktitle = {32nd {Conference} on {Neural} {Information} {Processing} {Systems}},
	author = {Melis, David Alvarez and Jaakkola, Tommi},
	year = {2018},
	pages = {10},
}

@inproceedings{jiang_trust_2018,
	title = {To {Trust} {Or} {Not} {To} {Trust} {A} {Classifier}},
	volume = {31},
	url = {https://papers.nips.cc/paper/2018/hash/7180cffd6a8e829dacfc2a31b3f72ece-Abstract.html},
	language = {en},
	urldate = {2020-12-21},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Jiang, Heinrich and Kim, Been and Guan, Melody and Gupta, Maya},
	year = {2018},
	pages = {5541--5552},
}

@inproceedings{bansal_sam_2020,
	title = {{SAM}: {The} {Sensitivity} of {Attribution} {Methods} to {Hyperparameters}},
	shorttitle = {{SAM}},
	url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Bansal_SAM_The_Sensitivity_of_Attribution_Methods_to_Hyperparameters_CVPR_2020_paper.html},
	urldate = {2020-12-21},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Bansal, Naman and Agarwal, Chirag and Nguyen, Anh},
	year = {2020},
	pages = {8673--8683},
}

@misc{ernst_oscar_2020,
	title = {{OSCAR}: {Occluded} {Stereo} dataset for {Convolutional} {Architectures} with {Recurrence}},
	copyright = {Embargoed Access},
	shorttitle = {{OSCAR}},
	url = {https://zenodo.org/record/3540900},
	doi = {10.5281/ZENODO.3540900},
	abstract = {Recurrent connectivity in the visual cortex is believed to aid object recognition for challenging conditions such as occlusion. Here we investigate if and how artiﬁcial neural networks also beneﬁt from recurrence. We compare architectures composed of bottom-up, lateral and top-down connections and evaluate their performance using two novel stereoscopic occluded object datasets. We ﬁnd that classiﬁcation accuracy is signiﬁcantly higher for recurrent models when compared to feedforward models of matched parametric complexity. Additionally we show that for challenging stimuli, the recurrent feedback is able to correctly revise the initial feedforward guess.},
	language = {en},
	urldate = {2020-12-22},
	publisher = {Zenodo},
	author = {Ernst, Markus Roland and Triesch, Jochen and Burwick, Thomas},
	year = {2020},
	note = {Proceedings Title: European Symposium on Artificial Neural Networks},
	keywords = {Computer Vision, Dataset, Deep Learning, Machine Learning, Object Recognition, Occlusion, Recurrent Neural Networks},
}

@inproceedings{liang_knowledge_2020,
	title = {Knowledge {Consistency} {Between} {Neural} {Networks} and {Beyond}},
	abstract = {This paper aims to analyze knowledge consistency between pre-trained deep neural networks. We propose a generic deﬁnition for knowledge consistency between neural networks at different fuzziness levels. A task-agnostic method is designed to disentangle feature components, which represent the consistent knowledge, from raw intermediate-layer features of each neural network. As a generic tool, our method can be broadly used for different applications. In preliminary experiments, we have used knowledge consistency as a tool to diagnose representations of neural networks. Knowledge consistency provides new insights to explain the success of existing deep-learning techniques, such as knowledge distillation and network compression. More crucially, knowledge consistency can also be used to reﬁne pre-trained networks and boost performance.},
	language = {en},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Liang, Ruofan and Li, Tianlin and Li, Longfei and Wang, Jing and Zhang, Quanshi},
	year = {2020},
	pages = {15},
}

@inproceedings{li_interpretable_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Interpretable {Neural} {Network} {Decoupling}},
	isbn = {978-3-030-58555-6},
	doi = {10.1007/978-3-030-58555-6_39},
	abstract = {The remarkable performance of convolutional neural networks (CNNs) is entangled with their huge number of uninterpretable parameters, which has become the bottleneck limiting the exploitation of their full potential. Towards network interpretation, previous endeavors mainly resort to the single filter analysis, which however ignores the relationship between filters. In this paper, we propose a novel architecture decoupling method to interpret the network from a perspective of investigating its calculation paths. More specifically, we introduce a novel architecture controlling module in each layer to encode the network architecture by a vector. By maximizing the mutual information between the vectors and input images, the module is trained to select specific filters to distill a unique calculation path for each input. Furthermore, to improve the interpretability and compactness of the decoupled network, the output of each layer is encoded to align the architecture encoding vector with the constraint of sparsity regularization. Unlike conventional pixel-level or filter-level network interpretation methods, we propose a path-level analysis to explore the relationship between the combination of filter and semantic concepts, which is more suitable to interpret the working rationale of the decoupled network. Extensive experiments show that the decoupled network achieves several applications, i.e., network interpretation, network acceleration, and adversarial samples detection.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Li, Yuchao and Ji, Rongrong and Lin, Shaohui and Zhang, Baochang and Yan, Chenqian and Wu, Yongjian and Huang, Feiyue and Shao, Ling},
	year = {2020},
	keywords = {Architecture decoupling, Network interpretation},
	pages = {653--669},
}

@inproceedings{konforti_inference_2020,
	address = {Cham},
	title = {Inference {Graphs} for {CNN} {Interpretation}},
	volume = {12370},
	isbn = {978-3-030-58594-5 978-3-030-58595-2},
	url = {http://link.springer.com/10.1007/978-3-030-58595-2_5},
	doi = {10.1007/978-3-030-58595-2_5},
	abstract = {Convolutional neural networks (CNNs) have achieved superior accuracy in many visual related tasks. However, the inference process through intermediate layers is opaque, making it diﬃcult to interpret such networks or develop trust in their operation. We propose to model the network hidden layers activity using probabilistic models. The activity patterns in layers of interest are modeled as Gaussian mixture models, and transition probabilities between clusters in consecutive modeled layers are estimated. Based on maximum-likelihood considerations, nodes and paths relevant for network prediction are chosen, connected, and visualized as an inference graph. We show that such graphs are useful for understanding the general inference process of a class, as well as explaining decisions the network makes regarding speciﬁc images.},
	language = {en},
	urldate = {2020-12-22},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Konforti, Yael and Shpigler, Alon and Lerner, Boaz and Bar-Hillel, Aharon and Konforti, Yael and Shpigler, Alon and Lerner, Boaz and Bar-Hillel, Aharon},
	year = {2020},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {69--84},
}

@inproceedings{jung_icaps_2020,
	address = {Cham},
	title = {{iCaps}: {An} {Interpretable} {Classifier} via {Disentangled} {Capsule} {Networks}},
	volume = {12364},
	isbn = {978-3-030-58528-0 978-3-030-58529-7},
	shorttitle = {{iCaps}},
	url = {http://link.springer.com/10.1007/978-3-030-58529-7_19},
	doi = {10.1007/978-3-030-58529-7_19},
	abstract = {We propose an interpretable Capsule Network, iCaps, for image classiﬁcation. A capsule is a group of neurons nested inside each layer, and the one in the last layer is called a class capsule, which is a vector whose norm indicates a predicted probability for the class. Using the class capsule, existing Capsule Networks already provide some level of interpretability. However, there are two limitations which degrade its interpretability: 1) the class capsule also includes classiﬁcation-irrelevant information, and 2) entities represented by the class capsule overlap. In this work, we address these two limitations using a novel class-supervised disentanglement algorithm and an additional regularizer, respectively. Through quantitative and qualitative evaluations on three datasets, we demonstrate that the resulting classiﬁer, iCaps, provides a prediction along with clear rationales behind it with no performance degradation.},
	language = {en},
	urldate = {2020-12-22},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Jung, Dahuin and Lee, Jonghyun and Yi, Jihun and Yoon, Sungroh and Jung, Dahuin and Lee, Jonghyun and Yi, Jihun and Yoon, Sungroh},
	year = {2020},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {314--330},
}

@inproceedings{ortiz-jimenez_hold_2020,
	title = {Hold me tight! {Inﬂuence} of discriminative features on deep network boundaries},
	abstract = {Important insights towards the explainability of neural networks reside in the characteristics of their decision boundaries. In this work, we borrow tools from the ﬁeld of adversarial robustness, and propose a new perspective that relates dataset features to the distance of samples to the decision boundary. This enables us to carefully tweak the position of the training samples and measure the induced changes on the boundaries of CNNs trained on large-scale vision datasets. We use this framework to reveal some intriguing properties of CNNs. Speciﬁcally, we rigorously conﬁrm that neural networks exhibit a high invariance to non-discriminative features, and show that the decision boundaries of a DNN can only exist as long as the classiﬁer is trained with some features that hold them together. Finally, we show that the construction of the decision boundary is extremely sensitive to small perturbations of the training samples, and that changes in certain directions can lead to sudden invariances in the orthogonal ones. This is precisely the mechanism that adversarial training uses to achieve robustness.},
	language = {en},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Ortiz-Jiménez, Guillermo and Moosavi-Dezfooli, Seyed-Mohsen},
	year = {2020},
	pages = {12},
}

@inproceedings{nie_theoretical_2018,
	title = {A {Theoretical} {Explanation} for {Perplexing} {Behaviors} of {Backpropagation}-based {Visualizations}},
	abstract = {Backpropagation-based visualizations have been proposed to interpret convolutional neural networks (CNNs), however a theory is missing to justify their behaviors: Guided backpropagation (GBP) and deconvolutional network (DeconvNet) generate more human-interpretable but less classsensitive visualizations than saliency map. Motivated by this, we develop a theoretical explanation revealing that GBP and DeconvNet are essentially doing (partial) image recovery which is unrelated to the network decisions. Speciﬁcally, our analysis shows that the backward ReLU introduced by GBP and DeconvNet, and the local connections in CNNs are the two main causes of compelling visualizations. Extensive experiments are provided that support the theoretical analysis.},
	language = {en},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}},
	author = {Nie, Weili and Zhang, Yang and Patel, Ankit B},
	year = {2018},
	pages = {10},
}

@inproceedings{wang_high-frequency_2020,
	title = {High-{Frequency} {Component} {Helps} {Explain} the {Generalization} of {Convolutional} {Neural} {Networks}},
	url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Wang_High-Frequency_Component_Helps_Explain_the_Generalization_of_Convolutional_Neural_Networks_CVPR_2020_paper.html},
	urldate = {2020-12-21},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Wang, Haohan and Wu, Xindi and Huang, Zeyi and Xing, Eric P.},
	year = {2020},
	pages = {8684--8694},
}

@article{lakkaraju_interpretable_2017,
	title = {Interpretable \& {Explorable} {Approximations} of {Black} {Box} {Models}},
	url = {http://arxiv.org/abs/1707.01154},
	abstract = {We propose Black Box Explanations through Transparent Approximations (BETA), a novel model agnostic framework for explaining the behavior of any black-box classifier by simultaneously optimizing for fidelity to the original model and interpretability of the explanation. To this end, we develop a novel objective function which allows us to learn (with optimality guarantees), a small number of compact decision sets each of which explains the behavior of the black box model in unambiguous, well-defined regions of feature space. Furthermore, our framework also is capable of accepting user input when generating these approximations, thus allowing users to interactively explore how the black-box model behaves in different subspaces that are of interest to the user. To the best of our knowledge, this is the first approach which can produce global explanations of the behavior of any given black box model through joint optimization of unambiguity, fidelity, and interpretability, while also allowing users to explore model behavior based on their preferences. Experimental evaluation with real-world datasets and user studies demonstrates that our approach can generate highly compact, easy-to-understand, yet accurate approximations of various kinds of predictive models compared to state-of-the-art baselines.},
	urldate = {2020-12-25},
	journal = {arXiv:1707.01154 [cs]},
	author = {Lakkaraju, Himabindu and Kamar, Ece and Caruana, Rich and Leskovec, Jure},
	month = jul,
	year = {2017},
	note = {arXiv: 1707.01154},
	keywords = {Computer Science - Artificial Intelligence},
}

@inproceedings{dinu_challenging_2020,
	title = {Challenging common interpretability assumptions in feature attribution explanations},
	abstract = {As machine learning and algorithmic decision making systems are increasingly being leveraged in high-stakes human-in-the-loop settings, there is a pressing need to understand the rationale of their predictions. Researchers have responded to this need with explainable AI (XAI), but often proclaim interpretability axiomatically without evaluation. When these systems are evaluated, they are often tested through ofﬂine simulations with proxy metrics of interpretability (such as model complexity). We empirically evaluate the veracity of three common interpretability assumptions through a large scale human-subjects experiment with a simple “placebo explanation” control. We ﬁnd that feature attribution explanations provide marginal utility in our task for a human decision maker and in certain cases result in worse decisions due to cognitive and contextual confounders. This result challenges the assumed universal beneﬁt of applying these methods and we hope this work will underscore the importance of human evaluation in XAI research. Supplemental materials—including anonymized data from the experiment, code to replicate the study, an interactive demo of the experiment, and the models used in the analysis—can be found at: https://doi.pizza/challenging-xai.},
	language = {en},
	booktitle = {{NeurIPS}  {Workshop}: {ML} {Retrospectives}, {Surveys} \& {Meta}-{Analyses} ({ML}-{RSA}).},
	author = {Dinu, Jonathan and Bigham, Jeffrey and Kolter, J Zico},
	year = {2020},
	pages = {15},
}

@inproceedings{rafique_transparency_2020,
	title = {Transparency {Promotion} with {Model}-{Agnostic} {Linear} {Competitors}},
	url = {http://proceedings.mlr.press/v119/rafique20a.html},
	abstract = {We propose a novel type of hybrid model for multi-class classification, which utilizes competing linear models to collaborate with an existing black-box model, promoting transparency in the decisio...},
	language = {en},
	urldate = {2020-12-25},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Rafique, Hassan and Wang, Tong and Lin, Qihang and Singhani, Arshia},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {7898--7908},
}

@inproceedings{pan_interpretable_2020,
	title = {Interpretable {Companions} for {Black}-{Box} {Models}},
	url = {http://proceedings.mlr.press/v108/pan20a.html},
	abstract = {We present an interpretable companion model for any pre-trained black-box classifiers. The idea is that for any input, a user can decide to either receive a prediction from the black-box model, wit...},
	language = {en},
	urldate = {2020-12-25},
	booktitle = {International {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Pan, Danqing and Wang, Tong and Hara, Satoshi},
	month = jun,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {2444--2454},
}

@inproceedings{kim_examples_2016,
	title = {Examples are not enough, learn to criticize! {Criticism} for {Interpretability}},
	volume = {29},
	url = {https://papers.nips.cc/paper/2016/hash/5680522b8e2bb01943234bce7bf84534-Abstract.html},
	abstract = {Example-based explanations are widely used in the effort to improve the interpretability of highly complex distributions. However, prototypes alone are rarely sufﬁcient to represent the gist of the complexity. In order for users to construct better mental models and understand complex data distributions, we also need criticism to explain what are not captured by prototypes. Motivated by the Bayesian model criticism framework, we develop MMD-critic which efﬁciently learns prototypes and criticism, designed to aid human interpretability. A human subject pilot study shows that the MMD-critic selects prototypes and criticism that are useful to facilitate human understanding and reasoning. We also evaluate the prototypes selected by MMD-critic via a nearest prototype classiﬁer, showing competitive performance compared to baselines.},
	language = {en},
	urldate = {2020-12-25},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Kim, Been and Khanna, Rajiv and Koyejo, Oluwasanmi O.},
	year = {2016},
	pages = {2280--2288},
}

@article{qi_embedding_2021,
	title = {Embedding {Deep} {Networks} into {Visual} {Explanations}},
	volume = {292},
	issn = {00043702},
	url = {http://arxiv.org/abs/1709.05360},
	doi = {10.1016/j.artint.2020.103435},
	abstract = {In this paper, we propose a novel Explanation Neural Network (XNN) to explain the predictions made by a deep network. The XNN works by learning a nonlinear embedding of a high-dimensional activation vector of a deep network layer into a low-dimensional explanation space while retaining faithfulness i.e., the original deep learning predictions can be constructed from the few concepts extracted by our explanation network. We then visualize such concepts for human to learn about the high-level concepts that the deep network is using to make decisions. We propose an algorithm called Sparse Reconstruction Autoencoder (SRAE) for learning the embedding to the explanation space. SRAE aims to reconstruct part of the original feature space while retaining faithfulness. A pull-away term is applied to SRAE to make the bases of the explanation space more orthogonal to each other. A visualization system is then introduced for human understanding of the features in the explanation space. The proposed method is applied to explain CNN models in image classiﬁcation tasks. We conducted a human study, which shows that the proposed approach outperforms single saliency map baselines, and improves human performance on a difﬁcult classiﬁcation tasks. Also, several novel metrics are introduced to evaluate the performance of explanations quantitatively without human involvement.},
	language = {en},
	urldate = {2020-12-24},
	journal = {Artificial Intelligence},
	author = {Qi, Zhongang and Khorram, Saeed and Li, Fuxin},
	month = mar,
	year = {2021},
	note = {arXiv: 1709.05360},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	pages = {103435},
}

@article{higgins_towards_2018,
	title = {Towards a {Definition} of {Disentangled} {Representations}},
	url = {http://arxiv.org/abs/1812.02230},
	abstract = {How can intelligent agents solve a diverse set of tasks in a data-efficient manner? The disentangled representation learning approach posits that such an agent would benefit from separating out (disentangling) the underlying structure of the world into disjoint parts of its representation. However, there is no generally agreed-upon definition of disentangling, not least because it is unclear how to formalise the notion of world structure beyond toy datasets with a known ground truth generative process. Here we propose that a principled solution to characterising disentangled representations can be found by focusing on the transformation properties of the world. In particular, we suggest that those transformations that change only some properties of the underlying world state, while leaving all other properties invariant, are what gives exploitable structure to any kind of data. Similar ideas have already been successfully applied in physics, where the study of symmetry transformations has revolutionised the understanding of the world structure. By connecting symmetry transformations to vector representations using the formalism of group and representation theory we arrive at the first formal definition of disentangled representations. Our new definition is in agreement with many of the current intuitions about disentangling, while also providing principled resolutions to a number of previous points of contention. While this work focuses on formally defining disentangling - as opposed to solving the learning problem - we believe that the shift in perspective to studying data transformations can stimulate the development of better representation learning algorithms.},
	language = {en},
	urldate = {2020-12-24},
	journal = {arXiv:1812.02230 [cs, stat]},
	author = {Higgins, Irina and Amos, David and Pfau, David and Racaniere, Sebastien and Matthey, Loic and Rezende, Danilo and Lerchner, Alexander},
	month = dec,
	year = {2018},
	note = {arXiv: 1812.02230},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{van_steenkiste_are_2019,
	title = {Are {Disentangled} {Representations} {Helpful} for {Abstract} {Visual} {Reasoning}?},
	volume = {32},
	url = {https://papers.nips.cc/paper/2019/hash/bc3c4a6331a8a9950945a1aa8c95ab8a-Abstract.html},
	language = {en},
	urldate = {2020-12-24},
	journal = {Advances in Neural Information Processing Systems},
	author = {van Steenkiste, Sjoerd and Locatello, Francesco and Schmidhuber, Jürgen and Bachem, Olivier},
	year = {2019},
	pages = {14245--14258},
}

@article{rudin_stop_2019,
	title = {Stop {Explaining} {Black} {Box} {Machine} {Learning} {Models} for {High} {Stakes} {Decisions} and {Use} {Interpretable} {Models} {Instead}},
	url = {http://arxiv.org/abs/1811.10154},
	abstract = {Black box machine learning models are currently being used for high stakes decision-making throughout society, causing problems throughout healthcare, criminal justice, and in other domains. People have hoped that creating methods for explaining these black box models will alleviate some of these problems, but trying to explain black box models, rather than creating models that are interpretable in the ﬁrst place, is likely to perpetuate bad practices and can potentially cause catastrophic harm to society. There is a way forward – it is to design models that are inherently interpretable. This manuscript clariﬁes the chasm between explaining black boxes and using inherently interpretable models, outlines several key reasons why explainable black boxes should be avoided in high-stakes decisions, identiﬁes challenges to interpretable machine learning, and provides several example applications where interpretable models could potentially replace black box models in criminal justice, healthcare, and computer vision.},
	language = {en},
	urldate = {2020-12-23},
	journal = {arXiv:1811.10154 [cs, stat]},
	author = {Rudin, Cynthia},
	month = sep,
	year = {2019},
	note = {arXiv: 1811.10154},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{wang_interpretability_2020,
	address = {Virtual Event CA USA},
	title = {Interpretability is a {Kind} of {Safety}: {An} {Interpreter}-based {Ensemble} for {Adversary} {Defense}},
	isbn = {978-1-4503-7998-4},
	shorttitle = {Interpretability is a {Kind} of {Safety}},
	url = {https://dl.acm.org/doi/10.1145/3394486.3403044},
	doi = {10.1145/3394486.3403044},
	abstract = {While having achieved great success in rich real-life applications, deep neural network (DNN) models have long been criticized for their vulnerability to adversarial attacks. Tremendous research efforts have been dedicated to mitigating the threats of adversarial attacks, but the essential trait of adversarial examples is not yet clear, and most existing methods are yet vulnerable to hybrid attacks and suffer from counterattacks. In light of this, in this paper, we first reveal a gradient-based correlation between sensitivity analysisbased DNN interpreters and the generation process of adversarial examples, which indicates the Achilles’s heel of adversarial attacks and sheds light on linking together the two long-standing challenges of DNN: fragility and unexplainability. We then propose an interpreter-based ensemble framework called X-Ensemble for robust adversary defense. X-Ensemble adopts a novel detectionrectification process and features in building multiple sub-detectors and a rectifier upon various types of interpretation information toward target classifiers. Moreover, X-Ensemble employs the Random Forests (RF) model to combine sub-detectors into an ensemble detector for adversarial hybrid attacks defense. The non-differentiable property of RF further makes it a precious choice against the counterattack of adversaries. Extensive experiments under various types of state-of-the-art attacks and diverse attack scenarios demonstrate the advantages of X-Ensemble to competitive baseline methods.},
	language = {en},
	urldate = {2020-12-23},
	booktitle = {Proceedings of the 26th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
	publisher = {ACM},
	author = {Wang, Jingyuan and Wu, Yufan and Li, Mingxuan and Lin, Xin and Wu, Junjie and Li, Chao},
	month = aug,
	year = {2020},
	pages = {15--24},
}

@article{lanfredi_quantifying_2020,
	title = {Quantifying the {Preferential} {Direction} of the {Model} {Gradient} in {Adversarial} {Training} {With} {Projected} {Gradient} {Descent}},
	url = {http://arxiv.org/abs/2009.04709},
	abstract = {Adversarial training, especially projected gradient descent (PGD), has been the most successful approach for improving robustness against adversarial attacks. After adversarial training, gradients of models with respect to their inputs are meaningful and interpretable by humans. However, the concept of interpretability is not mathematically well established, making it difﬁcult to evaluate it quantitatively. We deﬁne interpretability as the alignment of the model gradient with the vector pointing toward the closest point of the support of the other class. We propose a method for measuring this alignment for binary classiﬁcation problems, using generative adversarial model training to produce the smallest residual needed to change the class present in the image. We show that PGD-trained models are more interpretable than the baseline according to our deﬁnition, and our metric presents higher alignment values than a competing metric formulation. We also show that enforcing this alignment increases the robustness of models without adversarial training.},
	language = {en},
	urldate = {2020-12-23},
	journal = {arXiv:2009.04709 [cs, stat]},
	author = {Lanfredi, Ricardo Bigolin and Schroeder, Joyce D. and Tasdizen, Tolga},
	month = sep,
	year = {2020},
	note = {arXiv: 2009.04709},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{fukui_attention_2019,
	address = {Long Beach, CA, USA},
	title = {Attention {Branch} {Network}: {Learning} of {Attention} {Mechanism} for {Visual} {Explanation}},
	isbn = {978-1-72813-293-8},
	shorttitle = {Attention {Branch} {Network}},
	url = {https://ieeexplore.ieee.org/document/8953929/},
	doi = {10.1109/CVPR.2019.01096},
	abstract = {Visual explanation enables humans to understand the decision making of deep convolutional neural network (CNN), but it is insufﬁcient to contribute to improving CNN performance. In this paper, we focus on the attention map for visual explanation, which represents a high response value as the attention location in image recognition. This attention region signiﬁcantly improves the performance of CNN by introducing an attention mechanism that focuses on a speciﬁc region in an image. In this work, we propose Attention Branch Network (ABN), which extends a response-based visual explanation model by introducing a branch structure with an attention mechanism. ABN can be applicable to several image recognition tasks by introducing a branch for the attention mechanism and is trainable for visual explanation and image recognition in an end-to-end manner. We evaluate ABN on several image recognition tasks such as image classiﬁcation, ﬁne-grained recognition, and multiple facial attribute recognition. Experimental results indicate that ABN outperforms the baseline models on these image recognition tasks while generating an attention map for visual explanation. Our code is available 1.},
	language = {en},
	urldate = {2020-12-23},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Fukui, Hiroshi and Hirakawa, Tsubasa and Yamashita, Takayoshi and Fujiyoshi, Hironobu},
	month = jun,
	year = {2019},
	pages = {10697--10706},
}

@article{noack_empirical_2020,
	title = {An {Empirical} {Study} on the {Relation} between {Network} {Interpretability} and {Adversarial} {Robustness}},
	url = {http://arxiv.org/abs/1912.03430},
	abstract = {Deep neural networks (DNNs) have had many successes, but they suffer from two major issues: (1) a vulnerability to adversarial examples and (2) a tendency to elude human interpretation. Interestingly, recent empirical and theoretical evidence suggests these two seemingly disparate issues are actually connected. In particular, robust models tend to provide more interpretable gradients than non-robust models. However, whether this relationship works in the opposite direction remains obscure. With this paper, we seek empirical answers to the following question: can models acquire adversarial robustness when they are trained to have interpretable gradients? We introduce a theoretically inspired technique called Interpretation Regularization (IR), which encourages a model's gradients to (1) match the direction of interpretable target salience maps and (2) have small magnitude. To assess model performance and tease apart factors that contribute to adversarial robustness, we conduct extensive experiments on MNIST and CIFAR-10 with both \${\textbackslash}ell\_2\$ and \${\textbackslash}ell\_{\textbackslash}infty\$ attacks. We demonstrate that training the networks to have interpretable gradients improves their robustness to adversarial perturbations. Applying the network interpretation technique SmoothGrad yields additional performance gains, especially in cross-norm attacks and under heavy perturbations. The results indicate that the interpretability of the model gradients is a crucial factor for adversarial robustness. Code for the experiments can be found at https://github.com/a1noack/interp\_regularization.},
	language = {en},
	urldate = {2020-12-22},
	journal = {arXiv:1912.03430 [cs, stat]},
	author = {Noack, Adam and Ahern, Isaac and Dou, Dejing and Li, Boyang},
	month = dec,
	year = {2020},
	note = {arXiv: 1912.03430},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{yuan_causal_2020,
	title = {Causal inference using deep neural networks},
	url = {http://arxiv.org/abs/2011.12508},
	abstract = {Causal inference from observation data is a core problem in many scientiﬁc ﬁelds. Here we present a general supervised deep learning framework that infers causal interactions by transforming the input vectors to an image-like representation for every pair of inputs. Given a training dataset we ﬁrst construct a normalized empirical probability density distribution (N EP DF ) matrix. We then train a convolutional neural network (CNN) on N EP DF s for causality predictions. We tested the method on several different simulated and real world data and compared it to prior methods for causal inference. As we show, the method is general, can efﬁciently handle very large datasets and improves upon prior methods.},
	language = {en},
	urldate = {2020-12-22},
	journal = {arXiv:2011.12508 [cs, stat]},
	author = {Yuan, Ye and Ding, Xueying and Bar-Joseph, Ziv},
	month = nov,
	year = {2020},
	note = {arXiv: 2011.12508},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{balaji_instance_2019,
	title = {Instance adaptive adversarial training: {Improved} accuracy tradeoffs in neural nets},
	shorttitle = {Instance adaptive adversarial training},
	url = {http://arxiv.org/abs/1910.08051},
	abstract = {Adversarial training is by far the most successful strategy for improving robustness of neural networks to adversarial attacks. Despite its success as a defense mechanism, adversarial training fails to generalize well to unperturbed test set. We hypothesize that this poor generalization is a consequence of adversarial training with uniform perturbation radius around every training sample. Samples close to decision boundary can be morphed into a different class under a small perturbation budget, and enforcing large margins around these samples produce poor decision boundaries that generalize poorly. Motivated by this hypothesis, we propose instance adaptive adversarial training – a technique that enforces samplespeciﬁc perturbation margins around every training sample. We show that using our approach, test accuracy on unperturbed samples improve with a marginal drop in robustness. Extensive experiments on CIFAR-10, CIFAR-100 and Imagenet datasets demonstrate the effectiveness of our proposed approach.},
	language = {en},
	urldate = {2020-12-22},
	journal = {arXiv:1910.08051 [cs, stat]},
	author = {Balaji, Yogesh and Goldstein, Tom and Hoffman, Judy},
	month = oct,
	year = {2019},
	note = {arXiv: 1910.08051},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{sitawarin_improving_2020,
	title = {Improving {Adversarial} {Robustness} {Through} {Progressive} {Hardening}},
	url = {http://arxiv.org/abs/2003.09347},
	abstract = {Adversarial training (AT) has become a popular choice for training robust networks. However, it tends to sacriﬁce clean accuracy heavily in favor of robustness, and with a large perturbation, it can cause models to learn a trivial solution, always predicting the same class. To address the above concerns, we propose Adversarial Training with Early Stopping (ATES), guided by principles from curriculum learning that emphasizes on starting “easy” and gradually ramping up on the “difﬁculty” of training. ATES is derived from our formulation for curriculum learning in the adversarial setting which introduces an additional curriculum constraint to the normal adversarial loss. To satisfy this constraint, we apply early stopping on the adversarial example generation step when a speciﬁed level of difﬁculty is reached. ATES stabilizes network training even for a large perturbation norm and allows the network to operate at a better clean accuracy versus robustness trade-off curve compared to AT. This leads to a signiﬁcant improvement in both clean accuracy and robustness compared to AT, TRADES, and the other baselines.},
	language = {en},
	urldate = {2020-12-22},
	journal = {arXiv:2003.09347 [cs, stat]},
	author = {Sitawarin, Chawin and Chakraborty, Supriyo and Wagner, David},
	month = jun,
	year = {2020},
	note = {arXiv: 2003.09347},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{zhang_theoretically_2019,
	title = {Theoretically {Principled} {Trade}-off between {Robustness} and {Accuracy}},
	url = {http://proceedings.mlr.press/v97/zhang19p.html},
	abstract = {We identify a trade-off between robustness and accuracy that serves as a guiding principle in the design of defenses against adversarial examples. Although this problem has been widely studied empi...},
	language = {en},
	urldate = {2020-12-22},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Zhang, Hongyang and Yu, Yaodong and Jiao, Jiantao and Xing, Eric and Ghaoui, Laurent El and Jordan, Michael},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {7472--7482},
}

@article{miyato_virtual_2019,
	title = {Virtual {Adversarial} {Training}: {A} {Regularization} {Method} for {Supervised} and {Semi}-{Supervised} {Learning}},
	volume = {41},
	issn = {0162-8828, 2160-9292, 1939-3539},
	shorttitle = {Virtual {Adversarial} {Training}},
	url = {https://ieeexplore.ieee.org/document/8417973/},
	doi = {10.1109/TPAMI.2018.2858821},
	abstract = {We propose a new regularization method based on virtual adversarial loss: a new measure of local smoothness of the conditional label distribution given input. Virtual adversarial loss is deﬁned as the robustness of the conditional label distribution around each input data point against local perturbation. Unlike adversarial training, our method deﬁnes the adversarial direction without label information and is hence applicable to semi-supervised learning. Because the directions in which we smooth the model are only “virtually” adversarial, we call our method virtual adversarial training (VAT). The computational cost of VAT is relatively low. For neural networks, the approximated gradient of virtual adversarial loss can be computed with no more than two pairs of forward- and back-propagations. In our experiments, we applied VAT to supervised and semi-supervised learning tasks on multiple benchmark datasets. With a simple enhancement of the algorithm based on the entropy minimization principle, our VAT achieves state-of-the-art performance for semi-supervised learning tasks on SVHN and CIFAR-10.},
	language = {en},
	number = {8},
	urldate = {2020-12-22},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Miyato, Takeru and Maeda, Shin-Ichi and Koyama, Masanori and Ishii, Shin},
	month = aug,
	year = {2019},
	pages = {1979--1993},
}

@inproceedings{stutz_disentangling_2019,
	address = {Long Beach, CA, USA},
	title = {Disentangling {Adversarial} {Robustness} and {Generalization}},
	isbn = {978-1-72813-293-8},
	url = {https://ieeexplore.ieee.org/document/8953455/},
	doi = {10.1109/CVPR.2019.00714},
	abstract = {Obtaining deep networks that are robust against adversarial examples and generalize well is an open problem. A recent hypothesis [102, 95] even states that both robust and accurate models are impossible, i.e., adversarial robustness and generalization are conﬂicting goals. In an effort to clarify the relationship between robustness and generalization, we assume an underlying, low-dimensional data manifold and show that: 1. regular adversarial examples leave the manifold; 2. adversarial examples constrained to the manifold, i.e., on-manifold adversarial examples, exist; 3. on–manifold adversarial examples are generalization errors, and on-manifold adversarial training boosts generalization; 4. regular robustness and generalization are not necessarily contradicting goals. These assumptions imply that both robust and accurate models are possible. However, different models (architectures, training strategies etc.) can exhibit different robustness and generalization characteristics. To conﬁrm our claims, we present extensive experiments on synthetic data (with known manifold) as well as on EMNIST [19], Fashion-MNIST [106] and CelebA [58].},
	language = {en},
	urldate = {2020-12-22},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Stutz, David and Hein, Matthias and Schiele, Bernt},
	month = jun,
	year = {2019},
	pages = {6969--6980},
}

@article{cocarascu_argumentation_nodate,
	title = {Argumentation for {Machine} {Learning}: {A} {Survey}},
	abstract = {Existing approaches using argumentation to aid or improve machine learning differ in the type of machine learning technique they consider, in their use of argumentation and in their choice of argumentation framework and semantics. This paper presents a survey of this relatively young ﬁeld highlighting, in particular, its achievements to date, the applications it has been used for as well as the beneﬁts brought about by the use of argumentation, with an eye towards its future.},
	language = {en},
	author = {Cocarascu, Oana and Toni, Francesca},
	pages = {12},
}

@incollection{vedaldi_semanticadv_2020,
	address = {Cham},
	title = {{SemanticAdv}: {Generating} {Adversarial} {Examples} via {Attribute}-{Conditioned} {Image} {Editing}},
	volume = {12359},
	isbn = {978-3-030-58567-9 978-3-030-58568-6},
	shorttitle = {{SemanticAdv}},
	url = {http://link.springer.com/10.1007/978-3-030-58568-6_2},
	abstract = {Recent studies have shown that DNNs are vulnerable to adversarial examples which are manipulated instances targeting to mislead DNNs to make incorrect predictions. Currently, most such adversarial examples try to guarantee “subtle perturbation” by limiting the Lp norm of the perturbation. In this paper, we propose SemanticAdv to generate a new type of semantically realistic adversarial examples via attribute-conditioned image editing. Compared to existing methods, our SemanticAdv enables ﬁne-grained analysis and evaluation of DNNs with input variations in the attribute space. We conduct comprehensive experiments to show that our adversarial examples not only exhibit semantically meaningful appearances but also achieve high targeted attack success rates under both whitebox and blackbox settings. Moreover, we show that the existing pixel-based and attribute-based defense methods fail to defend against SemanticAdv . We demonstrate the applicability of SemanticAdv on both face recognition and general street-view images to show its generalization. We believe that our work can shed light on further understanding about vulnerabilities of DNNs as well as novel defense approaches. Our implementation is available at https://github.com/AIsecure/SemanticAdv.},
	language = {en},
	urldate = {2020-12-22},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Qiu, Haonan and Xiao, Chaowei and Yang, Lei and Yan, Xinchen and Lee, Honglak and Li, Bo},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	doi = {10.1007/978-3-030-58568-6_2},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {19--37},
}

@inproceedings{vedaldi_adversarial_2020,
	address = {Cham},
	title = {Adversarial {Robustness} on {In}- and {Out}-{Distribution} {Improves} {Explainability}},
	volume = {12371},
	isbn = {978-3-030-58573-0 978-3-030-58574-7},
	url = {http://link.springer.com/10.1007/978-3-030-58574-7_14},
	doi = {10.1007/978-3-030-58574-7_14},
	abstract = {Neural networks have led to major improvements in image classiﬁcation but suﬀer from being non-robust to adversarial changes, unreliable uncertainty estimates on out-distribution samples and their inscrutable black-box decisions. In this work we propose RATIO, a training procedure for Robustness via Adversarial Training on In- and Outdistribution, which leads to robust models with reliable and robust conﬁdence estimates on the out-distribution. RATIO has similar generative properties to adversarial training so that visual counterfactuals produce class speciﬁc features. While adversarial training comes at the price of lower clean accuracy, RATIO achieves state-of-the-art l2-adversarial robustness on CIFAR10 and maintains better clean accuracy.},
	language = {en},
	urldate = {2020-12-22},
	booktitle = {Proceedings of the {European} {Conference} on {Computer} {Vision} ({ECCV})},
	publisher = {Springer International Publishing},
	author = {Augustin, Maximilian and Meinke, Alexander and Hein, Matthias},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {228--245},
}

@article{goyal_inductive_2020,
	title = {Inductive {Biases} for {Deep} {Learning} of {Higher}-{Level} {Cognition}},
	url = {http://arxiv.org/abs/2011.15091},
	abstract = {A fascinating hypothesis is that human and animal intelligence could be explained by a few principles (rather than an encyclopedic list of heuristics). If that hypothesis was correct, we could more easily both understand our own intelligence and build intelligent machines. Just like in physics, the principles themselves would not be suﬃcient to predict the behavior of complex systems like brains, and substantial computation might be needed to simulate human-like intelligence. This hypothesis would suggest that studying the kind of inductive biases that humans and animals exploit could help both clarify these principles and provide inspiration for AI research and neuroscience theories. Deep learning already exploits several key inductive biases, and this work considers a larger list, focusing on those which concern mostly higher-level and sequential conscious processing. The objective of clarifying these particular principles is that they could potentially help us build AI systems beneﬁting from humans’ abilities in terms of ﬂexible out-of-distribution and systematic generalization, which is currently an area where a large gap exists between state-of-the-art machine learning and human intelligence.},
	language = {en},
	urldate = {2020-12-22},
	journal = {arXiv:2011.15091 [cs, stat]},
	author = {Goyal, Anirudh and Bengio, Yoshua},
	month = dec,
	year = {2020},
	note = {arXiv: 2011.15091},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{bastani_interpreting_2019,
	title = {Interpreting {Blackbox} {Models} via {Model} {Extraction}},
	url = {http://arxiv.org/abs/1705.08504},
	abstract = {Interpretability has become incredibly important as machine learning is increasingly used to inform consequential decisions. We propose to construct global explanations of complex, blackbox models in the form of a decision tree approximating the original model—as long as the decision tree is a good approximation, then it mirrors the computation performed by the blackbox model. We devise a novel algorithm for extracting decision tree explanations that actively samples new training points to avoid overﬁtting. We evaluate our algorithm on a random forest to predict diabetes risk and a learned controller for cart-pole. Compared to several baselines, our decision trees are both substantially more accurate and equally or more interpretable based on a user study. Finally, we describe several insights provided by our interpretations, including a causal issue validated by a physician.},
	language = {en},
	urldate = {2020-12-22},
	journal = {arXiv:1705.08504 [cs]},
	author = {Bastani, Osbert and Kim, Carolyn and Bastani, Hamsa},
	month = jan,
	year = {2019},
	note = {arXiv: 1705.08504},
	keywords = {Computer Science - Machine Learning},
}

@article{bouchacourt_educe_2019,
	title = {{EDUCE}: {Explaining} model {Decisions} through {Unsupervised} {Concepts} {Extraction}},
	shorttitle = {{EDUCE}},
	url = {http://arxiv.org/abs/1905.11852},
	abstract = {Providing explanations along with predictions is crucial in some text processing tasks. Therefore, we propose a new self-interpretable model that performs output prediction and simultaneously provides an explanation in terms of the presence of particular concepts in the input. To do so, our model’s prediction relies solely on a low-dimensional binary representation of the input, where each feature denotes the presence or absence of concepts. The presence of a concept is decided from an excerpt i.e. a small sequence of consecutive words in the text. Relevant concepts for the prediction task at hand are automatically deﬁned by our model, avoiding the need for concept-level annotations. To ease interpretability, we enforce that for each concept, the corresponding excerpts share similar semantics and are differentiable from each others. We experimentally demonstrate the relevance of our approach on text classiﬁcation and multi-sentiment analysis tasks.},
	language = {en},
	urldate = {2020-12-22},
	journal = {arXiv:1905.11852 [cs, stat]},
	author = {Bouchacourt, Diane and Denoyer, Ludovic},
	month = sep,
	year = {2019},
	note = {arXiv: 1905.11852},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{puri_magix_2018,
	title = {{MAGIX}: {Model} {Agnostic} {Globally} {Interpretable} {Explanations}},
	shorttitle = {{MAGIX}},
	url = {http://arxiv.org/abs/1706.07160},
	abstract = {Explaining the behavior of a black box machine learning model at the instance level is useful for building trust. However, it is also important to understand how the model behaves globally. Such an understanding provides insight into both the data on which the model was trained and the patterns that it learned. We present here an approach that learns if-then rules to globally explain the behavior of black box machine learning models that have been used to solve classiﬁcation problems. The approach works by ﬁrst extracting conditions that were important at the instance level and then evolving rules through a genetic algorithm with an appropriate ﬁtness function. Collectively, these rules represent the patterns followed by the model for decisioning and are useful for understanding its behavior. We demonstrate the validity and usefulness of the approach by interpreting black box models created using publicly available data sets as well as a private digital marketing data set.},
	language = {en},
	urldate = {2020-12-22},
	journal = {arXiv:1706.07160 [cs]},
	author = {Puri, Nikaash and Gupta, Piyush and Agarwal, Pratiksha and Verma, Sukriti and Krishnamurthy, Balaji},
	month = jun,
	year = {2018},
	note = {arXiv: 1706.07160},
	keywords = {Computer Science - Artificial Intelligence},
}

@article{he_extract_2020,
	title = {Extract interpretability-accuracy balanced rules from artificial neural networks: {A} review},
	volume = {387},
	issn = {0925-2312},
	shorttitle = {Extract interpretability-accuracy balanced rules from artificial neural networks},
	url = {http://www.sciencedirect.com/science/article/pii/S0925231220300801},
	doi = {10.1016/j.neucom.2020.01.036},
	abstract = {Artificial neural networks (ANN) have been widely used and have achieved remarkable achievements. However, neural networks with high accuracy and good performance often have extremely complex internal structures such as deep neural networks (DNN). This shortcoming makes the neural networks as incomprehensible as a black box, which is unacceptable in some practical applications. But pursuing excessive interpretation of the neural networks will make the performance of the model worse. Based on this contradictory issue, we first summarize the mainstream methods about quantitatively evaluating the accuracy and interpretability of rule set. And then review existing methods on extracting rules from Multilayer Perceptron (MLP) and DNN in three categories: Decomposition Approach (Extract rules in neuron level such as visualizing the structure of network), Pedagogical Approach (By studying the correspondence between input and output such as by computing gradient) and Eclectics Approach (Combine the above two ideas). Some potential research directions about extracting rules from DNN are discussed in the last.},
	language = {en},
	urldate = {2020-12-22},
	journal = {Neurocomputing},
	author = {He, Congjie and Ma, Meng and Wang, Ping},
	month = apr,
	year = {2020},
	keywords = {Accuracy, Deep neural network, Interpretability, Multilayer Perceptron, Rule extraction},
	pages = {346--358},
}

@inproceedings{foerster_input_2017,
	title = {Input {Switched} {Affine} {Networks}: {An} {RNN} {Architecture} {Designed} for {Interpretability}},
	shorttitle = {Input {Switched} {Affine} {Networks}},
	url = {http://proceedings.mlr.press/v70/foerster17a.html},
	abstract = {There exist many problem domains where the interpretability of neural network models is essential for deployment. Here we introduce a recurrent architecture composed of input-switched affine transf...},
	language = {en},
	urldate = {2020-12-22},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Foerster, Jakob N. and Gilmer, Justin and Sohl-Dickstein, Jascha and Chorowski, Jan and Sussillo, David},
	month = jul,
	year = {2017},
	note = {ISSN: 2640-3498},
	pages = {1136--1145},
}

@inproceedings{lakkaraju_faithful_2019,
	address = {New York, NY, USA},
	series = {{AIES} '19},
	title = {Faithful and {Customizable} {Explanations} of {Black} {Box} {Models}},
	isbn = {978-1-4503-6324-2},
	url = {https://doi.org/10.1145/3306618.3314229},
	doi = {10.1145/3306618.3314229},
	abstract = {As predictive models increasingly assist human experts (e.g., doctors) in day-to-day decision making, it is crucial for experts to be able to explore and understand how such models behave in different feature subspaces in order to know if and when to trust them. To this end, we propose Model Understanding through Subspace Explanations (MUSE), a novel model agnostic framework which facilitates understanding of a given black box model by explaining how it behaves in subspaces characterized by certain features of interest. Our framework provides end users (e.g., doctors) with the flexibility of customizing the model explanations by allowing them to input the features of interest. The construction of explanations is guided by a novel objective function that we propose to simultaneously optimize for fidelity to the original model, unambiguity and interpretability of the explanation. More specifically, our objective allows us to learn, with optimality guarantees, a small number of compact decision sets each of which captures the behavior of a given black box model in unambiguous, well-defined regions of the feature space. Experimental evaluation with real-world datasets and user studies demonstrate that our approach can generate customizable, highly compact, easy-to-understand, yet accurate explanations of various kinds of predictive models compared to state-of-the-art baselines.},
	urldate = {2020-12-21},
	booktitle = {Proceedings of the 2019 {AAAI}/{ACM} {Conference} on {AI}, {Ethics}, and {Society}},
	publisher = {Association for Computing Machinery},
	author = {Lakkaraju, Himabindu and Kamar, Ece and Caruana, Rich and Leskovec, Jure},
	month = jan,
	year = {2019},
	keywords = {black box models, decision making, interpretable machine learning},
	pages = {131--138},
}

@inproceedings{kumar_problems_2020,
	title = {Problems with {Shapley}-value-based explanations as feature importance measures},
	url = {http://proceedings.mlr.press/v119/kumar20e.html},
	abstract = {Game-theoretic formulations of feature importance have become popular as a way to},
	language = {en},
	urldate = {2020-12-22},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Kumar, I. Elizabeth and Venkatasubramanian, Suresh and Scheidegger, Carlos and Friedler, Sorelle},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {5491--5500},
}

@inproceedings{lakkaraju_interpretable_2016,
	address = {New York, NY, USA},
	series = {{KDD} '16},
	title = {Interpretable {Decision} {Sets}: {A} {Joint} {Framework} for {Description} and {Prediction}},
	isbn = {978-1-4503-4232-2},
	shorttitle = {Interpretable {Decision} {Sets}},
	url = {https://doi.org/10.1145/2939672.2939874},
	doi = {10.1145/2939672.2939874},
	abstract = {One of the most important obstacles to deploying predictive models is the fact that humans do not understand and trust them. Knowing which variables are important in a model's prediction and how they are combined can be very powerful in helping people understand and trust automatic decision making systems. Here we propose interpretable decision sets, a framework for building predictive models that are highly accurate, yet also highly interpretable. Decision sets are sets of independent if-then rules. Because each rule can be applied independently, decision sets are simple, concise, and easily interpretable. We formalize decision set learning through an objective function that simultaneously optimizes accuracy and interpretability of the rules. In particular, our approach learns short, accurate, and non-overlapping rules that cover the whole feature space and pay attention to small but important classes. Moreover, we prove that our objective is a non-monotone submodular function, which we efficiently optimize to find a near-optimal set of rules. Experiments show that interpretable decision sets are as accurate at classification as state-of-the-art machine learning techniques. They are also three times smaller on average than rule-based models learned by other methods. Finally, results of a user study show that people are able to answer multiple-choice questions about the decision boundaries of interpretable decision sets and write descriptions of classes based on them faster and more accurately than with other rule-based models that were designed for interpretability. Overall, our framework provides a new approach to interpretable machine learning that balances accuracy, interpretability, and computational efficiency.},
	urldate = {2020-12-21},
	booktitle = {Proceedings of the 22nd {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Lakkaraju, Himabindu and Bach, Stephen H. and Leskovec, Jure},
	month = aug,
	year = {2016},
	keywords = {classification, decision sets, interpretable machine learning, submodularity},
	pages = {1675--1684},
}

@inproceedings{tan_distill-and-compare_2018,
	address = {New York, NY, USA},
	series = {{AIES} '18},
	title = {Distill-and-{Compare}: {Auditing} {Black}-{Box} {Models} {Using} {Transparent} {Model} {Distillation}},
	isbn = {978-1-4503-6012-8},
	shorttitle = {Distill-and-{Compare}},
	url = {https://doi.org/10.1145/3278721.3278725},
	doi = {10.1145/3278721.3278725},
	abstract = {Black-box risk scoring models permeate our lives, yet are typically proprietary or opaque. We propose Distill-and-Compare, an approach to audit such models without probing the black-box model API or pre-defining features to audit. To gain insight into black-box models, we treat them as teachers, training transparent student models to mimic the risk scores assigned by the black-box models. We compare the mimic model trained with distillation to a second, un-distilled transparent model trained on ground truth outcomes, and use differences between the two models to gain insight into the black-box model. We demonstrate the approach on four data sets: COMPAS, Stop-and-Frisk, Chicago Police, and Lending Club. We also propose a statistical test to determine if a data set is missing key features used to train the black-box model. Our test finds that the ProPublica data is likely missing key feature(s) used in COMPAS.},
	urldate = {2020-12-21},
	booktitle = {Proceedings of the 2018 {AAAI}/{ACM} {Conference} on {AI}, {Ethics}, and {Society}},
	publisher = {Association for Computing Machinery},
	author = {Tan, Sarah and Caruana, Rich and Hooker, Giles and Lou, Yin},
	month = dec,
	year = {2018},
	keywords = {black-box models, distillation, fairness, interpretability},
	pages = {303--310},
}

@inproceedings{slack_fooling_2020,
	address = {New York, NY, USA},
	series = {{AIES} '20},
	title = {Fooling {LIME} and {SHAP}: {Adversarial} {Attacks} on {Post} hoc {Explanation} {Methods}},
	isbn = {978-1-4503-7110-0},
	shorttitle = {Fooling {LIME} and {SHAP}},
	url = {https://doi.org/10.1145/3375627.3375830},
	doi = {10.1145/3375627.3375830},
	abstract = {As machine learning black boxes are increasingly being deployed in domains such as healthcare and criminal justice, there is growing emphasis on building tools and techniques for explaining these black boxes in an interpretable manner. Such explanations are being leveraged by domain experts to diagnose systematic errors and underlying biases of black boxes. In this paper, we demonstrate that post hoc explanations techniques that rely on input perturbations, such as LIME and SHAP, are not reliable. Specifically, we propose a novel scaffolding technique that effectively hides the biases of any given classifier by allowing an adversarial entity to craft an arbitrary desired explanation. Our approach can be used to scaffold any biased classifier in such a way that its predictions on the input data distribution still remain biased, but the post hoc explanations of the scaffolded classifier look innocuous. Using extensive evaluation with multiple real world datasets (including COMPAS), we demonstrate how extremely biased (racist) classifiers crafted by our framework can easily fool popular explanation techniques such as LIME and SHAP into generating innocuous explanations which do not reflect the underlying biases.},
	urldate = {2020-12-21},
	booktitle = {Proceedings of the {AAAI}/{ACM} {Conference} on {AI}, {Ethics}, and {Society}},
	publisher = {Association for Computing Machinery},
	author = {Slack, Dylan and Hilgard, Sophie and Jia, Emily and Singh, Sameer and Lakkaraju, Himabindu},
	month = feb,
	year = {2020},
	keywords = {adversarial attacks, bias detection, black box explanations, model interpretability},
	pages = {180--186},
}

@article{lee_game-theoretic_2018,
	title = {Game-{Theoretic} {Interpretability} for {Temporal} {Modeling}},
	url = {http://arxiv.org/abs/1807.00130},
	abstract = {Interpretability has arisen as a key desideratum of machine learning models alongside performance. Approaches so far have been primarily concerned with ﬁxed dimensional inputs emphasizing feature relevance or selection. In contrast, we focus on temporal modeling and the problem of tailoring the predictor, functionally, towards an interpretable family. To this end, we propose a co-operative game between the predictor and an explainer without any a priori restrictions on the functional class of the predictor. The goal of the explainer is to highlight, locally, how well the predictor conforms to the chosen interpretable family of temporal models. Our co-operative game is setup asymmetrically in terms of information sets for efﬁciency reasons. We develop and illustrate the framework in the context of temporal sequence models with examples.},
	language = {en},
	urldate = {2020-12-21},
	journal = {arXiv:1807.00130 [cs, stat]},
	author = {Lee, Guang-He and Alvarez-Melis, David and Jaakkola, Tommi S.},
	month = jun,
	year = {2018},
	note = {arXiv: 1807.00130},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{saxe_information_2019,
	title = {On the information bottleneck theory of deep learning},
	volume = {2019},
	issn = {1742-5468},
	url = {https://doi.org/10.1088/1742-5468/ab3985},
	doi = {10.1088/1742-5468/ab3985},
	abstract = {The practical successes of deep neural networks have not been matched by theoretical progress that satisfyingly explains their behavior. In this work, we study the information bottleneck (IB) theory of deep learning, which makes three specific claims: first, that deep networks undergo two distinct phases consisting of an initial fitting phase and a subsequent compression phase; second, that the compression phase is causally related to the excellent generalization performance of deep networks; and third, that the compression phase occurs due to the diffusion-like behavior of stochastic gradient descent. Here we show that none of these claims hold true in the general case, and instead reflect assumptions made to compute a finite mutual information metric in deterministic networks. When computed using simple binning, we demonstrate through a combination of analytical results and simulation that the information plane trajectory observed in prior work is predominantly a function of the neural nonlinearity employed: double-sided saturating nonlinearities like yield a compression phase as neural activations enter the saturation regime, but linear activation functions and single-sided saturating nonlinearities like the widely used ReLU in fact do not. Moreover, we find that there is no evident causal connection between compression and generalization: networks that do not compress are still capable of generalization, and vice versa. Next, we show that the compression phase, when it exists, does not arise from stochasticity in training by demonstrating that we can replicate the IB findings using full batch gradient descent rather than stochastic gradient descent. Finally, we show that when an input domain consists of a subset of task-relevant and task-irrelevant information, hidden representations do compress the task-irrelevant information, although the overall information about the input may monotonically increase with training time, and that this compression happens concurrently with the fitting process rather than during a subsequent compression period.},
	language = {en},
	number = {12},
	urldate = {2020-12-21},
	journal = {Journal of Statistical Mechanics: Theory and Experiment},
	author = {Saxe, Andrew M. and Bansal, Yamini and Dapello, Joel and Advani, Madhu and Kolchinsky, Artemy and Tracey, Brendan D. and Cox, David D.},
	month = dec,
	year = {2019},
	note = {Publisher: IOP Publishing},
	pages = {124020},
}

@inproceedings{locatello_challenging_2019,
	title = {Challenging {Common} {Assumptions} in the {Unsupervised} {Learning} of {Disentangled} {Representations}},
	url = {http://proceedings.mlr.press/v97/locatello19a.html},
	abstract = {The key idea behind the unsupervised learning of disentangled representations is that real-world data is generated by a few explanatory factors of variation which can be recovered by unsupervised l...},
	language = {en},
	urldate = {2020-12-21},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Locatello, Francesco and Bauer, Stefan and Lucic, Mario and Raetsch, Gunnar and Gelly, Sylvain and Schölkopf, Bernhard and Bachem, Olivier},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {4114--4124},
}

@inproceedings{liu_adversarial_2018,
	address = {New York, NY, USA},
	series = {{KDD} '18},
	title = {Adversarial {Detection} with {Model} {Interpretation}},
	isbn = {978-1-4503-5552-0},
	url = {https://doi.org/10.1145/3219819.3220027},
	doi = {10.1145/3219819.3220027},
	abstract = {Machine learning (ML) systems have been increasingly applied in web security applications such as spammer detection, malware detection and fraud detection. These applications have an intrinsic adversarial nature where intelligent attackers can adaptively change their behaviors to avoid being detected by the deployed detectors. Existing efforts against adversaries are usually limited by the type of applied ML models or the specific applications such as image classification. Additionally, the working mechanisms of ML models usually cannot be well understood by users, which in turn impede them from understanding the vulnerabilities of models nor improving their robustness. To bridge the gap, in this paper, we propose to investigate whether model interpretation could potentially help adversarial detection. Specifically, we develop a novel adversary-resistant detection framework by utilizing the interpretation of ML models. The interpretation process explains the mechanism of how the target ML model makes prediction for a given instance, thus providing more insights for crafting adversarial samples. The robustness of detectors is then improved through adversarial training with the adversarial samples. A data-driven method is also developed to empirically estimate costs of adversaries in feature manipulation. Our approach is model-agnostic and can be applied to various types of classification models. Our experimental results on two real-world datasets demonstrate the effectiveness of interpretation-based attacks and how estimated feature manipulation cost would affect the behavior of adversaries.},
	urldate = {2020-12-21},
	booktitle = {Proceedings of the 24th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Liu, Ninghao and Yang, Hongxia and Hu, Xia},
	month = jul,
	year = {2018},
	keywords = {adversarial detection, machine learning interpretation, spammer detection},
	pages = {1803--1811},
}

@inproceedings{alvarez-melis_causal_2017,
	address = {Copenhagen, Denmark},
	title = {A causal framework for explaining the predictions of black-box sequence-to-sequence models},
	url = {https://www.aclweb.org/anthology/D17-1042},
	doi = {10.18653/v1/D17-1042},
	abstract = {We interpret the predictions of any black-box structured input-structured output model around a specific input-output pair. Our method returns an “explanation” consisting of groups of input-output tokens that are causally related. These dependencies are inferred by querying the model with perturbed inputs, generating a graph over tokens from the responses, and solving a partitioning problem to select the most relevant components. We focus the general approach on sequence-to-sequence problems, adopting a variational autoencoder to yield meaningful input perturbations. We test our method across several NLP sequence generation tasks.},
	urldate = {2020-12-21},
	booktitle = {Proceedings of the 2017 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Alvarez-Melis, David and Jaakkola, Tommi},
	month = sep,
	year = {2017},
	pages = {412--421},
}

@inproceedings{serrano_is_2019,
	address = {Florence, Italy},
	title = {Is {Attention} {Interpretable}?},
	url = {https://www.aclweb.org/anthology/P19-1282},
	doi = {10.18653/v1/P19-1282},
	abstract = {Attention mechanisms have recently boosted performance on a range of NLP tasks. Because attention layers explicitly weight input components' representations, it is also often assumed that attention can be used to identify information that models found important (e.g., specific contextualized word tokens). We test whether that assumption holds by manipulating attention weights in already-trained text classification models and analyzing the resulting differences in their predictions. While we observe some ways in which higher attention weights correlate with greater impact on model predictions, we also find many ways in which this does not hold, i.e., where gradient-based rankings of attention weights better predict their effects than their magnitudes. We conclude that while attention noisily predicts input components' overall importance to a model, it is by no means a fail-safe indicator.},
	urldate = {2020-12-21},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Serrano, Sofia and Smith, Noah A.},
	month = jul,
	year = {2019},
	pages = {2931--2951},
}

@inproceedings{lee_functional_2019,
	title = {Functional {Transparency} for {Structured} {Data}: a {Game}-{Theoretic} {Approach}},
	shorttitle = {Functional {Transparency} for {Structured} {Data}},
	url = {http://proceedings.mlr.press/v97/lee19b.html},
	abstract = {We provide a new approach to training neural models to exhibit transparency in a well-defined, functional manner. Our approach naturally operates over structured data and tailors the predictor, fun...},
	language = {en},
	urldate = {2020-12-21},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Lee, Guang-He and Jin, Wengong and Alvarez-Melis, David and Jaakkola, Tommi},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {3723--3733},
}

@article{darlow_what_2020,
	title = {What {Information} {Does} a {ResNet} {Compress}?},
	url = {http://arxiv.org/abs/2003.06254},
	abstract = {The information bottleneck principle (Shwartz-Ziv \& Tishby, 2017) suggests that SGD-based training of deep neural networks results in optimally compressed hidden layers, from an information theoretic perspective. However, this claim was established on toy data. The goal of the work we present here is to test whether the information bottleneck principle is applicable to a realistic setting using a larger and deeper convolutional architecture, a ResNet model. We trained PixelCNN++ models as inverse representation decoders to measure the mutual information between hidden layers of a ResNet and input image data, when trained for (1) classification and (2) autoencoding. We find that two stages of learning happen for both training regimes, and that compression does occur, even for an autoencoder. Sampling images by conditioning on hidden layers' activations offers an intuitive visualisation to understand what a ResNets learns to forget.},
	urldate = {2020-12-21},
	journal = {arXiv:2003.06254 [cs, stat]},
	author = {Darlow, Luke Nicholas and Storkey, Amos},
	month = mar,
	year = {2020},
	note = {arXiv: 2003.06254},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{rahaman_spectral_2019,
	title = {On the {Spectral} {Bias} of {Neural} {Networks}},
	url = {http://proceedings.mlr.press/v97/rahaman19a.html},
	abstract = {Neural networks are known to be a class of highly expressive functions able to fit even random input-output mappings with 100\% accuracy. In this work we present properties of neural networks that c...},
	language = {en},
	urldate = {2020-12-21},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Rahaman, Nasim and Baratin, Aristide and Arpit, Devansh and Draxler, Felix and Lin, Min and Hamprecht, Fred and Bengio, Yoshua and Courville, Aaron},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {5301--5310},
}

@inproceedings{bhatt_explainable_2020,
	address = {New York, NY, USA},
	series = {{FAT}* '20},
	title = {Explainable machine learning in deployment},
	isbn = {978-1-4503-6936-7},
	url = {https://doi.org/10.1145/3351095.3375624},
	doi = {10.1145/3351095.3375624},
	abstract = {Explainable machine learning offers the potential to provide stakeholders with insights into model behavior by using various methods such as feature importance scores, counterfactual explanations, or influential training data. Yet there is little understanding of how organizations use these methods in practice. This study explores how organizations view and use explainability for stakeholder consumption. We find that, currently, the majority of deployments are not for end users affected by the model but rather for machine learning engineers, who use explainability to debug the model itself. There is thus a gap between explainability in practice and the goal of transparency, since explanations primarily serve internal stakeholders rather than external ones. Our study synthesizes the limitations of current explainability techniques that hamper their use for end users. To facilitate end user interaction, we develop a framework for establishing clear goals for explainability. We end by discussing concerns raised regarding explainability.},
	urldate = {2020-12-21},
	booktitle = {Proceedings of the 2020 {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {Association for Computing Machinery},
	author = {Bhatt, Umang and Xiang, Alice and Sharma, Shubham and Weller, Adrian and Taly, Ankur and Jia, Yunhan and Ghosh, Joydeep and Puri, Ruchir and Moura, José M. F. and Eckersley, Peter},
	month = jan,
	year = {2020},
	keywords = {deployed systems, explainability, machine learning, qualitative study, transparency},
	pages = {648--657},
}

@inproceedings{zhou_interpretable_2018,
	title = {Interpretable {Basis} {Decomposition} for {Visual} {Explanation}},
	url = {https://openaccess.thecvf.com/content_ECCV_2018/html/Antonio_Torralba_Interpretable_Basis_Decomposition_ECCV_2018_paper.html},
	urldate = {2020-12-21},
	author = {Zhou, Bolei and Sun, Yiyou and Bau, David and Torralba, Antonio},
	year = {2018},
	pages = {119--134},
}

@article{xu_causality_2020,
	title = {Causality {Learning}: {A} {New} {Perspective} for {Interpretable} {Machine} {Learning}},
	shorttitle = {Causality {Learning}},
	url = {http://arxiv.org/abs/2006.16789},
	abstract = {Recent years have witnessed the rapid growth of machine learning in a wide range of fields such as image recognition, text classification, credit scoring prediction, recommendation system, etc. In spite of their great performance in different sectors, researchers still concern about the mechanism under any machine learning (ML) techniques that are inherently black-box and becoming more complex to achieve higher accuracy. Therefore, interpreting machine learning model is currently a mainstream topic in the research community. However, the traditional interpretable machine learning focuses on the association instead of the causality. This paper provides an overview of causal analysis with the fundamental background and key concepts, and then summarizes most recent causal approaches for interpretable machine learning. The evaluation techniques for assessing method quality, and open problems in causal interpretability are also discussed in this paper.},
	urldate = {2020-12-21},
	journal = {arXiv:2006.16789 [cs, stat]},
	author = {Xu, Guandong and Duong, Tri Dung and Li, Qian and Liu, Shaowu and Wang, Xianzhi},
	month = jun,
	year = {2020},
	note = {arXiv: 2006.16789},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{parafita_explaining_2019,
	title = {Explaining {Visual} {Models} by {Causal} {Attribution}},
	url = {http://arxiv.org/abs/1909.08891},
	abstract = {Model explanations based on pure observational data cannot compute the effects of features reliably, due to their inability to estimate how each factor alteration could affect the rest. We argue that explanations should be based on the causal model of the data and the derived intervened causal models, that represent the data distribution subject to interventions. With these models, we can compute counterfactuals, new samples that will inform us how the model reacts to feature changes on our input. We propose a novel explanation methodology based on Causal Counterfactuals and identify the limitations of current Image Generative Models in their application to counterfactual creation.},
	urldate = {2020-12-21},
	journal = {arXiv:1909.08891 [cs, stat]},
	author = {Parafita, Álvaro and Vitrià, Jordi},
	month = sep,
	year = {2019},
	note = {arXiv: 1909.08891},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@inproceedings{khanna_interpreting_2019,
	title = {Interpreting {Black} {Box} {Predictions} using {Fisher} {Kernels}},
	url = {http://proceedings.mlr.press/v89/khanna19a.html},
	abstract = {Research in both machine learning and psychology suggests that salient examples can help humans to interpret learning models. To this end, we take a novel look at black box interpretation of test p...},
	language = {en},
	urldate = {2020-12-21},
	booktitle = {The 22nd {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Khanna, Rajiv and Kim, Been and Ghosh, Joydeep and Koyejo, Sanmi},
	month = apr,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {3382--3390},
}

@inproceedings{carter_what_2019,
	title = {What made you do this? {Understanding} black-box decisions with sufficient input subsets},
	shorttitle = {What made you do this?},
	url = {http://proceedings.mlr.press/v89/carter19a.html},
	abstract = {Local explanation frameworks aim to rationalize particular decisions made by a black-box prediction model.  Existing techniques are often restricted to a specific type of predictor or based on inpu...},
	language = {en},
	urldate = {2020-12-21},
	booktitle = {The 22nd {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Carter, Brandon and Mueller, Jonas and Jain, Siddhartha and Gifford, David},
	month = apr,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {567--576},
}

@inproceedings{ghorbani_towards_2019,
	title = {Towards {Automatic} {Concept}-based {Explanations}},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper/2019/hash/77d2afcb31f6493e350fca61764efb9a-Abstract.html},
	language = {en},
	urldate = {2020-12-21},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Ghorbani, Amirata and Wexler, James and Zou, James Y. and Kim, Been},
	year = {2019},
	pages = {9277--9286},
}

@inproceedings{grimsley_why_2020,
	address = {Marseille, France},
	title = {Why {Attention} is {Not} {Explanation}: {Surgical} {Intervention} and {Causal} {Reasoning} about {Neural} {Models}},
	isbn = {979-10-95546-34-4},
	shorttitle = {Why {Attention} is {Not} {Explanation}},
	url = {https://www.aclweb.org/anthology/2020.lrec-1.220},
	abstract = {As the demand for explainable deep learning grows in the evaluation of language technologies, the value of a principled grounding for those explanations grows as well. Here we study the state-of-the-art in explanation for neural models for NLP tasks from the viewpoint of philosophy of science. We focus on recent evaluation work that finds brittleness in explanations obtained through attention mechanisms. We harness philosophical accounts of explanation to suggest broader conclusions from these studies. From this analysis, we assert the impossibility of causal explanations from attention layers over text data. We then introduce NLP researchers to contemporary philosophy of science theories that allow robust yet non-causal reasoning in explanation, giving computer scientists a vocabulary for future research.},
	language = {English},
	urldate = {2020-12-16},
	booktitle = {Proceedings of the 12th {Language} {Resources} and {Evaluation} {Conference}},
	publisher = {European Language Resources Association},
	author = {Grimsley, Christopher and Mayfield, Elijah and R.S. Bursten, Julia},
	month = may,
	year = {2020},
	pages = {1780--1790},
}

@inproceedings{plumb_regularizing_2020,
	title = {Regularizing {Black}-box {Models} for {Improved} {Interpretability}},
	abstract = {Most of the work on interpretable machine learning has focused on designing either inherently interpretable models, which typically trade-off accuracy for interpretability, or post-hoc explanation systems, whose explanation quality can be unpredictable. Our method, EXPO, is a hybridization of these approaches that regularizes a model for explanation quality at training time. Importantly, these regularizers are differentiable, model agnostic, and require no domain knowledge to deﬁne. We demonstrate that post-hoc explanations for EXPO-regularized models have better explanation quality, as measured by the common ﬁdelity and stability metrics. We verify that improving these metrics leads to signiﬁcantly more useful explanations with a user study on a realistic task.},
	language = {en},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Plumb, Gregory and Al-Shedivat, Maruan and Cabrera, Ángel Alexander and Perer, Adam and Xing, Eric and Talwalkar, Ameet},
	year = {2020},
	pages = {11},
}

@inproceedings{ignatiev_relating_2019,
	title = {On {Relating} {Explanations} and {Adversarial} {Examples}},
	abstract = {The importance of explanations (XP’s) of machine learning (ML) model predictions and of adversarial examples (AE’s) cannot be overstated, with both arguably being essential for the practical success of ML in different settings. There has been recent work on understanding and assessing the relationship between XP’s and AE’s. However, such work has been mostly experimental and a sound theoretical relationship has been elusive. This paper demonstrates that explanations and adversarial examples are related by a generalized form of hitting set duality, which extends earlier work on hitting set duality observed in model-based diagnosis and knowledge compilation. Furthermore, the paper proposes algorithms, which enable computing adversarial examples from explanations and vice-versa.},
	language = {en},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Ignatiev, Alexey and Narodytska, Nina and Marques-Silva, Joao},
	year = {2019},
	pages = {11},
}

@inproceedings{shih_symbolic_2018,
	address = {Stockholm, Sweden},
	title = {A {Symbolic} {Approach} to {Explaining} {Bayesian} {Network} {Classifiers}},
	isbn = {978-0-9992411-2-7},
	url = {https://www.ijcai.org/proceedings/2018/708},
	doi = {10.24963/ijcai.2018/708},
	abstract = {We propose an approach for explaining Bayesian network classiﬁers, which is based on compiling such classiﬁers into decision functions that have a tractable and symbolic form. We introduce two types of explanations for why a classiﬁer may have classiﬁed an instance positively or negatively and suggest algorithms for computing these explanations. The ﬁrst type of explanation identiﬁes a minimal set of the currently active features that is responsible for the current classiﬁcation, while the second type of explanation identiﬁes a minimal set of features whose current state (active or not) is sufﬁcient for the classiﬁcation. We consider in particular the compilation of Naive and Latent-Tree Bayesian network classiﬁers into Ordered Decision Diagrams (ODDs), providing a context for evaluating our proposal using case studies and experiments based on classiﬁers from the literature.},
	language = {en},
	urldate = {2020-12-16},
	booktitle = {Proceedings of the {Twenty}-{Seventh} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Shih, Andy and Choi, Arthur and Darwiche, Adnan},
	month = jul,
	year = {2018},
	pages = {5103--5111},
}

@inproceedings{ignatiev_abduction-based_2019,
	title = {Abduction-{Based} {Explanations} for {Machine} {Learning} {Models}},
	volume = {33},
	copyright = {Copyright (c) 2019 Association for the Advancement of Artificial Intelligence},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/3964},
	doi = {10.1609/aaai.v33i01.33011511},
	language = {en},
	urldate = {2020-12-16},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Ignatiev, Alexey and Narodytska, Nina and Marques-Silva, Joao},
	month = jul,
	year = {2019},
	note = {Number: 01},
	pages = {1511--1519},
}

@article{nguyen_quantitative_2020,
	title = {On quantitative aspects of model interpretability},
	url = {http://arxiv.org/abs/2007.07584},
	abstract = {Despite the growing body of work in interpretable machine learning, it remains unclear how to evaluate different explainability methods without resorting to qualitative assessment and user-studies. While interpretability is an inherently subjective matter, previous works in cognitive science and epistemology have shown that good explanations do possess aspects that can be objectively judged apart from ﬁdelity), such as simplicity and broadness. In this paper we propose a set of metrics to programmatically evaluate interpretability methods along these dimensions. In particular, we argue that the performance of methods along these dimensions can be orthogonally imputed to two conceptual parts, namely the feature extractor and the actual explainability method. We experimentally validate our metrics on different benchmark tasks and show how they can be used to guide a practitioner in the selection of the most appropriate method for the task at hand.},
	language = {en},
	urldate = {2020-12-16},
	journal = {arXiv:2007.07584 [cs, stat]},
	author = {Nguyen, An-phi and Martínez, María Rodríguez},
	month = jul,
	year = {2020},
	note = {arXiv: 2007.07584},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{lin_what_2020,
	title = {What {Do} {You} {See}? {Evaluation} of {Explainable} {Artificial} {Intelligence} ({XAI}) {Interpretability} through {Neural} {Backdoors}},
	shorttitle = {What {Do} {You} {See}?},
	url = {http://arxiv.org/abs/2009.10639},
	abstract = {EXplainable AI (XAI) methods have been proposed to interpret how a deep neural network predicts inputs through model saliency explanations that highlight the parts of the inputs deemed important to arrive a decision at a speciﬁc target. However, it remains challenging to quantify correctness of their interpretability as current evaluation approaches either require subjective input from humans or incur high computation cost with automated evaluation. In this paper, we propose backdoor trigger patterns–hidden malicious functionalities that cause misclassiﬁcation–to automate the evaluation of saliency explanations. Our key observation is that triggers provide ground truth for inputs to evaluate whether the regions identiﬁed by an XAI method are truly relevant to its output. Since backdoor triggers are the most important features that cause deliberate misclassiﬁcation, a robust XAI method should reveal their presence at inference time. We introduce three complementary metrics for systematic evaluation of explanations that an XAI method generates and evaluate seven state-of-the-art model-free and model-speciﬁc posthoc methods through 36 models trojaned with speciﬁcally crafted triggers using color, shape, texture, location, and size. We discovered six methods that use local explanation and feature relevance fail to completely highlight trigger regions, and only a model-free approach can uncover the entire trigger region.},
	language = {en},
	urldate = {2020-12-16},
	journal = {arXiv:2009.10639 [cs]},
	author = {Lin, Yi-Shan and Lee, Wen-Chuan and Celik, Z. Berkay},
	month = sep,
	year = {2020},
	note = {arXiv: 2009.10639},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{zhang_improving_2020,
	title = {Improving {Interpretability} of {CNN} {Models} {Using} {Non}-{Negative} {Concept} {Activation} {Vectors}},
	url = {http://arxiv.org/abs/2006.15417},
	abstract = {Convolutional neural network (CNN) models for computer vision are powerful but lack explainability in their most basic form. This deﬁciency remains a key challenge when applying CNNs in important domains. Recent work for explanations through feature importance of approximate linear models has moved from input-level features (pixels or segments) to features from mid-layer feature maps in the guise of concept activation vectors (CAVs). CAVs contain concept-level information and could be learnt via Clustering. In this work, we rethink the ACE algorithm of Ghorbani et al., proposing an alternative concept-based explanation framework. Based on the requirements of ﬁdelity (approximate models) and interpretability (being meaningful to people), we design measurements and evaluate a range of dimensionality reduction methods for alignment with our framework. We ﬁnd that non-negative concept activation vectors from non-negative matrix factorization provide superior performance in interpretability and ﬁdelity based on computational and human subject experiments. Our framework provides both local and global concept-level explanations for pre-trained CNN models.},
	language = {en},
	urldate = {2020-12-16},
	journal = {arXiv:2006.15417 [cs]},
	author = {Zhang, Ruihan and Madumal, Prashan and Miller, Tim and Ehinger, Krista A. and Rubinstein, Benjamin I. P.},
	month = jul,
	year = {2020},
	note = {arXiv: 2006.15417},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@inproceedings{wang_gaining_2019,
	title = {Gaining {Free} or {Low}-{Cost} {Interpretability} with {Interpretable} {Partial} {Substitute}},
	url = {http://proceedings.mlr.press/v97/wang19a.html},
	abstract = {This work addresses the situation where a black-box model with good predictive performance is chosen over its interpretable competitors, and we show interpretability is still achievable in this cas...},
	language = {en},
	urldate = {2020-12-16},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Wang, Tong},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {6505--6514},
}

@inproceedings{amorim_interpretability_2020,
	title = {Interpretability vs. {Complexity}: {The} {Friction} in {Deep} {Neural} {Networks}},
	shorttitle = {Interpretability vs. {Complexity}},
	doi = {10.1109/IJCNN48605.2020.9206800},
	abstract = {Saliency maps have been used as one possibility to interpret deep neural networks. This method estimates the relevance of each pixel in the image classification, with higher values representing pixels which contribute positively to classification. The goal of this study is to understand how the complexity of the network affects the interpretabilty of the saliency maps in classification tasks. To achieve that, we investigate how changes in the regularization affects the saliency maps produced, and their fidelity to the overall classification process of the network.The experimental setup consists in the calculation of the fidelity of five saliency map methods that were compare, applying them to models trained on the CIFAR-10 dataset, using different levels of weight decay on some or all the layers. Achieved results show that models with lower regularization are statistically (significance of 5\%) more interpretable than the other models. Also, regularization applied only to the higher convolutional layers or fully-connected layers produce saliency maps with more fidelity.},
	booktitle = {2020 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	author = {Amorim, J. P. and Abreu, P. H. and Reyes, M. and Santos, J.},
	month = jul,
	year = {2020},
	note = {ISSN: 2161-4407},
	keywords = {CIFAR-10 dataset, CNN training, Complexity theory, Convolutional neural network, Data models, Mathematical model, Measurement, Neural networks, Perturbation methods, Training, complexity, convolutional neural nets, convolutional neural network, deep neural networks, feature extraction, image classification, image representation, interpretability, learning (artificial intelligence), pixel representation, saliency map, saliency map methods, saliency maps},
	pages = {1--7},
}

@inproceedings{dong_improving_2017,
	title = {Improving {Interpretability} of {Deep} {Neural} {Networks} {With} {Semantic} {Information}},
	url = {https://openaccess.thecvf.com/content_cvpr_2017/html/Dong_Improving_Interpretability_of_CVPR_2017_paper.html},
	urldate = {2020-12-16},
	author = {Dong, Yinpeng and Su, Hang and Zhu, Jun and Zhang, Bo},
	year = {2017},
	pages = {4306--4314},
}

@inproceedings{tsang_neural_2018,
	title = {Neural {Interaction} {Transparency} ({NIT}): {Disentangling} {Learned} {Interactions} for {Improved} {Interpretability}},
	volume = {31},
	shorttitle = {Neural {Interaction} {Transparency} ({NIT})},
	url = {https://proceedings.neurips.cc/paper/2018/hash/74378afe5e8b20910cf1f939e57f0480-Abstract.html},
	language = {en},
	urldate = {2020-12-16},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Tsang, Michael and Liu, Hanpeng and Purushotham, Sanjay and Murali, Pavankumar and Liu, Yan},
	year = {2018},
	pages = {5804--5813},
}

@inproceedings{gilpin_explaining_2018,
	title = {Explaining {Explanations}: {An} {Overview} of {Interpretability} of {Machine} {Learning}},
	shorttitle = {Explaining {Explanations}},
	doi = {10.1109/DSAA.2018.00018},
	abstract = {There has recently been a surge of work in explanatory artificial intelligence (XAI). This research area tackles the important problem that complex machines and algorithms often cannot provide insights into their behavior and thought processes. XAI allows users and parts of the internal system to be more transparent, providing explanations of their decisions in some level of detail. These explanations are important to ensure algorithmic fairness, identify potential bias/problems in the training data, and to ensure that the algorithms perform as expected. However, explanations produced by these systems is neither standardized nor systematically assessed. In an effort to create best practices and identify open challenges, we describe foundational concepts of explainability and show how they can be used to classify existing literature. We discuss why current approaches to explanatory methods especially for deep neural networks are insufficient. Finally, based on our survey, we conclude with suggested future research directions for explanatory artificial intelligence.},
	booktitle = {2018 {IEEE} 5th {International} {Conference} on {Data} {Science} and {Advanced} {Analytics} ({DSAA})},
	author = {Gilpin, L. H. and Bau, D. and Yuan, B. Z. and Bajwa, A. and Specter, M. and Kagal, L.},
	month = oct,
	year = {2018},
	keywords = {Artificial intelligence, Biological neural networks, Complexity theory, Computational modeling, Decision trees, Deep learning and deep analytics, Fairness and transparency in data science, Machine learning theories, Models and systems, Taxonomy, XAI, algorithmic fairness, artificial intelligence, complex machines, data analysis, explaining explanations, explanatory artificial intelligence, learning (artificial intelligence), machine learning, neural nets, potential bias-problems, suggested future research directions, training data},
	pages = {80--89},
}

@inproceedings{kim_interpretability_2018,
	title = {Interpretability {Beyond} {Feature} {Attribution}: {Quantitative} {Testing} with {Concept} {Activation} {Vectors} ({TCAV})},
	shorttitle = {Interpretability {Beyond} {Feature} {Attribution}},
	url = {http://proceedings.mlr.press/v80/kim18d.html},
	abstract = {The interpretation of deep learning models is a challenge due to their size, complexity, and often opaque internal state. In addition, many systems, such as image classifiers, operate on low-level ...},
	language = {en},
	urldate = {2020-12-16},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Kim, Been and Wattenberg, Martin and Gilmer, Justin and Cai, Carrie and Wexler, James and Viegas, Fernanda and Sayres, Rory},
	month = jul,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {2668--2677},
}

@article{nakkiran_adversarial_2019,
	title = {Adversarial {Robustness} {May} {Be} at {Odds} {With} {Simplicity}},
	url = {http://arxiv.org/abs/1901.00532},
	abstract = {Current techniques in machine learning are so far are unable to learn classiﬁers that are robust to adversarial perturbations. However, they are able to learn non-robust classiﬁers with very high accuracy, even in the presence of random perturbations. Towards explaining this gap, we highlight the hypothesis that robust classiﬁcation may require more complex classiﬁers (i.e. more capacity) than standard classiﬁcation.},
	language = {en},
	urldate = {2020-12-16},
	journal = {arXiv:1901.00532 [cs, stat]},
	author = {Nakkiran, Preetum},
	month = jan,
	year = {2019},
	note = {arXiv: 1901.00532},
	keywords = {Computer Science - Computational Complexity, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{jain_attention_2019,
	address = {Minneapolis, Minnesota},
	title = {Attention is not {Explanation}},
	url = {https://www.aclweb.org/anthology/N19-1357},
	doi = {10.18653/v1/N19-1357},
	abstract = {Attention mechanisms have seen wide adoption in neural NLP models. In addition to improving predictive performance, these are often touted as affording transparency: models equipped with attention provide a distribution over attended-to input units, and this is often presented (at least implicitly) as communicating the relative importance of inputs. However, it is unclear what relationship exists between attention weights and model outputs. In this work we perform extensive experiments across a variety of NLP tasks that aim to assess the degree to which attention weights provide meaningful “explanations” for predictions. We find that they largely do not. For example, learned attention weights are frequently uncorrelated with gradient-based measures of feature importance, and one can identify very different attention distributions that nonetheless yield equivalent predictions. Our findings show that standard attention modules do not provide meaningful explanations and should not be treated as though they do.},
	urldate = {2020-12-16},
	booktitle = {Proceedings of the 2019 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} and {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Jain, Sarthak and Wallace, Byron C.},
	month = jun,
	year = {2019},
	pages = {3543--3556},
}

@incollection{bostrom_ethics_2014,
	title = {The {Ethics} of {Artificial} {Intelligence}},
	abstract = {The possibility of creating thinking machines raises a host of ethical issues. These questions relate both to ensuring that such machines do not harm humans and other morally relevant beings, and to the moral status of the machines themselves. The first section discusses issues that may arise in the near future of AI. The second section outlines challenges for ensuring that AI operates safely as it approaches humans in its intelligence. The third section outlines how we might assess whether, and in what circumstances, AIs themselves have moral status. In the fourth section, we consider how AIs might diﬀer from humans in certain basic respects relevant to our ethical assessment of them. The final section addresses the issues of creating AIs more intelligent than human, and ensuring that they use their advanced intelligence for good rather than ill.},
	language = {en},
	booktitle = {The {Cambridge} {Handbook} of {Artificial} {Intelligence}},
	publisher = {Cambridge University Press},
	author = {Bostrom, Nick and Yudkowsky, Eliezer},
	month = jun,
	year = {2014},
	pages = {21},
}

@inproceedings{herlocker_explaining_2000,
	address = {New York, NY, USA},
	series = {{CSCW} '00},
	title = {Explaining collaborative filtering recommendations},
	isbn = {978-1-58113-222-9},
	url = {https://doi.org/10.1145/358916.358995},
	doi = {10.1145/358916.358995},
	abstract = {Automated collaborative filtering (ACF) systems predict a person's affinity for items or information by connecting that person's recorded interests with the recorded interests of a community of people and sharing ratings between like-minded persons. However, current recommender systems are black boxes, providing no transparency into the working of the recommendation. Explanations provide that transparency, exposing the reasoning and data behind a recommendation. In this paper, we address explanation interfaces for ACF systems - how they should be implemented and why they should be implemented. To explore how, we present a model for explanations based on the user's conceptual model of the recommendation process. We then present experimental results demonstrating what components of an explanation are the most compelling. To address why, we present experimental evidence that shows that providing explanations can improve the acceptance of ACF systems. We also describe some initial explorations into measuring how explanations can improve the filtering performance of users.},
	urldate = {2020-12-14},
	booktitle = {Proceedings of the 2000 {ACM} conference on {Computer} supported cooperative work},
	publisher = {Association for Computing Machinery},
	author = {Herlocker, Jonathan L. and Konstan, Joseph A. and Riedl, John},
	month = dec,
	year = {2000},
	keywords = {GroupLens, MoviesLens, collaborative filtering, explanations, recommender systems},
	pages = {241--250},
}

@article{ching_opportunities_2018,
	title = {Opportunities and obstacles for deep learning in biology and medicine},
	volume = {15},
	url = {https://royalsocietypublishing.org/doi/full/10.1098/rsif.2017.0387},
	doi = {10.1098/rsif.2017.0387},
	abstract = {Deep learning describes a class of machine learning algorithms that are capable of combining raw inputs into layers of intermediate features. These algorithms have recently shown impressive results across a variety of domains. Biology and medicine are data-rich disciplines, but the data are complex and often ill-understood. Hence, deep learning techniques may be particularly well suited to solve problems of these fields. We examine applications of deep learning to a variety of biomedical problems—patient classification, fundamental biological processes and treatment of patients—and discuss whether deep learning will be able to transform these tasks or if the biomedical sphere poses unique challenges. Following from an extensive literature review, we find that deep learning has yet to revolutionize biomedicine or definitively resolve any of the most pressing challenges in the field, but promising advances have been made on the prior state of the art. Even though improvements over previous baselines have been modest in general, the recent progress indicates that deep learning methods will provide valuable means for speeding up or aiding human investigation. Though progress has been made linking a specific neural network's prediction to input features, understanding how users should interpret these models to make testable hypotheses about the system under study remains an open challenge. Furthermore, the limited amount of labelled data for training presents problems in some domains, as do legal and privacy constraints on work with sensitive health records. Nonetheless, we foresee deep learning enabling changes at both bench and bedside with the potential to transform several areas of biology and medicine.},
	number = {141},
	urldate = {2020-12-14},
	journal = {Journal of The Royal Society Interface},
	author = {Ching, Travers and Himmelstein, Daniel S. and Beaulieu-Jones, Brett K. and Kalinin, Alexandr A. and Do, Brian T. and Way, Gregory P. and Ferrero, Enrico and Agapow, Paul-Michael and Zietz, Michael and Hoffman, Michael M. and Xie, Wei and Rosen, Gail L. and Lengerich, Benjamin J. and Israeli, Johnny and Lanchantin, Jack and Woloszynek, Stephen and Carpenter, Anne E. and Shrikumar, Avanti and Xu, Jinbo and Cofer, Evan M. and Lavender, Christopher A. and Turaga, Srinivas C. and Alexandari, Amr M. and Lu, Zhiyong and Harris, David J. and DeCaprio, Dave and Qi, Yanjun and Kundaje, Anshul and Peng, Yifan and Wiley, Laura K. and Segler, Marwin H. S. and Boca, Simina M. and Swamidass, S. Joshua and Huang, Austin and Gitter, Anthony and Greene, Casey S.},
	month = apr,
	year = {2018},
	note = {Publisher: Royal Society},
	pages = {20170387},
}

@article{goodman_european_2017,
	title = {European {Union} {Regulations} on {Algorithmic} {Decision}-{Making} and a “{Right} to {Explanation}”},
	volume = {38},
	copyright = {Copyright (c) 2017 AI Magazine},
	issn = {2371-9621},
	url = {https://ojs.aaai.org/index.php/aimagazine/article/view/2741},
	doi = {10.1609/aimag.v38i3.2741},
	language = {en},
	number = {3},
	urldate = {2020-12-14},
	journal = {AI Magazine},
	author = {Goodman, Bryce and Flaxman, Seth},
	month = oct,
	year = {2017},
	note = {Number: 3},
	pages = {50--57},
}

@article{dzindolet_role_2003,
	series = {Trust and {Technology}},
	title = {The role of trust in automation reliance},
	volume = {58},
	issn = {1071-5819},
	url = {http://www.sciencedirect.com/science/article/pii/S1071581903000387},
	doi = {10.1016/S1071-5819(03)00038-7},
	abstract = {A recent and dramatic increase in the use of automation has not yielded comparable improvements in performance. Researchers have found human operators often underutilize (disuse) and overly rely on (misuse) automated aids (Parasuraman and Riley, 1997). Three studies were performed with Cameron University students to explore the relationship among automation reliability, trust, and reliance. With the assistance of an automated decision aid, participants viewed slides of Fort Sill terrain and indicated the presence or absence of a camouflaged soldier. Results from the three studies indicate that trust is an important factor in understanding automation reliance decisions. Participants initially considered the automated decision aid trustworthy and reliable. After observing the automated aid make errors, participants distrusted even reliable aids, unless an explanation was provided regarding why the aid might err. Knowing why the aid might err increased trust in the decision aid and increased automation reliance, even when the trust was unwarranted. Our studies suggest a need for future research focused on understanding automation use, examining individual differences in automation reliance, and developing valid and reliable self-report measures of trust in automation.},
	language = {en},
	number = {6},
	urldate = {2020-12-14},
	journal = {International Journal of Human-Computer Studies},
	author = {Dzindolet, Mary T. and Peterson, Scott A. and Pomranky, Regina A. and Pierce, Linda G. and Beck, Hall P.},
	month = jun,
	year = {2003},
	keywords = {Automation reliance, Automation trust, Disuse, Misuse},
	pages = {697--718},
}

@inproceedings{kim_bridging_2019,
	title = {Bridging {Adversarial} {Robustness} and {Gradient} {Interpretability}},
	url = {http://arxiv.org/abs/1903.11626},
	abstract = {Adversarial training is a training scheme designed to counter adversarial attacks by augmenting the training dataset with adversarial examples. Surprisingly, several studies have observed that loss gradients from adversarially trained DNNs are visually more interpretable than those from standard DNNs. Although this phenomenon is interesting, there are only few works that have offered an explanation. In this paper, we attempted to bridge this gap between adversarial robustness and gradient interpretability. To this end, we identiﬁed that loss gradients from adversarially trained DNNs align better with human perception because adversarial training restricts gradients closer to the image manifold. We then demonstrated that adversarial training causes loss gradients to be quantitatively meaningful. Finally, we showed that under the adversarial training framework, there exists an empirical trade-off between test accuracy and loss gradient interpretability and proposed two potential approaches to resolving this trade-off.},
	language = {en},
	urldate = {2020-12-10},
	booktitle = {International {Conference} on {Learning} {Representations} {Workshop}},
	author = {Kim, Beomsu and Seo, Junghoon and Jeon, Taegyun},
	month = apr,
	year = {2019},
	note = {arXiv: 1903.11626},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{andriushchenko_understanding_2020,
	title = {Understanding and {Improving} {Fast} {Adversarial} {Training}},
	abstract = {A recent line of work focused on making adversarial training computationally efﬁcient for deep learning models. In particular, Wong et al. [47] showed that ∞-adversarial training with fast gradient sign method (FGSM) can fail due to a phenomenon called catastrophic overﬁtting, when the model quickly loses its robustness over a single epoch of training. We show that adding a random step to FGSM, as proposed in [47], does not prevent catastrophic overﬁtting, and that randomness is not important per se — its main role being simply to reduce the magnitude of the perturbation. Moreover, we show that catastrophic overﬁtting is not inherent to deep and overparametrized networks, but can occur in a single-layer convolutional network with a few ﬁlters. In an extreme case, even a single ﬁlter can make the network highly non-linear locally, which is the main reason why FGSM training fails. Based on this observation, we propose a new regularization method, GradAlign, that prevents catastrophic overﬁtting by explicitly maximizing the gradient alignment inside the perturbation set and improves the quality of the FGSM solution. As a result, GradAlign allows to successfully apply FGSM training also for larger ∞-perturbations and reduce the gap to multi-step adversarial training. The code of our experiments is available at https://github.com/tml-epfl/ understanding-fast-adv-training.},
	language = {en},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Andriushchenko, Maksym and Flammarion, Nicolas},
	year = {2020},
	pages = {12},
}

@inproceedings{cai_curriculum_2018,
	title = {Curriculum {Adversarial} {Training}},
	abstract = {Recently, deep learning has been applied to many security-sensitive applications, such as facial authentication. The existence of adversarial examples hinders such applications. The state-of-theart result on defense shows that adversarial training can be applied to train a robust model on MNIST against adversarial examples; but it fails to achieve a high empirical worst-case accuracy on a more complex task, such as CIFAR-10 and SVHN. In our work, we propose curriculum adversarial training (CAT) to resolve this issue. The basic idea is to develop a curriculum of adversarial examples generated by attacks with a wide range of strengths. With two techniques to mitigate the catastrophic forgetting and the generalization issues, we demonstrate that CAT can improve the prior art’s empirical worst-case accuracy by a large margin of 25\% on CIFAR-10 and 35\% on SVHN. At the same, the model’s performance on non-adversarial inputs is comparable to the state-of-the-art models.},
	language = {en},
	booktitle = {Proceedings of the {Twenty}-{Seventh} {International} {Joint} {Conference} on {Artificial} {Intelligence} ({IJCAI}-18)},
	author = {Cai, Qi-Zhi and Liu, Chang and Song, Dawn},
	year = {2018},
	pages = {8},
}

@inproceedings{wang_convergence_2019,
	title = {On the {Convergence} and {Robustness} of {Adversarial} {Training}},
	abstract = {Improving the robustness of deep neural networks (DNNs) to adversarial examples is an important yet challenging problem for secure deep learning. Across existing defense techniques, adversarial training with Projected Gradient Decent (PGD) is amongst the most effective. Adversarial training solves a min-max optimization problem, with the inner maximization generating adversarial examples by maximizing the classiﬁcation loss, and the outer minimization ﬁnding model parameters by minimizing the loss on adversarial examples generated from the inner maximization. A criterion that measures how well the inner maximization is solved is therefore crucial for adversarial training. In this paper, we propose such a criterion, namely First-Order Stationary Condition for constrained optimization (FOSC), to quantitatively evaluate the convergence quality of adversarial examples found in the inner maximization. With FOSC, we ﬁnd that to ensure better robustness, it is essential to use adversarial examples with better convergence quality at the later stages of training. Yet at the early stages, high convergence quality adversarial examples are not necessary and may even lead to poor robustness. Based on these observations, we propose a dynamic training strategy to gradually increase the convergence quality of the generated adversarial examples, which signiﬁcantly improves the robustness of adversarial training. Our theoretical and empirical results show the effectiveness of the proposed method.},
	language = {en},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	author = {Wang, Yisen and Ma, Xingjun and Bailey, James and Yi, Jinfeng and Zhou, Bowen and Gu, Quanquan},
	year = {2019},
	pages = {13},
}

@article{wang_initializing_2020,
	title = {Initializing {Perturbations} in {Multiple} {Directions} for {Fast} {Adversarial} {Training}},
	url = {http://arxiv.org/abs/2005.07606},
	abstract = {Recent developments in the ﬁled of Deep Learning have demonstrated that Deep Neural Networks(DNNs) are vulnerable to adversarial examples. Speciﬁcally, in image classiﬁcation, an adversarial example can fool the well trained deep neural networks by adding barely imperceptible perturbations to clean images. Adversarial Training, one of the most direct and effective methods, minimizes the losses of perturbed-data to learn robust deep networks against adversarial attacks. It has been proven that using the fast gradient sign method (FGSM) can achieve Fast Adversarial Training. However, FGSM-based adversarial training may ﬁnally obtain a failed model because of overﬁtting to FGSM samples. In this paper, we proposed the Diversiﬁed Initialized Perturbations Adversarial Training (DIPFAT) which involves seeking the initialization of the perturbation via enlarging the output distances of the target model in a random directions. Due to the diversity of random directions, the embedded fast adversarial training using FGSM increases the information from the adversary and reduces the possibility of overﬁtting. In addition to preventing overﬁtting, the extensive results show that our proposed DIP-FAT technique can also improve the accuracy of the clean data. The biggest advantage of DIP-FAT method: achieving the best banlance among clean-data, perturbed-data and efﬁciency.},
	language = {en},
	urldate = {2020-09-30},
	journal = {arXiv:2005.07606 [cs, stat]},
	author = {Wang, Xunguang and Xu, Ship Peng and Wang, Eric Ke},
	month = may,
	year = {2020},
	note = {arXiv: 2005.07606},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{etmann_closer_2019,
	title = {A {Closer} {Look} at {Double} {Backpropagation}},
	url = {http://arxiv.org/abs/1906.06637},
	abstract = {In recent years, an increasing number of neural network models have included derivatives with respect to inputs in their loss functions, resulting in so-called double backpropagation for ﬁrstorder optimization. However, so far no general description of the involved derivatives exists. Here, we cover a wide array of special cases in a very general Hilbert space framework, which allows us to provide optimized backpropagation rules for many real-world scenarios. This includes the reduction of calculations for Frobenius-norm-penalties on Jacobians by roughly a third for locally linear activation functions. Furthermore, we provide a description of the discontinuous loss surface of ReLU networks both in the inputs and the parameters and demonstrate why the discontinuities do not pose a big problem in reality.},
	language = {en},
	urldate = {2020-09-30},
	journal = {arXiv:1906.06637 [cs, math, stat]},
	author = {Etmann, Christian},
	month = jun,
	year = {2019},
	note = {arXiv: 1906.06637},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
}

@article{ruder_overview_2017,
	title = {An overview of gradient descent optimization algorithms},
	url = {http://arxiv.org/abs/1609.04747},
	abstract = {Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.},
	language = {en},
	urldate = {2020-09-30},
	journal = {arXiv:1609.04747 [cs]},
	author = {Ruder, Sebastian},
	month = jun,
	year = {2017},
	note = {arXiv: 1609.04747},
	keywords = {Computer Science - Machine Learning},
}

@inproceedings{mustafa_adversarial_2019,
	address = {Seoul, Korea (South)},
	title = {Adversarial {Defense} by {Restricting} the {Hidden} {Space} of {Deep} {Neural} {Networks}},
	isbn = {978-1-72814-803-8},
	url = {https://ieeexplore.ieee.org/document/9010713/},
	doi = {10.1109/ICCV.2019.00348},
	abstract = {Deep neural networks are vulnerable to adversarial attacks, which can fool them by adding minuscule perturbations to the input images. The robustness of existing defenses suffers greatly under white-box attack settings, where an adversary has full knowledge about the network and can iterate several times to ﬁnd strong perturbations. We observe that the main reason for the existence of such perturbations is the close proximity of different class samples in the learned feature space. This allows model decisions to be totally changed by adding an imperceptible perturbation in the inputs. To counter this, we propose to class-wise disentangle the intermediate feature representations of deep networks. Speciﬁcally, we force the features for each class to lie inside a convex polytope that is maximally separated from the polytopes of other classes. In this manner, the network is forced to learn distinct and distant decision regions for each class. We observe that this simple constraint on the features greatly enhances the robustness of learned models, even against the strongest white-box attacks, without degrading the classiﬁcation performance on clean images. We report extensive evaluations in both black-box and whitebox attack scenarios and show signiﬁcant gains in comparison to state-of-the art defenses1.},
	language = {en},
	urldate = {2020-09-30},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Mustafa, Aamir and Khan, Salman and Hayat, Munawar and Goecke, Roland and Shen, Jianbing and Shao, Ling},
	month = oct,
	year = {2019},
	pages = {3384--3393},
}

@article{aggarwal_benefits_2020,
	title = {On the {Benefits} of {Models} with {Perceptually}-{Aligned} {Gradients}},
	url = {http://arxiv.org/abs/2005.01499},
	abstract = {Adversarial robust models have been shown to learn more robust and interpretable features than standard trained models. As shown in [Tsipras et al. (2018)], such robust models inherit useful interpretable properties where the gradient aligns perceptually well with images, and adding a large targeted adversarial perturbation leads to an image resembling the target class. We perform experiments to show that interpretable and perceptually aligned gradients are present even in models that do not show high robustness to adversarial attacks. Speciﬁcally, we perform adversarial training with attack for different max-perturbation bound. Adversarial training with low max-perturbation bound results in models that have interpretable features with only slight drop in performance over clean samples. In this paper, we leverage models with interpretable perceptually-aligned features and show that adversarial training with low max-perturbation bound can improve the performance of models for zero-shot and weakly supervised localization tasks.},
	language = {en},
	urldate = {2020-09-30},
	journal = {arXiv:2005.01499 [cs]},
	author = {Aggarwal, Gunjan and Sinha, Abhishek and Kumari, Nupur and Singh, Mayank},
	month = may,
	year = {2020},
	note = {arXiv: 2005.01499},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{shaeiri_towards_2020,
	title = {Towards {Deep} {Learning} {Models} {Resistant} to {Large} {Perturbations}},
	url = {http://arxiv.org/abs/2003.13370},
	abstract = {Adversarial robustness has proven to be a required property of machine learning algorithms. A key and often overlooked aspect of this problem is to try to make the adversarial noise magnitude as large as possible to enhance the beneﬁts of the model robustness. We show that the wellestablished algorithm called “adversarial training” fails to train a deep neural network given a large, but reasonable, perturbation magnitude. In this paper, we propose a simple yet effective initialization of the network weights that makes learning on higher levels of noise possible. We next evaluate this idea rigorously on MNIST ( up to ≈ 0.40) and CIFAR10 ( up to ≈ 32/255) datasets assuming the ∞ attack model. Additionally, in order to establish the limits of in which the learning is feasible, we study the optimal robust classiﬁer assuming full access to the joint data and label distribution. Then, we provide some theoretical results on the adversarial accuracy for a simple multi-dimensional Bernoulli distribution, which yields some insights on the range of feasible perturbations for the MNIST dataset.},
	language = {en},
	urldate = {2020-09-30},
	journal = {arXiv:2003.13370 [cs, stat]},
	author = {Shaeiri, Amirreza and Nobahari, Rozhin and Rohban, Mohammad Hossein},
	month = mar,
	year = {2020},
	note = {arXiv: 2003.13370},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{bau_understanding_2020,
	title = {Understanding the role of individual units in a deep neural network},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/lookup/doi/10.1073/pnas.1907375117},
	doi = {10.1073/pnas.1907375117},
	abstract = {Deep neural networks excel at finding hierarchical representations that solve complex tasks over large datasets. How can we humans understand these learned representations? In this work, we present network dissection, an analytic framework to systematically identify the semantics of individual hidden units within image classification and image generation networks. First, we analyze a convolutional neural network (CNN) trained on scene classification and discover units that match a diverse set of object concepts. We find evidence that the network has learned many object classes that play crucial roles in classifying scene classes. Second, we use a similar analytic method to analyze a generative adversarial network (GAN) model trained to generate scenes. By analyzing changes made when small sets of units are activated or deactivated, we find that objects can be added and removed from the output scenes while adapting to the context. Finally, we apply our analytic framework to understanding adversarial attacks and to semantic image editing.},
	language = {en},
	urldate = {2020-09-16},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Bau, David and Zhu, Jun-Yan and Strobelt, Hendrik and Lapedriza, Agata and Zhou, Bolei and Torralba, Antonio},
	month = sep,
	year = {2020},
	pages = {201907375},
}

@inproceedings{hein_formal_2017,
	title = {Formal {Guarantees} on the {Robustness} of a {Classifier} against {Adversarial} {Manipulation}},
	url = {http://papers.nips.cc/paper/6821-formal-guarantees-on-the-robustness-of-a-classifier-against-adversarial-manipulation.pdf},
	urldate = {2020-09-09},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 30},
	publisher = {Curran Associates, Inc.},
	author = {Hein, Matthias and Andriushchenko, Maksym},
	editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
	year = {2017},
	pages = {2266--2276},
}

@inproceedings{kalimeris_sgd_2019,
	title = {{SGD} on {Neural} {Networks} {Learns} {Functions} of {Increasing} {Complexity}},
	url = {http://papers.nips.cc/paper/8609-sgd-on-neural-networks-learns-functions-of-increasing-complexity.pdf},
	urldate = {2020-09-09},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 32},
	publisher = {Curran Associates, Inc.},
	author = {Kalimeris, Dimitris and Kaplun, Gal and Nakkiran, Preetum and Edelman, Benjamin and Yang, Tristan and Barak, Boaz and Zhang, Haofeng},
	year = {2019},
	pages = {3496--3506},
}

@inproceedings{uesato_adversarial_2018,
	title = {Adversarial {Risk} and the {Dangers} of {Evaluating} {Against} {Weak} {Attacks}},
	abstract = {This paper investigates recently proposed approaches for defending against adversarial examples and evaluating adversarial robustness. We motivate adversarial risk as an objective for achieving models robust to worst-case inputs. We then frame commonly used attacks and evaluation metrics as deﬁning a tractable surrogate objective to the true adversarial risk. This suggests that models may optimize this surrogate rather than the true adversarial risk. We formalize this notion as obscurity to an adversary, and develop tools and heuristics for identifying obscured models and designing transparent models. We demonstrate that this is a signiﬁcant problem in practice by repurposing gradient-free optimization techniques into adversarial attacks, which we use to decrease the accuracy of several recently proposed defenses to near zero. Our hope is that our formulations and results will help researchers to develop more powerful defenses.},
	language = {en},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}},
	author = {Uesato, Jonathan and O’Donoghue, Brendan},
	year = {2018},
	pages = {10},
}

@inproceedings{rice_overfitting_2020,
	title = {Overfitting in adversarially robust deep learning},
	abstract = {It is common practice in deep learning to use overparameterized networks and train for as long as possible; there are numerous studies that show, both theoretically and empirically, that such practices surprisingly do not unduly harm the generalization performance of the classiﬁer. In this paper, we empirically study this phenomenon in the setting of adversarially trained deep networks, which are trained to minimize the loss under worst-case adversarial perturbations. We ﬁnd that overﬁtting to the training set does in fact harm robust performance to a very large degree in adversarially robust training across multiple datasets (SVHN, CIFAR-10, CIFAR-100, and ImageNet) and perturbation models (`∞ and `2). Based upon this observed effect, we show that the performance gains of virtually all recent algorithmic improvements upon adversarial training can be matched by simply using early stopping. We also show that effects such as the double descent curve do still occur in adversarially trained models, yet fail to explain the observed overﬁtting. Finally, we study several classical and modern deep learning remedies for overﬁtting, including regularization and data augmentation, and ﬁnd that no approach in isolation improves signiﬁcantly upon the gains achieved by early stopping. All code for reproducing the experiments as well as pretrained model weights and training logs can be found at https://github.com/ locuslab/robust\_overfitting.},
	language = {en},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	author = {Rice, Leslie and Wong, Eric and Kolter, J Zico},
	year = {2020},
	pages = {12},
}

@inproceedings{zheng_efficient_2020,
	address = {Seattle, WA, USA},
	title = {Efficient {Adversarial} {Training} {With} {Transferable} {Adversarial} {Examples}},
	isbn = {978-1-72817-168-5},
	url = {https://ieeexplore.ieee.org/document/9157681/},
	doi = {10.1109/CVPR42600.2020.00126},
	abstract = {Adversarial training is an effective defense method to protect classiﬁcation models against adversarial attacks. However, one limitation of this approach is that it can require orders of magnitude additional training time due to high cost of generating strong adversarial examples during training. In this paper, we ﬁrst show that there is high transferability between models from neighboring epochs in the same training process, i.e., adversarial examples from one epoch continue to be adversarial in subsequent epochs. Leveraging this property, we propose a novel method, Adversarial Training with Transferable Adversarial Examples (ATTA), that can enhance the robustness of trained models and greatly improve the training efﬁciency by accumulating adversarial perturbations through epochs. Compared to state-of-the-art adversarial training methods, ATTA enhances adversarial accuracy by up to 7.2\% on CIFAR10 and requires 12 ∼ 14× less training time on MNIST and CIFAR10 datasets with comparable model robustness.},
	language = {en},
	urldate = {2020-08-27},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Zheng, Haizhong and Zhang, Ziqi and Gu, Juncheng and Lee, Honglak and Prakash, Atul},
	month = jun,
	year = {2020},
	pages = {1178--1187},
}

@inproceedings{xiao_one_2020,
	address = {Seattle, WA, USA},
	title = {One {Man}’s {Trash} {Is} {Another} {Man}’s {Treasure}: {Resisting} {Adversarial} {Examples} by {Adversarial} {Examples}},
	isbn = {978-1-72817-168-5},
	shorttitle = {One {Man}’s {Trash} {Is} {Another} {Man}’s {Treasure}},
	url = {https://ieeexplore.ieee.org/document/9156804/},
	doi = {10.1109/CVPR42600.2020.00049},
	abstract = {Modern image classiﬁcation systems are often built on deep neural networks, which suffer from adversarial examples—images with deliberately crafted, imperceptible noise to mislead the network’s classiﬁcation. To defend against adversarial examples, a plausible idea is to obfuscate the network’s gradient with respect to the input image. This general idea has inspired a long line of defense methods. Yet, almost all of them have proven vulnerable.},
	language = {en},
	urldate = {2020-08-27},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Xiao, Chang and Zheng, Changxi},
	month = jun,
	year = {2020},
	pages = {409--418},
}

@inproceedings{gupta_improving_2020,
	address = {Seattle, WA, USA},
	title = {Improving the affordability of robustness training for {DNNs}},
	isbn = {978-1-72819-360-1},
	url = {https://ieeexplore.ieee.org/document/9150984/},
	doi = {10.1109/CVPRW50498.2020.00398},
	abstract = {Projected Gradient Descent (PGD) based adversarial training has become one of the most prominent methods for building robust deep neural network models. However, the computational complexity associated with this approach, due to the maximization of the loss function when ﬁnding adversaries, is a longstanding problem and may be prohibitive when using larger and more complex models. In this paper we show that the initial phase of adversarial training is redundant and can be replaced with natural training which signiﬁcantly improves the computational efﬁciency. We demonstrate that this efﬁciency gain can be achieved without any loss in accuracy on natural and adversarial test samples. We support our argument with insights on the nature of the adversaries and their relative strength during the training process. We show that our proposed method can reduce the training time by a factor of up to 2.5 with comparable or better model test accuracy and generalization on various strengths of adversarial attacks.},
	language = {en},
	urldate = {2020-08-27},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops} ({CVPRW})},
	publisher = {IEEE},
	author = {Gupta, Sidharth and Dube, Parijat and Verma, Ashish},
	month = jun,
	year = {2020},
	pages = {3383--3392},
}

@inproceedings{simonyan_very_2015,
	title = {Very {Deep} {Convolutional} {Networks} for {Large}-{Scale} {Image} {Recognition}},
	url = {http://arxiv.org/abs/1409.1556},
	abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3 × 3) convolution ﬁlters, which shows that a signiﬁcant improvement on the prior-art conﬁgurations can be achieved by pushing the depth to 16–19 weight layers. These ﬁndings were the basis of our ImageNet Challenge 2014 submission, where our team secured the ﬁrst and the second places in the localisation and classiﬁcation tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
	language = {en},
	urldate = {2020-08-06},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Simonyan, Karen and Zisserman, Andrew},
	year = {2015},
	note = {arXiv: 1409.1556},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{s_single-step_2020,
	title = {Single-{Step} {Adversarial} {Training} {With} {Dropout} {Scheduling}},
	abstract = {Deep learning models have shown impressive performance across a spectrum of computer vision applications including medical diagnosis and autonomous driving. One of the major concerns that these models face is their susceptibility to adversarial attacks. Realizing the importance of this issue, more researchers are working towards developing robust models that are less affected by adversarial attacks. Adversarial training method shows promising results in this direction. In adversarial training regime, models are trained with mini-batches augmented with adversarial samples. Fast and simple methods (e.g., single-step gradient ascent) are used for generating adversarial samples, in order to reduce computational complexity. It is shown that models trained using single-step adversarial training method (adversarial samples are generated using noniterative method) are pseudo robust. Further, this pseudo robustness of models is attributed to the gradient masking effect. However, existing works fail to explain when and why gradient masking effect occurs during single-step adversarial training. In this work, (i) we show that models trained using single-step adversarial training method learn to prevent the generation of single-step adversaries, and this is due to over-ﬁtting of the model during the initial stages of training, and (ii) to mitigate this effect, we propose a singlestep adversarial training method with dropout scheduling. Unlike models trained using existing single-step adversarial training methods, models trained using the proposed single-step adversarial training method are robust against both single-step and multi-step adversarial attacks, and the performance is on par with models trained using computationally expensive multi-step adversarial training methods, in white-box and black-box settings.},
	language = {en},
	booktitle = {The {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {S., Vivek B. and Babu, R. Venkatesh},
	year = {2020},
	pages = {10},
}

@inproceedings{krizhevsky_imagenet_2012,
	title = {{ImageNet} classification with deep convolutional neural networks},
	abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of ﬁve convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a ﬁnal 1000-way softmax. To make training faster, we used non-saturating neurons and a very efﬁcient GPU implementation of the convolution operation. To reduce overﬁtting in the fully-connected layers we employed a recently-developed regularization method called “dropout” that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
	language = {en},
	urldate = {2020-08-04},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
	year = {2012},
}

@inproceedings{tan_efficientnet_2019,
	title = {{EfficientNet}: {Rethinking} {Model} {Scaling} for {Convolutional} {Neural} {Networks}},
	shorttitle = {{EfficientNet}},
	url = {http://proceedings.mlr.press/v97/tan19a.html},
	abstract = {Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are given. In this paper, we systematically stud...},
	language = {en},
	urldate = {2020-07-23},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	author = {Tan, Mingxing and Le, Quoc},
	month = may,
	year = {2019},
	note = {ISSN: 1938-7228
Section: Machine Learning},
	pages = {6105--6114},
}

@article{lai_recurrent_nodate,
	title = {Recurrent {Convolutional} {Neural} {Networks} for {Text} {Classification}},
	abstract = {Text classiﬁcation is a foundational task in many NLP applications. Traditional text classiﬁers often rely on many human-designed features, such as dictionaries, knowledge bases and special tree kernels. In contrast to traditional methods, we introduce a recurrent convolutional neural network for text classiﬁcation without human-designed features. In our model, we apply a recurrent structure to capture contextual information as far as possible when learning word representations, which may introduce considerably less noise compared to traditional window-based neural networks. We also employ a max-pooling layer that automatically judges which words play key roles in text classiﬁcation to capture the key components in texts. We conduct experiments on four commonly used datasets. The experimental results show that the proposed method outperforms the state-of-the-art methods on several datasets, particularly on document-level datasets.},
	language = {en},
	author = {Lai, Siwei and Xu, Liheng and Liu, Kang and Zhao, Jun},
	pages = {7},
}

@article{kalchbrenner_recurrent_nodate,
	title = {Recurrent {Convolutional} {Neural} {Networks} for {Discourse} {Compositionality}},
	abstract = {The compositionality of meaning extends beyond the single sentence. Just as words combine to form the meaning of sentences, so do sentences combine to form the meaning of paragraphs, dialogues and general discourse. We introduce both a sentence model and a discourse model corresponding to the two levels of compositionality. The sentence model adopts convolution as the central operation for composing semantic vectors and is based on a novel hierarchical convolutional neural network. The discourse model extends the sentence model and is based on a recurrent neural network that is conditioned in a novel way both on the current sentence and on the current speaker. The discourse model is able to capture both the sequentiality of sentences and the interaction between different speakers. Without feature engineering or pretraining and with simple greedy decoding, the discourse model coupled to the sentence model obtains state of the art performance on a dialogue act classiﬁcation experiment.},
	language = {en},
	author = {Kalchbrenner, Nal and Blunsom, Phil},
	pages = {8},
}

@article{zagoruyko_paying_2017,
	title = {{PAYING} {MORE} {ATTENTION} {TO} {ATTENTION}: {IMPROVING} {THE} {PERFORMANCE} {OF} {CONVOLUTIONAL} {NEURAL} {NETWORKS} {VIA} {ATTENTION} {TRANSFER}},
	abstract = {Attention plays a critical role in human visual experience. Furthermore, it has recently been demonstrated that attention can also play an important role in the context of applying artiﬁcial neural networks to a variety of tasks from ﬁelds such as computer vision and NLP. In this work we show that, by properly deﬁning attention for convolutional neural networks, we can actually use this type of information in order to signiﬁcantly improve the performance of a student CNN network by forcing it to mimic the attention maps of a powerful teacher network. To that end, we propose several novel methods of transferring attention, showing consistent improvement across a variety of datasets and convolutional neural network architectures. Code and models for our experiments are available at https://github.com/szagoruyko/attention-transfer.},
	language = {en},
	author = {Zagoruyko, Sergey and Komodakis, Nikos},
	year = {2017},
	pages = {13},
}

@techreport{kubilius_cornet_2018,
	type = {preprint},
	title = {{CORnet}: {Modeling} the {Neural} {Mechanisms} of {Core} {Object} {Recognition}},
	shorttitle = {{CORnet}},
	url = {http://biorxiv.org/lookup/doi/10.1101/408385},
	abstract = {Abstract
          Deep artificial neural networks with spatially repeated processing (a.k.a., deep convolutional ANNs) have been established as the best class of candidate models of visual processing in primate ventral visual processing stream. Over the past five years, these ANNs have evolved from a simple feedforward eight-layer architecture in AlexNet to extremely deep and branching NAS-Net architectures, demonstrating increasingly better object categorization performance and increasingly better explanatory power of both neural and behavioral responses. However, from the neuroscientist’s point of view, the relationship between such very deep architectures and the ventral visual pathway is incomplete in at least two ways. On the one hand, current state-of-the-art ANNs appear to be too complex (e.g., now over 100 levels) compared with the relatively shallow cortical hierarchy (4-8 levels), which makes it difficult to map their elements to those in the ventral visual stream and to understand what they are doing. On the other hand, current state-of-the-art ANNs appear to be not complex enough in that they lack recurrent connections and the resulting neural response dynamics that are commonplace in the ventral visual stream. Here we describe our ongoing efforts to resolve both of these issues by developing a “CORnet” family of deep neural network architectures. Rather than just seeking high object recognition performance (as the state-of-the-art ANNs above), we instead try to reduce the model family to its most important elements and then gradually build new ANNs with recurrent and skip connections while monitoring both performance and the match between each new CORnet model and a large body of primate brain and behavioral data. We report here that our current best ANN model derived from this approach (CORnet-S) is among the top models on Brain-Score, a composite benchmark for comparing models to the brain, but is simpler than other deep ANNs in terms of the number of convolutions performed along the longest path of information processing in the model. All CORnet models are available at github.com/dicarlolab/CORnet, and we plan to up-date this manuscript and the available models in this family as they are produced.},
	language = {en},
	urldate = {2020-07-23},
	institution = {Neuroscience},
	author = {Kubilius, Jonas and Schrimpf, Martin and Nayebi, Aran and Bear, Daniel and Yamins, Daniel L. K. and DiCarlo, James J.},
	month = sep,
	year = {2018},
	doi = {10.1101/408385},
}

@article{mhaskar_learning_nodate,
	title = {Learning {Real} and {Boolean} {Functions}: {When} {Is} {Deep} {Better} {Than} {Shallow}},
	language = {en},
	author = {Mhaskar, Hrushikesh and Liao, Qianli and Poggio, Tomaso},
	pages = {11},
}

@article{bear_learning_2020,
	title = {Learning {Physical} {Graph} {Representations} from {Visual} {Scenes}},
	url = {http://arxiv.org/abs/2006.12373},
	abstract = {Convolutional Neural Networks (CNNs) have proved exceptional at learning representations for visual object categorization. However, CNNs do not explicitly encode objects, parts, and their physical properties, which has limited CNNs’ success on tasks that require structured understanding of visual scenes. To overcome these limitations, we introduce the idea of “Physical Scene Graphs” (PSGs), which represent scenes as hierarchical graphs, with nodes in the hierarchy corresponding intuitively to object parts at different scales, and edges to physical connections between parts. Bound to each node is a vector of latent attributes that intuitively represent object properties such as surface shape and texture. We also describe PSGNet, a network architecture that learns to extract PSGs by reconstructing scenes through a PSG-structured bottleneck. PSGNet augments standard CNNs by including: recurrent feedback connections to combine low and high-level image information; graph pooling and vectorization operations that convert spatially-uniform feature maps into object-centric graph structures; and perceptual grouping principles to encourage the identiﬁcation of meaningful scene elements. We show that PSGNet outperforms alternative self-supervised scene representation algorithms at scene segmentation tasks, especially on complex real-world images, and generalizes well to unseen object types and scene arrangements. PSGNet is also able learn from physical motion, enhancing scene estimates even for static images. We present a series of ablation studies illustrating the importance of each component of the PSGNet architecture, analyses showing that learned latent attributes capture intuitive scene properties, and illustrate the use of PSGs for compositional scene inference.},
	language = {en},
	urldate = {2020-07-23},
	journal = {arXiv:2006.12373 [cs]},
	author = {Bear, Daniel M. and Fan, Chaofei and Mrowca, Damian and Li, Yunzhu and Alter, Seth and Nayebi, Aran and Schwartz, Jeremy and Fei-Fei, Li and Wu, Jiajun and Tenenbaum, Joshua B. and Yamins, Daniel L. K.},
	month = jun,
	year = {2020},
	note = {arXiv: 2006.12373},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, I.2.6, I.4.8},
}

@article{collins_capacity_2017,
	title = {{CAPACITY} {AND} {TRAINABILITY} {IN} {RECURRENT} {NEURAL} {NETWORKS}},
	abstract = {Two potential bottlenecks on the expressiveness of recurrent neural networks (RNNs) are their ability to store information about the task in their parameters, and to store information about the input history in their units. We show experimentally that all common RNN architectures achieve nearly the same per-task and per-unit capacity bounds with careful training, for a variety of tasks and stacking depths. They can store an amount of task information which is linear in the number of parameters, and is approximately 5 bits per parameter. They can additionally store approximately one real number from their input history per hidden unit. We further ﬁnd that for several tasks it is the per-task parameter capacity bound that determines performance. These results suggest that many previous results comparing RNN architectures are driven primarily by differences in training effectiveness, rather than differences in capacity. Supporting this observation, we compare training difﬁculty for several architectures, and show that vanilla RNNs are far more difﬁcult to train, yet have slightly higher capacity. Finally, we propose two novel RNN architectures, one of which is easier to train than the LSTM or GRU for deeply stacked architectures.},
	language = {en},
	author = {Collins, Jasmine and Sohl-Dickstein, Jascha and Sussillo, David},
	year = {2017},
	pages = {17},
}

@article{xu_show_nodate,
	title = {Show, {Attend} and {Tell}: {Neural} {Image} {CaptionGeneration} with {Visual} {Attention}},
	abstract = {Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to ﬁx its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-theart performance on three benchmark datasets: Flickr9k, Flickr30k and MS COCO.},
	language = {en},
	author = {Xu, Kelvin and Lei, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhutdinov, Ruslan and Zemel, Richard S and Bengio, Yoshua},
	pages = {10},
}

@incollection{leibe_deep_2016,
	address = {Cham},
	title = {Deep {Networks} with {Stochastic} {Depth}},
	volume = {9908},
	isbn = {978-3-319-46492-3 978-3-319-46493-0},
	url = {http://link.springer.com/10.1007/978-3-319-46493-0_39},
	abstract = {Very deep convolutional networks with hundreds of layers have led to signiﬁcant reductions in error on competitive benchmarks. Although the unmatched expressiveness of the many layers can be highly desirable at test time, training very deep networks comes with its own set of challenges. The gradients can vanish, the forward ﬂow often diminishes, and the training time can be painfully slow. To address these problems, we propose stochastic depth, a training procedure that enables the seemingly contradictory setup to train short networks and use deep networks at test time. We start with very deep networks but during training, for each mini-batch, randomly drop a subset of layers and bypass them with the identity function. This simple approach complements the recent success of residual networks. It reduces training time substantially and improves the test error signiﬁcantly on almost all data sets that we used for evaluation. With stochastic depth we can increase the depth of residual networks even beyond 1200 layers and still yield meaningful improvements in test error (4.91 \% on CIFAR-10).},
	language = {en},
	urldate = {2020-07-23},
	booktitle = {Computer {Vision} – {ECCV} 2016},
	publisher = {Springer International Publishing},
	author = {Huang, Gao and Sun, Yu and Liu, Zhuang and Sedra, Daniel and Weinberger, Kilian Q.},
	editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
	year = {2016},
	doi = {10.1007/978-3-319-46493-0_39},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {646--661},
}

@article{zilly_recurrent_nodate,
	title = {Recurrent {Highway} {Networks}},
	abstract = {Many sequential processing tasks require complex nonlinear transition functions from one step to the next. However, recurrent neural networks with "deep" transition functions remain difﬁcult to train, even when using Long Short-Term Memory (LSTM) networks. We introduce a novel theoretical analysis of recurrent networks based on Geršgorin’s circle theorem that illuminates several modeling and optimization issues and improves our understanding of the LSTM cell. Based on this analysis we propose Recurrent Highway Networks, which extend the LSTM architecture to allow step-to-step transition depths larger than one. Several language modeling experiments demonstrate that the proposed architecture results in powerful and efﬁcient models. On the Penn Treebank corpus, solely increasing the transition depth from 1 to 10 improves word-level perplexity from 90.6 to 65.4 using the same number of parameters. On the larger Wikipedia datasets for character prediction (text8 and enwik8), RHNs outperform all previous results and achieve an entropy of 1.27 bits per character.},
	language = {en},
	author = {Zilly, Julian Georg and Srivastava, Rupesh Kumar and Koutník, Jan and Schmidhuber, Jürgen},
	pages = {10},
}

@article{zhang_architectural_nodate,
	title = {Architectural {Complexity} {Measures} of {Recurrent} {Neural} {Networks}},
	abstract = {In this paper, we systematically analyze the connecting architectures of recurrent neural networks (RNNs). Our main contribution is twofold: ﬁrst, we present a rigorous graph-theoretic framework describing the connecting architectures of RNNs in general. Second, we propose three architecture complexity measures of RNNs: (a) the recurrent depth, which captures the RNN’s over-time nonlinear complexity, (b) the feedforward depth, which captures the local input-output nonlinearity (similar to the “depth” in feedforward neural networks (FNNs)), and (c) the recurrent skip coefﬁcient which captures how rapidly the information propagates over time. We rigorously prove each measure’s existence and computability. Our experimental results show that RNNs might beneﬁt from larger recurrent depth and feedforward depth. We further demonstrate that increasing recurrent skip coefﬁcient offers performance boosts on long term dependency problems.},
	language = {en},
	author = {Zhang, Saizheng and Wu, Yuhuai and Che, Tong and Lin, Zhouhan and Memisevic, Roland and Salakhutdinov, Russ R and Bengio, Yoshua},
	pages = {9},
}

@inproceedings{zaremoodi_adaptive_2018,
	address = {Melbourne, Australia},
	title = {Adaptive {Knowledge} {Sharing} in {Multi}-{Task} {Learning}: {Improving} {Low}-{Resource} {Neural} {Machine} {Translation}},
	shorttitle = {Adaptive {Knowledge} {Sharing} in {Multi}-{Task} {Learning}},
	url = {http://aclweb.org/anthology/P18-2104},
	doi = {10.18653/v1/P18-2104},
	abstract = {Neural Machine Translation (NMT) is notorious for its need for large amounts of bilingual data. An effective approach to compensate for this requirement is MultiTask Learning (MTL) to leverage different linguistic resources as a source of inductive bias. Current MTL architectures are based on the SEQ2SEQ transduction, and (partially) share different components of the models among the tasks. However, this MTL approach often suffers from task interference, and is not able to fully capture commonalities among subsets of tasks. We address this issue by extending the recurrent units with multiple blocks along with a trainable routing network. The routing network enables adaptive collaboration by dynamic sharing of blocks conditioned on the task at hand, input, and model state. Empirical evaluation of two low-resource translation tasks, English to Vietnamese and Farsi, show +1 BLEU score improvements compared to strong baselines.},
	language = {en},
	urldate = {2020-07-23},
	booktitle = {Proceedings of the 56th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 2: {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Zaremoodi, Poorya and Buntine, Wray and Haffari, Gholamreza},
	year = {2018},
	pages = {656--661},
}

@inproceedings{rony_decoupling_2019,
	address = {Long Beach, CA, USA},
	title = {Decoupling {Direction} and {Norm} for {Efficient} {Gradient}-{Based} {L2} {Adversarial} {Attacks} and {Defenses}},
	isbn = {978-1-72813-293-8},
	url = {https://ieeexplore.ieee.org/document/8954314/},
	doi = {10.1109/CVPR.2019.00445},
	abstract = {Research on adversarial examples in computer vision tasks has shown that small, often imperceptible changes to an image can induce misclassiﬁcation, which has security implications for a wide range of image processing systems. Considering L2 norm distortions, the Carlini and Wagner attack is presently the most effective white-box attack in the literature. However, this method is slow since it performs a line-search for one of the optimization terms, and often requires thousands of iterations. In this paper, an efﬁcient approach is proposed to generate gradient-based attacks that induce misclassiﬁcations with low L2 norm, by decoupling the direction and the norm of the adversarial perturbation that is added to the image. Experiments conducted on the MNIST, CIFAR-10 and ImageNet datasets indicate that our attack achieves comparable results to the state-of-the-art (in terms of L2 norm) with considerably fewer iterations (as few as 100 iterations), which opens the possibility of using these attacks for adversarial training. Models trained with our attack achieve state-of-the-art robustness against whitebox gradient-based L2 attacks on the MNIST and CIFAR-10 datasets, outperforming the Madry defense when the attacks are limited to a maximum norm.},
	language = {en},
	urldate = {2020-07-23},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Rony, Jerome and Hafemann, Luiz G. and Oliveira, Luiz S. and Ben Ayed, Ismail and Sabourin, Robert and Granger, Eric},
	month = jun,
	year = {2019},
	pages = {4317--4325},
}

@article{kannan_adversarial_2018,
	title = {Adversarial {Logit} {Pairing}},
	url = {http://arxiv.org/abs/1803.06373},
	abstract = {In this paper, we develop improved techniques for defending against adversarial examples at scale. First, we implement the state of the art version of adversarial training at unprecedented scale on ImageNet and investigate whether it remains effective in this setting—an important open scientiﬁc question (Athalye et al., 2018). Next, we introduce enhanced defenses using a technique we call logit pairing, a method that encourages logits for pairs of examples to be similar. When applied to clean examples and their adversarial counterparts, logit pairing improves accuracy on adversarial examples over vanilla adversarial training; we also ﬁnd that logit pairing on clean examples only is competitive with adversarial training in terms of accuracy on two datasets. Finally, we show that adversarial logit pairing achieves the state of the art defense on Imagenet against PGD white box attacks, with an accuracy improvement from 1.5\% to 27.9\%. Adversarial logit pairing also successfully damages the current state of the art defense against black box attacks on Imagenet (Trame`r et al., 2018), dropping its accuracy from 66.6\% to 47.1\%. With this new accuracy drop, adversarial logit pairing ties with Trame`r et al. (2018) for the state of the art on black box attacks on ImageNet.},
	language = {en},
	urldate = {2020-07-23},
	journal = {arXiv:1803.06373 [cs, stat]},
	author = {Kannan, Harini and Kurakin, Alexey and Goodfellow, Ian},
	month = mar,
	year = {2018},
	note = {arXiv: 1803.06373},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{szegedy_inception-v4_2017,
	title = {Inception-v4, {Inception}-{ResNet} and the {Impact} of {Residual} {Connections} on {Learning}},
	abstract = {Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question: Are there any beneﬁts to combining Inception architectures with residual connections? Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks signiﬁcantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and nonresidual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classiﬁcation task signiﬁcantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4 networks, we achieve 3.08\% top-5 error on the test set of the ImageNet classiﬁcation (CLS) challenge.},
	language = {en},
	booktitle = {Proceedings of the {Thirty}-{First} {AAAI} {Conference} on {Artificial} {Intelligence} ({AAAI}-17)},
	author = {Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent and Alemi, Alexander A},
	year = {2017},
	pages = {7},
}

@inproceedings{zhang_you_2019,
	title = {You {Only} {Propagate} {Once}: {Accelerating} {Adversarial} {Training} via {Maximal} {Principle}},
	language = {en},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Zhang, Dinghuai and Zhang, Tianyuan and Lu, Yiping and Zhu, Zhanxing and Dong, Bin},
	year = {2019},
	pages = {12},
}

@inproceedings{vivek_gray-box_2018,
	title = {Gray-{Box} {Adversarial} {Training}},
	volume = {11219},
	isbn = {978-3-030-01266-3 978-3-030-01267-0},
	url = {http://link.springer.com/10.1007/978-3-030-01267-0_13},
	doi = {10.1007/978-3-030-01267-0_13},
	abstract = {Adversarial samples are perturbed inputs crafted to mislead the machine learning systems. A training mechanism, called adversarial training, which presents adversarial samples along with clean samples has been introduced to learn robust models. In order to scale adversarial training for large datasets, these perturbations can only be crafted using fast and simple methods (e.g., gradient ascent). However, it is shown that adversarial training converges to a degenerate minimum, where the model appears to be robust by generating weaker adversaries. As a result, the models are vulnerable to simple black-box attacks.},
	language = {en},
	urldate = {2020-07-23},
	booktitle = {Proceedings of the {European} {Conference} on {Computer} {Vision} ({ECCV})},
	publisher = {Springer International Publishing},
	author = {Vivek, B. S. and Mopuri, Konda Reddy and Babu, R. Venkatesh},
	year = {2018},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {213--228},
}

@inproceedings{wang_bilateral_2019,
	address = {Seoul, Korea (South)},
	title = {Bilateral {Adversarial} {Training}: {Towards} {Fast} {Training} of {More} {Robust} {Models} {Against} {Adversarial} {Attacks}},
	isbn = {978-1-72814-803-8},
	shorttitle = {Bilateral {Adversarial} {Training}},
	url = {https://ieeexplore.ieee.org/document/9009088/},
	doi = {10.1109/ICCV.2019.00673},
	abstract = {In this paper, we study fast training of adversarially robust models. From the analyses of the state-of-the-art defense method, i.e., the multi-step adversarial training [34], we hypothesize that the gradient magnitude links to the model robustness. Motivated by this, we propose to perturb both the image and the label during training, which we call Bilateral Adversarial Training (BAT). To generate the adversarial label, we derive an closed-form heuristic solution. To generate the adversarial image, we use one-step targeted attack with the target label being the most confusing class. In the experiment, we ﬁrst show that random start and the most confusing target attack effectively prevent the label leaking and gradient masking problem. Then coupled with the adversarial label part, our model significantly improves the state-of-the-art results. For example, against PGD100 white-box attack with cross-entropy loss, on CIFAR10, we achieve 63.7\% versus 47.2\%; on SVHN, we achieve 59.1\% versus 42.1\%. At last, the experiment on the very (computationally) challenging ImageNet dataset further demonstrates the effectiveness of our fast method.},
	language = {en},
	urldate = {2020-07-22},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Wang, Jianyu and Zhang, Haichao},
	month = oct,
	year = {2019},
	pages = {6628--6637},
}

@inproceedings{kornblith_better_2019,
	address = {Long Beach, CA, USA},
	title = {Do {Better} {ImageNet} {Models} {Transfer} {Better}?},
	isbn = {978-1-72813-293-8},
	url = {https://ieeexplore.ieee.org/document/8954384/},
	doi = {10.1109/CVPR.2019.00277},
	abstract = {Transfer learning is a cornerstone of computer vision, yet little work has been done to evaluate the relationship between architecture and transfer. An implicit hypothesis in modern computer vision research is that models that perform better on ImageNet necessarily perform better on other vision tasks. However, this hypothesis has never been systematically tested. Here, we compare the performance of 16 classiﬁcation networks on 12 image classiﬁcation datasets. We ﬁnd that, when networks are used as ﬁxed feature extractors or ﬁne-tuned, there is a strong correlation between ImageNet accuracy and transfer accuracy (r = 0.99 and 0.96, respectively). In the former setting, we ﬁnd that this relationship is very sensitive to the way in which networks are trained on ImageNet; many common forms of regularization slightly improve ImageNet accuracy but yield penultimate layer features that are much worse for transfer learning. Additionally, we ﬁnd that, on two small ﬁne-grained image classiﬁcation datasets, pretraining on ImageNet provides minimal beneﬁts, indicating the learned features from ImageNet do not transfer well to ﬁne-grained tasks. Together, our results show that ImageNet architectures generalize well across datasets, but ImageNet features are less general than previously suggested.},
	language = {en},
	urldate = {2020-07-22},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Kornblith, Simon and Shlens, Jonathon and Le, Quoc V.},
	month = jun,
	year = {2019},
	pages = {2656--2666},
}

@article{schmidhuber_deep_2015,
	title = {Deep learning in neural networks: {An} overview},
	volume = {61},
	issn = {0893-6080},
	shorttitle = {Deep learning in neural networks},
	url = {http://www.sciencedirect.com/science/article/pii/S0893608014002135},
	doi = {10.1016/j.neunet.2014.09.003},
	abstract = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarizes relevant work, much of it from the previous millennium. Shallow and Deep Learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning \& evolutionary computation, and indirect search for short programs encoding deep and large networks.},
	language = {en},
	urldate = {2020-07-19},
	journal = {Neural Networks},
	author = {Schmidhuber, Jürgen},
	month = jan,
	year = {2015},
	keywords = {Deep learning, Evolutionary computation, Reinforcement learning, Supervised learning, Unsupervised learning},
	pages = {85--117},
}

@inproceedings{greff_highway_2017,
	title = {Highway and {Residual} {Networks} {Learn} {Unrolled} {Iterative} {Estimation}},
	abstract = {The past year saw the introduction of new architectures such as Highway networks (Srivastava et al., 2015a) and Residual networks (He et al., 2015) which, for the ﬁrst time, enabled the training of feedforward networks with dozens to hundreds of layers using simple gradient descent. While depth of representation has been posited as a primary reason for their success, there are indications that these architectures defy a popular view of deep learning as a hierarchical computation of increasingly abstract features at each layer.},
	language = {en},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Greff, Klaus and Srivastava, Rupesh K and Schmidhuber, Jürgen},
	year = {2017},
	pages = {14},
}

@inproceedings{jastrze_residual_2018,
	title = {Residual {Connections} {Encourage} {Iterative} {Inference}},
	abstract = {Residual networks (Resnets) have become a prominent architecture in deep learning. However, a comprehensive understanding of Resnets is still a topic of ongoing research. A recent view argues that Resnets perform iterative reﬁnement of features. We attempt to further expose properties of this aspect. To this end, we study Resnets both analytically and empirically. We formalize the notion of iterative reﬁnement in Resnets by showing that residual connections naturally encourage features of residual blocks to move along the negative gradient of loss as we go from one block to the next. In addition, our empirical analysis suggests that Resnets are able to perform both representation learning and iterative reﬁnement. In general, a Resnet block tends to concentrate representation learning behavior in the ﬁrst few layers while higher layers perform iterative reﬁnement of features. Finally we observe that sharing residual layers naively leads to representation explosion and counterintuitively, overﬁtting, and we show that simple existing strategies can help alleviating this problem.},
	language = {en},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Jastrze, Stanisław and Che, Tong and Bengio, Yoshua},
	year = {2018},
	pages = {14},
}

@inproceedings{veit_residual_2016,
	title = {Residual {Networks} {Behave} {Like} {Ensembles} of {Relatively} {Shallow} {Networks}},
	abstract = {In this work we propose a novel interpretation of residual networks showing that they can be seen as a collection of many paths of differing length. Moreover, residual networks seem to enable very deep networks by leveraging only the short paths during training. To support this observation, we rewrite residual networks as an explicit collection of paths. Unlike traditional models, paths through residual networks vary in length. Further, a lesion study reveals that these paths show ensemble-like behavior in the sense that they do not strongly depend on each other. Finally, and most surprising, most paths are shorter than one might expect, and only the short paths are needed during training, as longer paths do not contribute any gradient. For example, most of the gradient in a residual network with 110 layers comes from paths that are only 10-34 layers deep. Our results reveal one of the key characteristics that seem to enable the training of very deep networks: Residual networks avoid the vanishing gradient problem by introducing short paths which can carry gradient throughout the extent of very deep networks.},
	language = {en},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Veit, Andreas and Wilber, Michael and Belongie, Serge},
	year = {2016},
	pages = {9},
}

@inproceedings{savarese_learning_2019,
	title = {Learning {Implicitly} {Recurrent} {CNNs} through {Parameter} {Sharing}},
	abstract = {We introduce a parameter sharing scheme, in which different layers of a convolutional neural network (CNN) are deﬁned by a learned linear combination of parameter tensors from a global bank of templates. Restricting the number of templates yields a ﬂexible hybridization of traditional CNNs and recurrent networks. Compared to traditional CNNs, we demonstrate substantial parameter savings on standard image classiﬁcation tasks, while maintaining accuracy.},
	language = {en},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Savarese, Pedro and Maire, Michael},
	year = {2019},
	pages = {15},
}

@inproceedings{parikh_interactively_2011,
	address = {Colorado Springs, CO, USA},
	title = {Interactively building a discriminative vocabulary of nameable attributes},
	isbn = {978-1-4577-0394-2},
	url = {http://ieeexplore.ieee.org/document/5995451/},
	doi = {10.1109/CVPR.2011.5995451},
	abstract = {Human-nameable visual attributes offer many advantages when used as mid-level features for object recognition, but existing techniques to gather relevant attributes can be inefﬁcient (costing substantial effort or expertise) and/or insufﬁcient (descriptive properties need not be discriminative). We introduce an approach to deﬁne a vocabulary of attributes that is both human understandable and discriminative. The system takes object/scene-labeled images as input, and returns as output a set of attributes elicited from human annotators that distinguish the categories of interest. To ensure a compact vocabulary and efﬁcient use of annotators’ effort, we 1) show how to actively augment the vocabulary such that new attributes resolve inter-class confusions, and 2) propose a novel “nameability” manifold that prioritizes candidate attributes by their likelihood of being associated with a nameable property. We demonstrate the approach with multiple datasets, and show its clear advantages over baselines that lack a nameability model or rely on a list of expert-provided attributes.},
	language = {en},
	urldate = {2020-06-23},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Parikh, Devi and Grauman, Kristen},
	month = jun,
	year = {2011},
	pages = {1681--1688},
}

@inproceedings{yu_multi-scale_2016,
	title = {Multi-scale {Context} {Aggregation} by {Dilated} {Convolutions}},
	abstract = {State-of-the-art models for semantic segmentation are based on adaptations of convolutional networks that had originally been designed for image classiﬁcation. However, dense prediction problems such as semantic segmentation are structurally different from image classiﬁcation. In this work, we develop a new convolutional network module that is speciﬁcally designed for dense prediction. The presented module uses dilated convolutions to systematically aggregate multiscale contextual information without losing resolution. The architecture is based on the fact that dilated convolutions support exponential expansion of the receptive ﬁeld without loss of resolution or coverage. We show that the presented context module increases the accuracy of state-of-the-art semantic segmentation systems. In addition, we examine the adaptation of image classiﬁcation networks to dense prediction and show that simplifying the adapted network can increase accuracy.},
	language = {en},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Yu, Fisher and Koltun, Vladlen},
	year = {2016},
	pages = {13},
}

@inproceedings{dai_unsupervised_2014,
	address = {Columbus, OH, USA},
	title = {Unsupervised {Learning} of {Dictionaries} of {Hierarchical} {Compositional} {Models}},
	isbn = {978-1-4799-5118-5},
	url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6909717},
	doi = {10.1109/CVPR.2014.321},
	abstract = {This paper proposes an unsupervised method for learning dictionaries of hierarchical compositional models for representing natural images. Each model is in the form of a template that consists of a small group of part templates that are allowed to shift their locations and orientations relative to each other, and each part template is in turn a composition of Gabor wavelets that are also allowed to shift their locations and orientations relative to each other. Given a set of unannotated training images, a dictionary of such hierarchical templates are learned so that each training image can be represented by a small number of templates that are spatially translated, rotated and scaled versions of the templates in the learned dictionary. The learning algorithm iterates between the following two steps: (1) Image encoding by a template matching pursuit process that involves a bottom-up template matching sub-process and a top-down template localization sub-process. (2) Dictionary re-learning by a shared matching pursuit process. Experimental results show that the proposed approach is capable of learning meaningful templates, and the learned templates are useful for tasks such as domain adaption and image cosegmentation.},
	language = {en},
	urldate = {2020-07-11},
	booktitle = {2014 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Dai, Jifeng and Hong, Yi and Hu, Wenze and Zhu, Song-Chun and Wu, Ying Nian},
	month = jun,
	year = {2014},
	pages = {2505--2512},
}

@misc{noauthor_zoom_nodate,
	title = {Zoom {In}: {An} {Introduction} to {Circuits}},
	url = {https://distill.pub/2020/circuits/zoom-in/},
	urldate = {2020-07-11},
}

@article{friston_free-energy_2010,
	title = {The free-energy principle: a unified brain theory?},
	volume = {11},
	copyright = {2010 Nature Publishing Group},
	issn = {1471-0048},
	shorttitle = {The free-energy principle},
	url = {https://www.nature.com/articles/nrn2787/boxes/bx1},
	doi = {10.1038/nrn2787},
	abstract = {Adaptive agents must occupy a limited repertoire of states and therefore minimize the long-term average of surprise associated with sensory exchanges with the world. Minimizing surprise enables them to resist a natural tendency to disorder.Surprise rests on predictions about sensations, which depend on an internal generative model of the world. Although surprise cannot be measured directly, a free-energy bound on surprise can be, suggesting that agents minimize free energy by changing their predictions (perception) or by changing the predicted sensory inputs (action).Perception optimizes predictions by minimizing free energy with respect to synaptic activity (perceptual inference), efficacy (learning and memory) and gain (attention and salience). This furnishes Bayes-optimal (probabilistic) representations of what caused sensations (providing a link to the Bayesian brain hypothesis).Bayes-optimal perception is mathematically equivalent to predictive coding and maximizing the mutual information between sensations and the representations of their causes. This is a probabilistic generalization of the principle of efficient coding (the infomax principle) or the minimum-redundancy principle.Learning under the free-energy principle can be formulated in terms of optimizing the connection strengths in hierarchical models of the sensorium. This rests on associative plasticity to encode causal regularities and appeals to the same synaptic mechanisms as those underlying cell assembly formation.Action under the free-energy principle reduces to suppressing sensory prediction errors that depend on predicted (expected or desired) movement trajectories. This provides a simple account of motor control, in which action is enslaved by perceptual (proprioceptive) predictions.Perceptual predictions rest on prior expectations about the trajectory or movement through the agent's state space. These priors can be acquired (as empirical priors during hierarchical inference) or they can be innate (epigenetic) and therefore subject to selective pressure.Predicted motion or state transitions realized by action correspond to policies in optimal control theory and reinforcement learning. In this context, value is inversely proportional to surprise (and implicitly free energy), and rewards correspond to innate priors that constrain policies.},
	language = {en},
	number = {2},
	urldate = {2020-07-11},
	journal = {Nature Reviews Neuroscience},
	author = {Friston, Karl},
	month = feb,
	year = {2010},
	note = {Number: 2
Publisher: Nature Publishing Group},
	pages = {127--138},
}

@inproceedings{srivastava_training_2015,
	title = {Training {Very} {Deep} {Networks}},
	abstract = {Theoretical and empirical evidence indicates that the depth of neural networks is crucial for their success. However, training becomes more difﬁcult as depth increases, and training of very deep networks remains an open problem. Here we introduce a new architecture designed to overcome this. Our so-called highway networks allow unimpeded information ﬂow across many layers on information highways. They are inspired by Long Short-Term Memory recurrent networks and use adaptive gating units to regulate the information ﬂow. Even with hundreds of layers, highway networks can be trained directly through simple gradient descent. This enables the study of extremely deep and efﬁcient architectures.},
	language = {en},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Srivastava, Rupesh K and Greff, Klaus and Schmidhuber, Jürgen},
	year = {2015},
	pages = {9},
}

@inproceedings{chen_dual_2017,
	title = {Dual {Path} {Networks}},
	abstract = {In this work, we present a simple, highly efﬁcient and modularized Dual Path Network (DPN) for image classiﬁcation which presents a new topology of connection paths internally. By revealing the equivalence of the state-of-the-art Residual Network (ResNet) and Densely Convolutional Network (DenseNet) within the HORNN framework, we ﬁnd that ResNet enables feature re-usage while DenseNet enables new features exploration which are both important for learning good representations. To enjoy the beneﬁts from both path topologies, our proposed Dual Path Network shares common features while maintaining the ﬂexibility to explore new features through dual path architectures. Extensive experiments on three benchmark datasets, ImagNet-1k, Places365 and PASCAL VOC, clearly demonstrate superior performance of the proposed DPN over state-of-the-arts. In particular, on the ImagNet-1k dataset, a shallow DPN surpasses the best ResNeXt-101(64 × 4d) with 26\% smaller model size, 25\% less computational cost and 8\% lower memory consumption, and a deeper DPN (DPN-131) further pushes the state-of-the-art single model performance with about 2 times faster training speed. Experiments on the Places365 large-scale scene dataset, PASCAL VOC detection dataset, and PASCAL VOC segmentation dataset also demonstrate its consistently better performance than DenseNet, ResNet and the latest ResNeXt model over various applications.},
	language = {en},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Chen, Yunpeng and Li, Jianan and Xiao, Huaxin and Jin, Xiaojie and Yan, Shuicheng and Feng, Jiashi},
	year = {2017},
	pages = {9},
}

@article{huang_predictive_2011,
	title = {Predictive coding},
	volume = {2},
	copyright = {Copyright © 2011 John Wiley \& Sons, Ltd.},
	issn = {1939-5086},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/wcs.142},
	doi = {10.1002/wcs.142},
	abstract = {Predictive coding is a unifying framework for understanding redundancy reduction and efficient coding in the nervous system. By transmitting only the unpredicted portions of an incoming sensory signal, predictive coding allows the nervous system to reduce redundancy and make full use of the limited dynamic range of neurons. Starting with the hypothesis of efficient coding as a design principle in the sensory system, predictive coding provides a functional explanation for a range of neural responses and many aspects of brain organization. The lateral and temporal antagonism in receptive fields in the retina and lateral geniculate nucleus occur naturally as a consequence of predictive coding of natural images. In the higher visual system, predictive coding provides an explanation for oriented receptive fields and contextual effects as well as the hierarchical reciprocally connected organization of the cortex. Predictive coding has also been found to be consistent with a variety of neurophysiological and psychophysical data obtained from different areas of the brain. WIREs Cogni Sci 2011 2 580–593 DOI: 10.1002/wcs.142 This article is categorized under: Computer Science {\textgreater} Neural Networks},
	language = {en},
	number = {5},
	urldate = {2020-07-11},
	journal = {WIREs Cognitive Science},
	author = {Huang, Yanping and Rao, Rajesh P. N.},
	year = {2011},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/wcs.142},
	pages = {580--593},
}

@inproceedings{he_deep_2016,
	address = {Las Vegas, NV, USA},
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	isbn = {978-1-4673-8851-1},
	url = {http://ieeexplore.ieee.org/document/7780459/},
	doi = {10.1109/CVPR.2016.90},
	abstract = {Deeper neural networks are more difﬁcult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers—8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classiﬁcation task. We also present analysis on CIFAR-10 with 100 and 1000 layers.},
	language = {en},
	urldate = {2020-07-09},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = jun,
	year = {2016},
	pages = {770--778},
}

@article{clarke_understanding_2015,
	title = {Understanding {What} {We} {See}: {How} {We} {Derive} {Meaning} {From} {Vision}},
	volume = {19},
	issn = {1364-6613},
	shorttitle = {Understanding {What} {We} {See}},
	url = {http://www.sciencedirect.com/science/article/pii/S1364661315001989},
	doi = {10.1016/j.tics.2015.08.008},
	abstract = {Recognising objects goes beyond vision, and requires models that incorporate different aspects of meaning. Most models focus on superordinate categories (e.g., animals, tools) which do not capture the richness of conceptual knowledge. We argue that object recognition must be seen as a dynamic process of transformation from low-level visual input through categorical organisation to specific conceptual representations. Cognitive models based on large normative datasets are well-suited to capture statistical regularities within and between concepts, providing both category structure and basic-level individuation. We highlight recent research showing how such models capture important properties of the ventral visual pathway. This research demonstrates that significant advances in understanding conceptual representations can be made by shifting the focus from studying superordinate categories to basic-level concepts.},
	language = {en},
	number = {11},
	urldate = {2020-07-09},
	journal = {Trends in Cognitive Sciences},
	author = {Clarke, Alex and Tyler, Lorraine K.},
	month = nov,
	year = {2015},
	keywords = {Concepts, category, fusiform gyrus, perirhinal cortex, semantics, ventral visual pathway},
	pages = {677--687},
}

@article{guclu_deep_2015,
	title = {Deep {Neural} {Networks} {Reveal} a {Gradient} in the {Complexity} of {Neural} {Representations} across the {Ventral} {Stream}},
	volume = {35},
	copyright = {Copyright © 2015 the authors 0270-6474/15/3510005-10\$15.00/0},
	issn = {0270-6474, 1529-2401},
	url = {https://www.jneurosci.org/content/35/27/10005},
	doi = {10.1523/JNEUROSCI.5023-14.2015},
	abstract = {Converging evidence suggests that the primate ventral visual pathway encodes increasingly complex stimulus features in downstream areas. We quantitatively show that there indeed exists an explicit gradient for feature complexity in the ventral pathway of the human brain. This was achieved by mapping thousands of stimulus features of increasing complexity across the cortical sheet using a deep neural network. Our approach also revealed a fine-grained functional specialization of downstream areas of the ventral stream. Furthermore, it allowed decoding of representations from human brain activity at an unsurpassed degree of accuracy, confirming the quality of the developed approach. Stimulus features that successfully explained neural responses indicate that population receptive fields were explicitly tuned for object categorization. This provides strong support for the hypothesis that object categorization is a guiding principle in the functional organization of the primate ventral stream.},
	language = {en},
	number = {27},
	urldate = {2020-07-09},
	journal = {Journal of Neuroscience},
	author = {Güçlü, Umut and Gerven, Marcel A. J. van},
	month = jul,
	year = {2015},
	pmid = {26157000},
	note = {Publisher: Society for Neuroscience
Section: Articles},
	keywords = {deep learning, functional magnetic resonance imaging, neural coding},
	pages = {10005--10014},
}

@article{yamins_hierarchical_nodate,
	title = {Hierarchical {Modular} {Optimization} of {Convolutional} {Networks} {Achieves} {Representations} {Similar} to {Macaque} {IT} and {Human} {Ventral} {Stream}},
	abstract = {Humans recognize visually-presented objects rapidly and accurately. To understand this ability, we seek to construct models of the ventral stream, the series of cortical areas thought to subserve object recognition. One tool to assess the quality of a model of the ventral stream is the Representational Dissimilarity Matrix (RDM), which uses a set of visual stimuli and measures the distances produced in either the brain (i.e. fMRI voxel responses, neural ﬁring rates) or in models (features). Previous work has shown that all known models of the ventral stream fail to capture the RDM pattern observed in either IT cortex, the highest ventral area, or in the human ventral stream. In this work, we construct models of the ventral stream using a novel optimization procedure for category-level object recognition problems, and produce RDMs resembling both macaque IT and human ventral stream. The model, while novel in the optimization procedure, further develops a long-standing functional hypothesis that the ventral visual stream is a hierarchically arranged series of processing stages optimized for visual object recognition.},
	language = {en},
	author = {Yamins, Daniel L and Hong, Ha and Cadieu, Charles and DiCarlo, James J},
	pages = {9},
}

@article{liao_how_nodate,
	title = {How {Important} {Is} {Weight} {Symmetry} in {Backpropagation}?},
	abstract = {Gradient backpropagation (BP) requires symmetric feedforward and feedback connections—the same weights must be used for forward and backward passes. This “weight transport problem” (Grossberg 1987) is thought to be one of the main reasons to doubt BP’s biologically plausibility. Using 15 different classiﬁcation datasets, we systematically investigate to what extent BP really depends on weight symmetry. In a study that turned out to be surprisingly similar in spirit to Lillicrap et al.’s demonstration (Lillicrap et al. 2014) but orthogonal in its results, our experiments indicate that: (1) the magnitudes of feedback weights do not matter to performance (2) the signs of feedback weights do matter—the more concordant signs between feedforward and their corresponding feedback connections, the better (3) with feedback weights having random magnitudes and 100\% concordant signs, we were able to achieve the same or even better performance than SGD. (4) some normalizations/stabilizations are indispensable for such asymmetric BP to work, namely Batch Normalization (BN) (Ioffe and Szegedy 2015) and/or a “Batch Manhattan” (BM) update rule.},
	language = {en},
	author = {Liao, Qianli and Leibo, Joel Z and Poggio, Tomaso},
	pages = {8},
}

@article{lamme_feedforward_1998,
	title = {Feedforward, horizontal, and feedback processing in the visual cortex},
	volume = {8},
	issn = {0959-4388},
	url = {http://www.sciencedirect.com/science/article/pii/S0959438898800421},
	doi = {10.1016/S0959-4388(98)80042-1},
	abstract = {The cortical visual system consists of many richly interconnected areas. Each area is characterized by more or less specific receptive field tuning properties. However, these tuning properties reflect only a subset of the interactions that occur within and between areas. Neuronal responses may be modulation by perceptual context or attention. These modulations reflect lateral interactions within areas and feedback from higher to lower areas. Recent work is beginning to unravel how horizontal and feedback connections each contribute to modulatory effects and what the role of these modulations is in vision. Whereas receptive field tuning properties reflect feedforward processing, modulations evoked by horizontal and feedback connections may reflect the integration of information that underlies perception.},
	language = {en},
	number = {4},
	urldate = {2020-07-07},
	journal = {Current Opinion in Neurobiology},
	author = {Lamme, Victor AF and Supèr, Hans and Spekreijse, Henk},
	month = aug,
	year = {1998},
	pages = {529--535},
}

@article{serre_feedforward_2007,
	title = {A feedforward architecture accounts for rapid categorization},
	volume = {104},
	copyright = {© 2007 by The National Academy of Sciences of the USA.                    Freely available online through the PNAS open access option.},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/104/15/6424},
	doi = {10.1073/pnas.0700622104},
	abstract = {Primates are remarkably good at recognizing objects. The level of performance of their visual system and its robustness to image degradations still surpasses the best computer vision systems despite decades of engineering effort. In particular, the high accuracy of primates in ultra rapid object categorization and rapid serial visual presentation tasks is remarkable. Given the number of processing stages involved and typical neural latencies, such rapid visual processing is likely to be mostly feedforward. Here we show that a specific implementation of a class of feedforward theories of object recognition (that extend the Hubel and Wiesel simple-to-complex cell hierarchy and account for many anatomical and physiological constraints) can predict the level and the pattern of performance achieved by humans on a rapid masked animal vs. non-animal categorization task.},
	language = {en},
	number = {15},
	urldate = {2020-07-07},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Serre, Thomas and Oliva, Aude and Poggio, Tomaso},
	month = apr,
	year = {2007},
	pmid = {17404214},
	note = {Publisher: National Academy of Sciences
Section: Biological Sciences},
	keywords = {computational model, natural scenes, object recognition, preattentive vision, visual cortex},
	pages = {6424--6429},
}

@inproceedings{huang_densely_2017,
	address = {Honolulu, HI},
	title = {Densely {Connected} {Convolutional} {Networks}},
	isbn = {978-1-5386-0457-1},
	url = {https://ieeexplore.ieee.org/document/8099726/},
	doi = {10.1109/CVPR.2017.243},
	language = {en},
	urldate = {2020-07-07},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Huang, Gao and Liu, Zhuang and Van Der Maaten, Laurens and Weinberger, Kilian Q.},
	month = jul,
	year = {2017},
	pages = {2261--2269},
}

@article{kubilius_deep_2016,
	title = {Deep {Neural} {Networks} as a {Computational} {Model} for {Human} {Shape} {Sensitivity}},
	volume = {12},
	issn = {1553-7358},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004896},
	doi = {10.1371/journal.pcbi.1004896},
	abstract = {Theories of object recognition agree that shape is of primordial importance, but there is no consensus about how shape might be represented, and so far attempts to implement a model of shape perception that would work with realistic stimuli have largely failed. Recent studies suggest that state-of-the-art convolutional ‘deep’ neural networks (DNNs) capture important aspects of human object perception. We hypothesized that these successes might be partially related to a human-like representation of object shape. Here we demonstrate that sensitivity for shape features, characteristic to human and primate vision, emerges in DNNs when trained for generic object recognition from natural photographs. We show that these models explain human shape judgments for several benchmark behavioral and neural stimulus sets on which earlier models mostly failed. In particular, although never explicitly trained for such stimuli, DNNs develop acute sensitivity to minute variations in shape and to non-accidental properties that have long been implicated to form the basis for object recognition. Even more strikingly, when tested with a challenging stimulus set in which shape and category membership are dissociated, the most complex model architectures capture human shape sensitivity as well as some aspects of the category structure that emerges from human judgments. As a whole, these results indicate that convolutional neural networks not only learn physically correct representations of object categories but also develop perceptually accurate representational spaces of shapes. An even more complete model of human object representations might be in sight by training deep architectures for multiple tasks, which is so characteristic in human development.},
	language = {en},
	number = {4},
	urldate = {2020-07-07},
	journal = {PLOS Computational Biology},
	author = {Kubilius, Jonas and Bracci, Stefania and Beeck, Hans P. Op de},
	month = apr,
	year = {2016},
	note = {Publisher: Public Library of Science},
	keywords = {Behavior, Human performance, Monkeys, Neural networks, Object recognition, Sensory perception, Vision, Visual cortex},
	pages = {e1004896},
}

@inproceedings{zuo_convolutional_2015,
	address = {Boston, MA, USA},
	title = {Convolutional recurrent neural networks: {Learning} spatial dependencies for image representation},
	isbn = {978-1-4673-6759-2},
	shorttitle = {Convolutional recurrent neural networks},
	url = {http://ieeexplore.ieee.org/document/7301268/},
	doi = {10.1109/CVPRW.2015.7301268},
	abstract = {In existing convolutional neural networks (CNNs), both convolution and pooling are locally performed for image regions separately, no contextual dependencies between different image regions have been taken into consideration. Such dependencies represent useful spatial structure information in images. Whereas recurrent neural networks (RNNs) are designed for learning contextual dependencies among sequential data by using the recurrent (feedback) connections. In this work, we propose the convolutional recurrent neural network (C-RNN), which learns the spatial dependencies between image regions to enhance the discriminative power of image representation. The C-RNN is trained in an end-to-end manner from raw pixel images. CNN layers are ﬁrstly processed to generate middle level features. RNN layer is then learned to encode spatial dependencies. The C-RNN can learn better image representation, especially for images with obvious spatial contextual dependencies. Our method achieves competitive performance on ILSVRC 2012, SUN 397, and MIT indoor.},
	language = {en},
	urldate = {2020-07-07},
	booktitle = {2015 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops} ({CVPRW})},
	publisher = {IEEE},
	author = {Zuo, Zhen and Shuai, Bing and Wang, Gang and Liu, Xiao and Wang, Xingxing and Wang, Bing and Chen, Yushi},
	month = jun,
	year = {2015},
	pages = {18--26},
}

@inproceedings{zhang_progressive_2018,
	address = {Salt Lake City, UT},
	title = {Progressive {Attention} {Guided} {Recurrent} {Network} for {Salient} {Object} {Detection}},
	isbn = {978-1-5386-6420-9},
	url = {https://ieeexplore.ieee.org/document/8578179/},
	doi = {10.1109/CVPR.2018.00081},
	abstract = {Effective convolutional features play an important role in saliency estimation but how to learn powerful features for saliency is still a challenging task. FCN-based methods directly apply multi-level convolutional features without distinction, which leads to sub-optimal results due to the distraction from redundant details. In this paper, we propose a novel attention guided network which selectively integrates multi-level contextual information in a progressive manner. Attentive features generated by our network can alleviate distraction of background thus achieve better performance. On the other hand, it is observed that most of existing algorithms conduct salient object detection by exploiting side-output features of the backbone feature extraction network. However, shallower layers of backbone network lack the ability to obtain global semantic information, which limits the effective feature learning. To address the problem, we introduce multi-path recurrent feedback to enhance our proposed progressive attention driven framework. Through multi-path recurrent connections, global semantic information from the top convolutional layer is transferred to shallower layers, which intrinsically reﬁnes the entire network. Experimental results on six benchmark datasets demonstrate that our algorithm performs favorably against the state-of-the-art approaches.},
	language = {en},
	urldate = {2020-07-07},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Zhang, Xiaoning and Wang, Tiantian and Qi, Jinqing and Lu, Huchuan and Wang, Gang},
	month = jun,
	year = {2018},
	pages = {714--722},
}

@inproceedings{liu_semantic_2017,
	address = {Honolulu, HI},
	title = {Semantic {Regularisation} for {Recurrent} {Image} {Annotation}},
	isbn = {978-1-5386-0457-1},
	url = {http://ieeexplore.ieee.org/document/8099926/},
	doi = {10.1109/CVPR.2017.443},
	abstract = {The “CNN-RNN” design pattern is increasingly widely applied in a variety of image annotation tasks including multi-label classiﬁcation and captioning. Existing models use the weakly semantic CNN hidden layer or its transform as the image embedding that provides the interface between the CNN and RNN. This leaves the RNN overstretched with two jobs: predicting the visual concepts and modelling their correlations for generating structured annotation output. Importantly this makes the end-to-end training of the CNN and RNN slow and ineffective due to the difﬁculty of back propagating gradients through the RNN to train the CNN. We propose a simple modiﬁcation to the design pattern that makes learning more effective and efﬁcient. Speciﬁcally, we propose to use a semantically regularised embedding layer as the interface between the CNN and RNN. Regularising the interface can partially or completely decouple the learning problems, allowing each to be more effectively trained and jointly training much more efﬁcient. Extensive experiments show that state-of-the art performance is achieved on multi-label classiﬁcation as well as image captioning.},
	language = {en},
	urldate = {2020-07-07},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Liu, Feng and Xiang, Tao and Hospedales, Timothy M. and Yang, Wankou and Sun, Changyin},
	month = jul,
	year = {2017},
	pages = {4160--4168},
}

@article{kim_recurrent_nodate,
	title = {Recurrent {Transformer} {Networks} for {Semantic} {Correspondence}},
	abstract = {We present recurrent transformer networks (RTNs) for obtaining dense correspondences between semantically similar images. Our networks accomplish this through an iterative process of estimating spatial transformations between the input images and using these transformations to generate aligned convolutional activations. By directly estimating the transformations between an image pair, rather than employing spatial transformer networks to independently normalize each individual image, we show that greater accuracy can be achieved. This process is conducted in a recursive manner to reﬁne both the transformation estimates and the feature representations. In addition, a technique is presented for weakly-supervised training of RTNs that is based on a proposed classiﬁcation loss. With RTNs, state-of-the-art performance is attained on several benchmarks for semantic correspondence.},
	language = {en},
	author = {Kim, Seungryong and Lin, Stephen and Jeon, Sang Ryul and Min, Dongbo and Sohn, Kwanghoon},
	pages = {11},
}

@article{wyatte_early_2014,
	title = {Early recurrent feedback facilitates visual object recognition under challenging conditions},
	volume = {5},
	issn = {1664-1078},
	url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2014.00674/full},
	doi = {10.3389/fpsyg.2014.00674},
	abstract = {Standard models of the visual object recognition pathway hold that a largely feedforward process from the retina through inferotemporal cortex leads to object identification. A subsequent feedback process originating in frontoparietal areas through reciprocal connections to striate cortex provides attentional support to salient or behaviorally-relevant features. Here, we review mounting evidence that feedback signals also originate within extrastriate regions and begin during the initial feedforward process. This feedback process is temporally dissociable from attention and provides important functions such as grouping, associational reinforcement, and filling-in of features. Local feedback signals operating concurrently with feedforward processing are important for object identification in noisy real-world situations, particularly when objects are partially occluded, unclear, or otherwise ambiguous. Altogether, the dissociation of early and late feedback processes presented here expands on current models of object identification, and suggests a dual role for descending feedback projections.},
	language = {English},
	urldate = {2020-07-06},
	journal = {Frontiers in Psychology},
	author = {Wyatte, Dean and Jilk, David J. and O'Reilly, Randall C.},
	year = {2014},
	note = {Publisher: Frontiers},
	keywords = {Feedback, Illusory contours, amodal completion, object recognition, top-down attention},
}

@inproceedings{lee_recursive_2016,
	address = {Las Vegas, NV, USA},
	title = {Recursive {Recurrent} {Nets} with {Attention} {Modeling} for {OCR} in the {Wild}},
	isbn = {978-1-4673-8851-1},
	url = {http://ieeexplore.ieee.org/document/7780614/},
	doi = {10.1109/CVPR.2016.245},
	abstract = {We present recursive recurrent neural networks with attention modeling (R2AM) for lexicon-free optical character recognition in natural scene images. The primary advantages of the proposed method are: (1) use of recursive convolutional neural networks (CNNs), which allow for parametrically efﬁcient and effective image feature extraction; (2) an implicitly learned character-level language model, embodied in a recurrent neural network which avoids the need to use N-grams; and (3) the use of a soft-attention mechanism, allowing the model to selectively exploit image features in a coordinated way, and allowing for end-to-end training within a standard backpropagation framework.},
	language = {en},
	urldate = {2020-07-06},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Lee, Chen-Yu and Osindero, Simon},
	month = jun,
	year = {2016},
	pages = {2231--2239},
}

@inproceedings{shuai_dag-recurrent_2016,
	address = {Las Vegas, NV, USA},
	title = {{DAG}-{Recurrent} {Neural} {Networks} for {Scene} {Labeling}},
	isbn = {978-1-4673-8851-1},
	url = {http://ieeexplore.ieee.org/document/7780763/},
	doi = {10.1109/CVPR.2016.394},
	abstract = {In image labeling, local representations for image units are usually generated from their surrounding image patches, thus long-range contextual information is not effectively encoded. In this paper, we introduce recurrent neural networks (RNNs) to address this issue. Speciﬁcally, directed acyclic graph RNNs (DAG-RNNs) are proposed to process DAG-structured images, which enables the network to model long-range semantic dependencies among image units. Our DAG-RNNs are capable of tremendously enhancing the discriminative power of local representations, which signiﬁcantly beneﬁts the local classiﬁcation. Meanwhile, we propose a novel class weighting function that attends to rare classes, which phenomenally boosts the recognition accuracy for non-frequent classes. Integrating with convolution and deconvolution layers, our DAG-RNNs achieve new state-of-the-art results on the challenging SiftFlow, CamVid and Barcelona benchmarks.},
	language = {en},
	urldate = {2020-07-06},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Shuai, Bing and Zuo, Zhen and Wang, Bing and Wang, Gang},
	month = jun,
	year = {2016},
	pages = {3620--3629},
}

@article{schrimpf_brain-score_nodate,
	title = {Brain-{Score}: {Which} {Artificial} {Neural} {Network} for {Object} {Recognition} is most {Brain}-{Like}?},
	language = {en},
	author = {Schrimpf, Martin and Kubilius, Jonas and Hong, Ha and Majaj, Najib J and Rajalingham, Rishi and Issa, Elias B and Kar, Kohitij and Bashivan, Pouya and Prescott-Roy, Jonathan and Geiger, Franziska and Schmidt, Kailyn and Yamins, Daniel L K and DiCarlo, James J},
	pages = {9},
}

@inproceedings{bashivan_teacher_2019,
	address = {Seoul, Korea (South)},
	title = {Teacher {Guided} {Architecture} {Search}},
	isbn = {978-1-72814-803-8},
	url = {https://ieeexplore.ieee.org/document/9010650/},
	doi = {10.1109/ICCV.2019.00542},
	abstract = {Much of the recent improvement in neural networks for computer vision has resulted from discovery of new networks architectures. Most prior work has used the performance of candidate models following limited training to automatically guide the search in a feasible way. Could further gains in computational efﬁciency be achieved by guiding the search via measurements of a high performing network with unknown detailed architecture (e.g. the primate visual system)? As one step toward this goal, we use representational similarity analysis to evaluate the similarity of internal activations of candidate networks with those of a (ﬁxed, high performing) teacher network. We show that adopting this evaluation metric could produce up to an order of magnitude in search efﬁciency over performanceguided methods. Our approach ﬁnds a convolutional cell structure with similar performance as was previously found using other methods but at a total computational cost that is two orders of magnitude lower than Neural Architecture Search (NAS) and more than four times lower than progressive neural architecture search (PNAS). We further show that measurements from only ∼300 neurons from primate visual system provides enough signal to ﬁnd a network with an Imagenet top-1 error that is signiﬁcantly lower than that achieved by performance-guided architecture search alone. These results suggest that representational matching can be used to accelerate network architecture search in cases where one has access to some or all of the internal representations of a teacher network of interest, such as the brain’s sensory processing networks.},
	language = {en},
	urldate = {2020-07-06},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Bashivan, Pouya and Tensen, Mark and Dicarlo, James},
	month = oct,
	year = {2019},
	pages = {5319--5328},
}

@inproceedings{jin_multi-path_2017,
	title = {Multi-{Path} {Feedback} {Recurrent} {Neural} {Networks} for {Scene} {Parsing}},
	abstract = {In this paper, we consider the scene parsing problem and propose a novel Multi-Path Feedback recurrent neural network (MPF-RNN) for parsing scene images. MPF-RNN can enhance the capability of RNNs in modeling long-range context information at multiple levels and better distinguish pixels that are easy to confuse. Different from feedforward CNNs and RNNs with only single feedback, MPF-RNN propagates the contextual features learned at top layer through multiple weighted recurrent connections to learn bottom features. For better training MPF-RNN, we propose a new strategy that considers accumulative loss at multiple recurrent steps to improve performance of the MPF-RNN on parsing small objects. With these two novel components, MPF-RNN has achieved signiﬁcant improvement over strong baselines (VGG16 and Res101) on ﬁve challenging scene parsing benchmarks, including traditional SiftFlow, Barcelona, CamVid, Stanford Background as well as the recently released large-scale ADE20K.},
	language = {en},
	booktitle = {Thirty-{First} {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Jin, Xiaojie and Chen, Yunpeng and Jie, Zequn and Feng, Jiashi and Yan, Shuicheng},
	year = {2017},
	pages = {7},
}

@article{zuo_learning_2016,
	title = {Learning {Contextual} {Dependence} {With} {Convolutional} {Hierarchical} {Recurrent} {Neural} {Networks}},
	volume = {25},
	issn = {1941-0042},
	doi = {10.1109/TIP.2016.2548241},
	abstract = {Deep convolutional neural networks (CNNs) have shown their great success on image classification. CNNs mainly consist of convolutional and pooling layers, both of which are performed on local image areas without considering the dependence among different image regions. However, such dependence is very important for generating explicit image representation. In contrast, recurrent neural networks (RNNs) are well known for their ability of encoding contextual information in sequential data, and they only require a limited number of network parameters. Thus, we proposed the hierarchical RNNs (HRNNs) to encode the contextual dependence in image representation. In HRNNs, each RNN layer focuses on modeling spatial dependence among image regions from the same scale but different locations. While the cross RNN scale connections target on modeling scale dependencies among regions from the same location but different scales. Specifically, we propose two RNN models: 1) hierarchical simple recurrent network (HSRN), which is fast and has low computational cost and 2) hierarchical long-short term memory recurrent network, which performs better than HSRN with the price of higher computational cost. In this paper, we integrate CNNs with HRNNs, and develop end-to-end convolutional hierarchical RNNs (C-HRNNs) for image classification. C-HRNNs not only utilize the discriminative representation power of CNNs, but also utilize the contextual dependence learning ability of our HRNNs. On four of the most challenging object/scene image classification benchmarks, our C-HRNNs achieve the state-of-the-art results on Places 205, SUN 397, and MIT indoor, and the competitive results on ILSVRC 2012.},
	number = {7},
	journal = {IEEE Transactions on Image Processing},
	author = {Zuo, Zhen and Shuai, Bing and Wang, Gang and Liu, Xiao and Wang, Xingxing and Wang, Bing and Chen, Yushi},
	month = jul,
	year = {2016},
	note = {Conference Name: IEEE Transactions on Image Processing},
	keywords = {C-HRNN, Computer vision, Context modeling, Convolutional Neural Networks, Deep Learning, Deep learning, Image Classification, Image representation, Logic gates, Natural language processing, Recurrent Neural Networks, Recurrent neural networks, computational complexity, computational cost, contextual dependence learning, convolutional hierarchical recurrent neural networks, convolutional neural networks, hierarchical long-short term memory recurrent network, hierarchical simple recurrent network, image classification, image regions spatial dependence, image representation, object image classification, recurrent neural nets, recurrent neural networks, scene image classification},
	pages = {2983--2996},
}

@inproceedings{ronneberger_u-net_2015,
	address = {Cham},
	title = {U-{Net}: {Convolutional} {Networks} for {Biomedical} {Image} {Segmentation}},
	volume = {9351},
	isbn = {978-3-319-24573-7 978-3-319-24574-4},
	shorttitle = {U-{Net}},
	url = {http://link.springer.com/10.1007/978-3-319-24574-4_28},
	doi = {10.1007/978-3-319-24574-4_28},
	abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more eﬃciently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caﬀe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net.},
	language = {en},
	urldate = {2020-07-04},
	booktitle = {International {Conference} on {Medical} {Image} {Computing} and {Computer}-{Assisted} {Intervention}},
	publisher = {Springer International Publishing},
	author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	year = {2015},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {Convolutional Layer, Data Augmentation, Deep Network, Ground Truth Segmentation, Training Image},
	pages = {234--241},
}

@inproceedings{wen_deep_2018,
	title = {Deep {Predictive} {Coding} {Network} for {Object} {Recognition}},
	url = {http://proceedings.mlr.press/v80/wen18a.html},
	abstract = {Based on the predictive coding theory in neuro- science, we designed a bi-directional and recur- rent neural net, namely deep predictive coding networks (PCN), that has feedforward, feedback, and r...},
	language = {en},
	urldate = {2020-07-06},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Wen, Haiguang and Han, Kuan and Shi, Junxing and Zhang, Yizhen and Culurciello, Eugenio and Liu, Zhongming},
	month = jul,
	year = {2018},
	note = {ISSN: 1938-7228
Section: Machine Learning},
	pages = {5266--5275},
}

@article{manassi_what_2016,
	title = {What crowding can tell us about object representations},
	volume = {16},
	issn = {1534-7362},
	url = {http://jov.arvojournals.org/article.aspx?articleid=2497927},
	doi = {10.1167/16.3.35},
	language = {en},
	number = {3},
	urldate = {2020-07-06},
	journal = {Journal of Vision},
	author = {Manassi, Mauro and Lonchampt, Sophie and Clarke, Aaron and Herzog, Michael H.},
	month = feb,
	year = {2016},
	note = {Publisher: The Association for Research in Vision and Ophthalmology},
	pages = {35--35},
}

@article{doerig_crowding_2020,
	title = {Crowding reveals fundamental differences in local vs. global processing in humans and machines},
	volume = {167},
	issn = {0042-6989},
	url = {http://www.sciencedirect.com/science/article/pii/S0042698919302299},
	doi = {10.1016/j.visres.2019.12.006},
	abstract = {Feedforward Convolutional Neural Networks (ffCNNs) have become state-of-the-art models both in computer vision and neuroscience. However, human-like performance of ffCNNs does not necessarily imply human-like computations. Previous studies have suggested that current ffCNNs do not make use of global shape information. However, it is currently unclear whether this reflects fundamental differences between ffCNN and human processing or is merely an artefact of how ffCNNs are trained. Here, we use visual crowding as a well-controlled, specific probe to test global shape computations. Our results provide evidence that ffCNNs cannot produce human-like global shape computations for principled architectural reasons. We lay out approaches that may address shortcomings of ffCNNs to provide better models of the human visual system.},
	language = {en},
	urldate = {2020-07-06},
	journal = {Vision Research},
	author = {Doerig, A. and Bornet, A. and Choung, O. H. and Herzog, M. H.},
	month = feb,
	year = {2020},
	keywords = {Convolutional Neural Networks, Crowding, Deep Neural Networks, Global processing, Grouping, Segmentation},
	pages = {39--45},
}

@article{kietzmann_recurrence_2019,
	title = {Recurrence is required to capture the representational dynamics of the human visual system},
	volume = {116},
	copyright = {Copyright © 2019 the Author(s). Published by PNAS.. https://creativecommons.org/licenses/by-nc-nd/4.0/This open access article is distributed under Creative Commons Attribution-NonCommercial-NoDerivatives License 4.0 (CC BY-NC-ND).},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/116/43/21854},
	doi = {10.1073/pnas.1905544116},
	abstract = {The human visual system is an intricate network of brain regions that enables us to recognize the world around us. Despite its abundant lateral and feedback connections, object processing is commonly viewed and studied as a feedforward process. Here, we measure and model the rapid representational dynamics across multiple stages of the human ventral stream using time-resolved brain imaging and deep learning. We observe substantial representational transformations during the first 300 ms of processing within and across ventral-stream regions. Categorical divisions emerge in sequence, cascading forward and in reverse across regions, and Granger causality analysis suggests bidirectional information flow between regions. Finally, recurrent deep neural network models clearly outperform parameter-matched feedforward models in terms of their ability to capture the multiregion cortical dynamics. Targeted virtual cooling experiments on the recurrent deep network models further substantiate the importance of their lateral and top-down connections. These results establish that recurrent models are required to understand information processing in the human ventral stream.},
	language = {en},
	number = {43},
	urldate = {2020-07-06},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Kietzmann, Tim C. and Spoerer, Courtney J. and Sörensen, Lynn K. A. and Cichy, Radoslaw M. and Hauk, Olaf and Kriegeskorte, Nikolaus},
	month = oct,
	year = {2019},
	pmid = {31591217},
	note = {Publisher: National Academy of Sciences
Section: PNAS Plus},
	keywords = {deep recurrent neural networks, magnetoencephalography, object recognition, representational dynamics, virtual cooling},
	pages = {21854--21863},
}

@article{torralba_contextual_2006,
	title = {Contextual guidance of eye movements and attention in real-world scenes: {The} role of global features in object search.},
	volume = {113},
	issn = {1939-1471, 0033-295X},
	shorttitle = {Contextual guidance of eye movements and attention in real-world scenes},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0033-295X.113.4.766},
	doi = {10.1037/0033-295X.113.4.766},
	abstract = {Many experiments have shown that the human visual system makes extensive use of contextual information for facilitating object search in natural scenes. However, the question of how to formally model contextual influences is still open. On the basis of a Bayesian framework, the authors present an original approach of attentional guidance by global scene context. The model comprises 2 parallel pathways; one pathway computes local features (saliency) and the other computes global (scenecentered) features. The contextual guidance model of attention combines bottom-up saliency, scene context, and top-down mechanisms at an early stage of visual processing and predicts the image regions likely to be fixated by human observers performing natural search tasks in real-world scenes.},
	language = {en},
	number = {4},
	urldate = {2020-07-06},
	journal = {Psychological Review},
	author = {Torralba, Antonio and Oliva, Aude and Castelhano, Monica S. and Henderson, John M.},
	month = oct,
	year = {2006},
	pages = {766--786},
}

@article{hayhoe_eye_2005,
	title = {Eye movements in natural behavior},
	volume = {9},
	issn = {1364-6613},
	url = {http://www.sciencedirect.com/science/article/pii/S1364661305000598},
	doi = {10.1016/j.tics.2005.02.009},
	abstract = {The classic experiments of Yarbus over 50 years ago revealed that saccadic eye movements reflect cognitive processes. But it is only recently that three separate advances have greatly expanded our understanding of the intricate role of eye movements in cognitive function. The first is the demonstration of the pervasive role of the task in guiding where and when to fixate. The second has been the recognition of the role of internal reward in guiding eye and body movements, revealed especially in neurophysiological studies. The third important advance has been the theoretical developments in the fields of reinforcement learning and graphic simulation. All of these advances are proving crucial for understanding how behavioral programs control the selection of visual information.},
	language = {en},
	number = {4},
	urldate = {2020-07-06},
	journal = {Trends in Cognitive Sciences},
	author = {Hayhoe, Mary and Ballard, Dana},
	month = apr,
	year = {2005},
	pages = {188--194},
}

@inproceedings{alexe_what_2010,
	title = {What is an object?},
	doi = {10.1109/CVPR.2010.5540226},
	abstract = {We present a generic objectness measure, quantifying how likely it is for an image window to contain an object of any class. We explicitly train it to distinguish objects with a well-defined boundary in space, such as cows and telephones, from amorphous background elements, such as grass and road. The measure combines in a Bayesian framework several image cues measuring characteristics of objects, such as appearing different from their surroundings and having a closed boundary. This includes an innovative cue measuring the closed boundary characteristic. In experiments on the challenging PASCAL VOC 07 dataset, we show this new cue to outperform a state-of-the-art saliency measure, and the combined measure to perform better than any cue alone. Finally, we show how to sample windows from an image according to their objectness distribution and give an algorithm to employ them as location priors for modern class-specific object detectors. In experiments on PASCAL VOC 07 we show this greatly reduces the number of windows evaluated by class-specific object detectors.},
	booktitle = {2010 {IEEE} {Computer} {Society} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Alexe, Bogdan and Deselaers, Thomas and Ferrari, Vittorio},
	month = jun,
	year = {2010},
	note = {ISSN: 1063-6919},
	keywords = {Bayesian framework, Bayesian methods, Detectors, Image color analysis, Image edge detection, Image segmentation, Pixel, Training, belief networks, image cues measurement, image recognition, image window, object detection, objectness measurement},
	pages = {73--80},
}

@article{rensink_dynamic_2000,
	title = {The {Dynamic} {Representation} of {Scenes}},
	volume = {7},
	issn = {1350-6285},
	url = {https://doi.org/10.1080/135062800394667},
	doi = {10.1080/135062800394667},
	abstract = {One of the more powerful impressions created by vision is that of a coherent, richly detailed world where everything is present simultaneously. Indeed, this impression is so compelling that we tend to ascribe these properties not only to the external world, but to our internal representations as well. But results from several recent experiments argue against this latter ascription. For example, changes in images of real-world scenes often go unnoticed when made during a saccade, flicker, blink, or movie cut. This “change blindness” provides strong evidence against the idea that our brains contain a picture-like representation of the scene that is everywhere detailed and coherent. How then do we represent a scene? It is argued here that focused attention provides spatiotemporal coherence for the stable representation of one object at a time. It is then argued that the allocation of attention can be co-ordinated to create a “virtual representation”. In such a scheme, a stable object representation is formed whenever needed, making it appear to higher levels as if all objects in the scene are represented in detail simultaneously.},
	number = {1-3},
	urldate = {2020-07-06},
	journal = {Visual Cognition},
	author = {Rensink, Ronald A.},
	month = jan,
	year = {2000},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/135062800394667},
	pages = {17--42},
}

@article{oreilly_recurrent_2013,
	title = {Recurrent {Processing} during {Object} {Recognition}},
	volume = {4},
	issn = {1664-1078},
	url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2013.00124/full},
	doi = {10.3389/fpsyg.2013.00124},
	abstract = {How does the brain learn to recognize objects visually, and perform this difficult feat robustly in the face of many sources of ambiguity and variability? We present a computational model based on the biology of the relevant visual pathways that learns to reliably recognize 100 different object categories in the face of of naturally-occurring variability in location, rotation, size, and lighting. The model exhibits robustness to highly ambiguous, partially occluded inputs. Both the unified, biologically plausible learning mechanism and the robustness to occlusion derive from the role that recurrent connectivity and recurrent processing mechanisms play in the model. Furthermore, this interaction of recurrent connectivity and learning predicts that high-level visual representations should be shaped by error signals from nearby, associated brain areas over the course of visual learning. Consistent with this prediction, we show how semantic knowledge about object categories changes the nature of their learned visual representations, as well as how this representational shift supports the mapping between perceptual and conceptual knowledge. Altogether, these findings support the potential importance of ongoing recurrent processing throughout the brain's visual system and suggest ways in which object recognition can be understood in terms of interactions within and between processes over time.},
	language = {English},
	urldate = {2020-07-05},
	journal = {Frontiers in Psychology},
	author = {O'Reilly, Randall C. and Wyatte, Dean and Herd, Seth and Mingus, Brian and Jilk, David J.},
	year = {2013},
	note = {Publisher: Frontiers},
	keywords = {Feedback, Winners-take-all mechanism, computational model, object recognition, recurrent processing},
}

@article{felleman_distributed_1991,
	title = {Distributed {Hierarchical} {Processing} in the {Primate} {Cerebral} {Cortex}},
	volume = {1},
	issn = {1047-3211, 1460-2199},
	url = {https://academic.oup.com/cercor/article-lookup/doi/10.1093/cercor/1.1.1},
	doi = {10.1093/cercor/1.1.1},
	abstract = {Europe PMC is an archive of life sciences journal literature., Distributed hierarchical processing in the primate cerebral cortex.},
	language = {en},
	number = {1},
	urldate = {2020-07-05},
	journal = {Cerebral Cortex},
	author = {Felleman, D. J. and Van Essen, D. C.},
	month = jan,
	year = {1991},
	pages = {1--47},
}

@article{wyatte_limits_2012,
	title = {The {Limits} of {Feedforward} {Vision}: {Recurrent} {Processing} {Promotes} {Robust} {Object} {Recognition} when {Objects} {Are} {Degraded}},
	volume = {24},
	issn = {0898-929X, 1530-8898},
	shorttitle = {The {Limits} of {Feedforward} {Vision}},
	url = {http://www.mitpressjournals.org/doi/10.1162/jocn_a_00282},
	doi = {10.1162/jocn_a_00282},
	language = {en},
	number = {11},
	urldate = {2020-07-05},
	journal = {Journal of Cognitive Neuroscience},
	author = {Wyatte, Dean and Curran, Tim and O'Reilly, Randall},
	month = nov,
	year = {2012},
	pages = {2248--2261},
}

@article{hupe_cortical_1998,
	title = {Cortical feedback improves discrimination between figure and background by {V1}, {V2} and {V3} neurons},
	volume = {394},
	copyright = {1998 Macmillan Magazines Ltd.},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/29537/},
	doi = {10.1038/29537},
	abstract = {A single visual stimulus activates neurons in many different cortical areas. A major challenge in cortical physiology is to understand how the neural activity in these numerous active zones leads to a unified percept of the visual scene. The anatomical basis for these interactions is the dense network of connections that link the visual areas. Within this network, feedforward connections transmit signals from lower-order areas such as V1 or V2 to higher-order areas. In addition, there is a dense web of feedback connections which, despite their anatomical prominence1,2,3,4, remain functionally mysterious5,6,7,8. Here we show, using reversible inactivation of a higher-order area (monkey area V5/MT), that feedback connections serve to amplify and focus activity of neurons in lower-order areas, and that they are important in the differentiation of figure from ground, particularly in the case of stimuli of low visibility. More specifically, we show that feedback connections facilitate responses to objects moving within the classical receptive field; enhance suppression evoked by background stimuli in the surrounding region; and have the strongest effects for stimuli of low salience.},
	language = {en},
	number = {6695},
	urldate = {2020-07-05},
	journal = {Nature},
	author = {Hupé, J. M. and James, A. C. and Payne, B. R. and Lomber, S. G. and Girard, P. and Bullier, J.},
	month = aug,
	year = {1998},
	note = {Number: 6695
Publisher: Nature Publishing Group},
	pages = {784--787},
}

@article{gilbert_brain_2007,
	title = {Brain {States}: {Top}-{Down} {Influences} in {Sensory} {Processing}},
	volume = {54},
	issn = {0896-6273},
	shorttitle = {Brain {States}},
	url = {http://www.sciencedirect.com/science/article/pii/S0896627307003765},
	doi = {10.1016/j.neuron.2007.05.019},
	abstract = {All cortical and thalamic levels of sensory processing are subject to powerful top-down influences, the shaping of lower-level processes by more complex information. New findings on the diversity of top-down interactions show that cortical areas function as adaptive processors, being subject to attention, expectation, and perceptual task. Brain states are determined by the interactions between multiple cortical areas and the modulation of intrinsic circuits by feedback connections. In perceptual learning, both the encoding and recall of learned information involves a selection of the appropriate inputs that convey information about the stimulus being discriminated. Disruption of this interaction may lead to behavioral disorders, including schizophrenia.},
	language = {en},
	number = {5},
	urldate = {2020-07-05},
	journal = {Neuron},
	author = {Gilbert, Charles D. and Sigman, Mariano},
	month = jun,
	year = {2007},
	pages = {677--696},
}

@inproceedings{goodfellow_maxout_2013,
	title = {Maxout {Networks}},
	url = {http://proceedings.mlr.press/v28/goodfellow13.html},
	abstract = {We consider the problem of designing models to leverage a recently introduced approximate model averaging technique called dropout. We define a simple new model called maxout (so named because its ...},
	language = {en},
	urldate = {2020-07-05},
	booktitle = {Proceedings of the 30th {International} {Conference} on {Machine} {Learning}},
	author = {Goodfellow, Ian and Warde-Farley, David and Mirza, Mehdi and Courville, Aaron and Bengio, Yoshua},
	month = feb,
	year = {2013},
	note = {ISSN: 1938-7228
Section: Machine Learning},
	pages = {1319--1327},
}

@inproceedings{wang_saliency_2016,
	address = {Cham},
	title = {Saliency {Detection} with {Recurrent} {Fully} {Convolutional} {Networks}},
	volume = {9908},
	isbn = {978-3-319-46492-3 978-3-319-46493-0},
	url = {http://link.springer.com/10.1007/978-3-319-46493-0_50},
	doi = {10.1007/978-3-319-46493-0_50},
	abstract = {Deep networks have been proved to encode high level semantic features and delivered superior performance in saliency detection. In this paper, we go one step further by developing a new saliency model using recurrent fully convolutional networks (RFCNs). Compared with existing deep network based methods, the proposed network is able to incorporate saliency prior knowledge for more accurate inference. In addition, the recurrent architecture enables our method to automatically learn to reﬁne the saliency map by correcting its previous errors. To train such a network with numerous parameters, we propose a pre-training strategy using semantic segmentation data, which simultaneously leverages the strong supervision of segmentation tasks for better training and enables the network to capture generic representations of objects for saliency detection. Through extensive experimental evaluations, we demonstrate that the proposed method compares favorably against stateof-the-art approaches, and that the proposed recurrent deep model as well as the pre-training method can signiﬁcantly improve performance.},
	language = {en},
	urldate = {2020-07-04},
	booktitle = {Proceedings of the {European} {Conference} on {Computer} {Vision} ({ECCV})},
	publisher = {Springer International Publishing},
	author = {Wang, Linzhao and Wang, Lijun and Lu, Huchuan and Zhang, Pingping and Ruan, Xiang},
	year = {2016},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {825--841},
}

@article{eigen_understanding_2014,
	title = {Understanding {Deep} {Architectures} using a {Recursive} {Convolutional} {Network}},
	url = {http://arxiv.org/abs/1312.1847},
	abstract = {A key challenge in designing convolutional network models is sizing them appropriately. Many factors are involved in these decisions, including number of layers, feature maps, kernel sizes, etc. Complicating this further is the fact that each of these inﬂuence not only the numbers and dimensions of the activation units, but also the total number of parameters. In this paper we focus on assessing the independent contributions of three of these linked variables: The numbers of layers, feature maps, and parameters. To accomplish this, we employ a recursive convolutional network whose weights are tied between layers; this allows us to vary each of the three factors in a controlled setting. We ﬁnd that while increasing the numbers of layers and parameters each have clear beneﬁt, the number of feature maps (and hence dimensionality of the representation) appears ancillary, and ﬁnds most of its beneﬁt through the introduction of more weights. Our results (i) empirically conﬁrm the notion that adding layers alone increases computational power, within the context of convolutional layers, and (ii) suggest that precise sizing of convolutional feature map dimensions is itself of little concern; more attention should be paid to the number of parameters in these layers instead.},
	language = {en},
	urldate = {2020-07-04},
	journal = {arXiv:1312.1847 [cs]},
	author = {Eigen, David and Rolfe, Jason and Fergus, Rob and LeCun, Yann},
	month = feb,
	year = {2014},
	note = {arXiv: 1312.1847},
	keywords = {Computer Science - Machine Learning},
}

@inproceedings{zheng_conditional_2015,
	address = {Santiago, Chile},
	title = {Conditional {Random} {Fields} as {Recurrent} {Neural} {Networks}},
	isbn = {978-1-4673-8391-2},
	url = {http://ieeexplore.ieee.org/document/7410536/},
	doi = {10.1109/ICCV.2015.179},
	abstract = {Pixel-level labelling tasks, such as semantic segmentation, play a central role in image understanding. Recent approaches have attempted to harness the capabilities of deep learning techniques for image recognition to tackle pixellevel labelling tasks. One central issue in this methodology is the limited capacity of deep learning techniques to delineate visual objects. To solve this problem, we introduce a new form of convolutional neural network that combines the strengths of Convolutional Neural Networks (CNNs) and Conditional Random Fields (CRFs)-based probabilistic graphical modelling. To this end, we formulate Conditional Random Fields with Gaussian pairwise potentials and mean-ﬁeld approximate inference as Recurrent Neural Networks. This network, called CRF-RNN, is then plugged in as a part of a CNN to obtain a deep network that has desirable properties of both CNNs and CRFs. Importantly, our system fully integrates CRF modelling with CNNs, making it possible to train the whole deep network end-to-end with the usual back-propagation algorithm, avoiding ofﬂine post-processing methods for object delineation.},
	language = {en},
	urldate = {2020-07-04},
	booktitle = {2015 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Zheng, Shuai and Jayasumana, Sadeep and Romera-Paredes, Bernardino and Vineet, Vibhav and Su, Zhizhong and Du, Dalong and Huang, Chang and Torr, Philip H. S.},
	month = dec,
	year = {2015},
	pages = {1529--1537},
}

@inproceedings{chatfield_return_2014,
	address = {Nottingham},
	title = {Return of the {Devil} in the {Details}: {Delving} {Deep} into {Convolutional} {Nets}},
	isbn = {978-1-901725-52-0},
	shorttitle = {Return of the {Devil} in the {Details}},
	url = {http://www.bmva.org/bmvc/2014/papers/paper054/index.html},
	doi = {10.5244/C.28.6},
	abstract = {The latest generation of Convolutional Neural Networks (CNN) have achieved impressive results in challenging benchmarks on image recognition and object detection, signiﬁcantly raising the interest of the community in these methods. Nevertheless, it is still unclear how different CNN methods compare with each other and with previous state-of-the-art shallow representations such as the Bag-of-Visual-Words and the Improved Fisher Vector. This paper conducts a rigorous evaluation of these new techniques, exploring different deep architectures and comparing them on a common ground, identifying and disclosing important implementation details. We identify several useful properties of CNN-based representations, including the fact that the dimensionality of the CNN output layer can be reduced signiﬁcantly without having an adverse effect on performance. We also identify aspects of deep and shallow methods that can be successfully shared. In particular, we show that the data augmentation techniques commonly applied to CNN-based methods can also be applied to shallow methods, and result in an analogous performance boost. Source code and models to reproduce the experiments in the paper is made publicly available.},
	language = {en},
	urldate = {2020-07-04},
	booktitle = {Proceedings of the {British} {Machine} {Vision} {Conference} 2014},
	publisher = {British Machine Vision Association},
	author = {Chatfield, Ken and Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
	year = {2014},
	pages = {6.1--6.12},
}

@inproceedings{ballas_delving_2016,
	title = {Delving {Deeper} into {Convolutional} {Networks} for {Learning} {Video} {Representations}},
	url = {http://arxiv.org/abs/1511.06432},
	abstract = {We propose an approach to learn spatio-temporal features in videos from intermediate visual representations we call “percepts” using Gated-Recurrent-Unit Recurrent Networks (GRUs). Our method relies on percepts that are extracted from all levels of a deep convolutional network trained on the large ImageNet dataset. While high-level percepts contain highly discriminative information, they tend to have a low-spatial resolution. Low-level percepts, on the other hand, preserve a higher spatial resolution from which we can model ﬁner motion patterns.},
	language = {en},
	urldate = {2020-07-04},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Ballas, Nicolas and Yao, Li and Pal, Chris and Courville, Aaron},
	year = {2016},
	note = {arXiv: 1511.06432},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@article{alom_inception_2017,
	title = {Inception {Recurrent} {Convolutional} {Neural} {Network} for {Object} {Recognition}},
	url = {http://arxiv.org/abs/1704.07709},
	abstract = {Deep convolutional neural networks (DCNNs) are an inﬂuential tool for solving various problems in the machine learning and computer vision ﬁelds. In this paper, we introduce a new deep learning model called an InceptionRecurrent Convolutional Neural Network (IRCNN), which utilizes the power of an inception network combined with recurrent layers in DCNN architecture. We have empirically evaluated the recognition performance of the proposed IRCNN model using different benchmark datasets such as MNIST, CIFAR-10, CIFAR100, and SVHN. Experimental results show similar or higher recognition accuracy when compared to most of the popular DCNNs including the RCNN. Furthermore, we have investigated IRCNN performance against equivalent Inception Networks and Inception-Residual Networks using the CIFAR-100 dataset. We report about 3.5\%, 3.47\% and 2.54\% improvement in classiﬁcation accuracy when compared to the RCNN, equivalent Inception Networks, and InceptionResidual Networks on the augmented CIFAR100 dataset respectively.},
	language = {en},
	urldate = {2020-07-04},
	journal = {arXiv:1704.07709 [cs]},
	author = {Alom, Md Zahangir and Hasan, Mahmudul and Yakopcic, Chris and Taha, Tarek M.},
	month = apr,
	year = {2017},
	note = {arXiv: 1704.07709},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{cukur_attention_2013,
	title = {Attention during natural vision warps semantic representation across the human brain},
	volume = {16},
	copyright = {2013 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1546-1726},
	url = {https://www.nature.com/articles/nn.3381},
	doi = {10.1038/nn.3381},
	abstract = {The authors use functional magnetic resonance imaging to measure how the semantic representation changes when searching for different object categories in natural movies. They find tuning shifts that expand the representation of the attended category and of semantically related, but unattended, categories, and compress the representation of categories semantically dissimilar to the target.},
	language = {en},
	number = {6},
	urldate = {2020-07-04},
	journal = {Nature Neuroscience},
	author = {Çukur, Tolga and Nishimoto, Shinji and Huth, Alexander G. and Gallant, Jack L.},
	month = jun,
	year = {2013},
	note = {Number: 6
Publisher: Nature Publishing Group},
	pages = {763--770},
}

@article{rothenstein_attention_2008,
	series = {Cognitive {Vision}-{Special} {Issue}},
	title = {Attention links sensing to recognition},
	volume = {26},
	issn = {0262-8856},
	url = {http://www.sciencedirect.com/science/article/pii/S0262885606000680},
	doi = {10.1016/j.imavis.2005.08.011},
	abstract = {This paper presents arguments that explicit strategies for visual attentional selection are important for cognitive vision systems, and shows that a number of proposals currently exist for exactly how parts of this goal may be accomplished. A comprehensive survey of approaches to computational attention is given. A key characteristic of virtually all the models surveyed here is that they receive significant inspiration from the neurobiology and psychophysics of human and primate vision. This, although not necessarily a key component of mainstream computer vision, seems very appropriate for cognitive vision systems given a definition of the topic that always includes the goal of human-like visual performance. A particular model, the Selective Tuning model, is overviewed in some detail. The growing neurobiological and psychophysical evidence for its biological plausibility is cited highlighting the fact that it has more biological support than other models; it is further claimed that it may form an appropriate starting point for the difficult task of integrating attention into cognitive vision systems.},
	language = {en},
	number = {1},
	urldate = {2020-07-04},
	journal = {Image and Vision Computing},
	author = {Rothenstein, Albert L. and Tsotsos, John K.},
	month = jan,
	year = {2008},
	keywords = {Attention, Cognitive vision, Recognition, Selective tuning},
	pages = {114--126},
}

@article{baluch_mechanisms_2011,
	title = {Mechanisms of top-down attention},
	volume = {34},
	issn = {0166-2236},
	url = {http://www.sciencedirect.com/science/article/pii/S0166223611000191},
	doi = {10.1016/j.tins.2011.02.003},
	abstract = {Attention exhibits characteristic neural signatures in brain regions that process sensory signals. An important area of future research is to understand the nature of top-down signals that facilitate attentional guidance towards behaviorally relevant locations and features. In this review, we discuss recent studies that have made progress towards understanding: (i) the brain structures and circuits involved in attentional allocation; (ii) top-down attention pathways, particularly as elucidated by microstimulation and lesion studies; (iii) top-down modulatory influences involving subcortical structures and reward systems; (iv) plausible substrates and embodiments of top-down signals; and (v) information processing and theoretical constraints that might be helpful in guiding future experiments. Understanding top-down attention is crucial for elucidating the mechanisms by which we can filter sensory information to pay attention to the most behaviorally relevant events.},
	language = {en},
	number = {4},
	urldate = {2020-07-04},
	journal = {Trends in Neurosciences},
	author = {Baluch, Farhan and Itti, Laurent},
	month = apr,
	year = {2011},
	pages = {210--224},
}

@article{thorpe_speed_1996,
	title = {Speed of processing in the human visual system},
	volume = {381},
	copyright = {1996 Nature Publishing Group},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/381520a0},
	doi = {10.1038/381520a0},
	abstract = {How long does it take for the human visual system to process a complex natural image? Subjectively, recognition of familiar objects and scenes appears to be virtually instantaneous, but measuring this processing time experimentally has proved difficult. Behavioural measures such as reaction times can be used1, but these include not only visual processing but also the time required for response execution. However, event-related potentials (ERPs) can sometimes reveal signs of neural processing well before the motor output2. Here we use a go/no-go categorization task in which subjects have to decide whether a previously unseen photograph, flashed on for just 20 ms, contains an animal. ERP analysis revealed a frontal negativity specific to no-go trials that develops roughly 150 ms after stimulus onset. We conclude that the visual processing needed to perform this highly demanding task can be achieved in under 150 ms.},
	language = {en},
	number = {6582},
	urldate = {2020-07-04},
	journal = {Nature},
	author = {Thorpe, Simon and Fize, Denis and Marlot, Catherine},
	month = jun,
	year = {1996},
	note = {Number: 6582
Publisher: Nature Publishing Group},
	pages = {520--522},
}

@inproceedings{tianjun_xiao_application_2015,
	address = {Boston, MA, USA},
	title = {The application of two-level attention models in deep convolutional neural network for fine-grained image classification},
	isbn = {978-1-4673-6964-0},
	url = {http://ieeexplore.ieee.org/document/7298685/},
	doi = {10.1109/CVPR.2015.7298685},
	abstract = {Fine-grained classiﬁcation is challenging because categories can only be discriminated by subtle and local differences. Variances in the pose, scale or rotation usually make the problem more difﬁcult. Most ﬁne-grained classiﬁcation systems follow the pipeline of ﬁnding foreground object or object parts (where) to extract discriminative features (what).},
	language = {en},
	urldate = {2020-07-02},
	booktitle = {2015 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {{Tianjun Xiao} and {Yichong Xu} and {Kuiyuan Yang} and {Jiaxing Zhang} and {Yuxin Peng} and Zhang, Zheng},
	month = jun,
	year = {2015},
	pages = {842--850},
}

@inproceedings{wang_attentional_2014,
	title = {Attentional {Neural} {Network}: {Feature} {Selection} {Using} {Cognitive} {Feedback}},
	abstract = {Attentional Neural Network is a new framework that integrates top-down cognitive bias and bottom-up feature extraction in one coherent architecture. The top-down inﬂuence is especially effective when dealing with high noise or difﬁcult segmentation problems. Our system is modular and extensible. It is also easy to train and cheap to run, and yet can accommodate complex behaviors. We obtain classiﬁcation accuracy better than or competitive with state of art results on the MNIST variation dataset, and successfully disentangle overlaid digits with high success rates. We view such a general purpose framework as an essential foundation for a larger system emulating the cognitive abilities of the whole brain.},
	language = {en},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Wang, Qian and Zhang, Jiaxing and Song, Sen and Zhang, Zheng},
	year = {2014},
	pages = {9},
}

@inproceedings{stollenga_deep_2014,
	title = {Deep {Networks} with {Internal} {Selective} {Attention} through {Feedback} {Connections}},
	abstract = {Traditional convolutional neural networks (CNN) are stationary and feedforward. They neither change their parameters during evaluation nor use feedback from higher to lower layers. Real brains, however, do. So does our Deep Attention Selective Network (dasNet) architecture. DasNet’s feedback structure can dynamically alter its convolutional ﬁlter sensitivities during classiﬁcation. It harnesses the power of sequential processing to improve classiﬁcation performance, by allowing the network to iteratively focus its internal attention on some of its convolutional ﬁlters. Feedback is trained through direct policy search in a huge million-dimensional parameter space, through scalable natural evolution strategies (SNES). On the CIFAR-10 and CIFAR-100 datasets, dasNet outperforms the previous state-of-the-art model on unaugmented datasets.},
	language = {en},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Stollenga, Marijn F and Masci, Jonathan and Gomez, Faustino and Schmidhuber, Jürgen},
	year = {2014},
	pages = {9},
}

@inproceedings{salakhutdinov_deep_2009,
	title = {Deep {Boltzmann} {Machines}},
	url = {http://proceedings.mlr.press/v5/salakhutdinov09a.html},
	abstract = {We present a new learning algorithm for Boltzmann machines that contain many layers of hidden variables. Data-dependent expectations are estimated using a variational approximation that tends to fo...},
	language = {en},
	urldate = {2020-07-04},
	booktitle = {Proceedings of the {Twelth} {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	author = {Salakhutdinov, Ruslan and Hinton, Geoffrey},
	month = apr,
	year = {2009},
	note = {ISSN: 1938-7228
Section: Machine Learning},
	pages = {448--455},
}

@article{dhingra_differentiable_2020,
	title = {Differentiable {Reasoning} {Over} {A} {Virtual} {Knowledge} {Base}},
	language = {en},
	author = {Dhingra, Bhuwan and Zaheer, Manzil and Balachandran, Vidhisha and Neubig, Graham and Salakhutdinov, Ruslan and Cohen, William W},
	year = {2020},
	pages = {16},
}

@article{lee_hierarchical_2003,
	title = {Hierarchical {Bayesian} inference in the visual cortex},
	volume = {20},
	issn = {1084-7529, 1520-8532},
	url = {https://www.osapublishing.org/abstract.cfm?URI=josaa-20-7-1434},
	doi = {10.1364/JOSAA.20.001434},
	language = {en},
	number = {7},
	urldate = {2020-07-04},
	journal = {Journal of the Optical Society of America A},
	author = {Lee, Tai Sing and Mumford, David},
	month = jul,
	year = {2003},
	pages = {1434},
}

@article{cichy_resolving_2014,
	title = {Resolving human object recognition in space and time},
	volume = {17},
	issn = {1097-6256, 1546-1726},
	url = {http://www.nature.com/articles/nn.3635},
	doi = {10.1038/nn.3635},
	language = {en},
	number = {3},
	urldate = {2020-07-04},
	journal = {Nature Neuroscience},
	author = {Cichy, Radoslaw Martin and Pantazis, Dimitrios and Oliva, Aude},
	month = mar,
	year = {2014},
	pages = {455--462},
}

@article{felzenszwalb_object_2010,
	title = {Object {Detection} with {Discriminatively} {Trained} {Part}-{Based} {Models}},
	volume = {32},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2009.167},
	abstract = {We describe an object detection system based on mixtures of multiscale deformable part models. Our system is able to represent highly variable object classes and achieves state-of-the-art results in the PASCAL object detection challenges. While deformable part models have become quite popular, their value had not been demonstrated on difficult benchmarks such as the PASCAL data sets. Our system relies on new methods for discriminative training with partially labeled data. We combine a margin-sensitive approach for data-mining hard negative examples with a formalism we call latent SVM. A latent SVM is a reformulation of MI–SVM in terms of latent variables. A latent SVM is semiconvex, and the training problem becomes convex once latent information is specified for the positive examples. This leads to an iterative training algorithm that alternates between fixing latent values for positive examples and optimizing the latent SVM objective function.},
	number = {9},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Felzenszwalb, Pedro F. and Girshick, Ross B. and McAllester, David and Ramanan, Deva},
	month = sep,
	year = {2010},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Algorithms, Artificial Intelligence, Bicycles, Computer Society, Computer vision, Deformable models, Discriminant Analysis, Image Enhancement, Image Interpretation, Computer-Assisted, Imaging, Three-Dimensional, Iterative algorithms, Lighting, Object detection, Object recognition, PASCAL object detection, Pattern Recognition, Automated, Reproducibility of Results, Sensitivity and Specificity, Shape, Speech recognition, Support vector machines, data mining, deformable models, discriminative trained part-based models, discriminative training, iterative methods, iterative training algorithm, latent SVM objective function, latent SVM., margin-sensitive approach, multiscale deformable part models, object detection, object detection system, object recognition, pictorial structures, support vector machine, support vector machines},
	pages = {1627--1645},
}

@inproceedings{szegedy_going_2015,
	address = {Boston, MA, USA},
	title = {Going deeper with convolutions},
	isbn = {978-1-4673-6964-0},
	url = {http://ieeexplore.ieee.org/document/7298594/},
	doi = {10.1109/CVPR.2015.7298594},
	abstract = {We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classiﬁcation and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classiﬁcation and detection.},
	language = {en},
	urldate = {2020-07-04},
	booktitle = {2015 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Szegedy, Christian and {Wei Liu} and {Yangqing Jia} and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
	month = jun,
	year = {2015},
	pages = {1--9},
}

@inproceedings{he_delving_2015,
	address = {Santiago, Chile},
	title = {Delving {Deep} into {Rectifiers}: {Surpassing} {Human}-{Level} {Performance} on {ImageNet} {Classification}},
	isbn = {978-1-4673-8391-2},
	shorttitle = {Delving {Deep} into {Rectifiers}},
	url = {http://ieeexplore.ieee.org/document/7410480/},
	doi = {10.1109/ICCV.2015.123},
	abstract = {Rectiﬁed activation units (rectiﬁers) are essential for state-of-the-art neural networks. In this work, we study rectiﬁer neural networks for image classiﬁcation from two aspects. First, we propose a Parametric Rectiﬁed Linear Unit (PReLU) that generalizes the traditional rectiﬁed unit. PReLU improves model ﬁtting with nearly zero extra computational cost and little overﬁtting risk. Second, we derive a robust initialization method that particularly considers the rectiﬁer nonlinearities. This method enables us to train extremely deep rectiﬁed models directly from scratch and to investigate deeper or wider network architectures. Based on the learnable activation and advanced initialization, we achieve 4.94\% top-5 test error on the ImageNet 2012 classiﬁcation dataset. This is a 26\% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66\% [33]). To our knowledge, our result is the ﬁrst1 to surpass the reported human-level performance (5.1\%, [26]) on this dataset.},
	language = {en},
	urldate = {2020-07-04},
	booktitle = {2015 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = dec,
	year = {2015},
	pages = {1026--1034},
}

@article{kruger_deep_2013,
	title = {Deep {Hierarchies} in the {Primate} {Visual} {Cortex}: {What} {Can} {We} {Learn} for {Computer} {Vision}?},
	volume = {35},
	issn = {1939-3539},
	shorttitle = {Deep {Hierarchies} in the {Primate} {Visual} {Cortex}},
	doi = {10.1109/TPAMI.2012.272},
	abstract = {Computational modeling of the primate visual system yields insights of potential relevance to some of the challenges that computer vision is facing, such as object recognition and categorization, motion detection and activity recognition, or vision-based navigation and manipulation. This paper reviews some functional principles and structures that are generally thought to underlie the primate visual cortex, and attempts to extract biological principles that could further advance computer vision research. Organized for a computer vision audience, we present functional principles of the processing hierarchies present in the primate visual system considering recent discoveries in neurophysiology. The hierarchical processing in the primate visual system is characterized by a sequence of different levels of processing (on the order of 10) that constitute a deep hierarchy in contrast to the flat vision architectures predominantly used in today's mainstream computer vision. We hope that the functional description of the deep hierarchies realized in the primate visual system provides valuable insights for the design of computer vision algorithms, fostering increasingly productive interaction between biological and computer vision research.},
	number = {8},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Kruger, Norbert and Janssen, Peter and Kalkan, Sinan and Lappe, Markus and Leonardis, Ales and Piater, Justus and Rodriguez-Sanchez, Antonio J. and Wiskott, Laurenz},
	month = aug,
	year = {2013},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Animals, Artificial Intelligence, Computer vision, Humans, Neurons, Organizations, Pattern Recognition, Visual, Primates, Retina, Vision, Ocular, Visual Cortex, Visual systems, Visualization, activity recognition, advance computer vision research, biological modeling, biological principle extraction, biology computing, computational modeling, computer vision, computer vision algorithms, computer vision audience, deep hierarchies, flat vision architectures, functional principles, hierarchical processing, motion detection, neurophysiology, object categorization, object recognition, primate visual cortex, primate visual system, vision-based manipulation, vision-based navigation},
	pages = {1847--1871},
}

@article{beck_top-down_2009,
	series = {Visual {Attention}: {Psychophysics}, electrophysiology and neuroimaging},
	title = {Top-down and bottom-up mechanisms in biasing competition in the human brain},
	volume = {49},
	issn = {0042-6989},
	url = {http://www.sciencedirect.com/science/article/pii/S0042698908003635},
	doi = {10.1016/j.visres.2008.07.012},
	abstract = {The biased competition theory of selective attention has been an influential neural theory of attention, motivating numerous animal and human studies of visual attention and visual representation. There is now neural evidence in favor of all three of its most basic principles: that representation in the visual system is competitive; that both top-down and bottom-up biasing mechanisms influence the ongoing competition; and that competition is integrated across brain systems. We review the evidence in favor of these three principles, and in particular, findings related to six more specific neural predictions derived from these original principles.},
	language = {en},
	number = {10},
	urldate = {2020-07-04},
	journal = {Vision Research},
	author = {Beck, Diane M. and Kastner, Sabine},
	month = jun,
	year = {2009},
	keywords = {Bias, Brain, Suppression, Visual attention, fMRI},
	pages = {1154--1165},
}

@inproceedings{jetley_learn_2018,
	title = {Learn to {Pay} {Attention}},
	abstract = {We propose an end-to-end-trainable attention module for convolutional neural network (CNN) architectures built for image classiﬁcation. The module takes as input the 2D feature vector maps which form the intermediate representations of the input image at different stages in the CNN pipeline, and outputs a 2D matrix of scores for each map. Standard CNN architectures are modiﬁed through the incorporation of this module, and trained under the constraint that a convex combination of the intermediate 2D feature vectors, as parameterised by the score matrices, must alone be used for classiﬁcation. Incentivised to amplify the relevant and suppress the irrelevant or misleading, the scores thus assume the role of attention values. Our experimental observations provide clear evidence to this effect: the learned attention maps neatly highlight the regions of interest while suppressing background clutter. Consequently, the proposed function is able to bootstrap standard CNN architectures for the task of image classiﬁcation, demonstrating superior generalisation over 6 unseen benchmark datasets. When binarised, our attention maps outperform other CNN-based attention maps, traditional saliency maps, and top object proposals for weakly supervised segmentation as demonstrated on the Object Discovery dataset. We also demonstrate improved robustness against the fast gradient sign method of adversarial attack.},
	language = {en},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Jetley, Saumya and Lord, Nicholas A and Lee, Namhoon and Torr, Philip H S},
	year = {2018},
	pages = {14},
}

@inproceedings{cao_look_2015,
	address = {Santiago, Chile},
	title = {Look and {Think} {Twice}: {Capturing} {Top}-{Down} {Visual} {Attention} with {Feedback} {Convolutional} {Neural} {Networks}},
	isbn = {978-1-4673-8391-2},
	shorttitle = {Look and {Think} {Twice}},
	url = {http://ieeexplore.ieee.org/document/7410695/},
	doi = {10.1109/ICCV.2015.338},
	abstract = {While feedforward deep convolutional neural networks (CNNs) have been a great success in computer vision, it is important to note that the human visual cortex generally contains more feedback than feedforward connections. In this paper, we will brieﬂy introduce the background of feedbacks in the human visual cortex, which motivates us to develop a computational feedback mechanism in deep neural networks. In addition to the feedforward inference in traditional neural networks, a feedback loop is introduced to infer the activation status of hidden layer neurons according to the “goal” of the network, e.g., high-level semantic labels. We analogize this mechanism as “Look and Think Twice.” The feedback networks help better visualize and understand how deep neural networks work, and capture visual attention on expected objects, even in images with cluttered background and multiple objects. Experiments on ImageNet dataset demonstrate its effectiveness in solving tasks such as image classiﬁcation and object localization.},
	language = {en},
	urldate = {2020-07-04},
	booktitle = {2015 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Cao, Chunshui and Liu, Xianming and Yang, Yi and Yu, Yinan and Wang, Jiang and Wang, Zilei and Huang, Yongzhen and Wang, Liang and Huang, Chang and Xu, Wei and Ramanan, Deva and Huang, Thomas S.},
	month = dec,
	year = {2015},
	pages = {2956--2964},
}

@article{bengio_learning_1994,
	title = {Learning long-term dependencies with gradient descent is difficult},
	volume = {5},
	issn = {1941-0093},
	doi = {10.1109/72.279181},
	abstract = {Recurrent neural networks can be used to map input sequences to output sequences, such as for recognition, production or prediction problems. However, practical difficulties have been reported in training recurrent neural networks to perform tasks in which the temporal contingencies present in the input/output sequences span long intervals. We show why gradient based learning algorithms face an increasingly difficult problem as the duration of the dependencies to be captured increases. These results expose a trade-off between efficient learning by gradient descent and latching on information for long periods. Based on an understanding of this problem, alternatives to standard gradient descent are considered.{\textless}{\textgreater}},
	number = {2},
	journal = {IEEE Transactions on Neural Networks},
	author = {Bengio, Y. and Simard, P. and Frasconi, P.},
	month = mar,
	year = {1994},
	note = {Conference Name: IEEE Transactions on Neural Networks},
	keywords = {Computer networks, Cost function, Delay effects, Discrete transforms, Displays, Intelligent networks, Neural networks, Neurofeedback, Production, Recurrent neural networks, efficient learning, gradient descent, input/output sequence mapping, learning (artificial intelligence), long-term dependencies, numerical analysis, prediction problems, production problems, recognition, recurrent neural nets, recurrent neural network training, temporal contingencies},
	pages = {157--166},
}

@inproceedings{moosavi-dezfooli_universal_2017,
	title = {Universal {Adversarial} {Perturbations}},
	url = {https://openaccess.thecvf.com/content_cvpr_2017/html/Moosavi-Dezfooli_Universal_Adversarial_Perturbations_CVPR_2017_paper.html},
	urldate = {2020-07-03},
	author = {Moosavi-Dezfooli, Seyed-Mohsen and Fawzi, Alhussein and Fawzi, Omar and Frossard, Pascal},
	year = {2017},
	pages = {1765--1773},
}

@inproceedings{carlini_towards_2017,
	title = {Towards {Evaluating} the {Robustness} of {Neural} {Networks}},
	doi = {10.1109/SP.2017.49},
	abstract = {Neural networks provide state-of-the-art results for most machine learning tasks. Unfortunately, neural networks are vulnerable to adversarial examples: given an input x and any target classification t, it is possible to find a new input x' that is similar to x but classified as t. This makes it difficult to apply neural networks in security-critical areas. Defensive distillation is a recently proposed approach that can take an arbitrary neural network, and increase its robustness, reducing the success rate of current attacks' ability to find adversarial examples from 95\% to 0.5\%. In this paper, we demonstrate that defensive distillation does not significantly increase the robustness of neural networks by introducing three new attack algorithms that are successful on both distilled and undistilled neural networks with 100\% probability. Our attacks are tailored to three distance metrics used previously in the literature, and when compared to previous adversarial example generation algorithms, our attacks are often much more effective (and never worse). Furthermore, we propose using high-confidence adversarial examples in a simple transferability test we show can also be used to break defensive distillation. We hope our attacks will be used as a benchmark in future defense attempts to create neural networks that resist adversarial examples.},
	booktitle = {2017 {IEEE} {Symposium} on {Security} and {Privacy} ({SP})},
	author = {Carlini, Nicholas and Wagner, David},
	month = may,
	year = {2017},
	note = {ISSN: 2375-1207},
	keywords = {Malware, Measurement, Neural networks, Resists, Robustness, Security, Speech recognition, attack algorithms, defensive distillation, distance metrics, high-confidence adversarial examples, machine learning, neural nets, neural networks, security of data, transferability test},
	pages = {39--57},
}

@inproceedings{papernot_limitations_2016,
	title = {The {Limitations} of {Deep} {Learning} in {Adversarial} {Settings}},
	doi = {10.1109/EuroSP.2016.36},
	abstract = {Deep learning takes advantage of large datasets and computationally efficient training algorithms to outperform other approaches at various machine learning tasks. However, imperfections in the training phase of deep neural networks make them vulnerable to adversarial samples: inputs crafted by adversaries with the intent of causing deep neural networks to misclassify. In this work, we formalize the space of adversaries against deep neural networks (DNNs) and introduce a novel class of algorithms to craft adversarial samples based on a precise understanding of the mapping between inputs and outputs of DNNs. In an application to computer vision, we show that our algorithms can reliably produce samples correctly classified by human subjects but misclassified in specific targets by a DNN with a 97\% adversarial success rate while only modifying on average 4.02\% of the input features per sample. We then evaluate the vulnerability of different sample classes to adversarial perturbations by defining a hardness measure. Finally, we describe preliminary work outlining defenses against adversarial samples by defining a predictive measure of distance between a benign input and a target classification.},
	booktitle = {2016 {IEEE} {European} {Symposium} on {Security} and {Privacy} ({EuroS} {P})},
	author = {Papernot, Nicolas and McDaniel, Patrick and Jha, Somesh and Fredrikson, Matt and Celik, Z. Berkay and Swami, Ananthram},
	month = mar,
	year = {2016},
	keywords = {Biological neural networks, DNN, Distortion, Force, Machine learning, Neurons, Training, adversarial samples, adversarial samples:, computer vision, deep neural networks, hardness measure, human subjects, image classification, input features, large datasets, learning (artificial intelligence), neural nets, target classification, training algorithms, training phase},
	pages = {372--387},
}

@inproceedings{brown_adversarial_2017,
	title = {Adversarial {Patch}},
	url = {http://arxiv.org/abs/1712.09665},
	abstract = {We present a method to create universal, robust, targeted adversarial image patches in the real world. The patches are universal because they can be used to attack any scene, robust because they work under a wide variety of transformations, and targeted because they can cause a classiﬁer to output any target class. These adversarial patches can be printed, added to any scene, photographed, and presented to image classiﬁers; even when the patches are small, they cause the classiﬁers to ignore the other items in the scene and report a chosen target class.},
	language = {en},
	urldate = {2020-07-03},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} {Workshop}},
	author = {Brown, Tom B. and Mané, Dandelion and Roy, Aurko and Abadi, Martín and Gilmer, Justin},
	year = {2017},
	note = {arXiv: 1712.09665},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{li_contour_2006,
	title = {Contour {Saliency} in {Primary} {Visual} {Cortex}},
	volume = {50},
	issn = {0896-6273},
	url = {http://www.sciencedirect.com/science/article/pii/S0896627306004156},
	doi = {10.1016/j.neuron.2006.04.035},
	abstract = {Contour integration is an important intermediate stage of object recognition, in which line segments belonging to an object boundary are perceptually linked and segmented from complex backgrounds. Contextual influences observed in primary visual cortex (V1) suggest the involvement of V1 in contour integration. Here, we provide direct evidence that, in monkeys performing a contour detection task, there was a close correlation between the responses of V1 neurons and the perceptual saliency of contours. Receiver operating characteristic analysis showed that single neuronal responses encode the presence or absence of a contour as reliably as the animal's behavioral responses. We also show that the same visual contours elicited significantly weaker neuronal responses when they were not detected in the detection task, or when they were unattended. Our results demonstrate that contextual interactions in V1 play a pivotal role in contour integration and saliency.},
	language = {en},
	number = {6},
	urldate = {2020-07-02},
	journal = {Neuron},
	author = {Li, Wu and Piëch, Valentin and Gilbert, Charles D.},
	month = jun,
	year = {2006},
	keywords = {SYSNEURO},
	pages = {951--962},
}

@incollection{angelucci_contribution_2006,
	series = {Visual {Perception}},
	title = {Contribution of feedforward, lateral and feedback connections to the classical receptive field center and extra-classical receptive field surround of primate {V1} neurons},
	volume = {154},
	url = {http://www.sciencedirect.com/science/article/pii/S0079612306540051},
	abstract = {A central question in visual neuroscience is what circuits generate the responses of neurons in the primary visual cortex (V1). V1 neurons respond best to oriented stimuli of optimal size within their receptive field (RF) center. This size tuning is contrast dependent, i.e. a neuron's optimal stimulus size measured at high contrast (the high-contrast summation RF, or hsRF) is smaller than when measured using low-contrast stimuli (the low-contrast summation RF, or lsRF). Responses to stimuli in the RF center are usually suppressed by iso-oriented stimuli in the extra-classical RF surround. Iso-orientation surround suppression is fast and long range, extending well beyond the size of V1 cells’ lsRF. Geniculocortical feedforward (FF), V1 lateral and extrastriate feedback (FB) connections to V1 could all contribute to generating the RF center and surround of V1 neurons. Studies on the spatio-temporal properties and functional organization of these connections can help disclose their specific contributions to the responses of V1 cells. These studies, reviewed in this chapter, have shown that FF afferents to V1 integrate signals within the hsRF of V1 cells; V1 lateral connections are commensurate with the size of the lsRF and may, thus, underlie contrast-dependent changes in spatial summation, and modulatory effects arising from the surround region closer to the RF center (the “near” surround). The spatial and temporal properties of lateral connections cannot account for the dimensions and onset latency of modulation arising from more distant regions of the surround (the “far” surround). Inter-areal FB connections to V1, instead, are commensurate with the full spatial range of center and surround responses, and show fast conduction velocity consistent with the short onset latency of modulation arising from the “far” surround. We review data showing that a subset of FB connections terminate in a patchy fashion in V1, and show modular and orientation specificity, consistent with their proposed role in orientation-specific center–surround interactions. We propose specific mechanisms by which each connection type contributes to the RF center and surround of V1 neurons, and implement these hypotheses into a recurrent network model. We show physiological data in support of the model's predictions, revealing that modulation from the “far” surround is not always suppressive, but can be facilitatory under specific stimulus conditions.},
	language = {en},
	urldate = {2020-07-02},
	booktitle = {Progress in {Brain} {Research}},
	publisher = {Elsevier},
	author = {Angelucci, Alessandra and Bressloff, Paul C.},
	editor = {Martinez-Conde, S. and Macknik, S. L. and Martinez, L. M. and Alonso, J. -M. and Tse, P. U.},
	month = jan,
	year = {2006},
	doi = {10.1016/S0079-6123(06)54005-1},
	keywords = {extrastriate, geniculocortical, horizontal connections, macaque., striate cortex, surround modulation},
	pages = {93--120},
}

@inproceedings{wang_gated_2017,
	title = {Gated {Recurrent} {Convolution} {Neural} {Network} for {OCR}},
	abstract = {Optical Character Recognition (OCR) aims to recognize text in natural images. Inspired by a recently proposed model for general image classiﬁcation, Recurrent Convolution Neural Network (RCNN), we propose a new architecture named Gated RCNN (GRCNN) for solving this problem. Its critical component, Gated Recurrent Convolution Layer (GRCL), is constructed by adding a gate to the Recurrent Convolution Layer (RCL), the critical component of RCNN. The gate controls the context modulation in RCL and balances the feed-forward information and the recurrent information. In addition, an efﬁcient Bidirectional Long ShortTerm Memory (BLSTM) is built for sequence modeling. The GRCNN is combined with BLSTM to recognize text in natural images. The entire GRCNN-BLSTM model can be trained end-to-end. Experiments show that the proposed model outperforms existing methods on several benchmark datasets including the IIIT-5K, Street View Text (SVT) and ICDAR.},
	language = {en},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Wang, Jianfeng and Hu, Xiaolin},
	year = {2017},
	pages = {10},
}

@inproceedings{lin_feature_2017,
	title = {Feature {Pyramid} {Networks} for {Object} {Detection}},
	url = {https://openaccess.thecvf.com/content_cvpr_2017/html/Lin_Feature_Pyramid_Networks_CVPR_2017_paper.html},
	urldate = {2020-06-29},
	author = {Lin, Tsung-Yi and Dollar, Piotr and Girshick, Ross and He, Kaiming and Hariharan, Bharath and Belongie, Serge},
	year = {2017},
	pages = {2117--2125},
}

@article{liu_deep_2018,
	title = {A {Deep} {Spatial} {Contextual} {Long}-{Term} {Recurrent} {Convolutional} {Network} for {Saliency} {Detection}},
	volume = {27},
	issn = {1941-0042},
	doi = {10.1109/TIP.2018.2817047},
	abstract = {Traditional saliency models usually adopt hand-crafted image features and human-designed mechanisms to calculate local or global contrast. In this paper, we propose a novel computational saliency model, i.e., deep spatial contextual long-term recurrent convolutional network (DSCLRCN), to predict where people look in natural scenes. DSCLRCN first automatically learns saliency related local features on each image location in parallel. Then, in contrast with most other deep network based saliency models which infer saliency in local contexts, DSCLRCN can mimic the cortical lateral inhibition mechanisms in human visual system to incorporate global contexts to assess the saliency of each image location by leveraging the deep spatial long short-term memory (DSLSTM) model. Moreover, we also integrate scene context modulation in DSLSTM for saliency inference, leading to a novel deep spatial contextual LSTM (DSCLSTM) model. The whole network can be trained end-to-end and works efficiently when testing. Experimental results on two benchmark datasets show that DSCLRCN can achieve state-of-the-art performance on saliency detection. Furthermore, the proposed DSCLSTM model can significantly boost the saliency detection performance by incorporating both global spatial interconnections and scene context modulation, which may uncover novel inspirations for studies on them in computational saliency models.},
	number = {7},
	journal = {IEEE Transactions on Image Processing},
	author = {Liu, Nian and Han, Junwei},
	month = jul,
	year = {2018},
	note = {Conference Name: IEEE Transactions on Image Processing},
	keywords = {Computational modeling, Context modeling, DSCLRCN, DSCLSTM model, Feature extraction, Modulation, Saliency detection, Task analysis, Visualization, computational saliency model, convolutional neural networks, cortical lateral inhibition mechanisms, deep spatial contextual LSTM, deep spatial contextual LSTM model, deep spatial contextual long-term recurrent convolutional network, deep spatial long short-term memory, eye fixation prediction, feature extraction, global context, global spatial interconnections, hand-crafted image features, human visual system, human-designed mechanisms, image location, learning (artificial intelligence), long short-term memory, natural scenes, recurrent neural nets, saliency detection performance, saliency inference, saliency related local features, scene context, scene context modulation, short-term memory model, traditional saliency models},
	pages = {3264--3274},
}

@inproceedings{cho_learning_2014,
	address = {Doha, Qatar},
	title = {Learning {Phrase} {Representations} using {RNN} {Encoder}–{Decoder} for {Statistical} {Machine} {Translation}},
	url = {http://aclweb.org/anthology/D14-1179},
	doi = {10.3115/v1/D14-1179},
	abstract = {In this paper, we propose a novel neural network model called RNN Encoder–Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a ﬁxedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder–Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
	language = {en},
	urldate = {2020-06-29},
	booktitle = {Proceedings of the 2014 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Cho, Kyunghyun and van Merrienboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
	year = {2014},
	pages = {1724--1734},
}

@article{rumelhart_learning_1986,
	title = {Learning representations by back-propagating errors},
	volume = {323},
	copyright = {1986 Nature Publishing Group},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/323533a0},
	doi = {10.1038/323533a0},
	abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal ‘hidden’ units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.},
	language = {en},
	number = {6088},
	urldate = {2020-06-29},
	journal = {Nature},
	author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
	month = oct,
	year = {1986},
	note = {Number: 6088
Publisher: Nature Publishing Group},
	pages = {533--536},
}

@article{hochreiter_long_1997,
	title = {Long {Short}-{Term} {Memory}},
	volume = {9},
	language = {en},
	number = {8},
	journal = {Neural Computation},
	author = {Hochreiter, Sepp},
	month = nov,
	year = {1997},
	pages = {1735--1780},
}

@inproceedings{shi_convolutional_2015,
	title = {Convolutional {LSTM} {Network}: {A} {Machine} {Learning} {Approach} for {Precipitation} {Nowcasting}},
	abstract = {The goal of precipitation nowcasting is to predict the future rainfall intensity in a local region over a relatively short period of time. Very few previous studies have examined this crucial and challenging weather forecasting problem from the machine learning perspective. In this paper, we formulate precipitation nowcasting as a spatiotemporal sequence forecasting problem in which both the input and the prediction target are spatiotemporal sequences. By extending the fully connected LSTM (FC-LSTM) to have convolutional structures in both the input-to-state and state-to-state transitions, we propose the convolutional LSTM (ConvLSTM) and use it to build an end-to-end trainable model for the precipitation nowcasting problem. Experiments show that our ConvLSTM network captures spatiotemporal correlations better and consistently outperforms FC-LSTM and the state-of-theart operational ROVER algorithm for precipitation nowcasting.},
	language = {en},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Shi, Xingjian and Chen, Zhourong and Wang, Hao and Yeung, Dit-Yan and Wong, Wai-kin and Woo, Wang-chun},
	year = {2015},
	pages = {9},
}

@article{bar_top-down_2006,
	title = {Top-down facilitation of visual recognition},
	volume = {103},
	copyright = {Copyright © 2006, The National Academy of Sciences},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/103/2/449},
	doi = {10.1073/pnas.0507062103},
	abstract = {Cortical analysis related to visual object recognition is traditionally thought to propagate serially along a bottom-up hierarchy of ventral areas. Recent proposals gradually promote the role of top-down processing in recognition, but how such facilitation is triggered remains a puzzle. We tested a specific model, proposing that low spatial frequencies facilitate visual object recognition by initiating top-down processes projected from orbitofrontal to visual cortex. The present study combined magnetoencephalography, which has superior temporal resolution, functional magnetic resonance imaging, and a behavioral task that yields successful recognition with stimulus repetitions. Object recognition elicited differential activity that developed in the left orbitofrontal cortex 50 ms earlier than it did in recognition-related areas in the temporal cortex. This early orbitofrontal activity was directly modulated by the presence of low spatial frequencies in the image. Taken together, the dynamics we revealed provide strong support for the proposal of how top-down facilitation of object recognition is initiated, and our observations are used to derive predictions for future research.},
	language = {en},
	number = {2},
	urldate = {2020-06-29},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Bar, M. and Kassam, K. S. and Ghuman, A. S. and Boshyan, J. and Schmid, A. M. and Dale, A. M. and Hämäläinen, M. S. and Marinkovic, K. and Schacter, D. L. and Rosen, B. R. and Halgren, E.},
	month = jan,
	year = {2006},
	pmid = {16407167},
	note = {Publisher: National Academy of Sciences
Section: Biological Sciences},
	keywords = {feedback, low spatial frequency, object recognition, orbitofrontal cortex, visual cortex},
	pages = {449--454},
}

@article{lamme_distinct_2000,
	title = {The distinct modes of vision offered by feedforward and recurrent processing},
	volume = {23},
	issn = {0166-2236},
	url = {http://www.sciencedirect.com/science/article/pii/S016622360001657X},
	doi = {10.1016/S0166-2236(00)01657-X},
	abstract = {An analysis of response latencies shows that when an image is presented to the visual system, neuronal activity is rapidly routed to a large number of visual areas. However, the activity of cortical neurons is not determined by this feedforward sweep alone. Horizontal connections within areas, and higher areas providing feedback, result in dynamic changes in tuning. The differences between feedforward and recurrent processing could prove pivotal in understanding the distinctions between attentive and pre-attentive vision as well as between conscious and unconscious vision. The feedforward sweep rapidly groups feature constellations that are hardwired in the visual brain, yet is probably incapable of yielding visual awareness; in many cases, recurrent processing is necessary before the features of an object are attentively grouped and the stimulus can enter consciousness.},
	language = {en},
	number = {11},
	urldate = {2020-06-29},
	journal = {Trends in Neurosciences},
	author = {Lamme, Victor A. F. and Roelfsema, Pieter R.},
	month = nov,
	year = {2000},
	pages = {571--579},
}

@article{werbos_backpropagation_1990,
	title = {Backpropagation through time: what it does and how to do it},
	volume = {78},
	issn = {1558-2256},
	shorttitle = {Backpropagation through time},
	doi = {10.1109/5.58337},
	abstract = {Basic backpropagation, which is a simple method now being widely used in areas like pattern recognition and fault diagnosis, is reviewed. The basic equations for backpropagation through time, and applications to areas like pattern recognition involving dynamic systems, systems identification, and control are discussed. Further extensions of this method, to deal with systems other than neural networks, systems involving simultaneous equations, or true recurrent networks, and other practical issues arising with the method are described. Pseudocode is provided to clarify the algorithms. The chain rule for ordered derivatives-the theorem which underlies backpropagation-is briefly discussed. The focus is on designing a simpler version of backpropagation which can be translated into computer code and applied directly by neutral network users.{\textless}{\textgreater}},
	number = {10},
	journal = {Proceedings of the IEEE},
	author = {Werbos, P.J.},
	month = oct,
	year = {1990},
	note = {Conference Name: Proceedings of the IEEE},
	keywords = {Artificial neural networks, Backpropagation, Books, Control systems, Equations, Fluid dynamics, Neural networks, Pattern recognition, Power system modeling, Supervised learning, backpropagation, fault diagnosis, identification, neural nets, neural networks, pattern recognition, pseudocode, systems identification},
	pages = {1550--1560},
}

@inproceedings{mnih_recurrent_2014,
	title = {Recurrent {Models} of {Visual} {Attention}},
	abstract = {Applying convolutional neural networks to large images is computationally expensive because the amount of computation scales linearly with the number of image pixels. We present a novel recurrent neural network model that is capable of extracting information from an image or video by adaptively selecting a sequence of regions or locations and only processing the selected regions at high resolution. Like convolutional neural networks, the proposed model has a degree of translation invariance built-in, but the amount of computation it performs can be controlled independently of the input image size. While the model is non-differentiable, it can be trained using reinforcement learning methods to learn task-speciﬁc policies. We evaluate our model on several image classiﬁcation tasks, where it signiﬁcantly outperforms a convolutional neural network baseline on cluttered images, and on a dynamic visual control problem, where it learns to track a simple object without an explicit training signal for doing so.},
	language = {en},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Mnih, Volodymyr and Heess, Nicolas and Graves, Alex},
	year = {2014},
	pages = {9},
}

@inproceedings{pinheiro_recurrent_2014,
	title = {Recurrent {Convolutional} {Neural} {Networks} for {Scene} {Labeling}},
	abstract = {The goal of the scene labeling task is to assign a class label to each pixel in an image. To ensure a good visual coherence and a high class accuracy, it is essential for a model to capture long range (pixel) label dependencies in images. In a feed-forward architecture, this can be achieved simply by considering a sufﬁciently large input context patch, around each pixel to be labeled. We propose an approach that consists of a recurrent convolutional neural network which allows us to consider a large input context while limiting the capacity of the model. Contrary to most standard approaches, our method does not rely on any segmentation technique nor any taskspeciﬁc features. The system is trained in an end-to-end manner over raw pixels, and models complex spatial dependencies with low inference cost. As the context size increases with the built-in recurrence, the system identiﬁes and corrects its own errors. Our approach yields state-ofthe-art performance on both the Stanford Background Dataset and the SIFT Flow Dataset, while remaining very fast at test time.},
	language = {en},
	booktitle = {Proceedings of the 31th {International} {Conference} on {Machine} {Learning}},
	author = {Pinheiro, Pedro O and Collobert, Ronan},
	year = {2014},
	pages = {9},
}

@inproceedings{donahue_long-term_2015,
	title = {Long-{Term} {Recurrent} {Convolutional} {Networks} for {Visual} {Recognition} and {Description}},
	url = {https://openaccess.thecvf.com/content_cvpr_2015/html/Donahue_Long-Term_Recurrent_Convolutional_2015_CVPR_paper.html},
	urldate = {2020-06-29},
	author = {Donahue, Jeffrey and Anne Hendricks, Lisa and Guadarrama, Sergio and Rohrbach, Marcus and Venugopalan, Subhashini and Saenko, Kate and Darrell, Trevor},
	year = {2015},
	pages = {2625--2634},
}

@inproceedings{linsley_learning_2018,
	title = {Learning long-range spatial dependencies with horizontal gated-recurrent units},
	abstract = {Progress in deep learning has spawned great successes in many engineering applications. As a prime example, convolutional neural networks, a type of feedforward neural networks, are now approaching – and sometimes even surpassing – human accuracy on a variety of visual recognition tasks. Here, however, we show that these neural networks and their recent extensions struggle in recognition tasks where co-dependent visual features must be detected over long spatial ranges. We introduce a visual challenge, Pathﬁnder, and describe a novel recurrent neural network architecture called the horizontal gated recurrent unit (hGRU) to learn intrinsic horizontal connections – both within and across feature columns. We demonstrate that a single hGRU layer matches or outperforms all tested feedforward hierarchical baselines including state-of-the-art architectures with orders of magnitude more parameters.},
	language = {en},
	urldate = {2020-06-18},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Linsley, Drew and Kim, Junkyung and Veerabadran, Vijay and Serre, Thomas},
	year = {2018},
}

@inproceedings{hendricks_generating_2016,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Generating {Visual} {Explanations}},
	isbn = {978-3-319-46493-0},
	doi = {10.1007/978-3-319-46493-0_1},
	abstract = {Clearly explaining a rationale for a classification decision to an end user can be as important as the decision itself. Existing approaches for deep visual recognition are generally opaque and do not output any justification text; contemporary vision-language models can describe image content but fail to take into account class-discriminative image aspects which justify visual predictions. We propose a new model that focuses on the discriminating properties of the visible object, jointly predicts a class label, and explains why the predicted label is appropriate for the image. Through a novel loss function based on sampling and reinforcement learning, our model learns to generate sentences that realize a global sentence property, such as class specificity. Our results on the CUB dataset show that our model is able to generate explanations which are not only consistent with an image but also more discriminative than descriptions produced by existing captioning methods.},
	language = {en},
	booktitle = {Proceedings of the {European} {Conference} on {Computer} {Vision} ({ECCV})},
	publisher = {Springer International Publishing},
	author = {Hendricks, Lisa Anne and Akata, Zeynep and Rohrbach, Marcus and Donahue, Jeff and Schiele, Bernt and Darrell, Trevor},
	year = {2016},
	keywords = {Image description, Language and vision, Visual explanation},
	pages = {3--19},
}

@inproceedings{murdoch_automatic_2017,
	title = {Automatic {Rule} {Extraction} from {Long} {Short} {Term} {Memory} {Networks}},
	url = {http://arxiv.org/abs/1702.02540},
	abstract = {Although deep learning models have proven effective at solving problems in natural language processing, the mechanism by which they come to their conclusions is often unclear. As a result, these models are generally treated as black boxes, yielding no insight of the underlying learned patterns. In this paper we consider Long Short Term Memory networks (LSTMs) and demonstrate a new approach for tracking the importance of a given input to the LSTM for a given output. By identifying consistently important patterns of words, we are able to distill state of the art LSTMs on sentiment analysis and question answering into a set of representative phrases. This representation is then quantitatively validated by using the extracted phrases to construct a simple, rule-based classiﬁer which approximates the output of the LSTM.},
	language = {en},
	urldate = {2020-06-18},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Murdoch, W. James and Szlam, Arthur},
	year = {2017},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@inproceedings{parkash_attributes_2012,
	address = {Berlin, Heidelberg},
	title = {Attributes for {Classifier} {Feedback}},
	volume = {7574},
	isbn = {978-3-642-33711-6 978-3-642-33712-3},
	url = {http://link.springer.com/10.1007/978-3-642-33712-3_26},
	doi = {10.1007/978-3-642-33712-3_26},
	abstract = {Traditional active learning allows a (machine) learner to query the (human) teacher for labels on examples it ﬁnds confusing. The teacher then provides a label for only that instance. This is quite restrictive. In this paper, we propose a learning paradigm in which the learner communicates its belief (i.e. predicted label) about the actively chosen example to the teacher. The teacher then conﬁrms or rejects the predicted label. More importantly, if rejected, the teacher communicates an explanation for why the learner’s belief was wrong. This explanation allows the learner to propagate the feedback provided by the teacher to many unlabeled images. This allows a classiﬁer to better learn from its mistakes, leading to accelerated discriminative learning of visual concepts even with few labeled images. In order for such communication to be feasible, it is crucial to have a language that both the human supervisor and the machine learner understand. Attributes provide precisely this channel. They are human-interpretable mid-level visual concepts shareable across categories e.g. “furry”, “spacious”, etc. We advocate the use of attributes for a supervisor to provide feedback to a classiﬁer and directly communicate his knowledge of the world. We employ a straightforward approach to incorporate this feedback in the classiﬁer, and demonstrate its power on a variety of visual recognition scenarios such as image classiﬁcation and annotation. This application of attributes for providing classiﬁers feedback is very powerful, and has not been explored in the community. It introduces a new mode of supervision, and opens up several avenues for future research.},
	language = {en},
	urldate = {2020-06-23},
	booktitle = {Proceedings of the {European} {Conference} on {Computer} {Vision} ({ECCV})},
	publisher = {Springer Berlin Heidelberg},
	author = {Parkash, Amar and Parikh, Devi and Parkash, Amar and Parikh, Devi},
	editor = {Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Fitzgibbon, Andrew and Lazebnik, Svetlana and Perona, Pietro and Sato, Yoichi and Schmid, Cordelia},
	year = {2012},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {354--368},
}

@inproceedings{li_are_2019,
	title = {Are {Generative} {Classiﬁers} {More} {Robust} to {Adversarial} {Attacks}?},
	abstract = {There is a rising interest in studying the robustness of deep neural network classiﬁers against adversaries, with both advanced attack and defence techniques being actively developed. However, most recent work focuses on discriminative classiﬁers, which only model the conditional distribution of the labels given the inputs. In this paper, we propose and investigate the deep Bayes classiﬁer, which improves classical naive Bayes with conditional deep generative models. We further develop detection methods for adversarial examples, which reject inputs with low likelihood under the generative model. Experimental results suggest that deep Bayes classiﬁers are more robust than deep discriminative classiﬁers, and that the proposed detection methods are effective against many recently proposed attacks.},
	language = {en},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	author = {Li, Yingzhen and Bradshaw, John and Sharma, Yash},
	year = {2019},
	pages = {11},
}

@inproceedings{brendel_approximating_2019,
	title = {Approximating {CNNs} with {Bag}-of-local-features {Models} {Works} {Surprisingly} {Well} on {ImageNet}},
	abstract = {Deep Neural Networks (DNNs) excel on many complex perceptual tasks but it has proven notoriously difﬁcult to understand how they reach their decisions. We here introduce a high-performance DNN architecture on ImageNet whose decisions are considerably easier to explain. Our model, a simple variant of the ResNet50 architecture called BagNet, classiﬁes an image based on the occurrences of small local image features without taking into account their spatial ordering. This strategy is closely related to the bag-of-feature (BoF) models popular before the onset of deep learning and reaches a surprisingly high accuracy on ImageNet (87.6\% top-5 for 33 × 33 px features and Alexnet performance for 17 × 17 px features). The constraint on local features makes it straight-forward to analyse how exactly each part of the image inﬂuences the classiﬁcation. Furthermore, the BagNets behave similar to state-of-the art deep neural networks such as VGG-16, ResNet-152 or DenseNet-169 in terms of feature sensitivity, error distribution and interactions between image parts. This suggests that the improvements of DNNs over previous bag-of-feature classiﬁers in the last few years is mostly achieved by better ﬁne-tuning rather than by qualitatively different decision strategies.},
	language = {en},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Brendel, Wieland and Bethge, Matthias},
	year = {2019},
	pages = {15},
}

@inproceedings{ribeiro_anchors_2018,
	title = {Anchors: {High} {Precision} {Model}-{Agnostic} {Explanations}},
	abstract = {We introduce a novel model-agnostic system that explains the behavior of complex models with high-precision rules called anchors, representing local, “sufﬁcient” conditions for predictions. We propose an algorithm to efﬁciently compute these explanations for any black-box model with high-probability guarantees. We demonstrate the ﬂexibility of anchors by explaining a myriad of different models for different domains and tasks. In a user study, we show that anchors enable users to predict how a model would behave on unseen instances with less effort and higher precision, as compared to existing linear explanations or no explanations.},
	language = {en},
	booktitle = {Thirty-{Second} {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
	year = {2018},
	pages = {9},
}

@inproceedings{zhang_growing_2017,
	title = {Growing {Interpretable} {Part} {Graphs} on {ConvNets} via {Multi}-{Shot} {Learning}},
	abstract = {This paper proposes a learning strategy that extracts objectpart concepts from a pre-trained convolutional neural network (CNN), in an attempt to 1) explore explicit semantics hidden in CNN units and 2) gradually grow a semantically interpretable graphical model on the pre-trained CNN for hierarchical object understanding. Given part annotations on very few (e.g. 3–12) objects, our method mines certain latent patterns from the pre-trained CNN and associates them with different semantic parts. We use a four-layer And-Or graph to organize the mined latent patterns, so as to clarify their internal semantic hierarchy. Our method is guided by a small number of part annotations, and it achieves superior performance (about 13\%–107\% improvement) in part center prediction on the PASCAL VOC and ImageNet datasets1.},
	language = {en},
	booktitle = {Proceedings of the {Thirty}-{First} {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Zhang, Quanshi and Cao, Ruiming and Wu, Ying Nian and Zhu, Song-Chun},
	year = {2017},
	pages = {9},
}

@inproceedings{sabour_adversarial_2016,
	title = {Adversarial {Manipulation} of {Deep} {Representations}},
	abstract = {We show that the image representations in a deep neural network (DNN) can be manipulated to mimic those of other natural images, with only minor, imperceptible perturbations to the original image. Previous methods for generating adversarial images focused on image perturbations designed to produce erroneous class labels. Here we instead concentrate on the internal layers of DNN representations, to produce a new class of adversarial images that differs qualitatively from others. While the adversary is perceptually similar to one image, its internal representation appears remarkably similar to a different image, from a different class and bearing little if any apparent similarity to the input. Further, they appear generic and consistent with the space of natural images. This phenomenon demonstrates the possibility to trick a DNN to confound almost any image with any other chosen image, and raises questions about DNN representations, as well as the properties of natural images themselves.},
	language = {en},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Sabour, Sara and Cao, Yanshuai and Faghri, Fartash and Fleet, David J},
	year = {2016},
	pages = {18},
}

@inproceedings{singla_understanding_2019,
	title = {Understanding {Impacts} of {High}-{Order} {Loss} {Approximations} and {Features} in {Deep} {Learning} {Interpretation}},
	url = {http://proceedings.mlr.press/v97/singla19a.html},
	abstract = {Current saliency map interpretations for neural networks generally rely on two key assumptions. First, they use first-order approximations of the loss function, neglecting higher-order terms such a...},
	language = {en},
	urldate = {2020-06-27},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	author = {Singla, Sahil and Wallace, Eric and Feng, Shi and Feizi, Soheil},
	month = may,
	year = {2019},
	note = {ISSN: 1938-7228
Section: Machine Learning},
	pages = {5848--5856},
}

@article{olah_building_2018,
	title = {The {Building} {Blocks} of {Interpretability}},
	volume = {3},
	issn = {2476-0757},
	url = {https://distill.pub/2018/building-blocks},
	doi = {10.23915/distill.00010},
	abstract = {Interpretability techniques are normally studied in isolation. We explore the powerful interfaces that arise when you combine them -- and the rich structure of this combinatorial space.},
	language = {en},
	number = {3},
	urldate = {2020-06-25},
	journal = {Distill},
	author = {Olah, Chris and Satyanarayan, Arvind and Johnson, Ian and Carter, Shan and Schubert, Ludwig and Ye, Katherine and Mordvintsev, Alexander},
	month = mar,
	year = {2018},
	pages = {e10},
}

@article{khaligh-razavi_deep_2014,
	title = {Deep {Supervised}, but {Not} {Unsupervised}, {Models} {May} {Explain} {IT} {Cortical} {Representation}},
	volume = {10},
	issn = {1553-7358},
	url = {https://dx.plos.org/10.1371/journal.pcbi.1003915},
	doi = {10.1371/journal.pcbi.1003915},
	abstract = {Inferior temporal (IT) cortex in human and nonhuman primates serves visual object recognition. Computational objectvision models, although continually improving, do not yet reach human performance. It is unclear to what extent the internal representations of computational models can explain the IT representation. Here we investigate a wide range of computational model representations (37 in total), testing their categorization performance and their ability to account for the IT representational geometry. The models include well-known neuroscientific object-recognition models (e.g. HMAX, VisNet) along with several models from computer vision (e.g. SIFT, GIST, self-similarity features, and a deep convolutional neural network). We compared the representational dissimilarity matrices (RDMs) of the model representations with the RDMs obtained from human IT (measured with fMRI) and monkey IT (measured with cell recording) for the same set of stimuli (not used in training the models). Better performing models were more similar to IT in that they showed greater clustering of representational patterns by category. In addition, better performing models also more strongly resembled IT in terms of their within-category representational dissimilarities. Representational geometries were significantly correlated between IT and many of the models. However, the categorical clustering observed in IT was largely unexplained by the unsupervised models. The deep convolutional network, which was trained by supervision with over a million categorylabeled images, reached the highest categorization performance and also best explained IT, although it did not fully explain the IT data. Combining the features of this model with appropriate weights and adding linear combinations that maximize the margin between animate and inanimate objects and between faces and other objects yielded a representation that fully explained our IT data. Overall, our results suggest that explaining IT requires computational features trained through supervised learning to emphasize the behaviorally important categorical divisions prominently reflected in IT.},
	language = {en},
	number = {11},
	urldate = {2020-06-24},
	journal = {PLoS Computational Biology},
	author = {Khaligh-Razavi, Seyed-Mahdi and Kriegeskorte, Nikolaus},
	editor = {Diedrichsen, Jörn},
	month = nov,
	year = {2014},
	pages = {e1003915},
}

@inproceedings{hendrycks_benchmarking_2019,
	title = {Benchmarking {Neural} {Network} {Robustness} to {Common} {Corruptions} and {Perturbations}},
	url = {http://arxiv.org/abs/1903.12261},
	abstract = {In this paper we establish rigorous benchmarks for image classiﬁer robustness. Our ﬁrst benchmark, IMAGENET-C, standardizes and expands the corruption robustness topic, while showing which classiﬁers are preferable in safety-critical applications. Then we propose a new dataset called IMAGENET-P which enables researchers to benchmark a classiﬁer’s robustness to common perturbations. Unlike recent robustness research, this benchmark evaluates performance on common corruptions and perturbations not worst-case adversarial perturbations. We ﬁnd that there are negligible changes in relative corruption robustness from AlexNet classiﬁers to ResNet classiﬁers. Afterward we discover ways to enhance corruption and perturbation robustness. We even ﬁnd that a bypassed adversarial defense provides substantial common perturbation robustness. Together our benchmarks may aid future work toward networks that robustly generalize.},
	language = {en},
	urldate = {2020-06-18},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Hendrycks, Dan and Dietterich, Thomas},
	year = {2019},
	keywords = {dataset},
}

@inproceedings{yin_fourier_2019,
	title = {A {Fourier} {Perspective} on {Model} {Robustness} in {Computer} {Vision}},
	url = {http://arxiv.org/abs/1906.08988},
	abstract = {Achieving robustness to distributional shift is a longstanding and challenging goal of computer vision. Data augmentation is a commonly used approach for improving robustness, however robustness gains are typically not uniform across corruption types. Indeed increasing performance in the presence of random noise is often met with reduced performance on other corruptions such as contrast change. Understanding when and why these sorts of trade-offs occur is a crucial step towards mitigating them. Towards this end, we investigate recently observed tradeoffs caused by Gaussian data augmentation and adversarial training. We ﬁnd that both methods improve robustness to corruptions that are concentrated in the high frequency domain while reducing robustness to corruptions that are concentrated in the low frequency domain. This suggests that one way to mitigate these trade-offs via data augmentation is to use a more diverse set of augmentations. Towards this end we observe that AutoAugment [6], a recently proposed data augmentation policy optimized for clean accuracy, achieves state-of-the-art robustness on the CIFAR-10-C [17] benchmark.},
	language = {en},
	urldate = {2020-06-18},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Yin, Dong and Lopes, Raphael Gontijo and Shlens, Jonathon and Cubuk, Ekin D. and Gilmer, Justin},
	year = {2019},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{han_deep_2018,
	title = {Deep {Predictive} {Coding} {Network} with {Local} {Recurrent} {Processing} for {Object} {Recognition}},
	abstract = {Inspired by "predictive coding" - a theory in neuroscience, we develop a bidirectional and dynamic neural network with local recurrent processing, namely predictive coding network (PCN). Unlike feedforward-only convolutional neural networks, PCN includes both feedback connections, which carry top-down predictions, and feedforward connections, which carry bottom-up errors of prediction. Feedback and feedforward connections enable adjacent layers to interact locally and recurrently to reﬁne representations towards minimization of layer-wise prediction errors. When unfolded over time, the recurrent processing gives rise to an increasingly deeper hierarchy of non-linear transformation, allowing a shallow network to dynamically extend itself into an arbitrarily deep network. We train and test PCN for image classiﬁcation with SVHN, CIFAR and ImageNet datasets. Despite notably fewer layers and parameters, PCN achieves competitive performance compared to classical and state-of-the-art models. Further analysis shows that the internal representations in PCN converge over time and yield increasingly better accuracy in object recognition. Errors of top-down prediction also reveal visual saliency or bottom-up attention.},
	language = {en},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Han, Kuan and Wen, Haiguang and Zhang, Yizhen and Fu, Di and Culurciello, Eugenio and Liu, Zhongming},
	year = {2018},
	pages = {13},
}

@article{lotter_deep_2017,
	title = {Deep {Predictive} {Coding} {Networks} for {Video} {Prediction} and {Unsupervised} {Learning}},
	abstract = {While great strides have been made in using deep learning algorithms to solve supervised learning tasks, the problem of unsupervised learning — leveraging unlabeled examples to learn about the structure of a domain — remains a difﬁcult unsolved challenge. Here, we explore prediction of future frames in a video sequence as an unsupervised learning rule for learning about the structure of the visual world. We describe a predictive neural network (“PredNet”) architecture that is inspired by the concept of “predictive coding” from the neuroscience literature. These networks learn to predict future frames in a video sequence, with each layer in the network making local predictions and only forwarding deviations from those predictions to subsequent network layers. We show that these networks are able to robustly learn to predict the movement of synthetic (rendered) objects, and that in doing so, the networks learn internal representations that are useful for decoding latent object parameters (e.g. pose) that support object recognition with fewer training views. We also show that these networks can scale to complex natural image streams (car-mounted camera videos), capturing key aspects of both egocentric movement and the movement of objects in the visual scene, and the representation learned in this setting is useful for estimating the steering angle. Altogether, these results suggest that prediction represents a powerful framework for unsupervised learning, allowing for implicit learning of object and scene structure.},
	language = {en},
	author = {Lotter, William and Kreiman, Gabriel and Cox, David},
	year = {2017},
	pages = {18},
}

@incollection{calders_knowledge-powered_2014,
	address = {Berlin, Heidelberg},
	title = {Knowledge-{Powered} {Deep} {Learning} for {Word} {Embedding}},
	volume = {8724},
	isbn = {978-3-662-44847-2 978-3-662-44848-9},
	url = {http://link.springer.com/10.1007/978-3-662-44848-9_9},
	abstract = {The basis of applying deep learning to solve natural language processing tasks is to obtain high-quality distributed representations of words, i.e., word embeddings, from large amounts of text data. However, text itself usually contains incomplete and ambiguous information, which makes necessity to leverage extra knowledge to understand it. Fortunately, text itself already contains well-deﬁned morphological and syntactic knowledge; moreover, the large amount of texts on the Web enable the extraction of plenty of semantic knowledge. Therefore, it makes sense to design novel deep learning algorithms and systems in order to leverage the above knowledge to compute more effective word embeddings. In this paper, we conduct an empirical study on the capacity of leveraging morphological, syntactic, and semantic knowledge to achieve high-quality word embeddings. Our study explores these types of knowledge to deﬁne new basis for word representation, provide additional input information, and serve as auxiliary supervision in deep learning, respectively. Experiments on an analogical reasoning task, a word similarity task, and a word completion task have all demonstrated that knowledge-powered deep learning can enhance the effectiveness of word embedding.},
	language = {en},
	urldate = {2020-06-23},
	booktitle = {Machine {Learning} and {Knowledge} {Discovery} in {Databases}},
	publisher = {Springer Berlin Heidelberg},
	author = {Bian, Jiang and Gao, Bin and Liu, Tie-Yan},
	editor = {Calders, Toon and Esposito, Floriana and Hüllermeier, Eyke and Meo, Rosa},
	year = {2014},
	doi = {10.1007/978-3-662-44848-9_9},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {132--148},
}

@incollection{hutchison_visual_2010,
	address = {Berlin, Heidelberg},
	title = {Visual {Recognition} with {Humans} in the {Loop}},
	volume = {6314},
	isbn = {978-3-642-15560-4 978-3-642-15561-1},
	url = {http://link.springer.com/10.1007/978-3-642-15561-1_32},
	abstract = {We present an interactive, hybrid human-computer method for object classiﬁcation. The method applies to classes of objects that are recognizable by people with appropriate expertise (e.g., animal species or airplane model), but not (in general) by people without such expertise. It can be seen as a visual version of the 20 questions game, where questions based on simple visual attributes are posed interactively. The goal is to identify the true class while minimizing the number of questions asked, using the visual content of the image. We introduce a general framework for incorporating almost any oﬀ-the-shelf multi-class object recognition algorithm into the visual 20 questions game, and provide methodologies to account for imperfect user responses and unreliable computer vision algorithms. We evaluate our methods on Birds-200, a diﬃcult dataset of 200 tightly-related bird species, and on the Animals With Attributes dataset. Our results demonstrate that incorporating user input drives up recognition accuracy to levels that are good enough for practical applications, while at the same time, computer vision reduces the amount of human interaction required.},
	language = {en},
	urldate = {2020-06-23},
	booktitle = {Computer {Vision} – {ECCV} 2010},
	publisher = {Springer Berlin Heidelberg},
	author = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Branson, Steve and Wah, Catherine and Schroff, Florian and Babenko, Boris and Welinder, Peter and Perona, Pietro and Belongie, Serge},
	editor = {Daniilidis, Kostas and Maragos, Petros and Paragios, Nikos},
	year = {2010},
	doi = {10.1007/978-3-642-15561-1_32},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {438--451},
}

@inproceedings{wagner_interpretable_2019,
	address = {Long Beach, CA, USA},
	title = {Interpretable and {Fine}-{Grained} {Visual} {Explanations} for {Convolutional} {Neural} {Networks}},
	isbn = {978-1-72813-293-8},
	url = {https://ieeexplore.ieee.org/document/8953597/},
	doi = {10.1109/CVPR.2019.00931},
	abstract = {To verify and validate networks, it is essential to gain insight into their decisions, limitations as well as possible shortcomings of training data. In this work, we propose a post-hoc, optimization based visual explanation method, which highlights the evidence in the input image for a speciﬁc prediction. Our approach is based on a novel technique to defend against adversarial evidence (i.e. faulty evidence due to artefacts) by ﬁltering gradients during optimization. The defense does not depend on human-tuned parameters. It enables explanations which are both ﬁne-grained and preserve the characteristics of images, such as edges and colors. The explanations are interpretable, suited for visualizing detailed evidence and can be tested as they are valid model inputs. We qualitatively and quantitatively evaluate our approach on a multitude of models and datasets.},
	language = {en},
	urldate = {2020-06-23},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Wagner, Jorg and Kohler, Jan Mathias and Gindele, Tobias and Hetzel, Leon and Wiedemer, Jakob Thaddaus and Behnke, Sven},
	month = jun,
	year = {2019},
	pages = {9089--9099},
}

@inproceedings{donahue_annotator_2011,
	address = {Barcelona, Spain},
	title = {Annotator rationales for visual recognition},
	isbn = {978-1-4577-1102-2 978-1-4577-1101-5 978-1-4577-1100-8},
	url = {http://ieeexplore.ieee.org/document/6126394/},
	doi = {10.1109/ICCV.2011.6126394},
	abstract = {Traditional supervised visual learning simply asks annotators “what” label an image should have. We propose an approach for image classiﬁcation problems requiring subjective judgment that also asks “why”, and uses that information to enrich the learned model. We develop two forms of visual annotator rationales: in the ﬁrst, the annotator highlights the spatial region of interest he found most inﬂuential to the label selected, and in the second, he comments on the visual attributes that were most important. For either case, we show how to map the response to synthetic contrast examples, and then exploit an existing large-margin learning technique to reﬁne the decision boundary accordingly. Results on multiple scene categorization and human attractiveness tasks show the promise of our approach, which can more accurately learn complex categories with the explanations behind the label choices.},
	language = {en},
	urldate = {2020-06-23},
	booktitle = {2011 {International} {Conference} on {Computer} {Vision}},
	publisher = {IEEE},
	author = {Donahue, Jeff and Grauman, Kristen},
	month = nov,
	year = {2011},
	pages = {1395--1402},
}

@inproceedings{branson_strong_2011,
	address = {Barcelona, Spain},
	title = {Strong supervision from weak annotation: {Interactive} training of deformable part models},
	isbn = {978-1-4577-1102-2 978-1-4577-1101-5 978-1-4577-1100-8},
	shorttitle = {Strong supervision from weak annotation},
	url = {http://ieeexplore.ieee.org/document/6126450/},
	doi = {10.1109/ICCV.2011.6126450},
	abstract = {We propose a framework for large scale learning and annotation of structured models. The system interleaves interactive labeling (where the current model is used to semiautomate the labeling of a new example) and online learning (where a newly labeled example is used to update the current model parameters). This framework is scalable to large datasets and complex image models and is shown to have excellent theoretical and practical properties in terms of train time, optimality guarantees, and bounds on the amount of annotation effort per image. We apply this framework to part-based detection, and introduce a novel algorithm for interactive labeling of deformable part models. The labeling tool updates and displays in real-time the maximum likelihood location of all parts as the user clicks and drags the location of one or more parts. We demonstrate that the system can be used to efﬁciently and robustly train part and pose detectors on the CUB Birds-200–a challenging dataset of birds in unconstrained pose and environment.},
	language = {en},
	urldate = {2020-06-23},
	booktitle = {2011 {International} {Conference} on {Computer} {Vision}},
	publisher = {IEEE},
	author = {Branson, Steve and Perona, Pietro and Belongie, Serge},
	month = nov,
	year = {2011},
	pages = {1832--1839},
}

@inproceedings{deng_fine-grained_2013,
	address = {Portland, OR, USA},
	title = {Fine-{Grained} {Crowdsourcing} for {Fine}-{Grained} {Recognition}},
	isbn = {978-0-7695-4989-7},
	url = {http://ieeexplore.ieee.org/document/6618925/},
	doi = {10.1109/CVPR.2013.81},
	abstract = {Fine-grained recognition concerns categorization at sub-ordinate levels, where the distinction between object classes is highly local. Compared to basic level recognition, ﬁne-grained categorization can be more challenging as there are in general less data and fewer discriminative features. This necessitates the use of stronger prior for feature selection. In this work, we include humans in the loop to help computers select discriminative features. We introduce a novel online game called “Bubbles” that reveals discriminative features humans use. The player’s goal is to identify the category of a heavily blurred image. During the game, the player can choose to reveal full details of circular regions (“bubbles”), with a certain penalty. With proper setup the game generates discriminative bubbles with assured quality. We next propose the “BubbleBank” algorithm that uses the human selected bubbles to improve machine recognition performance. Experiments demonstrate that our approach yields large improvements over the previous state of the art on challenging benchmarks.},
	language = {en},
	urldate = {2020-06-23},
	booktitle = {2013 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Deng, Jia and Krause, Jonathan and Fei-Fei, Li},
	month = jun,
	year = {2013},
	pages = {580--587},
}

@inproceedings{kun_duan_discovering_2012,
	address = {Providence, RI},
	title = {Discovering localized attributes for fine-grained recognition},
	isbn = {978-1-4673-1228-8 978-1-4673-1226-4 978-1-4673-1227-1},
	url = {http://ieeexplore.ieee.org/document/6248089/},
	doi = {10.1109/CVPR.2012.6248089},
	abstract = {Attributes are visual concepts that can be detected by machines, understood by humans, and shared across categories. They are particularly useful for ﬁne-grained domains where categories are closely related to one other (e.g. bird species recognition). In such scenarios, relevant attributes are often local (e.g. “white belly”), but the question of how to choose these local attributes remains largely unexplored. In this paper, we propose an interactive approach that discovers local attributes that are both discriminative and semantically meaningful from image datasets annotated only with ﬁne-grained category labels and object bounding boxes. Our approach uses a latent conditional random ﬁeld model to discover candidate attributes that are detectable and discriminative, and then employs a recommender system that selects attributes likely to be semantically meaningful. Human interaction is used to provide semantic names for the discovered attributes. We demonstrate our method on two challenging datasets, CaltechUCSD Birds-200-2011 and Leeds Butterﬂies, and ﬁnd that our discovered attributes outperform those generated by traditional approaches.},
	language = {en},
	urldate = {2020-06-23},
	booktitle = {2012 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {{Kun Duan} and Parikh, D. and Crandall, D. and Grauman, K.},
	month = jun,
	year = {2012},
	pages = {3474--3481},
}

@article{visin_renet_2015,
	title = {{ReNet}: {A} {Recurrent} {Neural} {Network} {Based} {Alternative} to {Convolutional} {Networks}},
	shorttitle = {{ReNet}},
	url = {http://arxiv.org/abs/1505.00393},
	abstract = {In this paper, we propose a deep neural network architecture for object recognition based on recurrent neural networks. The proposed network, called ReNet, replaces the ubiquitous convolution+pooling layer of the deep convolutional neural network with four recurrent neural networks that sweep horizontally and vertically in both directions across the image. We evaluate the proposed ReNet on three widely-used benchmark datasets; MNIST, CIFAR-10 and SVHN. The result suggests that ReNet is a viable alternative to the deep convolutional neural network, and that further investigation is needed.},
	language = {en},
	urldate = {2020-06-23},
	journal = {arXiv:1505.00393 [cs]},
	author = {Visin, Francesco and Kastner, Kyle and Cho, Kyunghyun and Matteucci, Matteo and Courville, Aaron and Bengio, Yoshua},
	month = jul,
	year = {2015},
	note = {arXiv: 1505.00393},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{kriegeskorte_deep_2015,
	title = {Deep {Neural} {Networks}: {A} {New} {Framework} for {Modeling} {Biological} {Vision} and {Brain} {Information} {Processing}},
	volume = {1},
	issn = {2374-4642, 2374-4650},
	shorttitle = {Deep {Neural} {Networks}},
	url = {http://www.annualreviews.org/doi/10.1146/annurev-vision-082114-035447},
	doi = {10.1146/annurev-vision-082114-035447},
	abstract = {Recent advances in neural network modeling have enabled major strides in computer vision and other artiﬁcial intelligence applications. Human-level visual recognition abilities are coming within reach of artiﬁcial systems. Artiﬁcial neural networks are inspired by the brain, and their computations could be implemented in biological neurons. Convolutional feedforward networks, which now dominate computer vision, take further inspiration from the architecture of the primate visual hierarchy. However, the current models are designed with engineering goals, not to model brain computations. Nevertheless, initial studies comparing internal representations between these models and primate brains ﬁnd surprisingly similar representational spaces. With human-level performance no longer out of reach, we are entering an exciting new era, in which we will be able to build biologically faithful feedforward and recurrent computational models of how biological brains perform high-level feats of intelligence, including vision.},
	language = {en},
	number = {1},
	urldate = {2020-06-23},
	journal = {Annual Review of Vision Science},
	author = {Kriegeskorte, Nikolaus},
	month = nov,
	year = {2015},
	pages = {417--446},
}

@article{liang_incorporating_2016,
	title = {Incorporating image priors with deep convolutional neural networks for image super-resolution},
	volume = {194},
	issn = {09252312},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231216002836},
	doi = {10.1016/j.neucom.2016.02.046},
	abstract = {Deep convolutional neural network has been applied for single image super-resolution problem and demonstrated state-of-the-art quality. This paper presents several prior information that could be utilized during the training process of the deep convolutional neural network. The ﬁrst type of prior focuses on edges and texture restoration in the output, and the second type of prior utilizes multiple upscaling factors to consider the structure recurrence across different scales. As demonstrated by our experimental results, the proposed framework could signiﬁcantly accelerate the training speed for more than ten times and at the same time lead to better image quality. The generated super-resolution image achieves visually sharper and more pleasant restoration as well as superior objectively evaluation results compared to state-of-the-art methods.},
	language = {en},
	urldate = {2020-06-23},
	journal = {Neurocomputing},
	author = {Liang, Yudong and Wang, Jinjun and Zhou, Sanping and Gong, Yihong and Zheng, Nanning},
	month = jun,
	year = {2016},
	pages = {340--347},
}

@article{ghosh_incorporating_2016,
	title = {Incorporating priors for medical image segmentation using a genetic algorithm},
	volume = {195},
	issn = {09252312},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231216001065},
	doi = {10.1016/j.neucom.2015.09.123},
	abstract = {Medical image segmentation is typically performed manually by a physician to delineate gross tumor volumes for treatment planning and diagnosis. Manual segmentation is performed by medical experts using prior knowledge of organ shapes and locations but is prone to reader subjectivity and inconsistency. Automating the process is challenging due to poor tissue contrast and ill-deﬁned organ/tissue boundaries in medical images. This paper presents a genetic algorithm for combining representations of learned information such as known shapes, regional properties and relative position of objects into a single framework to perform automated three-dimensional segmentation. The algorithm has been tested for prostate segmentation on pelvic computed tomography and magnetic resonance images.},
	language = {en},
	urldate = {2020-06-23},
	journal = {Neurocomputing},
	author = {Ghosh, Payel and Mitchell, Melanie and Tanyi, James A. and Hung, Arthur Y.},
	month = jun,
	year = {2016},
	pages = {181--194},
}

@article{sinha_curriculum_2020,
	title = {Curriculum {By} {Texture}},
	url = {http://arxiv.org/abs/2003.01367},
	abstract = {Convolutional Neural Networks (CNNs) have shown impressive performance in computer vision tasks such as image classiﬁcation and segmentation. One factor for the success of CNNs is that they have an inductive bias that assumes a certain type of spatial structure is present in the data. Recent work by Geirhos et al. (2018) shows how learning in CNNs causes the learned CNN models to be biased towards high-frequency textural information, compared to low-frequency shape information in images. Many tasks generally requires both shape and textural information. Hence, we propose a simple curriculum based scheme which improves the ability of CNNs to be less biased towards textural information, and at the same time, being able to represent both the shape and textural information. We propose to augment the training of CNNs by controlling the amount of textural information that is available to the CNNs during the training process, by convolving the output of a CNN layer with a low-pass ﬁlter, or simply a Gaussian kernel. By reducing the standard deviation of the Gaussian kernel, we are able to gradually increase the amount of textural information available as training progresses, and hence reduce the texture bias. Such an augmented training scheme significantly improves the performance of CNNs on various image classiﬁcation tasks, while adding no additional trainable parameters or auxiliary regularization objectives. We also observe signiﬁcant improvements when using the trained CNNs to perform transfer learning on a different dataset, and transferring to a different task which shows how the learned CNNs using the proposed method act as better feature extractors.},
	language = {en},
	urldate = {2020-06-23},
	journal = {arXiv:2003.01367 [cs, stat]},
	author = {Sinha, Samarth and Garg, Animesh and Larochelle, Hugo},
	month = mar,
	year = {2020},
	note = {arXiv: 2003.01367},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{dharmaretnam_emergence_2018,
	address = {New Orleans, Louisiana},
	title = {The {Emergence} of {Semantics} in {Neural} {Network} {Representations} of {Visual} {Information}},
	url = {http://aclweb.org/anthology/N18-2122},
	doi = {10.18653/v1/N18-2122},
	abstract = {Word vector models learn about semantics through corpora. Convolutional Neural Networks (CNNs) can learn about semantics through images. At the most abstract level, some of the information in these models must be shared, as they model the same real-world phenomena. Here we employ techniques previously used to detect semantic representations in the human brain to detect semantic representations in CNNs. We show the accumulation of semantic information in the layers of the CNN, and discover that, for misclassiﬁed images, the correct class can be recovered in intermediate layers of a CNN.},
	language = {en},
	urldate = {2020-06-23},
	booktitle = {Proceedings of the 2018 {Conference} of the {North} {American} {Chapter} of           the {Association} for {Computational} {Linguistics}: {Human} {Language}           {Technologies}, {Volume} 2 ({Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Dharmaretnam, Dhanush and Fyshe, Alona},
	year = {2018},
	pages = {776--780},
}

@article{baker_deep_2018,
	title = {Deep convolutional networks do not classify based on global object shape},
	volume = {14},
	issn = {1553-7358},
	url = {https://dx.plos.org/10.1371/journal.pcbi.1006613},
	doi = {10.1371/journal.pcbi.1006613},
	abstract = {Deep convolutional networks (DCNNs) are achieving previously unseen performance in object classification, raising questions about whether DCNNs operate similarly to human vision. In biological vision, shape is arguably the most important cue for recognition. We tested the role of shape information in DCNNs trained to recognize objects. In Experiment 1, we presented a trained DCNN with object silhouettes that preserved overall shape but were filled with surface texture taken from other objects. Shape cues appeared to play some role in the classification of artifacts, but little or none for animals. In Experiments 2–4, DCNNs showed no ability to classify glass figurines or outlines but correctly classified some silhouettes. Aspects of these results led us to hypothesize that DCNNs do not distinguish object’s bounding contours from other edges, and that DCNNs access some local shape features, but not global shape. In Experiment 5, we tested this hypothesis with displays that preserved local features but disrupted global shape, and vice versa. With disrupted global shape, which reduced human accuracy to 28\%, DCNNs gave the same classification labels as with ordinary shapes. Conversely, local contour changes eliminated accurate DCNN classification but caused no difficulty for human observers. These results provide evidence that DCNNs have access to some local shape information in the form of local edge relations, but they have no access to global object shapes.},
	language = {en},
	number = {12},
	urldate = {2020-06-23},
	journal = {PLOS Computational Biology},
	author = {Baker, Nicholas and Lu, Hongjing and Erlikhman, Gennady and Kellman, Philip J.},
	editor = {Einhäuser, Wolfgang},
	month = dec,
	year = {2018},
	pages = {e1006613},
}

@inproceedings{kortylewski_combining_2020,
	address = {Snowmass Village, CO, USA},
	title = {Combining {Compositional} {Models} and {Deep} {Networks} {For} {Robust} {Object} {Classification} under {Occlusion}},
	isbn = {978-1-72816-553-0},
	url = {https://ieeexplore.ieee.org/document/9093560/},
	doi = {10.1109/WACV45572.2020.9093560},
	abstract = {Deep convolutional neural networks (DCNNs) are powerful models that yield impressive results at object classiﬁcation. However, recent work has shown that they do not generalize well to partially occluded objects and to mask attacks. In contrast to DCNNs, compositional models are robust to partial occlusion, however, they are not as discriminative as deep models. In this work, we combine DCNNs and compositional object models to retain the best of both approaches: a discriminative model that is robust to partial occlusion and mask attacks. Our model is learned in two steps. First, a standard DCNN is trained for image classiﬁcation. Subsequently, we cluster the DCNN features into dictionaries. We show that the dictionary components resemble object part detectors and learn the spatial distribution of parts for each object class. We propose mixtures of compositional models to account for large changes in the spatial activation patterns (e.g. due to changes in the 3D pose of an object). At runtime, an image is ﬁrst classiﬁed by the DCNN in a feedforward manner. The prediction uncertainty is used to detect partially occluded objects, which in turn are classiﬁed by the compositional model. Our experimental results demonstrate that combining compositional models and DCNNs resolves a fundamental problem of current deep learning approaches to computer vision: The combined model recognizes occluded objects, even when it has not been exposed to occluded objects during training, while at the same time maintaining high discriminative performance for non-occluded objects.},
	language = {en},
	urldate = {2020-06-23},
	booktitle = {2020 {IEEE} {Winter} {Conference} on {Applications} of {Computer} {Vision} ({WACV})},
	publisher = {IEEE},
	author = {Kortylewski, Adam and Liu, Qing and Wang, Huiyu and Zhang, Zhishuai and Yuille, Alan},
	month = mar,
	year = {2020},
	pages = {1322--1330},
}

@inproceedings{wang_detecting_2017,
	address = {London, UK},
	title = {Detecting {Semantic} {Parts} on {Partially} {Occluded} {Objects}},
	isbn = {978-1-901725-60-5},
	url = {http://www.bmva.org/bmvc/2017/papers/paper073/index.html},
	doi = {10.5244/C.31.73},
	abstract = {In this paper, we address the task of detecting semantic parts on partially occluded objects. We consider a scenario where the model is trained using non-occluded images but tested on occluded images. The motivation is that there are inﬁnite number of occlusion patterns in real world, which cannot be fully covered in the training data. So the models should be inherently robust and adaptive to occlusions instead of ﬁtting / learning the occlusion patterns in the training data. Our approach detects semantic parts by accumulating the conﬁdence of local visual cues. Speciﬁcally, the method uses a simple voting method, based on log-likelihood ratio tests and spatial constraints, to combine the evidence of local cues. These cues are called visual concepts, which are derived by clustering the internal states of deep networks. We evaluate our voting scheme on the VehicleSemanticPart dataset with dense part annotations. We randomly place two, three or four irrelevant objects onto the target object to generate testing images with various occlusions. Experiments show that our algorithm outperforms several competitors in semantic part detection when occlusions are present.},
	language = {en},
	urldate = {2020-06-23},
	booktitle = {Procedings of the {British} {Machine} {Vision} {Conference} 2017},
	publisher = {British Machine Vision Association},
	author = {Wang, Jianyu and Zhang, Zhishuai and Xie, Cihang and Zhu, Jun and Xie, Lingxi and Yuille, Alan},
	year = {2017},
	pages = {73},
}

@article{villalobos_neural_nodate,
	title = {Do {Neural} {Networks} for {Segmentation} {Understand} {Insideness}?},
	abstract = {The insideness problem is an image segmentation modality that consists of determining which pixels are inside and outside a region. Deep Neural Networks (DNNs) excel in segmentation benchmarks, but it is unclear that they have the ability to solve the insideness problem as it requires evaluating longrange spatial dependencies. In this paper, the insideness problem is analysed in isolation, without texture or semantic cues, such that other aspects of segmentation do not interfere in the analysis. We demonstrate that DNNs for segmentation with few units have sufﬁcient complexity to solve insideness for any curve. Yet, such DNNs have severe problems to learn general solutions. Only recurrent networks trained with small images learn solutions that generalize well to almost any curve. Recurrent networks can decompose the evaluation of long-range dependencies into a sequence of local operations, and learning with small images alleviates the common difﬁculties of training recurrent networks with a large number of unrolling steps.},
	language = {en},
	author = {Villalobos, Kimberly and Štih, Vilim and Ahmadinejad, Amineh and Sundaram, Shobhita and Dozier, Jamell and Francl, Andrew and Azevedo, Frederico and Sasaki, Tomotake and Boix, Xavier},
	pages = {25},
}

@article{lindsay_feature-based_2015,
	title = {Feature-based {Attention} in {Convolutional} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1511.06408},
	abstract = {Convolutional neural networks (CNNs) have proven effective for image processing tasks, such as object recognition and classification. Recently, CNNs have been enhanced with concepts of attention, similar to those found in biology. Much of this work on attention has focused on effective serial spatial processing. In this paper, I introduce a simple procedure for applying feature-based attention (FBA) to CNNs and compare multiple implementation options. FBA is a top-down signal applied globally to an input image which aides in detecting chosen objects in cluttered or noisy settings. The concept of FBA and the implementation details tested here were derived from what is known (and debated) about biological object- and feature-based attention. The implementations of FBA described here increase performance on challenging object detection tasks using a procedure that is simple, fast, and does not require additional iterative training. Furthermore, the comparisons performed here suggest that a proposed model of biological FBA (the "feature similarity gain model") is effective in increasing performance.},
	language = {en},
	urldate = {2020-06-23},
	journal = {arXiv:1511.06408 [cs]},
	author = {Lindsay, Grace W.},
	month = dec,
	year = {2015},
	note = {arXiv: 1511.06408},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{mottaghi_role_2014,
	title = {The {Role} of {Context} for {Object} {Detection} and {Semantic} {Segmentation} in the {Wild}},
	url = {https://www.cv-foundation.org/openaccess/content_cvpr_2014/html/Mottaghi_The_Role_of_2014_CVPR_paper.html},
	urldate = {2020-06-23},
	author = {Mottaghi, Roozbeh and Chen, Xianjie and Liu, Xiaobai and Cho, Nam-Gyu and Lee, Seong-Whan and Fidler, Sanja and Urtasun, Raquel and Yuille, Alan},
	year = {2014},
	pages = {891--898},
}

@inproceedings{zhang_making_2019,
	title = {Making {Convolutional} {Networks} {Shift}-{Invariant} {Again}},
	url = {http://proceedings.mlr.press/v97/zhang19a.html},
	abstract = {Modern convolutional networks are not shift-invariant, as small input shifts or translations can cause drastic changes in the output. Commonly used downsampling methods, such as max-pooling, stride...},
	language = {en},
	urldate = {2020-06-23},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	author = {Zhang, Richard},
	month = may,
	year = {2019},
	note = {ISSN: 1938-7228
Section: Machine Learning},
	pages = {7324--7334},
}

@article{azulay_why_2019,
	title = {Why do deep convolutional networks generalize so poorly to small image transformations?},
	volume = {20},
	url = {http://jmlr.org/papers/v20/19-519.html},
	abstract = {Convolutional Neural Networks (CNNs) are commonly assumed to be invariant to small image transformations: either because of the convolutional architecture or because they were trained using data augmentation. Recently, several authors have shown that this is not the case: small translations or rescalings of the input image can drastically change the network’s prediction. In this paper, we quantify this phenomena and ask why neither the convolutional architecture nor data augmentation are suﬃcient to achieve the desired invariance. Speciﬁcally, we show that the convolutional architecture does not give invariance since architectures ignore the classical sampling theorem, and data augmentation does not give invariance because the CNNs learn to be invariant to transformations only for images that are very similar to typical images from the training set. We discuss two possible solutions to this problem: (1) antialiasing the intermediate representations and (2) increasing data augmentation and show that they provide only a partial solution at best. Taken together, our results indicate that the problem of insuring invariance to small image transformations in neural networks while preserving high accuracy remains unsolved.},
	number = {184},
	journal = {Journal of Machine Learning Research},
	author = {Azulay, Aharon and Weiss, Yair},
	year = {2019},
	pages = {1--25},
}

@inproceedings{nayebi_task-driven_2018,
	title = {Task-{Driven} {Convolutional} {Recurrent} {Models} of the {Visual} {System}},
	abstract = {Feed-forward convolutional neural networks (CNNs) are currently state-of-the-art for object classiﬁcation tasks such as ImageNet. Further, they are quantitatively accurate models of temporally-averaged responses of neurons in the primate brain’s visual system. However, biological visual systems have two ubiquitous architectural features not shared with typical CNNs: local recurrence within cortical areas, and long-range feedback from downstream areas to upstream areas. Here we explored the role of recurrence in improving classiﬁcation performance. We found that standard forms of recurrence (vanilla RNNs and LSTMs) do not perform well within deep CNNs on the ImageNet task. In contrast, novel cells that incorporated two structural features, bypassing and gating, were able to boost task accuracy substantially. We extended these design principles in an automated search over thousands of model architectures, which identiﬁed novel local recurrent cells and long-range feedback connections useful for object recognition. Moreover, these task-optimized ConvRNNs matched the dynamics of neural activity in the primate visual system better than feedforward networks, suggesting a role for the brain’s recurrent connections in performing difﬁcult visual behaviors.},
	language = {en},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Nayebi, Aran and Bear, Daniel and Kubilius, Jonas and Kar, Kohitij and Ganguli, Surya and Sussillo, David and DiCarlo, James J and Yamins, Daniel L},
	year = {2018},
	pages = {12},
}

@inproceedings{zamir_feedback_2017,
	title = {Feedback {Networks}},
	url = {http://openaccess.thecvf.com/content_cvpr_2017/html/Zamir_Feedback_Networks_CVPR_2017_paper.html},
	urldate = {2020-06-23},
	booktitle = {Proceedings of the {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Zamir, Amir R. and Wu, Te-Lin and Sun, Lin and Shen, William B. and Shi, Bertram E. and Malik, Jitendra and Savarese, Silvio},
	year = {2017},
	pages = {1308--1317},
}

@article{gilbert_top-down_2013,
	title = {Top-down influences on visual processing},
	volume = {14},
	copyright = {2013 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1471-0048},
	url = {https://www.nature.com/articles/nrn3476},
	doi = {10.1038/nrn3476},
	abstract = {Vision is an active process. Higher-order cognitive influences, including attention, expectation and perceptual task, as well as motor signals, are fed into the sensory apparatus. This enables neurons to dynamically tune their receptive field properties to carry information that is relevant for executing the current behavioural tasks.},
	language = {en},
	number = {5},
	urldate = {2020-06-23},
	journal = {Nature Reviews Neuroscience},
	author = {Gilbert, Charles D. and Li, Wu},
	month = may,
	year = {2013},
	note = {Number: 5
Publisher: Nature Publishing Group},
	pages = {350--363},
}

@article{rajaei_beyond_2019,
	title = {Beyond core object recognition: {Recurrent} processes account for object recognition under occlusion},
	volume = {15},
	issn = {1553-7358},
	shorttitle = {Beyond core object recognition},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1007001},
	doi = {10.1371/journal.pcbi.1007001},
	abstract = {Core object recognition, the ability to rapidly recognize objects despite variations in their appearance, is largely solved through the feedforward processing of visual information. Deep neural networks are shown to achieve human-level performance in these tasks, and explain the primate brain representation. On the other hand, object recognition under more challenging conditions (i.e. beyond the core recognition problem) is less characterized. One such example is object recognition under occlusion. It is unclear to what extent feedforward and recurrent processes contribute in object recognition under occlusion. Furthermore, we do not know whether the conventional deep neural networks, such as AlexNet, which were shown to be successful in solving core object recognition, can perform similarly well in problems that go beyond the core recognition. Here, we characterize neural dynamics of object recognition under occlusion, using magnetoencephalography (MEG), while participants were presented with images of objects with various levels of occlusion. We provide evidence from multivariate analysis of MEG data, behavioral data, and computational modelling, demonstrating an essential role for recurrent processes in object recognition under occlusion. Furthermore, the computational model with local recurrent connections, used here, suggests a mechanistic explanation of how the human brain might be solving this problem.},
	language = {en},
	number = {5},
	urldate = {2020-06-23},
	journal = {PLOS Computational Biology},
	author = {Rajaei, Karim and Mohsenzadeh, Yalda and Ebrahimpour, Reza and Khaligh-Razavi, Seyed-Mahdi},
	month = may,
	year = {2019},
	note = {Publisher: Public Library of Science},
	keywords = {Behavior, Human performance, Information processing, Magnetoencephalography, Recurrent neural networks, Support vector machines, Vision, Visual object recognition},
	pages = {e1007001},
}

@article{rajalingham_large-scale_2018,
	title = {Large-{Scale}, {High}-{Resolution} {Comparison} of the {Core} {Visual} {Object} {Recognition} {Behavior} of {Humans}, {Monkeys}, and {State}-of-the-{Art} {Deep} {Artificial} {Neural} {Networks}},
	volume = {38},
	copyright = {Copyright © 2018 the authors 0270-6474/18/387255-15\$15.00/0},
	issn = {0270-6474, 1529-2401},
	url = {https://www.jneurosci.org/content/38/33/7255},
	doi = {10.1523/JNEUROSCI.0388-18.2018},
	abstract = {Primates, including humans, can typically recognize objects in visual images at a glance despite naturally occurring identity-preserving image transformations (e.g., changes in viewpoint). A primary neuroscience goal is to uncover neuron-level mechanistic models that quantitatively explain this behavior by predicting primate performance for each and every image. Here, we applied this stringent behavioral prediction test to the leading mechanistic models of primate vision (specifically, deep, convolutional, artificial neural networks; ANNs) by directly comparing their behavioral signatures against those of humans and rhesus macaque monkeys. Using high-throughput data collection systems for human and monkey psychophysics, we collected more than one million behavioral trials from 1472 anonymous humans and five male macaque monkeys for 2400 images over 276 binary object discrimination tasks. Consistent with previous work, we observed that state-of-the-art deep, feedforward convolutional ANNs trained for visual categorization (termed DCNNIC models) accurately predicted primate patterns of object-level confusion. However, when we examined behavioral performance for individual images within each object discrimination task, we found that all tested DCNNIC models were significantly nonpredictive of primate performance and that this prediction failure was not accounted for by simple image attributes nor rescued by simple model modifications. These results show that current DCNNIC models cannot account for the image-level behavioral patterns of primates and that new ANN models are needed to more precisely capture the neural mechanisms underlying primate object vision. To this end, large-scale, high-resolution primate behavioral benchmarks such as those obtained here could serve as direct guides for discovering such models.
SIGNIFICANCE STATEMENT Recently, specific feedforward deep convolutional artificial neural networks (ANNs) models have dramatically advanced our quantitative understanding of the neural mechanisms underlying primate core object recognition. In this work, we tested the limits of those ANNs by systematically comparing the behavioral responses of these models with the behavioral responses of humans and monkeys at the resolution of individual images. Using these high-resolution metrics, we found that all tested ANN models significantly diverged from primate behavior. Going forward, these high-resolution, large-scale primate behavioral benchmarks could serve as direct guides for discovering better ANN models of the primate visual system.},
	language = {en},
	number = {33},
	urldate = {2020-06-23},
	journal = {Journal of Neuroscience},
	author = {Rajalingham, Rishi and Issa, Elias B. and Bashivan, Pouya and Kar, Kohitij and Schmidt, Kailyn and DiCarlo, James J.},
	month = aug,
	year = {2018},
	pmid = {30006365},
	note = {Publisher: Society for Neuroscience
Section: Research Articles},
	keywords = {deep neural network, human, monkey, object recognition, vision},
	pages = {7255--7269},
}

@article{dicarlo_how_2012,
	title = {How {Does} the {Brain} {Solve} {Visual} {Object} {Recognition}?},
	volume = {73},
	issn = {0896-6273},
	url = {http://www.sciencedirect.com/science/article/pii/S089662731200092X},
	doi = {10.1016/j.neuron.2012.01.010},
	abstract = {Mounting evidence suggests that ‘core object recognition,’ the ability to rapidly recognize objects despite substantial appearance variation, is solved in the brain via a cascade of reflexive, largely feedforward computations that culminate in a powerful neuronal representation in the inferior temporal cortex. However, the algorithm that produces this solution remains poorly understood. Here we review evidence ranging from individual neurons and neuronal populations to behavior and computational models. We propose that understanding this algorithm will require using neuronal and psychophysical data to sift through many computational models, each based on building blocks of small, canonical subnetworks with a common functional goal.},
	language = {en},
	number = {3},
	urldate = {2020-06-23},
	journal = {Neuron},
	author = {DiCarlo, James J. and Zoccolan, Davide and Rust, Nicole C.},
	month = feb,
	year = {2012},
	pages = {415--434},
}

@article{pinto_why_2008,
	title = {Why is {Real}-{World} {Visual} {Object} {Recognition} {Hard}?},
	volume = {4},
	issn = {1553-7358},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.0040027},
	doi = {10.1371/journal.pcbi.0040027},
	abstract = {Progress in understanding the brain mechanisms underlying vision requires the construction of computational models that not only emulate the brain's anatomy and physiology, but ultimately match its performance on visual tasks. In recent years, “natural” images have become popular in the study of vision and have been used to show apparently impressive progress in building such models. Here, we challenge the use of uncontrolled “natural” images in guiding that progress. In particular, we show that a simple V1-like model—a neuroscientist's “null” model, which should perform poorly at real-world visual object recognition tasks—outperforms state-of-the-art object recognition systems (biologically inspired and otherwise) on a standard, ostensibly natural image recognition test. As a counterpoint, we designed a “simpler” recognition test to better span the real-world variation in object pose, position, and scale, and we show that this test correctly exposes the inadequacy of the V1-like model. Taken together, these results demonstrate that tests based on uncontrolled natural images can be seriously misleading, potentially guiding progress in the wrong direction. Instead, we reexamine what it means for images to be natural and argue for a renewed focus on the core problem of object recognition—real-world image variation.},
	language = {en},
	number = {1},
	urldate = {2020-06-23},
	journal = {PLOS Computational Biology},
	author = {Pinto, Nicolas and Cox, David D. and DiCarlo, James J.},
	month = jan,
	year = {2008},
	note = {Publisher: Public Library of Science},
	keywords = {Grayscale, Human performance, Imaging techniques, Object recognition, Primates, Principal component analysis, Support vector machines, Vision},
	pages = {e27},
}

@inproceedings{koporec_deep_2019,
	address = {Seoul, Korea (South)},
	title = {Deep {Learning} {Performance} in the {Presence} of {Significant} {Occlusions} - {An} {Intelligent} {Household} {Refrigerator} {Case}},
	isbn = {978-1-72815-023-9},
	url = {https://ieeexplore.ieee.org/document/9022381/},
	doi = {10.1109/ICCVW.2019.00310},
	abstract = {Real-world environments, inhabited by people, still pose signiﬁcant challenges to deep learning methods. Object occlusion is one of such problems. Humans deal with the occlusion in a complex way, by changing the viewpoint and using hands to manipulate the scene. However, not all robotic systems can do that due to cost or design constraints. The question we address in this paper is, how well modern object detection methods work on a model case of an intelligent household refrigerator, where numerous occlusions occur. To motivate our research, we actually performed a worldwide survey of refrigerator occupancy to realistically judge the extent of the problem, but the results could be generalized to any unstructured storage environment where people are in charge. The survey results enabled us to generate a dataset of photo-realistic renderings of a typical refrigerator interior, where the object identity, location, and the degree of the refrigerator occupancy are all readily available. Our results are represented as the Average Precision depending on a refrigerator occupancy for two well known deep models.},
	language = {en},
	urldate = {2020-06-15},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} {Workshop} ({ICCVW})},
	publisher = {IEEE},
	author = {Koporec, Gregor and Pers, Janez},
	month = oct,
	year = {2019},
	keywords = {dataset},
	pages = {2532--2540},
}

@inproceedings{geirhos_generalisation_2018,
	title = {Generalisation in humans and deep neural networks},
	abstract = {We compare the robustness of humans and current convolutional deep neural networks (DNNs) on object recognition under twelve different types of image degradations. First, using three well known DNNs (ResNet-152, VGG-19, GoogLeNet) we ﬁnd the human visual system to be more robust to nearly all of the tested image manipulations, and we observe progressively diverging classiﬁcation error-patterns between humans and DNNs when the signal gets weaker. Secondly, we show that DNNs trained directly on distorted images consistently surpass human performance on the exact distortion types they were trained on, yet they display extremely poor generalisation abilities when tested on other distortion types. For example, training on salt-and-pepper noise does not imply robustness on uniform white noise and vice versa. Thus, changes in the noise distribution between training and testing constitutes a crucial challenge to deep learning vision systems that can be systematically addressed in a lifelong machine learning approach. Our new dataset consisting of 83K carefully measured human psychophysical trials provide a useful reference for lifelong robustness against image degradations set by the human visual system.},
	language = {en},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Geirhos, Robert and Temme, Carlos R M and Rauber, Jonas and Schütt, Heiko H and Bethge, Matthias and Wichmann, Felix A},
	year = {2018},
	keywords = {dataset},
	pages = {13},
}

@inproceedings{recht_imagenet_2019,
	title = {Do {ImageNet} {Classiﬁers} {Generalize} to {ImageNet}?},
	abstract = {We build new test sets for the CIFAR-10 and ImageNet datasets. Both benchmarks have been the focus of intense research for almost a decade, raising the danger of overﬁtting to excessively re-used test sets. By closely following the original dataset creation processes, we test to what extent current classiﬁcation models generalize to new data. We evaluate a broad range of models and ﬁnd accuracy drops of 3\% – 15\% on CIFAR-10 and 11\% – 14\% on ImageNet. However, accuracy gains on the original test sets translate to larger gains on the new test sets. Our results suggest that the accuracy drops are not caused by adaptivity, but by the models’ inability to generalize to slightly “harder” images than those found in the original test sets.},
	language = {en},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	author = {Recht, Benjamin and Roelofs, Rebecca and Schmidt, Ludwig and Shankar, Vaishaal},
	year = {2019},
	pages = {12},
}

@article{li_learning_2018,
	title = {Learning with rethinking: {Recurrently} improving convolutional neural networks through feedback},
	volume = {79},
	issn = {0031-3203},
	shorttitle = {Learning with rethinking},
	url = {http://www.sciencedirect.com/science/article/pii/S0031320318300153},
	doi = {10.1016/j.patcog.2018.01.015},
	abstract = {Recent years have witnessed the great success of convolutional neural network (CNN) based models in the field of computer vision. CNN is able to learn hierarchically abstracted features from images in an end-to-end training manner. However, most of the existing CNN models only learn features through a feedforward structure and no feedback information from top to bottom layers is exploited to enable the networks to refine themselves. In this paper, we propose a Learning with Rethinking algorithm. By adding a feedback layer and producing the emphasis vector, the model is able to recurrently boost the performance based on previous prediction. Particularly, it can be employed to boost any pre-trained models. This algorithm is tested on four object classification benchmark datasets: CIFAR-100, CIFAR-10, MNIST-background-image and ILSVRC-2012 dataset, and the results have demonstrated the advantage of training CNN models with the proposed feedback mechanism.},
	language = {en},
	urldate = {2020-06-23},
	journal = {Pattern Recognition},
	author = {Li, Xin and Jie, Zequn and Feng, Jiashi and Liu, Changsong and Yan, Shuicheng},
	month = jul,
	year = {2018},
	keywords = {Convolutional neural network, Deep learning, Image classification},
	pages = {183--194},
}

@book{poggio_visual_2016,
	title = {Visual {Cortex} and {Deep} {Networks}: {Learning} {Invariant} {Representations}},
	isbn = {978-0-262-03472-2},
	shorttitle = {Visual {Cortex} and {Deep} {Networks}},
	abstract = {A mathematical framework that describes learning of invariant representations in the ventral stream, offering both theoretical development and applications.The ventral visual stream is believed to underlie object recognition in primates. Over the past fifty years, researchers have developed a series of quantitative models that are increasingly faithful to the biological architecture. Recently, deep learning convolution networks—which do not reflect several important features of the ventral stream architecture and physiology—have been trained with extremely large datasets, resulting in model neurons that mimic object recognition but do not explain the nature of the computations carried out in the ventral stream. This book develops a mathematical framework that describes learning of invariant representations of the ventral stream and is particularly relevant to deep convolutional learning networks. The authors propose a theory based on the hypothesis that the main computational goal of the ventral stream is to compute neural representations of images that are invariant to transformations commonly encountered in the visual environment and are learned from unsupervised experience. They describe a general theoretical framework of a computational theory of invariance (with details and proofs offered in appendixes) and then review the application of the theory to the feedforward path of the ventral stream in the primate visual cortex.},
	language = {en},
	publisher = {MIT Press},
	author = {Poggio, Tomaso and Anselmi, Fabio},
	month = sep,
	year = {2016},
	note = {Google-Books-ID: RP8WDQAAQBAJ},
	keywords = {Computers / Intelligence (AI) \& Semantics, Science / Life Sciences / Neuroscience},
}

@article{tramer_space_2017,
	title = {The {Space} of {Transferable} {Adversarial} {Examples}},
	url = {http://arxiv.org/abs/1704.03453},
	abstract = {Adversarial examples are maliciously perturbed inputs designed to mislead machine learning (ML) models at test-time. They often transfer: the same adversarial example fools more than one model.},
	language = {en},
	urldate = {2020-06-15},
	journal = {arXiv:1704.03453 [cs, stat]},
	author = {Tramèr, Florian and Papernot, Nicolas and Goodfellow, Ian and Boneh, Dan and McDaniel, Patrick},
	month = may,
	year = {2017},
	keywords = {Computer Science - Cryptography and Security, Statistics - Machine Learning},
}

@inproceedings{lipton_mythos_2017,
	title = {The {Mythos} of {Model} {Interpretability}},
	url = {http://arxiv.org/abs/1606.03490},
	abstract = {Supervised machine learning models boast remarkable predictive capabilities. But can you trust your model? Will it work in deployment? What else can it tell you about the world? We want models to be not only good, but interpretable. And yet the task of interpretation appears underspeciﬁed. Papers provide diverse and sometimes non-overlapping motivations for interpretability, and offer myriad notions of what attributes render models interpretable. Despite this ambiguity, many papers proclaim interpretability axiomatically, absent further explanation. In this paper, we seek to reﬁne the discourse on interpretability. First, we examine the motivations underlying interest in interpretability, ﬁnding them to be diverse and occasionally discordant. Then, we address model properties and techniques thought to confer interpretability, identifying transparency to humans and post-hoc explanations as competing notions. Throughout, we discuss the feasibility and desirability of different notions, and question the oft-made assertions that linear models are interpretable and that deep neural networks are not.},
	language = {en},
	urldate = {2020-06-16},
	booktitle = {{ICML} {Workshop} on {Human} {Interpretability} in {Machine} {Learning}},
	author = {Lipton, Zachary C.},
	year = {2017},
	keywords = {Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{garcez_neural-symbolic_2019,
	title = {Neural-{Symbolic} {Computing}: {An} {Effective} {Methodology} for {Principled} {Integration} of {Machine} {Learning} and {Reasoning}},
	volume = {6},
	shorttitle = {Neural-{Symbolic} {Computing}},
	url = {http://arxiv.org/abs/1905.06088},
	abstract = {Current advances in Artiﬁcial Intelligence and machine learning in general, and deep learning in particular have reached unprecedented impact not only across research communities, but also over popular media channels. However, concerns about interpretability and accountability of AI have been raised by inﬂuential thinkers. In spite of the recent impact of AI, several works have identiﬁed the need for principled knowledge representation and reasoning mechanisms integrated with deep learning-based systems to provide sound and explainable models for such systems. Neural-symbolic computing aims at integrating, as foreseen by Valiant, two most fundamental cognitive abilities: the ability to learn from the environment, and the ability to reason from what has been learned. Neural-symbolic computing has been an active topic of research for many years, reconciling the advantages of robust learning in neural networks and reasoning and interpretability of symbolic representation. In this paper, we survey recent accomplishments of neural-symbolic computing as a principled methodology for integrated machine learning and reasoning. We illustrate the eﬀectiveness of the approach by outlining the main characteristics of the methodology: principled integration of neural learning with symbolic knowledge representation and reasoning allowing for the construction of explainable AI systems. The insights provided by neural-symbolic computing shed new light on the increasingly prominent need for interpretable and accountable AI systems.},
	language = {en},
	number = {4},
	urldate = {2020-06-14},
	journal = {Journal of Applied Logics},
	author = {Garcez, Artur d'Avila and Gori, Marco and Lamb, Luis C. and Serafini, Luciano and Spranger, Michael and Tran, Son N.},
	month = may,
	year = {2019},
	pages = {611--631},
}

@incollection{escalante_considerations_2018,
	title = {Considerations for {Evaluation} and {Generalization} in {Interpretable} {Machine} {Learning}},
	isbn = {978-3-319-98130-7 978-3-319-98131-4},
	url = {http://link.springer.com/10.1007/978-3-319-98131-4_1},
	language = {en},
	urldate = {2020-06-16},
	booktitle = {Explainable and {Interpretable} {Models} in {Computer} {Vision} and {Machine} {Learning}},
	publisher = {Springer International Publishing},
	author = {Doshi-Velez, Finale and Kim, Been},
	editor = {Escalante, Hugo Jair and Escalera, Sergio and Guyon, Isabelle and Baró, Xavier and Güçlütürk, Yağmur and Güçlü, Umut and van Gerven, Marcel},
	year = {2018},
	doi = {10.1007/978-3-319-98131-4_1},
	note = {Series Title: The Springer Series on Challenges in Machine Learning},
	pages = {3--17},
}

@inproceedings{ribeiro_why_2016,
	address = {San Francisco, California, USA},
	series = {{KDD} '16},
	title = {"{Why} {Should} {I} {Trust} {You}?": {Explaining} the {Predictions} of {Any} {Classifier}},
	isbn = {978-1-4503-4232-2},
	shorttitle = {"{Why} {Should} {I} {Trust} {You}?},
	url = {https://doi.org/10.1145/2939672.2939778},
	doi = {10.1145/2939672.2939778},
	abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally varound the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
	urldate = {2020-06-20},
	booktitle = {Proceedings of the 22nd {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
	month = aug,
	year = {2016},
	keywords = {black box classifier, explaining machine learning, interpretability, interpretable machine learning},
	pages = {1135--1144},
}

@inproceedings{fleet_visualizing_2014,
	title = {Visualizing and {Understanding} {Convolutional} {Networks}},
	volume = {8689},
	isbn = {978-3-319-10589-5 978-3-319-10590-1},
	url = {http://link.springer.com/10.1007/978-3-319-10590-1_53},
	abstract = {Large Convolutional Network models have recently demonstrated impressive classiﬁcation performance on the ImageNet benchmark Krizhevsky et al. [18]. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we explore both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classiﬁer. Used in a diagnostic role, these visualizations allow us to ﬁnd model architectures that outperform Krizhevsky et al. on the ImageNet classiﬁcation benchmark. We also perform an ablation study to discover the performance contribution from diﬀerent model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classiﬁer is retrained, it convincingly beats the current state-ofthe-art results on Caltech-101 and Caltech-256 datasets.},
	language = {en},
	urldate = {2020-06-16},
	booktitle = {Proceedings of the {European} {Conference} on {Computer} {Vision} ({ECCV})},
	publisher = {Springer International Publishing},
	author = {Zeiler, Matthew D. and Fergus, Rob},
	editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
	year = {2014},
	pages = {818--833},
}

@inproceedings{koh_understanding_2017,
	title = {Understanding {Black}-box {Predictions} via {Influence} {Functions}},
	url = {http://arxiv.org/abs/1703.04730},
	abstract = {How can we explain the predictions of a blackbox model? In this paper, we use inﬂuence functions — a classic technique from robust statistics — to trace a model’s prediction through the learning algorithm and back to its training data, thereby identifying training points most responsible for a given prediction. To scale up inﬂuence functions to modern machine learning settings, we develop a simple, efﬁcient implementation that requires only oracle access to gradients and Hessian-vector products. We show that even on non-convex and non-differentiable models where the theory breaks down, approximations to inﬂuence functions can still provide valuable information. On linear models and convolutional neural networks, we demonstrate that inﬂuence functions are useful for multiple purposes: understanding model behavior, debugging models, detecting dataset errors, and even creating visuallyindistinguishable training-set attacks.},
	language = {en},
	urldate = {2020-06-16},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Machine} {Learning}},
	author = {Koh, Pang Wei and Liang, Percy},
	year = {2017},
	keywords = {Statistics - Machine Learning},
}

@inproceedings{nguyen_synthesizing_2016,
	title = {Synthesizing the preferred inputs for neurons in neural networks via deep generator networks},
	abstract = {Deep neural networks (DNNs) have demonstrated state-of-the-art results on many pattern recognition tasks, especially vision classiﬁcation problems. Understanding the inner workings of such computational brains is both fascinating basic science that is interesting in its own right—similar to why we study the human brain—and will enable researchers to further improve DNNs. One path to understanding how a neural network functions internally is to study what each of its neurons has learned to detect. One such method is called activation maximization (AM), which synthesizes an input (e.g. an image) that highly activates a neuron. Here we dramatically improve the qualitative state of the art of activation maximization by harnessing a powerful, learned prior: a deep generator network (DGN). The algorithm (1) generates qualitatively state-of-the-art synthetic images that look almost real, (2) reveals the features learned by each neuron in an interpretable way, (3) generalizes well to new datasets and somewhat well to different network architectures without requiring the prior to be relearned, and (4) can be considered as a high-quality generative method (in this case, by generating novel, creative, interesting, recognizable images).},
	language = {en},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Nguyen, Anh and Dosovitskiy, Alexey and Yosinski, Jason and Brox, Thomas and Clune, Jeff},
	year = {2016},
	pages = {9},
}

@inproceedings{springenberg_striving_2015,
	title = {Striving for {Simplicity}: {The} {All} {Convolutional} {Net}},
	shorttitle = {Striving for {Simplicity}},
	url = {http://arxiv.org/abs/1412.6806},
	abstract = {Most modern convolutional neural networks (CNNs) used for object recognition are built using the same principles: Alternating convolution and max-pooling layers followed by a small number of fully connected layers. We re-evaluate the state of the art for object recognition from small images with convolutional networks, questioning the necessity of different components in the pipeline. We ﬁnd that max-pooling can simply be replaced by a convolutional layer with increased stride without loss in accuracy on several image recognition benchmarks. Following this ﬁnding – and building on other recent work for ﬁnding simple network structures – we propose a new architecture that consists solely of convolutional layers and yields competitive or state of the art performance on several object recognition datasets (CIFAR-10, CIFAR-100, ImageNet). To analyze the network we introduce a new variant of the “deconvolution approach” for visualizing features learned by CNNs, which can be applied to a broader range of network structures than existing approaches.},
	language = {en},
	urldate = {2020-06-16},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Springenberg, Jost Tobias and Dosovitskiy, Alexey and Brox, Thomas and Riedmiller, Martin},
	year = {2015},
	keywords = {Computer Science - Neural and Evolutionary Computing},
}

@inproceedings{smilkov_smoothgrad_2017,
	title = {{SmoothGrad}: removing noise by adding noise},
	shorttitle = {{SmoothGrad}},
	url = {http://arxiv.org/abs/1706.03825},
	abstract = {Explaining the output of a deep network remains a challenge. In the case of an image classiﬁer, one type of explanation is to identify pixels that strongly inﬂuence the ﬁnal decision. A starting point for this strategy is the gradient of the class score function with respect to the input image. This gradient can be interpreted as a sensitivity map, and there are several techniques that elaborate on this basic idea. This paper makes two contributions: it introduces SMOOTHGRAD, a simple method that can help visually sharpen gradient-based sensitivity maps, and it discusses lessons in the visualization of these maps. We publish the code for our experiments and a website with our results.},
	language = {en},
	urldate = {2020-06-16},
	booktitle = {{ICML} {Workshop} on {Visualization} for {Deep} {Learning}},
	author = {Smilkov, Daniel and Thorat, Nikhil and Kim, Been and Viégas, Fernanda and Wattenberg, Martin},
	year = {2017},
	keywords = {Statistics - Machine Learning},
}

@inproceedings{bau_network_2017,
	title = {Network {Dissection}: {Quantifying} {Interpretability} of {Deep} {Visual} {Representations}},
	shorttitle = {Network {Dissection}},
	url = {http://openaccess.thecvf.com/content_cvpr_2017/html/Bau_Network_Dissection_Quantifying_CVPR_2017_paper.html},
	urldate = {2020-06-21},
	author = {Bau, David and Zhou, Bolei and Khosla, Aditya and Oliva, Aude and Torralba, Antonio},
	year = {2017},
	pages = {6541--6549},
}

@inproceedings{shrikumar_learning_2017,
	title = {Learning {Important} {Features} {Through} {Propagating} {Activation} {Differences}},
	url = {http://proceedings.mlr.press/v70/shrikumar17a.html},
	abstract = {The purported “black box” nature of neural networks is a barrier to adoption in applications where interpretability is essential. Here we present DeepLIFT (Deep Learning Important FeaTures), a meth...},
	language = {en},
	urldate = {2020-06-21},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Machine} {Learning}},
	author = {Shrikumar, Avanti and Greenside, Peyton and Kundaje, Anshul},
	month = jul,
	year = {2017},
	note = {ISSN: 1938-7228
Section: Machine Learning},
	pages = {3145--3153},
}

@article{zhou_interpreting_2019,
	title = {Interpreting {Deep} {Visual} {Representations} via {Network} {Dissection}},
	volume = {41},
	issn = {0162-8828, 2160-9292, 1939-3539},
	url = {https://ieeexplore.ieee.org/document/8417924/},
	doi = {10.1109/TPAMI.2018.2858759},
	abstract = {The success of recent deep convolutional neural networks (CNNs) depends on learning hidden representations that can summarize the important factors of variation behind the data. In this work, we describe Network Dissection, a method that interprets networks by providing meaningful labels to their individual units. The proposed method quantiﬁes the interpretability of CNN representations by evaluating the alignment between individual hidden units and visual semantic concepts. By identifying the best alignments, units are given interpretable labels ranging from colors, materials, textures, parts, objects and scenes. The method reveals that deep representations are more transparent and interpretable than they would be under a random equivalently powerful basis. We apply our approach to interpret and compare the latent representations of several network architectures trained to solve a wide range of supervised and self-supervised tasks. We then examine factors affecting the network interpretability such as the number of the training iterations, regularizations, different initialization parameters, as well as networks depth and width. Finally we show that the interpreted units can be used to provide explicit explanations of a given CNN prediction for an image. Our results highlight that interpretability is an important property of deep neural networks that provides new insights into what hierarchical structures can learn.},
	language = {en},
	number = {9},
	urldate = {2020-06-21},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Zhou, Bolei and Bau, David and Oliva, Aude and Torralba, Antonio},
	month = sep,
	year = {2019},
	pages = {2131--2145},
}

@inproceedings{zhou_learning_2016,
	title = {Learning {Deep} {Features} for {Discriminative} {Localization}},
	url = {https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Zhou_Learning_Deep_Features_CVPR_2016_paper.html},
	urldate = {2020-06-21},
	booktitle = {Proceedings of the {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Zhou, Bolei and Khosla, Aditya and Lapedriza, Agata and Oliva, Aude and Torralba, Antonio},
	year = {2016},
	pages = {2921--2929},
}

@article{baehrens_how_2010,
	title = {How to {Explain} {Individual} {Classification} {Decisions}},
	volume = {11},
	url = {http://jmlr.org/papers/v11/baehrens10a.html},
	abstract = {After building a classiﬁer with modern tools of machine learning we typically have a black box at hand that is able to predict well for unseen data. Thus, we get an answer to the question what is the most likely label of a given unseen data point. However, most methods will provide no answer why the model predicted a particular label for a single instance and what features were most inﬂuential for that particular instance. The only method that is currently able to provide such explanations are decision trees. This paper proposes a procedure which (based on a set of assumptions) allows to explain the decisions of any classiﬁcation method.},
	number = {61},
	journal = {Journal of Machine Learning Research},
	author = {Baehrens, David and Schroeter, Timon and Harmeling, Stefan and Kawanabe, Motoaki and Hansen, Katja and Müller, Klaus-Robert},
	year = {2010},
	pages = {1803--1831},
}

@inproceedings{selvaraju_grad-cam_2017,
	title = {Grad-{CAM}: {Visual} {Explanations} {From} {Deep} {Networks} via {Gradient}-{Based} {Localization}},
	shorttitle = {Grad-{CAM}},
	url = {http://openaccess.thecvf.com/content_iccv_2017/html/Selvaraju_Grad-CAM_Visual_Explanations_ICCV_2017_paper.html},
	urldate = {2020-06-21},
	booktitle = {Proceedings of the {IEEE} {International} {Conference} on {Computer} {Vision}},
	author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
	year = {2017},
	pages = {618--626},
}

@inproceedings{lundberg_unified_2017,
	title = {A {Unified} {Approach} to {Interpreting} {Model} {Predictions}},
	abstract = {Understanding why a model makes a certain prediction can be as crucial as the prediction’s accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a uniﬁed framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identiﬁcation of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class uniﬁes six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this uniﬁcation, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.},
	language = {en},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Lundberg, Scott M and Lee, Su-In},
	year = {2017},
	pages = {10},
}

@inproceedings{sundararajan_axiomatic_2017,
	title = {Axiomatic {Attribution} for {Deep} {Networks}},
	url = {http://proceedings.mlr.press/v70/sundararajan17a.html},
	abstract = {We study the problem of attributing the prediction of a deep network to its input features, a problem previously studied by several other works. We identify two fundamental axioms—Sensitivity and I...},
	language = {en},
	urldate = {2020-06-21},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Machine} {Learning}},
	author = {Sundararajan, Mukund and Taly, Ankur and Yan, Qiqi},
	month = jul,
	year = {2017},
	pages = {3319--3328},
}

@inproceedings{hendricks_women_2018,
	address = {Cham},
	title = {Women {Also} {Snowboard}: {Overcoming} {Bias} in {Captioning} {Models}},
	volume = {11207},
	isbn = {978-3-030-01218-2 978-3-030-01219-9},
	shorttitle = {Women {Also} {Snowboard}},
	url = {http://link.springer.com/10.1007/978-3-030-01219-9_47},
	doi = {10.1007/978-3-030-01219-9_47},
	language = {en},
	urldate = {2020-06-16},
	booktitle = {Proceedings of the {European} {Conference} on {Computer} {Vision} ({ECCV})},
	publisher = {Springer International Publishing},
	author = {Hendricks, Lisa Anne and Burns, Kaylee and Saenko, Kate and Darrell, Trevor and Rohrbach, Anna},
	year = {2018},
	pages = {793--811},
}

@inproceedings{donadello_logic_2017,
	title = {Logic {Tensor} {Networks} for {Semantic} {Image} {Interpretation}},
	url = {http://arxiv.org/abs/1705.08968},
	abstract = {Semantic Image Interpretation (SII) is the task of extracting structured semantic descriptions from images. It is widely agreed that the combined use of visual data and background knowledge is of great importance for SII. Recently, Statistical Relational Learning (SRL) approaches have been developed for reasoning under uncertainty and learning in the presence of data and rich knowledge. Logic Tensor Networks (LTNs) are an SRL framework which integrates neural networks with ﬁrst-order fuzzy logic to allow (i) efﬁcient learning from noisy data in the presence of logical constraints, and (ii) reasoning with logical formulas describing general properties of the data. In this paper, we develop and apply LTNs to two of the main tasks of SII, namely, the classiﬁcation of an image’s bounding boxes and the detection of the relevant part-of relations between objects. To the best of our knowledge, this is the ﬁrst successful application of SRL to such SII tasks. The proposed approach is evaluated on a standard image processing benchmark. Experiments show that the use of background knowledge in the form of logical constraints can improve the performance of purely data-driven approaches, including the state-of-the-art Fast Region-based Convolutional Neural Networks (Fast R-CNN). Moreover, we show that the use of logical background knowledge adds robustness to the learning system when errors are present in the labels of the training data.},
	language = {en},
	urldate = {2020-06-14},
	booktitle = {Proceedings of the {Twenty}-{Sixth} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	author = {Donadello, Ivan and Serafini, Luciano and Garcez, Artur d'Avila},
	month = may,
	year = {2017},
	note = {arXiv: 1705.08968},
}

@inproceedings{hooker_benchmark_2019,
	title = {A {Benchmark} for {Interpretability} {Methods} in {Deep} {Neural} {Networks}},
	abstract = {We propose an empirical measure of the approximate accuracy of feature importance estimates in deep neural networks. Our results across several large-scale image classiﬁcation datasets show that many popular interpretability methods produce estimates of feature importance that are not better than a random designation of feature importance. Only certain ensemble based approaches—VarGrad and SmoothGrad-Squared—outperform such a random assignment of importance. The manner of ensembling remains critical, we show that some approaches do no better then the underlying method but carry a far higher computational burden.},
	language = {en},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Hooker, Sara and Erhan, Dumitru and Kindermans, Pieter-Jan and Kim, Been},
	year = {2019},
	pages = {12},
}

@inproceedings{alvarez-melis_robustness_2018,
	title = {On the {Robustness} of {Interpretability} {Methods}},
	url = {http://arxiv.org/abs/1806.08049},
	abstract = {We argue that robustness of explanations---i.e., that similar inputs should give rise to similar explanations---is a key desideratum for interpretability. We introduce metrics to quantify robustness and demonstrate that current methods do not perform well according to these metrics. Finally, we propose ways that robustness can be enforced on existing interpretability approaches.},
	language = {en},
	urldate = {2020-06-16},
	booktitle = {{ICML} {Workshop} on {Human} {Interpretability} in {Machine} {Learning} ({WHI} 2018)},
	author = {Alvarez-Melis, David and Jaakkola, Tommi S.},
	year = {2018},
	keywords = {Statistics - Machine Learning},
}

@inproceedings{strout_human_2019,
	address = {Florence, Italy},
	title = {Do {Human} {Rationales} {Improve} {Machine} {Explanations}?},
	url = {https://www.aclweb.org/anthology/W19-4807},
	doi = {10.18653/v1/W19-4807},
	abstract = {Work on “learning with rationales” shows that humans providing explanations to a machine learning system can improve the system's predictive accuracy. However, this work has not been connected to work in “explainable AI” which concerns machines explaining their reasoning to humans. In this work, we show that learning with rationales can also improve the quality of the machine's explanations as evaluated by human judges. Specifically, we present experiments showing that, for CNN-based text classification, explanations generated using “supervised attention” are judged superior to explanations generated using normal unsupervised attention.},
	urldate = {2020-06-21},
	booktitle = {Proceedings of the 2019 {ACL} {Workshop} {BlackboxNLP}: {Analyzing} and {Interpreting} {Neural} {Networks} for {NLP}},
	publisher = {Association for Computational Linguistics},
	author = {Strout, Julia and Zhang, Ye and Mooney, Raymond},
	month = aug,
	year = {2019},
	pages = {56--62},
}

@article{tang_recurrent_2018,
	title = {Recurrent computations for visual pattern completion},
	volume = {115},
	copyright = {Copyright © 2018 the Author(s). Published by PNAS.. https://creativecommons.org/licenses/by-nc-nd/4.0/This open access article is distributed under Creative Commons Attribution-NonCommercial-NoDerivatives License 4.0 (CC BY-NC-ND).},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/115/35/8835},
	doi = {10.1073/pnas.1719397115},
	abstract = {Making inferences from partial information constitutes a critical aspect of cognition. During visual perception, pattern completion enables recognition of poorly visible or occluded objects. We combined psychophysics, physiology, and computational models to test the hypothesis that pattern completion is implemented by recurrent computations and present three pieces of evidence that are consistent with this hypothesis. First, subjects robustly recognized objects even when they were rendered {\textless}15\% visible, but recognition was largely impaired when processing was interrupted by backward masking. Second, invasive physiological responses along the human ventral cortex exhibited visually selective responses to partially visible objects that were delayed compared with whole objects, suggesting the need for additional computations. These physiological delays were correlated with the effects of backward masking. Third, state-of-the-art feed-forward computational architectures were not robust to partial visibility. However, recognition performance was recovered when the model was augmented with attractor-based recurrent connectivity. The recurrent model was able to predict which images of heavily occluded objects were easier or harder for humans to recognize, could capture the effect of introducing a backward mask on recognition behavior, and was consistent with the physiological delays along the human ventral visual stream. These results provide a strong argument of plausibility for the role of recurrent computations in making visual inferences from partial information.},
	language = {en},
	number = {35},
	urldate = {2020-06-21},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Tang, Hanlin and Schrimpf, Martin and Lotter, William and Moerman, Charlotte and Paredes, Ana and Caro, Josue Ortega and Hardesty, Walter and Cox, David and Kreiman, Gabriel},
	month = aug,
	year = {2018},
	pmid = {30104363},
	note = {Publisher: National Academy of Sciences
Section: Biological Sciences},
	keywords = {artificial intelligence, computational neuroscience, machine learning, pattern completion, visual object recognition},
	pages = {8835--8840},
}

@inproceedings{kubilius_brain-like_2019,
	title = {Brain-{Like} {Object} {Recognition} with {High}-{Performing} {Shallow} {Recurrent} {ANNs}},
	url = {http://arxiv.org/abs/1909.06161},
	abstract = {Deep convolutional artiﬁcial neural networks (ANNs) are the leading class of candidate models of the mechanisms of visual processing in the primate ventral stream. While initially inspired by brain anatomy, over the past years, these ANNs have evolved from a simple eight-layer architecture in AlexNet to extremely deep and branching architectures, demonstrating increasingly better object categorization performance, yet bringing into question how brain-like they still are. In particular, typical deep models from the machine learning community are often hard to map onto the brain’s anatomy due to their vast number of layers and missing biologically-important connections, such as recurrence. Here we demonstrate that better anatomical alignment to the brain and high performance on machine learning as well as neuroscience measures do not have to be in contradiction. We developed CORnet-S, a shallow ANN with four anatomically mapped areas and recurrent connectivity, guided by Brain-Score, a new large-scale composite of neural and behavioral benchmarks for quantifying the functional ﬁdelity of models of the primate ventral visual stream. Despite being signiﬁcantly shallower than most models, CORnet-S is the top model on Brain-Score and outperforms similarly compact models on ImageNet. Moreover, our extensive analyses of CORnet-S circuitry variants reveal that recurrence is the main predictive factor of both BrainScore and ImageNet top-1 performance. Finally, we report that the temporal evolution of the CORnet-S "IT" neural population resembles the actual monkey IT population dynamics. Taken together, these results establish CORnet-S, a compact, recurrent ANN, as the current best model of the primate ventral visual stream.},
	language = {en},
	urldate = {2020-06-14},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Kubilius, Jonas and Schrimpf, Martin and Kar, Kohitij and Rajalingham, Rishi and Hong, Ha and Majaj, Najib J. and Issa, Elias B. and Bashivan, Pouya and Prescott-Roy, Jonathan and Schmidt, Kailyn and Nayebi, Aran and Bear, Daniel and Yamins, Daniel L. K. and DiCarlo, James J.},
	year = {2019},
	keywords = {Computer Science - Neural and Evolutionary Computing, Electrical Engineering and Systems Science - Image and Video Processing, Quantitative Biology - Neurons and Cognition},
}

@article{bengio_representation_2013,
	title = {Representation {Learning}: {A} {Review} and {New} {Perspectives}},
	volume = {35},
	issn = {1939-3539},
	shorttitle = {Representation {Learning}},
	doi = {10.1109/TPAMI.2013.50},
	abstract = {The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, autoencoders, manifold learning, and deep networks. This motivates longer term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation, and manifold learning.},
	number = {8},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
	month = aug,
	year = {2013},
	keywords = {AI, Abstracts, Algorithms, Artificial Intelligence, Boltzmann machine, Deep learning, Feature extraction, Humans, Learning systems, Machine learning, Manifolds, Neural Networks (Computer), Neural networks, Speech recognition, artificial intelligence, autoencoder, autoencoders, data representation, data structures, density estimation, feature learning, geometrical connections, machine learning algorithms, manifold learning, neural nets, probabilistic models, probability, representation learning, unsupervised feature learning, unsupervised learning},
	pages = {1798--1828},
}

@article{lecun_deep_2015,
	title = {Deep learning},
	volume = {521},
	copyright = {2015 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature14539},
	doi = {10.1038/nature14539},
	abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
	language = {en},
	number = {7553},
	urldate = {2020-06-21},
	journal = {Nature},
	author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	month = may,
	year = {2015},
	pages = {436--444},
}

@inproceedings{cremers_part_2015,
	title = {Part {Detector} {Discovery} in {Deep} {Convolutional} {Neural} {Networks}},
	volume = {9004},
	isbn = {978-3-319-16807-4 978-3-319-16808-1},
	url = {http://link.springer.com/10.1007/978-3-319-16808-1_12},
	abstract = {Current ﬁne-grained classiﬁcation approaches often rely on a robust localization of object parts to extract localized feature representations suitable for discrimination. However, part localization is a challenging task due to the large variation of appearance and pose. In this paper, we show how pre-trained convolutional neural networks can be used for robust and eﬃcient object part discovery and localization without the necessity to actually train the network on the current dataset. Our approach called “part detector discovery” (PDD) is based on analyzing the gradient maps of the network outputs and ﬁnding activation centers spatially related to annotated semantic parts or bounding boxes. This allows us not just to obtain excellent performance on the CUB200-2011 dataset, but in contrast to previous approaches also to perform detection and bird classiﬁcation jointly without requiring a given bounding box annotation during testing and ground-truth parts during training.},
	language = {en},
	urldate = {2020-06-21},
	booktitle = {Asian {Conference} on {Computer} {Vision}},
	publisher = {Springer International Publishing},
	author = {Simon, Marcel and Rodner, Erik and Denzler, Joachim},
	editor = {Cremers, Daniel and Reid, Ian and Saito, Hideo and Yang, Ming-Hsuan},
	year = {2015},
	pages = {162--177},
}

@inproceedings{morcos_importance_2018,
	title = {On the importance of single directions for generalization},
	url = {http://arxiv.org/abs/1803.06959},
	abstract = {Despite their ability to memorize large datasets, deep neural networks often achieve good generalization performance. However, the differences between the learned solutions of networks which generalize and those which do not remain unclear. Additionally, the tuning properties of single directions (deﬁned as the activation of a single unit or some linear combination of units in response to some input) have been highlighted, but their importance has not been evaluated. Here, we connect these lines of inquiry to demonstrate that a network’s reliance on single directions is a good predictor of its generalization performance, across networks trained on datasets with different fractions of corrupted labels, across ensembles of networks trained on datasets with unmodiﬁed labels, across different hyperparameters, and over the course of training. While dropout only regularizes this quantity up to a point, batch normalization implicitly discourages single direction reliance, in part by decreasing the class selectivity of individual units. Finally, we ﬁnd that class selectivity is a poor predictor of task importance, suggesting not only that networks which generalize well minimize their dependence on individual units by reducing their selectivity, but also that individually selective units may not be necessary for strong network performance.},
	language = {en},
	urldate = {2020-06-16},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Morcos, Ari S. and Barrett, David G. T. and Rabinowitz, Neil C. and Botvinick, Matthew},
	year = {2018},
	keywords = {Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@inproceedings{zhou_object_2015,
	title = {Object {Detectors} {Emerge} in {Deep} {Scene} {CNNs}},
	url = {http://arxiv.org/abs/1412.6856},
	abstract = {With the success of new computational architectures for visual processing, such as convolutional neural networks (CNN) and access to image databases with millions of labeled examples (e.g., ImageNet, Places), the state of the art in computer vision is advancing rapidly. One important factor for continued progress is to understand the representations that are learned by the inner layers of these deep architectures. Here we show that object detectors emerge from training CNNs to perform scene classiﬁcation. As scenes are composed of objects, the CNN for scene classiﬁcation automatically discovers meaningful objects detectors, representative of the learned scene categories. With object detectors emerging as a result of learning to recognize scenes, our work demonstrates that the same network can perform both scene recognition and object localization in a single forward-pass, without ever having been explicitly taught the notion of objects.},
	language = {en},
	urldate = {2020-06-16},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Zhou, Bolei and Khosla, Aditya and Lapedriza, Agata and Oliva, Aude and Torralba, Antonio},
	year = {2015},
	keywords = {Computer Science - Neural and Evolutionary Computing},
}

@inproceedings{yosinski_how_2014,
	title = {How transferable are features in deep neural networks?},
	url = {http://arxiv.org/abs/1411.1792},
	abstract = {Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the ﬁrst layer they learn features similar to Gabor ﬁlters and color blobs. Such ﬁrst-layer features appear not to be speciﬁc to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to speciﬁc by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus speciﬁcity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difﬁculties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A ﬁnal surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after ﬁne-tuning to the target dataset.},
	language = {en},
	urldate = {2020-06-16},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Yosinski, Jason and Clune, Jeff and Bengio, Yoshua and Lipson, Hod},
	year = {2014},
	keywords = {Computer Science - Neural and Evolutionary Computing},
}

@article{gonzalez-garcia_semantic_2018,
	title = {Do {Semantic} {Parts} {Emerge} in {Convolutional} {Neural} {Networks}?},
	volume = {126},
	issn = {1573-1405},
	url = {https://doi.org/10.1007/s11263-017-1048-0},
	doi = {10.1007/s11263-017-1048-0},
	abstract = {Semantic object parts can be useful for several visual recognition tasks. Lately, these tasks have been addressed using Convolutional Neural Networks (CNN), achieving outstanding results. In this work we study whether CNNs learn semantic parts in their internal representation. We investigate the responses of convolutional filters and try to associate their stimuli with semantic parts. We perform two extensive quantitative analyses. First, we use ground-truth part bounding-boxes from the PASCAL-Part dataset to determine how many of those semantic parts emerge in the CNN. We explore this emergence for different layers, network depths, and supervision levels. Second, we collect human judgements in order to study what fraction of all filters systematically fire on any semantic part, even if not annotated in PASCAL-Part. Moreover, we explore several connections between discriminative power and semantics. We find out which are the most discriminative filters for object recognition, and analyze whether they respond to semantic parts or to other image patches. We also investigate the other direction: we determine which semantic parts are the most discriminative and whether they correspond to those parts emerging in the network. This enables to gain an even deeper understanding of the role of semantic parts in the network.},
	language = {en},
	number = {5},
	urldate = {2020-06-21},
	journal = {International Journal of Computer Vision},
	author = {Gonzalez-Garcia, Abel and Modolo, Davide and Ferrari, Vittorio},
	month = may,
	year = {2018},
	pages = {476--494},
}

@article{yan_recurrent_2019,
	title = {Recurrent {Feedback} {Improves} {Feedforward} {Representations} in {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1912.10489},
	abstract = {The abundant recurrent horizontal and feedback connections in the primate visual cortex are thought to play an important role in bringing global and semantic contextual information to early visual areas during perceptual inference, helping to resolve local ambiguity and ﬁll in missing details. In this study, we ﬁnd that introducing feedback loops and horizontal recurrent connections to a deep convolution neural network (VGG16) allows the network to become more robust against noise and occlusion during inference, even in the initial feedforward pass. This suggests that recurrent feedback and contextual modulation transform the feedforward representations of the network in a meaningful and interesting way. We study the population codes of neurons in the network, before and after learning with feedback, and ﬁnd that learning with feedback yielded an increase in discriminability (measured by d-prime) between the different object classes in the population codes of the neurons in the feedforward path, even at the earliest layer that receives feedback. We ﬁnd that recurrent feedback, by injecting top-down semantic meaning to the population activities, helps the network learn better feedforward paths to robustly map noisy image patches to the latent representations corresponding to important visual concepts of each object class, resulting in greater robustness of the network against noises and occlusion as well as better ﬁne-grained recognition.},
	language = {en},
	urldate = {2020-06-20},
	journal = {arXiv:1912.10489 [cs, q-bio]},
	author = {Yan, Siming and Fang, Xuyang and Xiao, Bowen and Rockwell, Harold and Zhang, Yimeng and Lee, Tai Sing},
	month = dec,
	year = {2019},
	note = {arXiv: 1912.10489},
	keywords = {Computer Science - Machine Learning, Quantitative Biology - Neurons and Cognition},
}

@inproceedings{zagoruyko_wide_2016,
	address = {York, UK},
	title = {Wide {Residual} {Networks}},
	isbn = {978-1-901725-59-9},
	url = {http://www.bmva.org/bmvc/2016/papers/paper087/index.html},
	doi = {10.5244/C.30.87},
	abstract = {Deep residual networks were shown to be able to scale up to thousands of layers and still have improving performance. However, each fraction of a percent of improved accuracy costs nearly doubling the number of layers, and so training very deep residual networks has a problem of diminishing feature reuse, which makes these networks very slow to train. To tackle these problems, in this paper we conduct a detailed experimental study on the architecture of ResNet blocks, based on which we propose a novel architecture where we decrease depth and increase width of residual networks. We call the resulting network structures wide residual networks (WRNs) and show that these are far superior over their commonly used thin and very deep counterparts. For example, we demonstrate that even a simple 16-layer-deep wide residual network outperforms in accuracy and efﬁciency all previous deep residual networks, including thousand-layer-deep networks, achieving new state-of-the-art results on CIFAR-10, CIFAR-100 and SVHN. Our code is available at https://github.com/szagoruyko/wide-residual-networks.},
	language = {en},
	urldate = {2020-06-20},
	booktitle = {Procedings of the {British} {Machine} {Vision} {Conference} 2016},
	publisher = {British Machine Vision Association},
	author = {Zagoruyko, Sergey and Komodakis, Nikos},
	year = {2016},
	pages = {87.1--87.12},
}

@inproceedings{he_identity_2016,
	title = {Identity {Mappings} in {Deep} {Residual} {Networks}},
	url = {http://arxiv.org/abs/1603.05027},
	abstract = {Deep residual networks [1] have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR-10 (4.62\% error) and CIFAR-100, and a 200-layer ResNet on ImageNet. Code is available at: https://github.com/KaimingHe/ resnet-1k-layers.},
	language = {en},
	urldate = {2020-06-15},
	booktitle = {Proceedings of the {European} {Conference} on {Computer} {Vision} ({ECCV})},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	year = {2016},
	keywords = {Computer Science - Machine Learning},
}

@inproceedings{madry_towards_2018,
	title = {Towards {Deep} {Learning} {Models} {Resistant} to {Adversarial} {Attacks}},
	url = {http://arxiv.org/abs/1706.06083},
	abstract = {Recent work has demonstrated that deep neural networks are vulnerable to adversarial examples---inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. Code and pre-trained models are available at https://github.com/MadryLab/mnist\_challenge and https://github.com/MadryLab/cifar10\_challenge.},
	language = {en},
	urldate = {2020-06-15},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
	year = {2018},
	keywords = {Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@inproceedings{chan_jacobian_2020,
	title = {Jacobian {Adversarially} {Regularized} {Networks} {For} {Robustness}},
	abstract = {Adversarial examples are crafted with imperceptible perturbations with the intent to fool neural networks. Against such attacks, adversarial training and its variants stand as the strongest defense to date. Previous studies have pointed out that robust models that have undergone adversarial training tend to produce more salient and interpretable Jacobian matrices than their non-robust counterparts. A natural question is whether a model trained with an objective to produce salient Jacobian can result in better robustness. This paper answers this question with afﬁrmative empirical results. We propose Jacobian Adversarially Regularized Networks (JARN) as a method to optimize the saliency of a classiﬁer’s Jacobian by adversarially regularizing the model’s Jacobian to resemble natural training images1. Image classiﬁers trained with JARN show improved robust accuracy compared to standard models on the MNIST, SVHN and CIFAR-10 datasets, uncovering a new angle to boost robustness without using adversarial training examples.},
	language = {en},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Chan, Alvin and Tay, Yi and Ong, Yew-Soon and Fu, Jie},
	year = {2020},
	pages = {13},
}

@inproceedings{tramer_ensemble_2018,
	title = {Ensemble {Adversarial} {Training}: {Attacks} and {Defenses}},
	shorttitle = {Ensemble {Adversarial} {Training}},
	url = {http://arxiv.org/abs/1705.07204},
	abstract = {Adversarial examples are perturbed inputs designed to fool machine learning models. Adversarial training injects such examples into training data to increase robustness. To scale this technique to large datasets, perturbations are crafted using fast single-step methods that maximize a linear approximation of the model’s loss. We show that this form of adversarial training converges to a degenerate global minimum, wherein small curvature artifacts near the data points obfuscate a linear approximation of the loss. The model thus learns to generate weak perturbations, rather than defend against strong ones. As a result, we ﬁnd that adversarial training remains vulnerable to black-box attacks, where we transfer perturbations computed on undefended models, as well as to a powerful novel single-step attack that escapes the non-smooth vicinity of the input data via a small random step.},
	language = {en},
	urldate = {2020-06-14},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Tramèr, Florian and Kurakin, Alexey and Papernot, Nicolas and Goodfellow, Ian and Boneh, Dan and McDaniel, Patrick},
	year = {2018},
	keywords = {Computer Science - Cryptography and Security, Statistics - Machine Learning},
}

@inproceedings{goodfellow_explaining_2015,
	title = {Explaining and {Harnessing} {Adversarial} {Examples}},
	url = {http://arxiv.org/abs/1412.6572},
	abstract = {Several machine learning models, including neural networks, consistently misclassify adversarial examples—inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high conﬁdence. Early attempts at explaining this phenomenon focused on nonlinearity and overﬁtting. We argue instead that the primary cause of neural networks’ vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the ﬁrst explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.},
	language = {en},
	urldate = {2020-06-14},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Goodfellow, Ian J. and Shlens, Jonathon and Szegedy, Christian},
	month = mar,
	year = {2015},
	keywords = {Statistics - Machine Learning},
}

@inproceedings{wong_fast_2020,
	title = {Fast is better than free: {Revisiting} adversarial training},
	shorttitle = {Fast is better than free},
	url = {http://arxiv.org/abs/2001.03994},
	abstract = {Adversarial training, a method for learning robust deep networks, is typically assumed to be more expensive than traditional training due to the necessity of constructing adversarial examples via a ﬁrst-order method like projected gradient decent (PGD). In this paper, we make the surprising discovery that it is possible to train empirically robust models using a much weaker and cheaper adversary, an approach that was previously believed to be ineffective, rendering the method no more costly than standard training in practice. Speciﬁcally, we show that adversarial training with the fast gradient sign method (FGSM), when combined with random initialization, is as effective as PGD-based training but has signiﬁcantly lower cost. Furthermore we show that FGSM adversarial training can be further accelerated by using standard techniques for efﬁcient training of deep networks, allowing us to learn a robust CIFAR10 classiﬁer with 45\% robust accuracy to PGD attacks with = 8/255 in 6 minutes, and a robust ImageNet classiﬁer with 43\% robust accuracy at = 2/255 in 12 hours, in comparison to past work based on “free” adversarial training which took 10 and 50 hours to reach the same respective thresholds. Finally, we identify a failure mode referred to as “catastrophic overﬁtting” which may have caused previous attempts to use FGSM adversarial training to fail. All code for reproducing the experiments in this paper as well as pretrained model weights are at https://github.com/locuslab/fast\_adversarial.},
	language = {en},
	urldate = {2020-06-14},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Wong, Eric and Rice, Leslie and Kolter, J. Zico},
	year = {2020},
	keywords = {Statistics - Machine Learning},
}

@inproceedings{xie_feature_2019,
	title = {Feature {Denoising} for {Improving} {Adversarial} {Robustness}},
	abstract = {Adversarial attacks to image classiﬁcation systems present challenges to convolutional networks and opportunities for understanding them. This study suggests that adversarial perturbations on images lead to noise in the features constructed by these networks. Motivated by this observation, we develop new network architectures that increase adversarial robustness by performing feature denoising. Speciﬁcally, our networks contain blocks that denoise the features using non-local means or other ﬁlters; the entire networks are trained end-to-end. When combined with adversarial training, our feature denoising networks substantially improve the state-of-the-art in adversarial robustness in both white-box and black-box attack settings. On ImageNet, under 10-iteration PGD white-box attacks where prior art has 27.9\% accuracy, our method achieves 55.7\%; even under extreme 2000-iteration PGD white-box attacks, our method secures 42.6\% accuracy. Our method was ranked ﬁrst in Competition on Adversarial Attacks and Defenses (CAAD) 2018 — it achieved 50.6\% classiﬁcation accuracy on a secret, ImageNet-like test dataset against 48 unknown attackers, surpassing the runner-up approach by ∼10\%. Code is available at https://github.com/facebookresearch/ ImageNet-Adversarial-Training.},
	language = {en},
	booktitle = {The {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Xie, Cihang and Wu, Yuxin and Maaten, Laurens van der and Yuille, Alan and He, Kaiming},
	year = {2019},
	pages = {9},
}

@inproceedings{ross_improving_2018,
	title = {Improving the {Adversarial} {Robustness} and {Interpretability} of {Deep} {Neural} {Networks} by {Regularizing} their {Input} {Gradients}},
	url = {https://aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17337},
	abstract = {Deep neural networks have proven remarkably effective at solving many classiﬁcation problems, but have been criticized recently for two major weaknesses: the reasons behind their predictions are uninterpretable, and the predictions themselves can often be fooled by small adversarial perturbations. These problems pose major obstacles for the adoption of neural networks in domains that require security or transparency. In this work, we evaluate the effectiveness of defenses that differentiably penalize the degree to which small changes in inputs can alter model predictions. Across multiple attacks, architectures, defenses, and datasets, we ﬁnd that neural networks trained with this input gradient regularization exhibit robustness to transferred adversarial examples generated to fool all of the other models. We also ﬁnd that adversarial examples generated to fool gradient-regularized models fool all other models equally well, and actually lead to more “legitimate,” interpretable misclassiﬁcations as rated by people (which we conﬁrm in a human subject experiment). Finally, we demonstrate that regularizing input gradients makes them more naturally interpretable as rationales for model predictions. We conclude by discussing this relationship between interpretability and robustness in deep neural networks.},
	language = {en},
	booktitle = {{AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Ross, Andrew Slavin and Doshi-Velez, Finale},
	year = {2018},
	pages = {10},
}

@inproceedings{jakubovitz_improving_2018,
	title = {Improving {DNN} {Robustness} to {Adversarial} {Attacks} using {Jacobian} {Regularization}},
	url = {http://openaccess.thecvf.com/content_ECCV_2018/html/Daniel_Jakubovitz_Improving_DNN_Robustness_ECCV_2018_paper.html},
	urldate = {2020-06-20},
	booktitle = {Proceedings of the {European} {Conference} on {Computer} {Vision} ({ECCV})},
	author = {Jakubovitz, Daniel and Giryes, Raja},
	year = {2018},
	pages = {514--529},
}

@inproceedings{kurakin_adversarial_2017,
	title = {Adversarial {Machine} {Learning} at {Scale}},
	url = {http://arxiv.org/abs/1611.01236},
	abstract = {Adversarial examples are malicious inputs designed to fool machine learning models. They often transfer from one model to another, allowing attackers to mount black box attacks without knowledge of the target model’s parameters. Adversarial training is the process of explicitly training a model on adversarial examples, in order to make it more robust to attack or to reduce its test error on clean inputs. So far, adversarial training has primarily been applied to small problems. In this research, we apply adversarial training to ImageNet (Russakovsky et al., 2014). Our contributions include: (1) recommendations for how to succesfully scale adversarial training to large models and datasets, (2) the observation that adversarial training confers robustness to single-step attack methods, (3) the ﬁnding that multi-step attack methods are somewhat less transferable than singlestep attack methods, so single-step attacks are the best for mounting black-box attacks, and (4) resolution of a “label leaking” effect that causes adversarially trained models to perform better on adversarial examples than on clean examples, because the adversarial example construction process uses the true label and the model can learn to exploit regularities in the construction process.},
	language = {en},
	urldate = {2020-06-15},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Kurakin, Alexey and Goodfellow, Ian and Bengio, Samy},
	year = {2017},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{papernot_distillation_2016,
	title = {Distillation as a {Defense} to {Adversarial} {Perturbations} against {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1511.04508},
	abstract = {Deep learning algorithms have been shown to perform extremely well on many classical machine learning problems. However, recent studies have shown that deep learning, like other machine learning techniques, is vulnerable to adversarial samples: inputs crafted to force a deep neural network (DNN) to provide adversary-selected outputs. Such attacks can seriously undermine the security of the system supported by the DNN, sometimes with devastating consequences. For example, autonomous vehicles can be crashed, illicit or illegal content can bypass content ﬁlters, or biometric authentication systems can be manipulated to allow improper access. In this work, we introduce a defensive mechanism called defensive distillation to reduce the effectiveness of adversarial samples on DNNs. We analytically investigate the generalizability and robustness properties granted by the use of defensive distillation when training DNNs. We also empirically study the effectiveness of our defense mechanisms on two DNNs placed in adversarial settings. The study shows that defensive distillation can reduce effectiveness of sample creation from 95\% to less than 0.5\% on a studied DNN. Such dramatic gains can be explained by the fact that distillation leads gradients used in adversarial sample creation to be reduced by a factor of 1030. We also ﬁnd that distillation increases the average minimum number of features that need to be modiﬁed to create adversarial samples by about 800\% on one of the DNNs we tested.},
	language = {en},
	urldate = {2020-06-15},
	booktitle = {37th {IEEE} {Symposium} on {Security} \& {Privacy}},
	author = {Papernot, Nicolas and McDaniel, Patrick and Wu, Xi and Jha, Somesh and Swami, Ananthram},
	year = {2016},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@inproceedings{shafahi_adversarial_2019,
	title = {Adversarial {Training} for {Free}!},
	url = {http://arxiv.org/abs/1904.12843},
	abstract = {Adversarial training, in which a network is trained on adversarial examples, is one of the few defenses against adversarial attacks that withstands strong attacks. Unfortunately, the high cost of generating strong adversarial examples makes standard adversarial training impractical on large-scale problems like ImageNet. We present an algorithm that eliminates the overhead cost of generating adversarial examples by recycling the gradient information computed when updating model parameters. Our “free” adversarial training algorithm achieves state-of-the-art robustness on CIFAR-10 and CIFAR-100 datasets at negligible additional cost compared to natural training, and can be 7 to 30 times faster than other strong adversarial training methods. Using a single workstation with 4 P100 GPUs and 2 days of runtime, we can train a robust model for the large-scale ImageNet classiﬁcation task that maintains 40\% accuracy against PGD attacks.},
	language = {en},
	urldate = {2020-06-15},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Shafahi, Ali and Najibi, Mahyar and Ghiasi, Amin and Xu, Zheng and Dickerson, John and Studer, Christoph and Davis, Larry S. and Taylor, Gavin and Goldstein, Tom},
	year = {2019},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{tsipras_robustness_2019,
	title = {Robustness {May} {Be} at {Odds} with {Accuracy}},
	url = {http://arxiv.org/abs/1805.12152},
	abstract = {We show that there may exist an inherent tension between the goal of adversarial robustness and that of standard generalization. Speciﬁcally, training robust models may not only be more resource-consuming, but also lead to a reduction of standard accuracy. We demonstrate that this trade-off between the standard accuracy of a model and its robustness to adversarial perturbations provably exists in a fairly simple and natural setting. These ﬁndings also corroborate a similar phenomenon observed empirically in more complex settings. Further, we argue that this phenomenon is a consequence of robust classiﬁers learning fundamentally different feature representations than standard classiﬁers. These differences, in particular, seem to result in unexpected beneﬁts: the representations learned by robust models tend to align better with salient data characteristics and human perception.},
	language = {en},
	urldate = {2020-06-15},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Tsipras, Dimitris and Santurkar, Shibani and Engstrom, Logan and Turner, Alexander and Madry, Aleksander},
	year = {2019},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@inproceedings{etmann_connection_2019,
	title = {On the {Connection} {Between} {Adversarial} {Robustness} and {Saliency} {Map} {Interpretability}},
	abstract = {Recent studies on the adversarial vulnerability of neural networks have shown that models trained to be more robust to adversarial attacks exhibit more interpretable saliency maps than their nonrobust counterparts. We aim to quantify this behavior by considering the alignment between input image and saliency map. We hypothesize that as the distance to the decision boundary grows, so does the alignment. This connection is strictly true in the case of linear models. We conﬁrm these theoretical ﬁndings with experiments based on models trained with a local Lipschitz regularization and identify where the non-linear nature of neural networks weakens the relation.},
	language = {en},
	booktitle = {{thProceedings} of the 36th {International} {Conference} on {Machine} {Learning}},
	author = {Etmann, Christian and Lunz, Sebastian and Maass, Peter and Schonlieb, Carola-Bibiane},
	year = {2019},
	pages = {10},
}

@inproceedings{zhang_interpreting_2019,
	title = {Interpreting {Adversarially} {Trained} {Convolutional} {Neural} {Networks}},
	abstract = {We attempt to interpret how adversarially trained convolutional neural networks (AT-CNNs) recognize objects. We design systematic approaches to interpret AT-CNNs in both qualitative and quantitative ways and compare them with normally trained models. Surprisingly, we ﬁnd that adversarial training alleviates the texture bias of standard CNNs when trained on object recognition tasks, and helps CNNs learn a more shape-biased representation. We validate our hypothesis from two aspects. First, we compare the salience maps of AT-CNNs and standard CNNs on clean images and images under different transformations. The comparison could visually show that the prediction of the two types of CNNs is sensitive to dramatically different types of features. Second, to achieve quantitative veriﬁcation, we construct additional test datasets that destroy either textures or shapes, such as style-transferred version of clean data, saturated images and patch-shufﬂed ones, and then evaluate the classiﬁcation accuracy of AT-CNNs and normal CNNs on these datasets. Our ﬁndings shed some light on why AT-CNNs are more robust than those normally trained ones and contribute to a better understanding of adversarial training over CNNs from an interpretation perspective.},
	language = {en},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	author = {Zhang, Tianyuan and Zhu, Zhanxing},
	year = {2019},
	pages = {10},
}

@inproceedings{ilyas_adversarial_2019,
	title = {Adversarial {Examples} {Are} {Not} {Bugs}, {They} {Are} {Features}},
	url = {https://papers.nips.cc/paper/8307-adversarial-examples-are-not-bugs-they-are-features.pdf},
	abstract = {Adversarial examples have attracted signiﬁcant attention in machine learning, but the reasons for their existence and pervasiveness remain unclear. We demonstrate that adversarial examples can be directly attributed to the presence of non-robust features: features (derived from patterns in the data distribution) that are highly predictive, yet brittle and (thus) incomprehensible to humans. After capturing these features within a theoretical framework, we establish their widespread existence in standard datasets. Finally, we present a simple setting where we can rigorously tie the phenomena we observe in practice to a misalignment between the (human-speciﬁed) notion of robustness and the inherent geometry of the data.},
	language = {en},
	urldate = {2020-06-15},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Ilyas, Andrew and Santurkar, Shibani and Tsipras, Dimitris and Engstrom, Logan and Tran, Brandon and Madry, Aleksander},
	year = {2019},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{galloway_batch_2019,
	title = {Batch {Normalization} is a {Cause} of {Adversarial} {Vulnerability}},
	url = {http://arxiv.org/abs/1905.02161},
	abstract = {Batch normalization (batch norm) is often used in an attempt to stabilize and accelerate training in deep neural networks. In many cases it indeed decreases the number of parameter updates required to achieve low training error. However, it also reduces robustness to small adversarial input perturbations and noise by double-digit percentages, as we show on ﬁve standard datasets. Furthermore, substituting weight decay for batch norm is suﬃcient to nullify the relationship between adversarial vulnerability and the input dimension. Our work is consistent with a mean-ﬁeld analysis that found that batch norm causes exploding gradients.},
	language = {en},
	urldate = {2020-06-15},
	journal = {arXiv:1905.02161 [cs, stat]},
	author = {Galloway, Angus and Golubeva, Anna and Tanay, Thomas and Moussa, Medhat and Taylor, Graham W.},
	month = may,
	year = {2019},
	keywords = {Statistics - Machine Learning},
}

@inproceedings{szegedy_intriguing_2014,
	title = {Intriguing properties of neural networks},
	url = {http://arxiv.org/abs/1312.6199},
	abstract = {Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties.},
	language = {en},
	urldate = {2020-06-16},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
	year = {2014},
	keywords = {Computer Science - Neural and Evolutionary Computing},
}

@inproceedings{sinha_harnessing_2019,
	title = {Harnessing the {Vulnerability} of {Latent} {Layers} in {Adversarially} {Trained} {Models}},
	abstract = {Neural networks are vulnerable to adversarial attacks - small visually imperceptible crafted noise which when added to the input drastically changes the output. The most effective method of defending against these adversarial attacks is to use the methodology of adversarial training. We analyze the adversarially trained robust models to study their vulnerability against adversarial attacks at the level of the latent layers. Our analysis reveals that contrary to the input layer which is robust to adversarial attack, the latent layer of these robust models are highly susceptible to adversarial perturbations of small magnitude. Leveraging this information, we introduce a new technique Latent Adversarial Training (LAT) which comprises of ﬁne-tuning the adversarially trained models to ensure the robustness at the feature layers. We also propose Latent Attack (LA), a novel algorithm for construction of adversarial examples. LAT results in minor improvement in test accuracy and leads to a state-of-the-art adversarial accuracy against the universal ﬁrst-order adversarial PGD attack which is shown for the MNIST, CIFAR-10, CIFAR-100 datasets.},
	language = {en},
	urldate = {2020-06-16},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Sinha, Abhishek and Singh, Mayank and Kumari, Nupur and Krishnamurthy, Balaji and Machiraju, Harshitha and Balasubramanian, Vineeth N},
	year = {2019},
}

@inproceedings{brendel_decision-based_2018,
	title = {Decision-{Based} {Adversarial} {Attacks}: {Reliable} {Attacks} {Against} {Black}-{Box} {Machine} {Learning} {Models}},
	shorttitle = {Decision-{Based} {Adversarial} {Attacks}},
	url = {http://arxiv.org/abs/1712.04248},
	abstract = {Many machine learning algorithms are vulnerable to almost imperceptible perturbations of their inputs. So far it was unclear how much risk adversarial perturbations carry for the safety of real-world machine learning applications because most methods used to generate such perturbations rely either on detailed model information (gradient-based attacks) or on conﬁdence scores such as class probabilities (score-based attacks), neither of which are available in most real-world scenarios. In many such cases one currently needs to retreat to transfer-based attacks which rely on cumbersome substitute models, need access to the training data and can be defended against. Here we emphasise the importance of attacks which solely rely on the ﬁnal model decision. Such decision-based attacks are (1) applicable to real-world black-box models such as autonomous cars, (2) need less knowledge and are easier to apply than transfer-based attacks and (3) are more robust to simple defences than gradient- or score-based attacks. Previous attacks in this category were limited to simple models or simple datasets. Here we introduce the Boundary Attack, a decision-based attack that starts from a large adversarial perturbation and then seeks to reduce the perturbation while staying adversarial. The attack is conceptually simple, requires close to no hyperparameter tuning, does not rely on substitute models and is competitive with the best gradient-based attacks in standard computer vision tasks like ImageNet. We apply the attack on two black-box algorithms from Clarifai.com. The Boundary Attack in particular and the class of decision-based attacks in general open new avenues to study the robustness of machine learning models and raise new questions regarding the safety of deployed machine learning systems. An implementation of the attack is available as part of Foolbox (https://github.com/bethgelab/foolbox).},
	language = {en},
	urldate = {2020-06-14},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Brendel, Wieland and Rauber, Jonas and Bethge, Matthias},
	month = feb,
	year = {2018},
	note = {arXiv: 1712.04248},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@inproceedings{zhang_interpreting_2019-1,
	address = {Long Beach, CA, USA},
	title = {Interpreting {CNNs} via {Decision} {Trees}},
	isbn = {978-1-72813-293-8},
	url = {https://ieeexplore.ieee.org/document/8953917/},
	doi = {10.1109/CVPR.2019.00642},
	abstract = {This paper1 aims to quantitatively explain the rationales of each prediction that is made by a pre-trained convolutional neural network (CNN). We propose to learn a decision tree, which clariﬁes the speciﬁc reason for each prediction made by the CNN at the semantic level. I.e. the decision tree decomposes feature representations in high conv-layers of the CNN into elementary concepts of object parts. In this way, the decision tree tells people which object parts activate which ﬁlters for the prediction and how much each object part contributes to the prediction score. Such semantic and quantitative explanations for CNN predictions have speciﬁc values beyond the traditional pixel-level analysis of CNNs. More speciﬁcally, our method mines all potential decision modes of the CNN, where each mode represents a typical case of how the CNN uses object parts for prediction. The decision tree organizes all potential decision modes in a coarse-to-ﬁne manner to explain CNN predictions at different ﬁne-grained levels. Experiments have demonstrated the effectiveness of the proposed method.},
	language = {en},
	urldate = {2020-06-18},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Zhang, Quanshi and Yang, Yu and Ma, Haotian and Wu, Ying Nian},
	month = jun,
	year = {2019},
	pages = {6254--6263},
}

@inproceedings{zhang_interpretable_2018,
	address = {Salt Lake City, UT},
	title = {Interpretable {Convolutional} {Neural} {Networks}},
	isbn = {978-1-5386-6420-9},
	url = {https://ieeexplore.ieee.org/document/8579018/},
	doi = {10.1109/CVPR.2018.00920},
	abstract = {This paper proposes a method to modify a traditional convolutional neural network (CNN) into an interpretable CNN, in order to clarify knowledge representations in high conv-layers of the CNN. In an interpretable CNN, each ﬁlter in a high conv-layer represents a speciﬁc object part. Our interpretable CNNs use the same training data as ordinary CNNs without a need for any annotations of object parts or textures for supervision. The interpretable CNN automatically assigns each ﬁlter in a high conv-layer with an object part during the learning process. We can apply our method to different types of CNNs with various structures. The explicit knowledge representation in an interpretable CNN can help people understand the logic inside a CNN, i.e. what patterns are memorized by the CNN for prediction. Experiments have shown that ﬁlters in an interpretable CNN are more semantically meaningful than those in a traditional CNN. The code is available at https: //github.com/zqs1022/interpretableCNN .},
	language = {en},
	urldate = {2020-06-18},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Zhang, Quanshi and Wu, Ying Nian and Zhu, Song-Chun},
	month = jun,
	year = {2018},
	pages = {8827--8836},
}

@article{lu_visual_2016,
	title = {Visual {Relationship} {Detection} with {Language} {Priors}},
	url = {http://arxiv.org/abs/1608.00187},
	abstract = {Visual relationships capture a wide variety of interactions between pairs of objects in images (e.g. “man riding bicycle” and “man pushing bicycle”). Consequently, the set of possible relationships is extremely large and it is diﬃcult to obtain suﬃcient training examples for all possible relationships. Because of this limitation, previous work on visual relationship detection has concentrated on predicting only a handful of relationships. Though most relationships are infrequent, their objects (e.g. “man” and “bicycle”) and predicates (e.g. “riding” and “pushing”) independently occur more frequently. We propose a model that uses this insight to train visual models for objects and predicates individually and later combines them together to predict multiple relationships per image. We improve on prior work by leveraging language priors from semantic word embeddings to ﬁnetune the likelihood of a predicted relationship. Our model can scale to predict thousands of types of relationships from a few examples. Additionally, we localize the objects in the predicted relationships as bounding boxes in the image. We further demonstrate that understanding relationships can improve content based image retrieval.},
	language = {en},
	urldate = {2020-06-18},
	journal = {arXiv:1608.00187 [cs]},
	author = {Lu, Cewu and Krishna, Ranjay and Bernstein, Michael and Fei-Fei, Li},
	month = jul,
	year = {2016},
	note = {arXiv: 1608.00187},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{richardson_markov_2006,
	title = {Markov logic networks},
	volume = {62},
	issn = {0885-6125, 1573-0565},
	url = {http://link.springer.com/10.1007/s10994-006-5833-1},
	doi = {10.1007/s10994-006-5833-1},
	abstract = {We propose a simple approach to combining ﬁrst-order logic and probabilistic graphical models in a single representation. A Markov logic network (MLN) is a ﬁrstorder knowledge base with a weight attached to each formula (or clause). Together with a set of constants representing objects in the domain, it speciﬁes a ground Markov network containing one feature for each possible grounding of a ﬁrst-order formula in the KB, with the corresponding weight. Inference in MLNs is performed by MCMC over the minimal subset of the ground network required for answering the query. Weights are efﬁciently learned from relational databases by iteratively optimizing a pseudo-likelihood measure. Optionally, additional clauses are learned using inductive logic programming techniques. Experiments with a real-world database and knowledge base in a university domain illustrate the promise of this approach.},
	language = {en},
	number = {1-2},
	urldate = {2020-06-18},
	journal = {Machine Learning},
	author = {Richardson, Matthew and Domingos, Pedro},
	month = feb,
	year = {2006},
	pages = {107--136},
}

@article{reed_training_2015,
	title = {Training {Deep} {Neural} {Networks} on {Noisy} {Labels} with {Bootstrapping}},
	url = {http://arxiv.org/abs/1412.6596},
	abstract = {Current state-of-the-art deep learning systems for visual object recognition and detection use purely supervised training with regularization such as dropout to avoid overﬁtting. The performance depends critically on the amount of labeled examples, and in current practice the labels are assumed to be unambiguous and accurate. However, this assumption often does not hold; e.g. in recognition, class labels may be missing; in detection, objects in the image may not be localized; and in general, the labeling may be subjective. In this work we propose a generic way to handle noisy and incomplete labeling by augmenting the prediction objective with a notion of consistency. We consider a prediction consistent if the same prediction is made given similar percepts, where the notion of similarity is between deep network features computed from the input data. In experiments we demonstrate that our approach yields substantial robustness to label noise on several datasets. On MNIST handwritten digits, we show that our model is robust to label corruption. On the Toronto Face Database, we show that our model handles well the case of subjective labels in emotion recognition, achieving state-of-theart results, and can also beneﬁt from unlabeled face images with no modiﬁcation to our method. On the ILSVRC2014 detection challenge data, we show that our approach extends to very deep networks, high resolution images and structured outputs, and results in improved scalable detection.},
	language = {en},
	urldate = {2020-06-18},
	journal = {arXiv:1412.6596 [cs]},
	author = {Reed, Scott and Lee, Honglak and Anguelov, Dragomir and Szegedy, Christian and Erhan, Dumitru and Rabinovich, Andrew},
	month = apr,
	year = {2015},
	note = {arXiv: 1412.6596},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@incollection{fleet_reasoning_2014,
	address = {Cham},
	title = {Reasoning about {Object} {Affordances} in a {Knowledge} {Base} {Representation}},
	volume = {8690},
	isbn = {978-3-319-10604-5 978-3-319-10605-2},
	url = {http://link.springer.com/10.1007/978-3-319-10605-2_27},
	abstract = {Reasoning about objects and their aﬀordances is a fundamental problem for visual intelligence. Most of the previous work casts this problem as a classiﬁcation task where separate classiﬁers are trained to label objects, recognize attributes, or assign aﬀordances. In this work, we consider the problem of object aﬀordance reasoning using a knowledge base representation. Diverse information of objects are ﬁrst harvested from images and other meta-data sources. We then learn a knowledge base (KB) using a Markov Logic Network (MLN). Given the learned KB, we show that a diverse set of visual inference tasks can be done in this uniﬁed framework without training separate classiﬁers, including zeroshot aﬀordance prediction and object recognition given human poses.},
	language = {en},
	urldate = {2020-06-18},
	booktitle = {Computer {Vision} – {ECCV} 2014},
	publisher = {Springer International Publishing},
	author = {Zhu, Yuke and Fathi, Alireza and Fei-Fei, Li},
	editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
	year = {2014},
	doi = {10.1007/978-3-319-10605-2_27},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {408--424},
}

@inproceedings{marszalek_semantic_2007,
	address = {Minneapolis, MN, USA},
	title = {Semantic {Hierarchies} for {Visual} {Object} {Recognition}},
	isbn = {978-1-4244-1179-5 978-1-4244-1180-1},
	url = {http://ieeexplore.ieee.org/document/4270297/},
	doi = {10.1109/CVPR.2007.383272},
	abstract = {In this paper we propose to use lexical semantic networks to extend the state-of-the-art object recognition techniques. We use the semantics of image labels to integrate prior knowledge about inter-class relationships into the visual appearance learning. We show how to build and train a semantic hierarchy of discriminative classiﬁers and how to use it to perform object detection. We evaluate how our approach inﬂuences the classiﬁcation accuracy and speed on the PASCAL VOC challenge 2006 dataset, a set of challenging real-world images. We also demonstrate additional features that become available to object recognition due to the extension with semantic inference tools—we can classify high-level categories, such as animals, and we can train part detectors, for example a window detector, by pure inference in the semantic network.},
	language = {en},
	urldate = {2020-06-18},
	booktitle = {2007 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Marszalek, Marcin and Schmid, Cordelia},
	month = jun,
	year = {2007},
	pages = {1--7},
}

@article{schott_towards_2019,
	title = {Towards {The} {First} {Adversarially} {Robust} {Neural} {Network} {Model} on {MNIST}},
	abstract = {Despite much effort, deep neural networks remain highly susceptible to tiny input perturbations and even for MNIST, one of the most common toy datasets in computer vision, no neural network model exists for which adversarial perturbations are large and make semantic sense to humans. We show that even the widely recognized and by far most successful L∞ defense by Madry et al. (1) has lower L0 robustness than undefended networks and is still highly susceptible to L2 perturbations, (2) classiﬁes unrecognizable images with high certainty, (3) performs not much better than simple input binarization and (4) features adversarial perturbations that make little sense to humans. These results suggest that MNIST is far from being solved in terms of adversarial robustness. We present a novel robust classiﬁcation model that performs analysis by synthesis using learned class-conditional data distributions. We derive bounds on the robustness and go to great length to empirically evaluate our model using maximally effective adversarial attacks by (a) applying decision-based, score-based, gradient-based and transfer-based attacks for several different Lp norms, (b) by designing a new attack that exploits the structure of our defended model and (c) by devising a novel decision-based attack that seeks to minimize the number of perturbed pixels (L0). The results suggest that our approach yields state-of-the-art robustness on MNIST against L0, L2 and L∞ perturbations and we demonstrate that most adversarial examples are strongly perturbed towards the perceptual boundary between the original and the adversarial class.},
	language = {en},
	author = {Schott, Lukas and Rauber, Jonas and Bethge, Matthias and Brendel, Wieland},
	year = {2019},
	pages = {17},
}

@article{grill-spector_human_2004,
	title = {The {Human} {Visual} {Cortex}},
	volume = {27},
	issn = {0147-006X, 1545-4126},
	url = {http://www.annualreviews.org/doi/10.1146/annurev.neuro.27.070203.144220},
	doi = {10.1146/annurev.neuro.27.070203.144220},
	language = {en},
	number = {1},
	urldate = {2020-06-18},
	journal = {Annual Review of Neuroscience},
	author = {Grill-Spector, Kalanit and Malach, Rafael},
	month = jul,
	year = {2004},
	pages = {649--677},
}

@inproceedings{ramanathan_learning_2015,
	address = {Boston, MA, USA},
	title = {Learning semantic relationships for better action retrieval in images},
	isbn = {978-1-4673-6964-0},
	url = {http://ieeexplore.ieee.org/document/7298713/},
	doi = {10.1109/CVPR.2015.7298713},
	abstract = {Human actions capture a wide variety of interactions between people and objects. As a result, the set of possible actions is extremely large and it is difﬁcult to obtain sufﬁcient training examples for all actions. However, we could compensate for this sparsity in supervision by leveraging the rich semantic relationship between different actions. A single action is often composed of other smaller actions and is exclusive of certain others. We need a method which can reason about such relationships and extrapolate unobserved actions from known actions. Hence, we propose a novel neural network framework which jointly extracts the relationship between actions and uses them for training better action retrieval models. Our model incorporates linguistic, visual and logical consistency based cues to effectively identify these relationships. We train and test our model on a largescale image dataset of human actions. We show a signiﬁcant improvement in mean AP compared to different baseline methods including the HEX-graph approach from Deng et al. [8].},
	language = {en},
	urldate = {2020-06-18},
	booktitle = {2015 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Ramanathan, Vignesh and Li, Congcong and Deng, Jia and Han, Wei and Li, Zhen and Gu, Kunlong and Song, Yang and Bengio, Samy and Rossenberg, Chuck and Fei-Fei, Li},
	month = jun,
	year = {2015},
	pages = {1100--1109},
}

@article{percy_need_nodate,
	title = {The {Need} for {Knowledge} {Extraction}: {Understanding} {Harmful} {Gambling} {Behavior} with {Neural} {Networks}},
	language = {en},
	author = {Percy, Chris and França, Manoel V M and Slabaugh, Greg and Weyde, Tillman},
	pages = {8},
}

@article{hinton_fast_2006,
	title = {A {Fast} {Learning} {Algorithm} for {Deep} {Belief} {Nets}},
	volume = {18},
	issn = {0899-7667, 1530-888X},
	url = {http://www.mitpressjournals.org/doi/10.1162/neco.2006.18.7.1527},
	doi = {10.1162/neco.2006.18.7.1527},
	language = {en},
	number = {7},
	urldate = {2020-06-18},
	journal = {Neural Computation},
	author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee-Whye},
	month = jul,
	year = {2006},
	pages = {1527--1554},
}

@inproceedings{feng_pathologies_2018,
	address = {Brussels, Belgium},
	title = {Pathologies of {Neural} {Models} {Make} {Interpretations} {Difficult}},
	url = {http://aclweb.org/anthology/D18-1407},
	doi = {10.18653/v1/D18-1407},
	abstract = {One way to interpret neural model predictions is to highlight the most important input features—for example, a heatmap visualization over the words in an input sentence. In existing interpretation methods for NLP, a word’s importance is determined by either input perturbation—measuring the decrease in model conﬁdence when that word is removed—or by the gradient with respect to that word. To understand the limitations of these methods, we use input reduction, which iteratively removes the least important word from the input. This exposes pathological behaviors of neural models: the remaining words appear nonsensical to humans and do not match the words interpretation methods deem important. As we conﬁrm with human experiments, the reduced examples lack information to support the prediction of any label, but models still make the same predictions with high conﬁdence. To explain these counterintuitive results, we draw connections to adversarial examples and conﬁdence calibration: pathological behaviors reveal difﬁculties in interpreting neural models trained with maximum likelihood. To mitigate their deﬁciencies, we ﬁne-tune the models by encouraging high entropy outputs on reduced examples. Fine-tuned models become more interpretable under input reduction without accuracy loss on regular examples.},
	language = {en},
	urldate = {2020-06-18},
	booktitle = {Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Feng, Shi and Wallace, Eric and Grissom II, Alvin and Iyyer, Mohit and Rodriguez, Pedro and Boyd-Graber, Jordan},
	year = {2018},
	pages = {3719--3728},
}

@article{li_visualizing_2016,
	title = {Visualizing and {Understanding} {Neural} {Models} in {NLP}},
	url = {http://arxiv.org/abs/1506.01066},
	abstract = {While neural networks have been successfully applied to many NLP tasks the resulting vector-based models are very difﬁcult to interpret. For example it’s not clear how they achieve compositionality, building sentence meaning from the meanings of words and phrases. In this paper we describe strategies for visualizing compositionality in neural models for NLP, inspired by similar work in computer vision. We ﬁrst plot unit values to visualize compositionality of negation, intensiﬁcation, and concessive clauses, allowing us to see wellknown markedness asymmetries in negation. We then introduce methods for visualizing a unit’s salience, the amount that it contributes to the ﬁnal composed meaning from ﬁrst-order derivatives. Our generalpurpose methods may have wide applications for understanding compositionality and other semantic properties of deep networks.},
	language = {en},
	urldate = {2020-06-18},
	journal = {arXiv:1506.01066 [cs]},
	author = {Li, Jiwei and Chen, Xinlei and Hovy, Eduard and Jurafsky, Dan},
	month = jan,
	year = {2016},
	note = {arXiv: 1506.01066},
	keywords = {Computer Science - Computation and Language},
}

@article{frosst_distilling_2017,
	title = {Distilling a {Neural} {Network} {Into} a {Soft} {Decision} {Tree}},
	url = {http://arxiv.org/abs/1711.09784},
	abstract = {Deep neural networks have proved to be a very eﬀective way to perform classiﬁcation tasks. They excel when the input data is high dimensional, the relationship between the input and the output is complicated, and the number of labeled training examples is large [Szegedy et al., 2015, Wu et al., 2016, Jozefowicz et al., 2016, Graves et al., 2013]. But it is hard to explain why a learned network makes a particular classiﬁcation decision on a particular test case. This is due to their reliance on distributed hierarchical representations. If we could take the knowledge acquired by the neural net and express the same knowledge in a model that relies on hierarchical decisions instead, explaining a particular decision would be much easier. We describe a way of using a trained neural net to create a type of soft decision tree that generalizes better than one learned directly from the training data.},
	language = {en},
	urldate = {2020-06-18},
	journal = {arXiv:1711.09784 [cs, stat]},
	author = {Frosst, Nicholas and Hinton, Geoffrey},
	month = nov,
	year = {2017},
	note = {arXiv: 1711.09784},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{guidotti_local_2018,
	title = {Local {Rule}-{Based} {Explanations} of {Black} {Box} {Decision} {Systems}},
	url = {http://arxiv.org/abs/1805.10820},
	abstract = {The recent years have witnessed the rise of accurate but obscure decision systems which hide the logic of their internal decision processes to the users. The lack of explanations for the decisions of black box systems is a key ethical issue, and a limitation to the adoption of machine learning components in socially sensitive and safety-critical contexts. In this paper we focus on the problem of black box outcome explanation, i.e., explaining the reasons of the decision taken on a specific instance. We propose LORE, an agnostic method able to provide interpretable and faithful explanations. LORE first leans a local interpretable predictor on a synthetic neighborhood generated by a genetic algorithm. Then it derives from the logic of the local interpretable predictor a meaningful explanation consisting of: a decision rule, which explains the reasons of the decision; and a set of counterfactual rules, suggesting the changes in the instance’s features that lead to a different outcome. Wide experiments show that LORE outperforms existing methods and baselines both in the quality of explanations and in the accuracy in mimicking the black box.},
	language = {en},
	urldate = {2020-06-18},
	journal = {arXiv:1805.10820 [cs]},
	author = {Guidotti, Riccardo and Monreale, Anna and Ruggieri, Salvatore and Pedreschi, Dino and Turini, Franco and Giannotti, Fosca},
	month = may,
	year = {2018},
	note = {arXiv: 1805.10820},
	keywords = {Computer Science - Artificial Intelligence},
}

@article{pedreschi_open_2018,
	title = {Open the {Black} {Box} {Data}-{Driven} {Explanation} of {Black} {Box} {Decision} {Systems}},
	url = {http://arxiv.org/abs/1806.09936},
	abstract = {Black box systems for automated decision making, often based on machine learning over (big) data, map a user's features into a class or a score without exposing the reasons why. This is problematic not only for lack of transparency, but also for possible biases hidden in the algorithms, due to human prejudices and collection artifacts hidden in the training data, which may lead to unfair or wrong decisions. We introduce the local-to-global framework for black box explanation, a novel approach with promising early results, which paves the road for a wide spectrum of future developments along three dimensions: (i) the language for expressing explanations in terms of highly expressive logic-based rules, with a statistical and causal interpretation; (ii) the inference of local explanations aimed at revealing the logic of the decision adopted for a specific instance by querying and auditing the black box in the vicinity of the target instance; (iii), the bottom-up generalization of the many local explanations into simple global ones, with algorithms that optimize the quality and comprehensibility of explanations.},
	language = {en},
	urldate = {2020-06-18},
	journal = {arXiv:1806.09936 [cs]},
	author = {Pedreschi, Dino and Giannotti, Fosca and Guidotti, Riccardo and Monreale, Anna and Pappalardo, Luca and Ruggieri, Salvatore and Turini, Franco},
	month = jun,
	year = {2018},
	note = {arXiv: 1806.09936},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computers and Society, Computer Science - Machine Learning},
}

@inproceedings{park_multimodal_2018,
	address = {Salt Lake City, UT},
	title = {Multimodal {Explanations}: {Justifying} {Decisions} and {Pointing} to the {Evidence}},
	isbn = {978-1-5386-6420-9},
	shorttitle = {Multimodal {Explanations}},
	url = {https://ieeexplore.ieee.org/document/8579013/},
	doi = {10.1109/CVPR.2018.00915},
	abstract = {Deep models that are both effective and explainable are desirable in many settings; prior explainable models have been unimodal, offering either image-based visualization of attention weights or text-based generation of post-hoc justiﬁcations. We propose a multimodal approach to explanation, and argue that the two modalities provide complementary explanatory strengths. We collect two new datasets to deﬁne and evaluate this task, and propose a novel model which can provide joint textual rationale generation and attention visualization. Our datasets deﬁne visual and textual justiﬁcations of a classiﬁcation decision for activity recognition tasks (ACT-X) and for visual question answering tasks (VQA-X). We quantitatively show that training with the textual explanations not only yields better textual justiﬁcation models, but also better localizes the evidence that supports the decision. We also qualitatively show cases where visual explanation is more insightful than textual explanation, and vice versa, supporting our thesis that multimodal explanation models offer signiﬁcant beneﬁts over unimodal approaches.},
	language = {en},
	urldate = {2020-06-18},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Park, Dong Huk and Hendricks, Lisa Anne and Akata, Zeynep and Rohrbach, Anna and Schiele, Bernt and Darrell, Trevor and Rohrbach, Marcus},
	month = jun,
	year = {2018},
	pages = {8779--8788},
}

@inproceedings{wang_vqa-machine_2017,
	address = {Honolulu, HI},
	title = {The {VQA}-{Machine}: {Learning} {How} to {Use} {Existing} {Vision} {Algorithms} to {Answer} {New} {Questions}},
	isbn = {978-1-5386-0457-1},
	shorttitle = {The {VQA}-{Machine}},
	url = {https://ieeexplore.ieee.org/document/8099899/},
	doi = {10.1109/CVPR.2017.416},
	abstract = {One of the most intriguing features of the Visual Question Answering (VQA) challenge is the unpredictability of the questions. Extracting the information required to answer them demands a variety of image operations from detection and counting, to segmentation and reconstruction. To train a method to perform even one of these operations accurately from \{image,question,answer\} tuples would be challenging, but to aim to achieve them all with a limited set of such training data seems ambitious at best. We propose here instead a more general and scalable approach which exploits the fact that very good methods to achieve these operations already exist, and thus do not need to be trained. Our method thus learns how to exploit a set of external off-the-shelf algorithms to achieve its goal, an approach that has something in common with the Neural Turing Machine [10]. The core of our proposed method is a new co-attention model. In addition, the proposed approach generates human-readable reasons for its decision, and can still be trained end-to-end without ground truth reasons being given. We demonstrate the effectiveness on two publicly available datasets, Visual Genome and VQA, and show that it produces the state-of-the-art results in both cases.},
	language = {en},
	urldate = {2020-06-18},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Wang, Peng and Wu, Qi and Shen, Chunhua and van den Hengel, Anton},
	month = jul,
	year = {2017},
	pages = {3909--3918},
}

@article{mccoy_rnns_2019,
	title = {{RNNs} {Implicitly} {Implement} {Tensor} {Product} {Representations}},
	url = {http://arxiv.org/abs/1812.08718},
	abstract = {Recurrent neural networks (RNNs) can learn continuous vector representations of symbolic structures such as sequences and sentences; these representations often exhibit linear regularities (analogies). Such regularities motivate our hypothesis that RNNs that show such regularities implicitly compile symbolic structures into tensor product representations (TPRs; Smolensky, 1990), which additively combine tensor products of vectors representing roles (e.g., sequence positions) and vectors representing ﬁllers (e.g., particular words). To test this hypothesis, we introduce Tensor Product Decomposition Networks (TPDNs), which use TPRs to approximate existing vector representations. We demonstrate using synthetic data that TPDNs can successfully approximate linear and tree-based RNN autoencoder representations, suggesting that these representations exhibit interpretable compositional structure; we explore the settings that lead RNNs to induce such structuresensitive representations. By contrast, further TPDN experiments show that the representations of four models trained to encode naturally-occurring sentences can be largely approximated with a bag of words, with only marginal improvements from more sophisticated structures. We conclude that TPDNs provide a powerful method for interpreting vector representations, and that standard RNNs can induce compositional sequence representations that are remarkably well approximated by TPRs; at the same time, existing training tasks for sentence representation learning may not be sufﬁcient for inducing robust structural representations.},
	language = {en},
	urldate = {2020-06-18},
	journal = {arXiv:1812.08718 [cs]},
	author = {McCoy, R. Thomas and Linzen, Tal and Dunbar, Ewan and Smolensky, Paul},
	month = mar,
	year = {2019},
	note = {arXiv: 1812.08718},
	keywords = {Computer Science - Computation and Language},
}

@article{arras_explaining_2017,
	title = {Explaining {Recurrent} {Neural} {Network} {Predictions} in {Sentiment} {Analysis}},
	url = {http://arxiv.org/abs/1706.07206},
	abstract = {Recently, a technique called Layer-wise Relevance Propagation (LRP) was shown to deliver insightful explanations in the form of input space relevances for understanding feed-forward neural network classiﬁcation decisions. In the present work, we extend the usage of LRP to recurrent neural networks. We propose a speciﬁc propagation rule applicable to multiplicative connections as they arise in recurrent network architectures such as LSTMs and GRUs. We apply our technique to a word-based bi-directional LSTM model on a ﬁve-class sentiment prediction task, and evaluate the resulting LRP relevances both qualitatively and quantitatively, obtaining better results than a gradient-based related method which was used in previous work.},
	language = {en},
	urldate = {2020-06-18},
	journal = {arXiv:1706.07206 [cs, stat]},
	author = {Arras, Leila and Montavon, Grégoire and Müller, Klaus-Robert and Samek, Wojciech},
	month = aug,
	year = {2017},
	note = {arXiv: 1706.07206},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{hinton_distilling_2015,
	title = {Distilling the {Knowledge} in a {Neural} {Network}},
	url = {http://arxiv.org/abs/1503.02531},
	abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions [3]. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators [1] have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can signiﬁcantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish ﬁne-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
	language = {en},
	urldate = {2020-06-18},
	journal = {arXiv:1503.02531 [cs, stat]},
	author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
	month = mar,
	year = {2015},
	note = {arXiv: 1503.02531},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{han_neural_nodate,
	title = {Neural {Knowledge} {Acquisition} via {Mutual} {Attention} between {Knowledge} {Graph} and {Text}},
	abstract = {We propose a general joint representation learning framework for knowledge acquisition (KA) on two tasks, knowledge graph completion (KGC) and relation extraction (RE) from text. In this framework, we learn representations of knowledge graphs (KGs) and text within a uniﬁed parameter sharing semantic space. To achieve better fusion, we propose an effective mutual attention between KGs and text. The reciprocal attention mechanism enables us to highlight important features and perform better KGC and RE. Different from conventional joint models, no complicated linguistic analysis or strict alignments between KGs and text are required to train our models. Experiments on relation extraction and entity link prediction show that models trained under our joint framework are signiﬁcantly improved in comparison with other baselines. Most existing methods for KGC and RE can be easily integrated into our framework due to its ﬂexible architectures. The source code of this paper can be obtained from https://github.com/thunlp/JointNRE.},
	language = {en},
	author = {Han, Xu and Liu, Zhiyuan and Sun, Maosong},
	pages = {8},
}

@article{saphra_sparsity_nodate,
	title = {Sparsity {Emerges} {Naturally} in {Neural} {Language} {Models}},
	abstract = {Concerns about interpretability, computational resources, and principled inductive priors have motivated efforts to engineer sparse neural models for NLP tasks. If sparsity is important for NLP, might well-trained neural models naturally become roughly sparse? Using the Taxi-Euclidean norm to measure sparsity, we ﬁnd that frequent input words are associated with concentrated or sparse activations, while frequent target words are associated with dispersed activations but concentrated gradients. We ﬁnd that gradients associated with function words are more concentrated than the gradients of content words, even controlling for word frequency.},
	language = {en},
	author = {Saphra, Naomi and Lopez, Adam},
	pages = {5},
}

@inproceedings{liu_multi-task_2015,
	address = {Boston, MA, USA},
	title = {Multi-task deep visual-semantic embedding for video thumbnail selection},
	isbn = {978-1-4673-6964-0},
	url = {http://ieeexplore.ieee.org/document/7298994/},
	doi = {10.1109/CVPR.2015.7298994},
	abstract = {Given the tremendous growth of online videos, video thumbnail, as the common visualization form of video content, is becoming increasingly important to inﬂuence user’s browsing and searching experience. However, conventional methods for video thumbnail selection often fail to produce satisfying results as they ignore the side semantic information (e.g., title, description, and query) associated with the video. As a result, the selected thumbnail cannot always represent video semantics and the click-through rate is adversely affected even when the retrieved videos are relevant. In this paper, we have developed a multi-task deep visualsemantic embedding model, which can automatically select query-dependent video thumbnails according to both visual and side information. Different from most existing methods, the proposed approach employs the deep visual-semantic embedding model to directly compute the similarity between the query and video thumbnails by mapping them into a common latent semantic space, where even unseen querythumbnail pairs can be correctly matched. In particular, we train the embedding model by exploring the large-scale and freely accessible click-through video and image data, as well as employing a multi-task learning strategy to holistically exploit the query-thumbnail relevance from these two highly related datasets. Finally, a thumbnail is selected by fusing both the representative and query relevance scores. The evaluations on 1,000 query-thumbnail dataset labeled by 191 workers in Amazon Mechanical Turk have demonstrated the effectiveness of our proposed method.},
	language = {en},
	urldate = {2020-06-18},
	booktitle = {2015 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Liu, Wu and Mei, Tao and Zhang, Yongdong and Che, Cherry and Luo, Jiebo},
	month = jun,
	year = {2015},
	pages = {3707--3715},
}

@inproceedings{lee_convolutional_2009,
	address = {Montreal, Quebec, Canada},
	title = {Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations},
	isbn = {978-1-60558-516-1},
	url = {http://portal.acm.org/citation.cfm?doid=1553374.1553453},
	doi = {10.1145/1553374.1553453},
	abstract = {There has been much interest in unsupervised learning of hierarchical generative models such as deep belief networks. Scaling such models to full-sized, high-dimensional images remains a diﬃcult problem. To address this problem, we present the convolutional deep belief network, a hierarchical generative model which scales to realistic image sizes. This model is translation-invariant and supports eﬃcient bottom-up and top-down probabilistic inference. Key to our approach is probabilistic max-pooling, a novel technique which shrinks the representations of higher layers in a probabilistically sound way. Our experiments show that the algorithm learns useful high-level visual features, such as object parts, from unlabeled images of objects and natural scenes. We demonstrate excellent performance on several visual recognition tasks and show that our model can perform hierarchical (bottom-up and top-down) inference over full-sized images.},
	language = {en},
	urldate = {2020-06-18},
	booktitle = {Proceedings of the 26th {Annual} {International} {Conference} on {Machine} {Learning} - {ICML} '09},
	publisher = {ACM Press},
	author = {Lee, Honglak and Grosse, Roger and Ranganath, Rajesh and Ng, Andrew Y.},
	year = {2009},
	pages = {1--8},
}

@article{lee_sparse_nodate,
	title = {Sparse deep belief net model for visual area {V2}},
	abstract = {Motivated in part by the hierarchical organization of the cortex, a number of algorithms have recently been proposed that try to learn hierarchical, or “deep,” structure from unlabeled data. While several authors have formally or informally compared their algorithms to computations performed in visual area V1 (and the cochlea), little attempt has been made thus far to evaluate these algorithms in terms of their ﬁdelity for mimicking computations at deeper levels in the cortical hierarchy. This paper presents an unsupervised learning model that faithfully mimics certain properties of visual area V2. Speciﬁcally, we develop a sparse variant of the deep belief networks of Hinton et al. (2006). We learn two layers of nodes in the network, and demonstrate that the ﬁrst layer, similar to prior work on sparse coding and ICA, results in localized, oriented, edge ﬁlters, similar to the Gabor functions known to model V1 cell receptive ﬁelds. Further, the second layer in our model encodes correlations of the ﬁrst layer responses in the data. Speciﬁcally, it picks up both colinear (“contour”) features as well as corners and junctions. More interestingly, in a quantitative comparison, the encoding of these more complex “corner” features matches well with the results from the Ito \& Komatsu’s study of biological V2 responses. This suggests that our sparse variant of deep belief networks holds promise for modeling more higher-order features.},
	language = {en},
	author = {Lee, Honglak and Ekanadham, Chaitanya and Ng, Andrew Y},
	pages = {8},
}

@article{bengio_consciousness_2019,
	title = {The {Consciousness} {Prior}},
	url = {http://arxiv.org/abs/1709.08568},
	abstract = {A new prior is proposed for learning representations of high-level concepts of the kind we manipulate with language. This prior can be combined with other priors in order to help disentangling abstract factors from each other. It is inspired by cognitive neuroscience theories of consciousness, seen as a bottleneck through which just a few elements, after having been selected by attention from a broader pool, are then broadcast and condition further processing, both in perception and decision-making. The set of recently selected elements one becomes aware of is seen as forming a low-dimensional conscious state. This conscious state is combining the few concepts constituting a conscious thought, i.e., what one is immediately conscious of at a particular moment. We claim that this architectural and information-processing constraint corresponds to assumptions about the joint distribution between high-level concepts. To the extent that these assumptions are generally true (and the form of natural language seems consistent with them), they can form a useful prior for representation learning. A low-dimensional thought or conscious state is analogous to a sentence: it involves only a few variables and yet can make a statement with very high probability of being true. This is consistent with a joint distribution (over high-level concepts) which has the form of a sparse factor graph, i.e., where the dependencies captured by each factor of the factor graph involve only very few variables while creating a strong dip in the overall energy function. Instead of making predictions in the sensory (e.g. pixel) space, one can thus make predictions in this high-level abstract space, which do not have to be limited to just the next time step but can relate events far away from each other in time. The consciousness prior also makes it natural to map conscious states to natural language utterances or to express classical AI knowledge in a form similar to facts and rules, albeit capturing uncertainty as well as efﬁcient search mechanisms implemented by attention mechanisms.},
	language = {en},
	urldate = {2020-06-18},
	journal = {arXiv:1709.08568 [cs, stat]},
	author = {Bengio, Yoshua},
	month = dec,
	year = {2019},
	note = {arXiv: 1709.08568},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{das_human_2017,
	title = {Human {Attention} in {Visual} {Question} {Answering}: {Do} {Humans} and {Deep} {Networks} {Look} at the {Same} {Regions}?},
	volume = {163},
	issn = {10773142},
	shorttitle = {Human {Attention} in {Visual} {Question} {Answering}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1077314217301649},
	doi = {10.1016/j.cviu.2017.10.001},
	abstract = {We conduct large-scale studies on ‘human attention’ in Visual Question Answering (VQA) to understand where humans choose to look to answer questions about images. We design and test multiple game-inspired novel attention-annotation interfaces that require the subject to sharpen regions of a blurred image to answer a question. Thus, we introduce the VQA-HAT (Human ATtention) dataset. We evaluate attention maps generated by state-of-the-art VQA models against human attention both qualitatively (via visualizations) and quantitatively (via rank-order correlation). Our experiments show that current attention models in VQA do not seem to be looking at the same regions as humans. Finally, we train VQA models with explicit attention supervision, and ﬁnd that it improves VQA performance.},
	language = {en},
	urldate = {2020-06-18},
	journal = {Computer Vision and Image Understanding},
	author = {Das, Abhishek and Agrawal, Harsh and Zitnick, Larry and Parikh, Devi and Batra, Dhruv},
	month = oct,
	year = {2017},
	pages = {90--100},
}

@article{towell_knowledge-based_1994,
	title = {Knowledge-based artificial neural networks},
	volume = {70},
	issn = {00043702},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0004370294901058},
	doi = {10.1016/0004-3702(94)90105-8},
	language = {en},
	number = {1-2},
	urldate = {2020-06-18},
	journal = {Artificial Intelligence},
	author = {Towell, Geoffrey G. and Shavlik, Jude W.},
	month = oct,
	year = {1994},
	pages = {119--165},
}

@article{shokri-kojori_network_2012,
	title = {The {Network} {Architecture} of {Cortical} {Processing} in {Visuo}-spatial {Reasoning}},
	volume = {2},
	issn = {2045-2322},
	url = {http://www.nature.com/articles/srep00411},
	doi = {10.1038/srep00411},
	language = {en},
	number = {1},
	urldate = {2020-06-18},
	journal = {Scientific Reports},
	author = {Shokri-Kojori, Ehsan and Motes, Michael A. and Rypma, Bart and Krawczyk, Daniel C.},
	month = dec,
	year = {2012},
	pages = {411},
}

@inproceedings{szegedy_rethinking_2016,
	address = {Las Vegas, NV, USA},
	title = {Rethinking the {Inception} {Architecture} for {Computer} {Vision}},
	isbn = {978-1-4673-8851-1},
	url = {http://ieeexplore.ieee.org/document/7780677/},
	doi = {10.1109/CVPR.2016.308},
	abstract = {Convolutional networks are at the core of most stateof-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efﬁciency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we are exploring ways to scale up networks in ways that aim at utilizing the added computation as efﬁciently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classiﬁcation challenge validation set demonstrate substantial gains over the state of the art: 21.2\% top-1 and 5.6\% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3.5\% top-5 error and 17.3\% top-1 error on the validation set and 3.6\% top-5 error on the ofﬁcial test set.},
	language = {en},
	urldate = {2020-06-18},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jon and Wojna, Zbigniew},
	month = jun,
	year = {2016},
	pages = {2818--2826},
}

@article{lake_building_2017,
	title = {Building machines that learn and think like people},
	volume = {40},
	issn = {0140-525X, 1469-1825},
	url = {https://www.cambridge.org/core/product/identifier/S0140525X16001837/type/journal_article},
	doi = {10.1017/S0140525X16001837},
	abstract = {Recent progress in artiﬁcial intelligence has renewed interest in building systems that learn and think like people. Many advances have come from using deep neural networks trained end-to-end in tasks such as object recognition, video games, and board games, achieving performance that equals or even beats that of humans in some respects. Despite their biological inspiration and performance achievements, these systems differ from human intelligence in crucial ways. We review progress in cognitive science suggesting that truly human-like learning and thinking machines will have to reach beyond current engineering trends in both what they learn and how they learn it. Speciﬁcally, we argue that these machines should (1) build causal models of the world that support explanation and understanding, rather than merely solving pattern recognition problems; (2) ground learning in intuitive theories of physics and psychology to support and enrich the knowledge that is learned; and (3) harness compositionality and learning-to-learn to rapidly acquire and generalize knowledge to new tasks and situations. We suggest concrete challenges and promising routes toward these goals that can combine the strengths of recent neural network advances with more structured cognitive models.},
	language = {en},
	urldate = {2020-06-18},
	journal = {Behavioral and Brain Sciences},
	author = {Lake, Brenden M. and Ullman, Tomer D. and Tenenbaum, Joshua B. and Gershman, Samuel J.},
	year = {2017},
	pages = {e253},
}

@techreport{kietzmann_deep_2017,
	type = {preprint},
	title = {Deep {Neural} {Networks} in {Computational} {Neuroscience}},
	url = {http://biorxiv.org/lookup/doi/10.1101/133504},
	abstract = {The goal of computational neuroscience is to find mechanistic explanations of how the nervous system processes information to give rise to cognitive function and behaviour. At the heart of the field are its models, i.e. mathematical and computational descriptions of the system being studied, which map sensory stimuli to neural responses and/or neural to behavioural responses. These models range from simple to complex. Recently, deep neural networks (DNNs) have come to dominate several domains of artificial intelligence (AI). As the term “neural network” suggests, these models are inspired by biological brains. However, current DNNs neglect many details of biological neural networks. These simplifications contribute to their computational efficiency, enabling them to perform complex feats of intelligence, ranging from perceptual (e.g. visual object and auditory speech recognition) to cognitive tasks (e.g. machine translation), and on to motor control (e.g. playing computer games or controlling a robot arm). In addition to their ability to model complex intelligent behaviours, DNNs excel at predicting neural responses to novel sensory stimuli with accuracies well beyond any other currently available model type. DNNs can have millions of parameters, which are required to capture the domain knowledge needed for successful task performance. Contrary to the intuition that this renders them into impenetrable black boxes, the computational properties of the network units are the result of four directly manipulable elements: input statistics, network structure, functional objective, and learning algorithm. With full access to the activity and connectivity of all units, advanced visualization techniques, and analytic tools to map network representations to neural data, DNNs represent a powerful framework for building task-performing models and will drive substantial insights in computational neuroscience.},
	language = {en},
	urldate = {2020-06-18},
	institution = {Neuroscience},
	author = {Kietzmann, Tim C and McClure, Patrick and Kriegeskorte, Nikolaus},
	month = may,
	year = {2017},
	doi = {10.1101/133504},
}

@inproceedings{chen_deep_2013,
	address = {Portland, OR, USA},
	title = {Deep {Learning} {Shape} {Priors} for {Object} {Segmentation}},
	isbn = {978-0-7695-4989-7},
	url = {http://ieeexplore.ieee.org/document/6619088/},
	doi = {10.1109/CVPR.2013.244},
	abstract = {In this paper we introduce a new shape-driven approach for object segmentation. Given a training set of shapes, we first use deep Boltzmann machine to learn the hierarchical architecture of shape priors. This learned hierarchical architecture is then used to model shape variations of global and local structures in an energetic form. Finally, it is applied to data-driven variational methods to perform object extraction of corrupted data based on shape probabilistic representation. Experiments demonstrate that our model can be applied to dataset of arbitrary prior shapes, and can cope with image noise and clutter, as well as partial occlusions.},
	language = {en},
	urldate = {2020-06-18},
	booktitle = {2013 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Chen, Fei and Yu, Huimin and Hu, Roland and Zeng, Xunxun},
	month = jun,
	year = {2013},
	pages = {1870--1877},
}

@article{si_learning_2013,
	title = {Learning {AND}-{OR} {Templates} for {Object} {Recognition} and {Detection}},
	volume = {35},
	issn = {0162-8828, 2160-9292},
	url = {http://ieeexplore.ieee.org/document/6425379/},
	doi = {10.1109/TPAMI.2013.35},
	abstract = {This paper presents a framework for unsupervised learning of a hierarchical reconfigurable image template—the AND-OR Template (AOT) for visual objects. The AOT includes: 1) hierarchical composition as “AND” nodes, 2) deformation and articulation of parts as geometric “OR” nodes, and 3) multiple ways of composition as structural “OR” nodes. The terminal nodes are hybrid image templates (HIT) [17] that are fully generative to the pixels. We show that both the structures and parameters of the AOT model can be learned in an unsupervised way from images using an information projection principle. The learning algorithm consists of two steps: 1) a recursive block pursuit procedure to learn the hierarchical dictionary of primitives, parts, and objects, and 2) a graph compression procedure to minimize model structure for better generalizability. We investigate the factors that influence how well the learning algorithm can identify the underlying AOT. And we propose a number of ways to evaluate the performance of the learned AOTs through both synthesized examples and real-world images. Our model advances the state of the art for object detection by improving the accuracy of template matching.},
	language = {en},
	number = {9},
	urldate = {2020-06-18},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Si, Zhangzhang and Zhu, Song-Chun},
	month = sep,
	year = {2013},
	pages = {2189--2205},
}

@inproceedings{ming_liang_recurrent_2015,
	address = {Boston, MA, USA},
	title = {Recurrent convolutional neural network for object recognition},
	isbn = {978-1-4673-6964-0},
	url = {http://ieeexplore.ieee.org/document/7298958/},
	doi = {10.1109/CVPR.2015.7298958},
	abstract = {In recent years, the convolutional neural network (CNN) has achieved great success in many computer vision tasks. Partially inspired by neuroscience, CNN shares many properties with the visual system of the brain. A prominent difference is that CNN is typically a feed-forward architecture while in the visual system recurrent connections are abundant. Inspired by this fact, we propose a recurrent CNN (RCNN) for object recognition by incorporating recurrent connections into each convolutional layer. Though the input is static, the activities of RCNN units evolve over time so that the activity of each unit is modulated by the activities of its neighboring units. This property enhances the ability of the model to integrate the context information, which is important for object recognition. Like other recurrent neural networks, unfolding the RCNN through time can result in an arbitrarily deep network with a ﬁxed number of parameters. Furthermore, the unfolded network has multiple paths, which can facilitate the learning process. The model is tested on four benchmark object recognition datasets: CIFAR-10, CIFAR-100, MNIST and SVHN. With fewer trainable parameters, RCNN outperforms the state-of-the-art models on all of these datasets. Increasing the number of parameters leads to even better performance. These results demonstrate the advantage of the recurrent structure over purely feed-forward structure for object recognition.},
	language = {en},
	urldate = {2020-06-18},
	booktitle = {2015 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {{Ming Liang} and {Xiaolin Hu}},
	month = jun,
	year = {2015},
	pages = {3367--3375},
}

@article{spoerer_recurrent_2017,
	title = {Recurrent {Convolutional} {Neural} {Networks}: {A} {Better} {Model} of {Biological} {Object} {Recognition}},
	volume = {8},
	issn = {1664-1078},
	shorttitle = {Recurrent {Convolutional} {Neural} {Networks}},
	url = {https://www.frontiersin.org/article/10.3389/fpsyg.2017.01551/full},
	doi = {10.3389/fpsyg.2017.01551},
	abstract = {Feedforward neural networks provide the dominant model of how the brain performs visual object recognition. However, these networks lack the lateral and feedback connections, and the resulting recurrent neuronal dynamics, of the ventral visual pathway in the human and non-human primate brain. Here we investigate recurrent convolutional neural networks with bottom-up (B), lateral (L), and top-down (T) connections. Combining these types of connections yields four architectures (B, BT, BL, and BLT), which we systematically test and compare. We hypothesized that recurrent dynamics might improve recognition performance in the challenging scenario of partial occlusion. We introduce two novel occluded object recognition tasks to test the efﬁcacy of the models, digit clutter (where multiple target digits occlude one another) and digit debris (where target digits are occluded by digit fragments). We ﬁnd that recurrent neural networks outperform feedforward control models (approximately matched in parametric complexity) at recognizing objects, both in the absence of occlusion and in all occlusion conditions. Recurrent networks were also found to be more robust to the inclusion of additive Gaussian noise. Recurrent neural networks are better in two respects: (1) they are more neurobiologically realistic than their feedforward counterparts; (2) they are better in terms of their ability to recognize objects, especially under challenging conditions. This work shows that computer vision can beneﬁt from using recurrent convolutional architectures and suggests that the ubiquitous recurrent connections in biological brains are essential for task performance.},
	language = {en},
	urldate = {2020-06-18},
	journal = {Frontiers in Psychology},
	author = {Spoerer, Courtney J. and McClure, Patrick and Kriegeskorte, Nikolaus},
	month = sep,
	year = {2017},
	pages = {1551},
}

@article{liao_bridging_2016,
	title = {Bridging the {Gaps} {Between} {Residual} {Learning}, {Recurrent} {Neural} {Networks} and {Visual} {Cortex}},
	url = {http://arxiv.org/abs/1604.03640},
	abstract = {We discuss relations between Residual Networks (ResNet), Recurrent Neural Networks (RNNs) and the primate visual cortex. We begin with the observation that a shallow RNN is exactly equivalent to a very deep ResNet with weight sharing among the layers. A direct implementation of such a RNN, although having orders of magnitude fewer parameters, leads to a performance similar to the corresponding ResNet. We propose 1) a generalization of both RNN and ResNet architectures and 2) the conjecture that a class of moderately deep RNNs is a biologically-plausible model of the ventral stream in visual cortex. We demonstrate the eﬀectiveness of the architectures by testing them on the CIFAR-10 dataset.},
	language = {en},
	urldate = {2020-06-18},
	journal = {arXiv:1604.03640 [cs]},
	author = {Liao, Qianli and Poggio, Tomaso},
	month = apr,
	year = {2016},
	note = {arXiv: 1604.03640},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@article{kar_evidence_2019,
	title = {Evidence that recurrent circuits are critical to the ventral stream’s execution of core object recognition behavior},
	volume = {22},
	issn = {1097-6256, 1546-1726},
	url = {http://www.nature.com/articles/s41593-019-0392-5},
	doi = {10.1038/s41593-019-0392-5},
	language = {en},
	number = {6},
	urldate = {2020-06-18},
	journal = {Nature Neuroscience},
	author = {Kar, Kohitij and Kubilius, Jonas and Schmidt, Kailyn and Issa, Elias B. and DiCarlo, James J.},
	month = jun,
	year = {2019},
	pages = {974--983},
}

@article{sabour_dynamic_nodate,
	title = {Dynamic {Routing} {Between} {Capsules}},
	abstract = {A capsule is a group of neurons whose activity vector represents the instantiation parameters of a speciﬁc type of entity such as an object or an object part. We use the length of the activity vector to represent the probability that the entity exists and its orientation to represent the instantiation parameters. Active capsules at one level make predictions, via transformation matrices, for the instantiation parameters of higher-level capsules. When multiple predictions agree, a higher level capsule becomes active. We show that a discrimininatively trained, multi-layer capsule system achieves state-of-the-art performance on MNIST and is considerably better than a convolutional net at recognizing highly overlapping digits. To achieve these results we use an iterative routing-by-agreement mechanism: A lower-level capsule prefers to send its output to higher level capsules whose activity vectors have a big scalar product with the prediction coming from the lower-level capsule.},
	language = {en},
	author = {Sabour, Sara and Frosst, Nicholas and Hinton, Geoffrey E},
	pages = {11},
}

@article{levi_crowdingessential_2008,
	title = {Crowding—{An} essential bottleneck for object recognition: {A} mini-review},
	volume = {48},
	issn = {00426989},
	shorttitle = {Crowding—{An} essential bottleneck for object recognition},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0042698907005561},
	doi = {10.1016/j.visres.2007.12.009},
	abstract = {Crowding, generally deﬁned as the deleterious inﬂuence of nearby contours on visual discrimination, is ubiquitous in spatial vision. Crowding impairs the ability to recognize objects in clutter. It has been extensively studied over the last 80 years or so, and much of the renewed interest is the hope that studying crowding may lead to a better understanding of the processes involved in object recognition. Crowding also has important clinical implications for patients with macular degeneration, amblyopia and dyslexia.},
	language = {en},
	number = {5},
	urldate = {2020-06-18},
	journal = {Vision Research},
	author = {Levi, Dennis M.},
	month = feb,
	year = {2008},
	pages = {635--654},
}

@article{whitney_visual_2011,
	title = {Visual crowding: a fundamental limit on conscious perception and object recognition},
	volume = {15},
	issn = {13646613},
	shorttitle = {Visual crowding},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1364661311000325},
	doi = {10.1016/j.tics.2011.02.005},
	language = {en},
	number = {4},
	urldate = {2020-06-18},
	journal = {Trends in Cognitive Sciences},
	author = {Whitney, David and Levi, Dennis M.},
	month = apr,
	year = {2011},
	pages = {160--168},
}

@inproceedings{ross_right_2017,
	address = {Melbourne, Australia},
	title = {Right for the {Right} {Reasons}: {Training} {Differentiable} {Models} by {Constraining} their {Explanations}},
	isbn = {978-0-9992411-0-3},
	shorttitle = {Right for the {Right} {Reasons}},
	url = {https://www.ijcai.org/proceedings/2017/371},
	doi = {10.24963/ijcai.2017/371},
	abstract = {Neural networks are among the most accurate supervised learning methods in use today. However, their opacity makes them difﬁcult to trust in critical applications, especially if conditions in training may differ from those in test. Recent work on explanations for black-box models has produced tools (e.g. LIME) to show the implicit rules behind predictions. These tools can help us identify when models are right for the wrong reasons. However, these methods do not scale to explaining entire datasets and cannot correct the problems they reveal. We introduce a method for efﬁciently explaining and regularizing differentiable models by examining and selectively penalizing their input gradients. We apply these penalties both based on expert annotation and in an unsupervised fashion that produces multiple classiﬁers with qualitatively different decision boundaries. On multiple datasets, we show our approach generates faithful explanations and models that generalize much better when conditions differ between training and test.},
	language = {en},
	urldate = {2020-06-16},
	booktitle = {Proceedings of the {Twenty}-{Sixth} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Ross, Andrew Slavin and Hughes, Michael C. and Doshi-Velez, Finale},
	month = aug,
	year = {2017},
	pages = {2662--2670},
}

@article{manhaeve_deepproblog_nodate,
	title = {{DeepProbLog}:  {Neural} {Probabilistic} {Logic} {Programming}},
	abstract = {We introduce DeepProbLog, a probabilistic logic programming language that incorporates deep learning by means of neural predicates. We show how existing inference and learning techniques can be adapted for the new language. Our experiments demonstrate that DeepProbLog supports (i) both symbolic and subsymbolic representations and inference, (ii) program induction, (iii) probabilistic (logic) programming, and (iv) (deep) learning from examples. To the best of our knowledge, this work is the ﬁrst to propose a framework where general-purpose neural networks and expressive probabilistic-logical modeling and reasoning are integrated in a way that exploits the full expressiveness and strengths of both worlds and can be trained end-to-end based on examples.},
	language = {en},
	author = {Manhaeve, Robin and Dumancic, Sebastijan and Kimmig, Angelika and Demeester, Thomas and Raedt, Luc De},
	pages = {11},
}

@article{bach_pixel-wise_2015,
	title = {On {Pixel}-{Wise} {Explanations} for {Non}-{Linear} {Classifier} {Decisions} by {Layer}-{Wise} {Relevance} {Propagation}},
	volume = {10},
	issn = {1932-6203},
	url = {https://dx.plos.org/10.1371/journal.pone.0130140},
	doi = {10.1371/journal.pone.0130140},
	abstract = {Understanding and interpreting classification decisions of automated image classification systems is of high value in many applications, as it allows to verify the reasoning of the system and provides additional information to the human expert. Although machine learning methods are solving very successfully a plethora of tasks, they have in most cases the disadvantage of acting as a black box, not providing any information about what made them arrive at a particular decision. This work proposes a general solution to the problem of understanding classification decisions by pixel-wise decomposition of nonlinear classifiers. We introduce a methodology that allows to visualize the contributions of single pixels to predictions for kernel-based classifiers over Bag of Words features and for multilayered neural networks. These pixel contributions can be visualized as heatmaps and are provided to a human expert who can intuitively not only verify the validity of the classification decision, but also focus further analysis on regions of potential interest. We evaluate our method for classifiers trained on PASCAL VOC 2009 images, synthetic image data containing geometric shapes, the MNIST handwritten digits data set and for the pre-trained ImageNet model available as part of the Caffe open source package.},
	language = {en},
	number = {7},
	urldate = {2020-06-16},
	journal = {PLOS ONE},
	author = {Bach, Sebastian and Binder, Alexander and Montavon, Grégoire and Klauschen, Frederick and Müller, Klaus-Robert and Samek, Wojciech},
	editor = {Suarez, Oscar Deniz},
	month = jul,
	year = {2015},
	pages = {e0130140},
}

@article{tran_deep_2018,
	title = {Deep {Logic} {Networks}: {Inserting} and {Extracting} {Knowledge} {From} {Deep} {Belief} {Networks}},
	volume = {29},
	issn = {2162-237X, 2162-2388},
	shorttitle = {Deep {Logic} {Networks}},
	url = {http://ieeexplore.ieee.org/document/7738566/},
	doi = {10.1109/TNNLS.2016.2603784},
	abstract = {Developments in deep learning have seen the use of layerwise unsupervised learning combined with supervised learning for ﬁne-tuning. With this layerwise approach, a deep network can be seen as a more modular system that lends itself well to learning representations. In this paper, we investigate whether such modularity can be useful to the insertion of background knowledge into deep networks, whether it can improve learning performance when it is available, and to the extraction of knowledge from trained deep networks, and whether it can offer a better understanding of the representations learned by such networks. To this end, we use a simple symbolic language—a set of logical rules that we call conﬁdence rules—and show that it is suitable for the representation of quantitative reasoning in deep networks. We show by knowledge extraction that conﬁdence rules can offer a low-cost representation for layerwise networks (or restricted Boltzmann machines). We also show that layerwise extraction can produce an improvement in the accuracy of deep belief networks. Furthermore, the proposed symbolic characterization of deep networks provides a novel method for the insertion of prior knowledge and training of deep networks. With the use of this method, a deep neural–symbolic system is proposed and evaluated, with the experimental results indicating that modularity through the use of conﬁdence rules and knowledge insertion can be beneﬁcial to network performance.},
	language = {en},
	number = {2},
	urldate = {2020-06-16},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Tran, Son N. and d'Avila Garcez, Artur S.},
	month = feb,
	year = {2018},
	pages = {246--258},
}

@article{zhou_revisiting_2018,
	title = {Revisiting the {Importance} of {Individual} {Units} in {CNNs} via {Ablation}},
	url = {http://arxiv.org/abs/1806.02891},
	abstract = {We revisit the importance of the individual units in Convolutional Neural Networks (CNNs) for visual recognition. By conducting unit ablation experiments on CNNs trained on large scale image datasets, we demonstrate that, though ablating any individual unit does not hurt overall classiﬁcation accuracy, it does lead to signiﬁcant damage on the accuracy of speciﬁc classes. This result shows that an individual unit is specialized to encode information relevant to a subset of classes. We compute the correlation between the accuracy drop under unit ablation and various attributes of an individual unit such as class selectivity and weight L1 norm. We conﬁrm that unit attributes such as class selectivity are a poor predictor for impact on overall accuracy as found previously in recent work [11]. However, our results show that class selectivity along with other attributes are good predictors of the importance of one unit to individual classes. We evaluate the impact of random rotation, batch normalization, and dropout to the importance of units to speciﬁc classes. Our results show that units with high selectivity play an important role in network classiﬁcation power at the individual class level. Understanding and interpreting the behavior of these units is necessary and meaningful.},
	language = {en},
	urldate = {2020-06-16},
	journal = {arXiv:1806.02891 [cs]},
	author = {Zhou, Bolei and Sun, Yiyou and Bau, David and Torralba, Antonio},
	month = jun,
	year = {2018},
	note = {arXiv: 1806.02891},
}

@inproceedings{subramanya_fooling_2019,
	address = {Seoul, Korea (South)},
	title = {Fooling {Network} {Interpretation} in {Image} {Classification}},
	isbn = {978-1-72814-803-8},
	url = {https://ieeexplore.ieee.org/document/9010911/},
	doi = {10.1109/ICCV.2019.00211},
	abstract = {Deep neural networks have been shown to be fooled rather easily using adversarial attack algorithms. Practical methods such as adversarial patches have been shown to be extremely effective in causing misclassiﬁcation. However, these patches are highlighted using standard network interpretation algorithms, thus revealing the identity of the adversary. We show that it is possible to create adversarial patches which not only fool the prediction, but also change what we interpret regarding the cause of the prediction. Moreover, we introduce our attack as a controlled setting to measure the accuracy of interpretation algorithms. We show this using extensive experiments for Grad-CAM interpretation that transfers to occluding patch interpretation as well. We believe our algorithms can facilitate developing more robust network interpretation tools that truly explain the network’s underlying decision making process.},
	language = {en},
	urldate = {2020-06-16},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Subramanya, Akshayvarun and Pillai, Vipin and Pirsiavash, Hamed},
	month = oct,
	year = {2019},
	pages = {2020--2029},
}

@article{kindermans_reliability_2017,
	title = {The ({Un})reliability of saliency methods},
	url = {http://arxiv.org/abs/1711.00867},
	abstract = {Saliency methods aim to explain the predictions of deep neural networks. These methods lack reliability when the explanation is sensitive to factors that do not contribute to the model prediction. We use a simple and common pre-processing step —adding a constant shift to the input data— to show that a transformation with no effect on the model can cause numerous methods to incorrectly attribute. In order to guarantee reliability, we posit that methods should fulﬁll input invariance, the requirement that a saliency method mirror the sensitivity of the model with respect to transformations of the input. We show, through several examples, that saliency methods that do not satisfy input invariance result in misleading attribution.},
	language = {en},
	urldate = {2020-06-16},
	journal = {arXiv:1711.00867 [cs, stat]},
	author = {Kindermans, Pieter-Jan and Hooker, Sara and Adebayo, Julius and Alber, Maximilian and Schütt, Kristof T. and Dähne, Sven and Erhan, Dumitru and Kim, Been},
	month = nov,
	year = {2017},
	note = {arXiv: 1711.00867},
	keywords = {Statistics - Machine Learning},
}

@article{simonyan_deep_2014,
	title = {Deep {Inside} {Convolutional} {Networks}: {Visualising} {Image} {Classification} {Models} and {Saliency} {Maps}},
	shorttitle = {Deep {Inside} {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1312.6034},
	abstract = {This paper addresses the visualisation of image classiﬁcation models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The ﬁrst one generates an image, which maximises the class score [5], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, speciﬁc to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classiﬁcation ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [13].},
	language = {en},
	urldate = {2020-06-16},
	journal = {arXiv:1312.6034 [cs]},
	author = {Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
	month = apr,
	year = {2014},
	note = {arXiv: 1312.6034},
}

@inproceedings{mahendran_understanding_2015,
	address = {Boston, MA, USA},
	title = {Understanding deep image representations by inverting them},
	isbn = {978-1-4673-6964-0},
	url = {http://ieeexplore.ieee.org/document/7299155/},
	doi = {10.1109/CVPR.2015.7299155},
	abstract = {Image representations, from SIFT and Bag of Visual Words to Convolutional Neural Networks (CNNs), are a crucial component of almost any image understanding system. Nevertheless, our understanding of them remains limited. In this paper we conduct a direct analysis of the visual information contained in representations by asking the following question: given an encoding of an image, to which extent is it possible to reconstruct the image itself? To answer this question we contribute a general framework to invert representations. We show that this method can invert representations such as HOG more accurately than recent alternatives while being applicable to CNNs too. We then use this technique to study the inverse of recent state-of-theart CNN image representations for the ﬁrst time. Among our ﬁndings, we show that several layers in CNNs retain photographically accurate information about the image, with different degrees of geometric and photometric invariance.},
	language = {en},
	urldate = {2020-06-16},
	booktitle = {2015 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Mahendran, Aravindh and Vedaldi, Andrea},
	month = jun,
	year = {2015},
	pages = {5188--5196},
}

@inproceedings{fong_interpretable_2017,
	address = {Venice},
	title = {Interpretable {Explanations} of {Black} {Boxes} by {Meaningful} {Perturbation}},
	isbn = {978-1-5386-1032-9},
	url = {http://ieeexplore.ieee.org/document/8237633/},
	doi = {10.1109/ICCV.2017.371},
	abstract = {As machine learning algorithms are increasingly applied to high impact yet high risk tasks, such as medical diagnosis or autonomous driving, it is critical that researchers can explain how such algorithms arrived at their predictions. In recent years, a number of image saliency methods have been developed to summarize where highly complex neural networks “look” in an image for evidence for their predictions. However, these techniques are limited by their heuristic nature and architectural constraints.},
	language = {en},
	urldate = {2020-06-16},
	booktitle = {2017 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Fong, Ruth C. and Vedaldi, Andrea},
	month = oct,
	year = {2017},
	pages = {3449--3457},
}

@article{montavon_methods_2018,
	title = {Methods for {Interpreting} and {Understanding} {Deep} {Neural} {Networks}},
	volume = {73},
	issn = {10512004},
	url = {http://arxiv.org/abs/1706.07979},
	doi = {10.1016/j.dsp.2017.10.011},
	abstract = {This paper provides an entry point to the problem of interpreting a deep neural network model and explaining its predictions. It is based on a tutorial given at ICASSP 2017. It introduces some recently proposed techniques of interpretation, along with theory, tricks and recommendations, to make most eﬃcient use of these techniques on real data. It also discusses a number of practical applications.},
	language = {en},
	urldate = {2020-06-16},
	journal = {Digital Signal Processing},
	author = {Montavon, Grégoire and Samek, Wojciech and Müller, Klaus-Robert},
	month = feb,
	year = {2018},
	note = {arXiv: 1706.07979},
	keywords = {Statistics - Machine Learning},
	pages = {1--15},
}

@article{doshi-velez_towards_2017,
	title = {Towards {A} {Rigorous} {Science} of {Interpretable} {Machine} {Learning}},
	url = {http://arxiv.org/abs/1702.08608},
	abstract = {As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is very little consensus on what interpretable machine learning is and how it should be measured. In this position paper, we first define interpretability and describe when interpretability is needed (and when it is not). Next, we suggest a taxonomy for rigorous evaluation and expose open questions towards a more rigorous science of interpretable machine learning.},
	language = {en},
	urldate = {2020-06-16},
	journal = {arXiv:1702.08608 [cs, stat]},
	author = {Doshi-Velez, Finale and Kim, Been},
	month = mar,
	year = {2017},
	note = {arXiv: 1702.08608},
	keywords = {Statistics - Machine Learning},
}

@article{adadi_peeking_2018,
	title = {Peeking {Inside} the {Black}-{Box}: {A} {Survey} on {Explainable} {Artificial} {Intelligence} ({XAI})},
	volume = {6},
	issn = {2169-3536},
	shorttitle = {Peeking {Inside} the {Black}-{Box}},
	url = {https://ieeexplore.ieee.org/document/8466590/},
	doi = {10.1109/ACCESS.2018.2870052},
	abstract = {At the dawn of the fourth industrial revolution, we are witnessing a fast and widespread adoption of artiﬁcial intelligence (AI) in our daily life, which contributes to accelerating the shift towards a more algorithmic society. However, even with such unprecedented advancements, a key impediment to the use of AI-based systems is that they often lack transparency. Indeed, the black-box nature of these systems allows powerful predictions, but it cannot be directly explained. This issue has triggered a new debate on explainable AI (XAI). A research ﬁeld holds substantial promise for improving trust and transparency of AI-based systems. It is recognized as the sine qua non for AI to continue making steady progress without disruption. This survey provides an entry point for interested researchers and practitioners to learn key aspects of the young and rapidly growing body of research related to XAI. Through the lens of the literature, we review the existing approaches regarding the topic, discuss trends surrounding its sphere, and present major research trajectories.},
	language = {en},
	urldate = {2020-06-16},
	journal = {IEEE Access},
	author = {Adadi, Amina and Berrada, Mohammed},
	year = {2018},
	pages = {52138--52160},
}

@article{carlini_evaluating_2019,
	title = {On {Evaluating} {Adversarial} {Robustness}},
	url = {http://arxiv.org/abs/1902.06705},
	abstract = {Correctly evaluating defenses against adversarial examples has proven to be extremely diﬃcult. Despite the signiﬁcant amount of recent work attempting to design defenses that withstand adaptive attacks, few have succeeded; most papers that propose defenses are quickly shown to be incorrect.},
	language = {en},
	urldate = {2020-06-15},
	journal = {arXiv:1902.06705 [cs, stat]},
	author = {Carlini, Nicholas and Athalye, Anish and Papernot, Nicolas and Brendel, Wieland and Rauber, Jonas and Tsipras, Dimitris and Goodfellow, Ian and Madry, Aleksander and Kurakin, Alexey},
	month = feb,
	year = {2019},
	note = {arXiv: 1902.06705},
	keywords = {Computer Science - Cryptography and Security, Statistics - Machine Learning},
}

@article{van_bergen_going_2020,
	title = {Going in circles is the way forward: the role of recurrence in visual inference},
	shorttitle = {Going in circles is the way forward},
	url = {http://arxiv.org/abs/2003.12128},
	abstract = {Biological visual systems exhibit abundant recurrent connectivity. State-of-the-art neural network models for visual recognition, by contrast, rely heavily or exclusively on feedforward computation. Any finite-time recurrent neural network (RNN) can be unrolled along time to yield an equivalent feedforward neural network (FNN). This important insight suggests that computational neuroscientists may not need to engage recurrent computation, and that computer-vision engineers may be limiting themselves to a special case of FNN if they build recurrent models. Here we argue, to the contrary, that FNNs are a special case of RNNs and that computational neuroscientists and engineers should engage recurrence to understand how brains and machines can (1) achieve greater and more flexible computational depth, (2) compress complex computations into limited hardware, (3) integrate priors and priorities into visual inference through expectation and attention, (4) exploit sequential dependencies in their data for better inference and prediction, and (5) leverage the power of iterative computation.},
	language = {en},
	urldate = {2020-06-14},
	journal = {arXiv:2003.12128 [cs, q-bio]},
	author = {van Bergen, Ruben S. and Kriegeskorte, Nikolaus},
	month = mar,
	year = {2020},
	note = {arXiv: 2003.12128},
	keywords = {Quantitative Biology - Neurons and Cognition},
}

@article{noauthor_neural-symbolic_nodate,
	title = {Neural-{Symbolic} {Learning} and {Reasoning}: {Contributions} and {Challenges}},
	abstract = {The goal of neural-symbolic computation is to integrate robust connectionist learning and sound symbolic reasoning. With the recent advances in connectionist learning, in particular deep neural networks, forms of representation learning have emerged. However, such representations have not become useful for reasoning. Results from neural-symbolic computation have shown to offer powerful alternatives for knowledge representation, learning and reasoning in neural computation. This paper recalls the main contributions and discusses key challenges for neural-symbolic integration which have been identified at a recent Dagstuhl seminar.},
	language = {en},
	pages = {4},
}

@article{besold_neural-symbolic_2017,
	title = {Neural-{Symbolic} {Learning} and {Reasoning}: {A} {Survey} and {Interpretation}},
	shorttitle = {Neural-{Symbolic} {Learning} and {Reasoning}},
	url = {http://arxiv.org/abs/1711.03902},
	abstract = {The study and understanding of human behaviour is relevant to computer science, artificial intelligence, neural computation, cognitive science, philosophy, psychology, and several other areas. Presupposing cognition as basis of behaviour, among the most prominent tools in the modelling of behaviour are computational-logic systems, connectionist models of cognition, and models of uncertainty. Recent studies in cognitive science, artificial intelligence, and psychology have produced a number of cognitive models of reasoning, learning, and language that are underpinned by computation. In addition, efforts in computer science research have led to the development of cognitive computational systems integrating machine learning and automated reasoning. Such systems have shown promise in a range of applications, including computational biology, fault diagnosis, training and assessment in simulators, and software verification. This joint survey reviews the personal ideas and views of several researchers on neural-symbolic learning and reasoning. The article is organised in three parts: Firstly, we frame the scope and goals of neural-symbolic computation and have a look at the theoretical foundations. We then proceed to describe the realisations of neural-symbolic computation, systems, and applications. Finally we present the challenges facing the area and avenues for further research.},
	language = {en},
	urldate = {2020-06-14},
	journal = {arXiv:1711.03902 [cs]},
	author = {Besold, Tarek R. and Garcez, Artur d'Avila and Bader, Sebastian and Bowman, Howard and Domingos, Pedro and Hitzler, Pascal and Kuehnberger, Kai-Uwe and Lamb, Luis C. and Lowd, Daniel and Lima, Priscila Machado Vieira and de Penning, Leo and Pinkas, Gadi and Poon, Hoifung and Zaverucha, Gerson},
	month = nov,
	year = {2017},
	note = {arXiv: 1711.03902},
}

@article{hailesilassie_rule_2016,
	title = {Rule {Extraction} {Algorithm} for {Deep} {Neural} {Networks}: {A} {Review}},
	volume = {14},
	abstract = {Despite the highest classification accuracy in wide varieties of application areas, artificial neural network has one disadvantage. The way this Network comes to a decision is not easily comprehensible. The lack of explanation ability reduces the acceptability of neural network in data mining and decision system. This drawback is the reason why researchers have proposed many rule extraction algorithms to solve the problem. Recently, Deep Neural Network (DNN) is achieving a profound result over the standard neural network for classification and recognition problems. It is a hot machine learning area proven both useful and innovative. This paper has thoroughly reviewed various rule extraction algorithms, considering the classification scheme: decompositional, pedagogical, and eclectics. It also presents the evaluation of these algorithms based on the neural network structure with which the algorithm is intended to work. The main contribution of this review is to show that there is a limited study of rule extraction algorithm from DNN.},
	language = {en},
	number = {7},
	author = {Hailesilassie, Tameru},
	year = {2016},
	pages = {6},
}

@article{futia_integration_2020,
	title = {On the {Integration} of {Knowledge} {Graphs} into {Deep} {Learning} {Models} for a {More} {Comprehensible} {AI}—{Three} {Challenges} for {Future} {Research}},
	volume = {11},
	issn = {2078-2489},
	url = {https://www.mdpi.com/2078-2489/11/2/122},
	doi = {10.3390/info11020122},
	abstract = {Deep learning models contributed to reaching unprecedented results in prediction and classiﬁcation tasks of Artiﬁcial Intelligence (AI) systems. However, alongside this notable progress, they do not provide human-understandable insights on how a speciﬁc result was achieved. In contexts where the impact of AI on human life is relevant (e.g., recruitment tools, medical diagnoses, etc.), explainability is not only a desirable property, but it is -or, in some cases, it will be soon-a legal requirement. Most of the available approaches to implement eXplainable Artiﬁcial Intelligence (XAI) focus on technical solutions usable only by experts able to manipulate the recursive mathematical functions in deep learning algorithms. A complementary approach is represented by symbolic AI, where symbols are elements of a lingua franca between humans and deep learning. In this context, Knowledge Graphs (KGs) and their underlying semantic technologies are the modern implementation of symbolic AI—while being less ﬂexible and robust to noise compared to deep learning models, KGs are natively developed to be explainable. In this paper, we review the main XAI approaches existing in the literature, underlying their strengths and limitations, and we propose neural-symbolic integration as a cornerstone to design an AI which is closer to non-insiders comprehension. Within such a general direction, we identify three speciﬁc challenges for future research—knowledge matching, cross-disciplinary explanations and interactive explanations.},
	language = {en},
	number = {2},
	urldate = {2020-06-14},
	journal = {Information},
	author = {Futia, Giuseppe and Vetrò, Antonio},
	month = feb,
	year = {2020},
	pages = {122},
}

@article{townsend_extracting_2019,
	title = {Extracting {Relational} {Explanations} {From} {Deep} {Neural} {Networks}: {A} {Survey} {From} a {Neural}-{Symbolic} {Perspective}},
	issn = {2162-237X, 2162-2388},
	shorttitle = {Extracting {Relational} {Explanations} {From} {Deep} {Neural} {Networks}},
	url = {https://ieeexplore.ieee.org/document/8889997/},
	doi = {10.1109/TNNLS.2019.2944672},
	abstract = {The term “explainable AI” refers to the goal of producing artiﬁcially intelligent agents that are capable of providing explanations for their decisions. Some models (e.g., rulebased systems) are designed to be explainable, while others are less explicit “black boxes” for which their reasoning remains a mystery. One example of the latter is the neural network, and over the past few decades, researchers in the ﬁeld of neuralsymbolic integration (NSI) have sought to extract relational knowledge from such networks. Extraction from deep neural networks, however, has remained a challenge until recent years in which many methods of extracting distinct, salient features from input or hidden feature spaces of deep neural networks have been proposed. Furthermore, methods of identifying relationships between these features have also emerged. This article presents examples of old and new developments in extracting relational explanations in order to argue that the latter have analogies in the former and, as such, can be described in terms of long-established taxonomies and frameworks presented in early neural-symbolic literature. We also outline potential future research directions that come to light from this refreshed perspective.},
	language = {en},
	urldate = {2020-06-14},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Townsend, Joe and Chaton, Thomas and Monteiro, Joao M.},
	year = {2019},
	pages = {1--15},
}

@article{lamb_graph_2020,
	title = {Graph {Neural} {Networks} {Meet} {Neural}-{Symbolic} {Computing}: {A} {Survey} and {Perspective}},
	shorttitle = {Graph {Neural} {Networks} {Meet} {Neural}-{Symbolic} {Computing}},
	url = {http://arxiv.org/abs/2003.00330},
	abstract = {Neural-symbolic computing has now become the subject of interest of both academic and industry research laboratories. Graph Neural Networks (GNNs) have been widely used in relational and symbolic domains, with widespread application of GNNs in combinatorial optimization, constraint satisfaction, relational reasoning and other scientiﬁc domains. The need for improved explainability, interpretability and trust of AI systems in general demands principled methodologies, as suggested by neural-symbolic computing. In this paper, we review the state-of-the-art on the use of GNNs as a model of neural-symbolic computing. This includes the application of GNNs in several domains as well as their relationship to current developments in neural-symbolic computing.},
	language = {en},
	urldate = {2020-06-14},
	journal = {arXiv:2003.00330 [cs]},
	author = {Lamb, Luis C. and Garcez, Artur and Gori, Marco and Prates, Marcelo and Avelar, Pedro and Vardi, Moshe},
	month = may,
	year = {2020},
	note = {arXiv: 2003.00330},
}
