\chapter*{\centering\Large{Tóm tắt đề tài}}
\addcontentsline{toc}{chapter}{Tóm tắt đề tài}

Đồ án tập trung nghiên cứu tính bền vững (robustness) của các mô hình thị giác--ngôn ngữ (Vision-Language Models -- VLMs), với trọng tâm là mô hình CLIP, dưới các tấn công đối kháng (adversarial attacks). Mặc dù các hệ thống AI hiện đại đạt hiệu năng cao trên dữ liệu sạch, độ nhạy trước những nhiễu loạn nhỏ khó nhận biết (adversarial perturbations) vẫn là thách thức đáng kể đối với triển khai trong thực tế.

Trong khuôn khổ đồ án, chúng tôi triển khai và đánh giá phương pháp \textbf{Adversarial Prompt Tuning (APT)} nhằm cải thiện độ bền vững mà không cần tinh chỉnh toàn bộ mô hình (full fine-tuning). APT giữ cố định các trọng số của mô hình và chỉ tối ưu hoá các vector ngữ cảnh (context vectors) ở đầu vào văn bản, dưới dạng một bài toán tối ưu hoá cực đại--cực tiểu (min--max optimization). Dựa trên quan sát trong công trình nền tảng, đồ án cũng xem xét giả thuyết ``one-word is enough'', theo đó việc tối ưu hoá một số lượng rất nhỏ token (thậm chí một token) có thể mang lại cải thiện đáng kể về độ bền vững, đồng thời giảm chi phí bộ nhớ và thời gian huấn luyện.

Ngoài ra, APT được kỳ vọng hỗ trợ cải thiện độ bền vững liên phương thức (cross-modal robustness), trong đó khả năng phòng thủ trước tấn công trên nhánh văn bản có thể được cải thiện ngay cả khi quá trình huấn luyện đối kháng chủ yếu thực hiện trên nhánh hình ảnh. Các kết quả thực nghiệm trên một số bộ dữ liệu benchmark được sử dụng để phân tích mức độ đánh đổi giữa độ chính xác trên dữ liệu sạch và độ bền vững trước tấn công, qua đó làm rõ tiềm năng ứng dụng của APT trong các hệ thống VLM theo hướng an toàn và tiết kiệm tài nguyên.